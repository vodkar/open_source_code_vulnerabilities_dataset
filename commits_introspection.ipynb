{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from src.paths import (\n",
    "    PYTHON_VULNERABILITY_FIXES_DATA_PATH,\n",
    "    PYTHON_CODE_FIXES_DATA_PATH,\n",
    "    PYTHON_CODE_FIXES_WITH_CONTEXT_DATA_PATH,\n",
    "    PYTHON_CODE_UNITS_DATA_PATH,\n",
    "    PYTHON_CODE_CONTEXT_DATA_PATH,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import jedi\n",
    "from git import Repo\n",
    "from typing import Any\n",
    "import jedi.api\n",
    "import jedi.common\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from src.process_code_changes import get_changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560\n",
      "1556\n"
     ]
    }
   ],
   "source": [
    "# python_commit_data = commit_data_only_top_langs.filter(pl.col(\"language\") == \"Python\")\n",
    "python_vulnerability_fixes = pl.read_parquet(PYTHON_VULNERABILITY_FIXES_DATA_PATH)\n",
    "print(python_vulnerability_fixes.unique(\"vulnerability_id\").shape[0])\n",
    "python_vulnerability_fixes = python_vulnerability_fixes.group_by('vulnerability_id', 'repo', 'commit', 'pull_request_number', 'file', 'patch', 'patch_time', 'commit_source', 'file_extension', 'language').agg(pl.col(\"cwe_id\"))\n",
    "python_vulnerability_fixes = python_vulnerability_fixes.unique(\"patch\")\n",
    "print(python_vulnerability_fixes.unique(\"vulnerability_id\").shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out data connected with test functionallity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1550"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_vulnerability_fixes = python_vulnerability_fixes.filter(\n",
    "    (\n",
    "        pl.col(\"file\").str.contains(r\"\\/{0,1}[tT][eE][sS][tT][sS]{0,1}\\/\")\n",
    "        | pl.col(\"patch\").str.contains(\"pytest\")\n",
    "        | pl.col(\"patch\").str.contains(\"unittest\")\n",
    "    ).not_()\n",
    ")\n",
    "python_vulnerability_fixes.unique(\"vulnerability_id\").shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1506"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_langs = [\n",
    "    \"txt\",\n",
    "    \"md\",\n",
    "    \"JSON\",\n",
    "    \"YAML\",\n",
    "    \"bugfix\",\n",
    "    \"cfg\",\n",
    "    \"rst\",\n",
    "    \"toml\",\n",
    "    \"lock\",\n",
    "    \"ini\",\n",
    "    \"in\",\n",
    "    \"gitignore\",\n",
    "    \"sample\",\n",
    "    \"pem\",\n",
    "    \"feature\",\n",
    "    \"tif\",\n",
    "    \"security\",\n",
    "    \"proto\",\n",
    "    \"conf\",\n",
    "    \"spec\",\n",
    "    \"bin\",\n",
    "    \"misc\",\n",
    "    \"pyi\",\n",
    "    \"pxi\",\n",
    "    \"fli\",\n",
    "    \"gif\",\n",
    "    \"tpl\",\n",
    "    \"graphql\",\n",
    "    \"http\",\n",
    "    \"sgi\",\n",
    "    \"pyx\",\n",
    "    \"inc\"\n",
    "]\n",
    "python_vulnerability_fixes = python_vulnerability_fixes.filter(\n",
    "    (\n",
    "        pl.col(\"file\").str.split(\".\").list.last().is_in(exclude_langs)\n",
    "    ).not_(),\n",
    "    pl.col(\"file\").str.contains(r\"__.*__.py\").not_()\n",
    ")\n",
    "python_vulnerability_fixes.unique(\"vulnerability_id\").shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process missing commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (32, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ vulnerabil â”† repo       â”† commit â”† pull_reque â”† â€¦ â”† commit_so â”† file_exte â”† language â”† cwe_id    â”‚\n",
      "â”‚ ity_id     â”† ---        â”† ---    â”† st_number  â”†   â”† urce      â”† nsion     â”† ---      â”† ---       â”‚\n",
      "â”‚ ---        â”† str        â”† str    â”† ---        â”†   â”† ---       â”† ---       â”† str      â”† list[str] â”‚\n",
      "â”‚ str        â”†            â”†        â”† i64        â”†   â”† str       â”† str       â”†          â”†           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2014-3146  â”† lxml/lxml  â”† null   â”† 273        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-79\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2014-8991  â”† pypa/pip   â”† null   â”† 2122       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ â€¦          â”† â€¦          â”† â€¦      â”† â€¦          â”† â€¦ â”† â€¦         â”† â€¦         â”† â€¦        â”† â€¦         â”‚\n",
      "â”‚ 2014-9601  â”† python-pil â”† null   â”† 1060       â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”† low/Pillow â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-4439  â”† saltstack/ â”† null   â”† 7356       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚            â”† salt       â”†        â”†            â”†   â”†           â”†           â”†          â”†           â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-4439  â”† saltstack/ â”† null   â”† 7356       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚            â”† salt       â”†        â”†            â”†   â”†           â”†           â”†          â”†           â”‚\n",
      "â”‚ 2013-4439  â”† saltstack/ â”† null   â”† 7356       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚            â”† salt       â”†        â”†            â”†   â”†           â”†           â”†          â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "shape: (32, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ vulnerabil â”† repo       â”† commit â”† pull_reque â”† â€¦ â”† commit_so â”† file_exte â”† language â”† cwe_id    â”‚\n",
      "â”‚ ity_id     â”† ---        â”† ---    â”† st_number  â”†   â”† urce      â”† nsion     â”† ---      â”† ---       â”‚\n",
      "â”‚ ---        â”† str        â”† str    â”† ---        â”†   â”† ---       â”† ---       â”† str      â”† list[str] â”‚\n",
      "â”‚ str        â”†            â”†        â”† i64        â”†   â”† str       â”† str       â”†          â”†           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 2014-3146  â”† lxml/lxml  â”† null   â”† 273        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-79\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2014-8991  â”† pypa/pip   â”† null   â”† 2122       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ â€¦          â”† â€¦          â”† â€¦      â”† â€¦          â”† â€¦ â”† â€¦         â”† â€¦         â”† â€¦        â”† â€¦         â”‚\n",
      "â”‚ 2014-9601  â”† python-pil â”† null   â”† 1060       â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”† low/Pillow â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-4439  â”† saltstack/ â”† null   â”† 7356       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚            â”† salt       â”†        â”†            â”†   â”†           â”†           â”†          â”†           â”‚\n",
      "â”‚ 2013-1629  â”† pypa/pip   â”† null   â”† 791        â”† â€¦ â”† github    â”† py        â”† Python   â”† [\"CWE-20\" â”‚\n",
      "â”‚            â”†            â”†        â”†            â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
      "â”‚ 2013-4439  â”† saltstack/ â”† null   â”† 7356       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚            â”† salt       â”†        â”†            â”†   â”†           â”†           â”†          â”†           â”‚\n",
      "â”‚ 2013-4439  â”† saltstack/ â”† null   â”† 7356       â”† â€¦ â”† github    â”† py        â”† Python   â”† [null]    â”‚\n",
      "â”‚            â”† salt       â”†        â”†            â”†   â”†           â”†           â”†          â”†           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "shape: (0, 11)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ vulnerabili â”† repo â”† commit â”† pull_reques â”† â€¦ â”† commit_sour â”† file_extens â”† language â”† cwe_id    â”‚\n",
      "â”‚ ty_id       â”† ---  â”† ---    â”† t_number    â”†   â”† ce          â”† ion         â”† ---      â”† ---       â”‚\n",
      "â”‚ ---         â”† str  â”† str    â”† ---         â”†   â”† ---         â”† ---         â”† str      â”† list[str] â”‚\n",
      "â”‚ str         â”†      â”†        â”† i64         â”†   â”† str         â”† str         â”†          â”†           â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "print(python_vulnerability_fixes.filter(pl.col(\"commit\").is_null()))\n",
    "python_vulnerability_fixes = python_vulnerability_fixes.with_columns(\n",
    "    pl.when(pl.col(\"pull_request_number\") == 24391)\n",
    "    .then(pl.lit(\"86664c9405136a4904775c52e6caf100a474ec58\"))\n",
    "    .otherwise(pl.col(\"commit\"))\n",
    "    .alias(\"commit\")\n",
    ")\n",
    "print(python_vulnerability_fixes.filter(pl.col(\"commit\").is_null()))\n",
    "# No changes related to python: https://github.com/pyca/pyopenssl/commit/6bbf44a00b35fb28df1f66aa194b2fe95eab1ab2\n",
    "# Very big change: https://github.com/transifex/transifex-client/commit/e0d1f8b38ec1a24e2999d63420554d8393206f58\n",
    "python_vulnerability_fixes = python_vulnerability_fixes.filter(\n",
    "    ~pl.col(\"commit\").is_in(\n",
    "        [\n",
    "            \"6bbf44a00b35fb28df1f66aa194b2fe95eab1ab2\",\n",
    "            \"e0d1f8b38ec1a24e2999d63420554d8393206f58\",\n",
    "            \"5f7496481bd3db1d06a2d2e62c0dce960a1fe12b\",\n",
    "            # Not exists in repo\n",
    "            \"13336272e32872247fa7d17e964ccd88ec8d1376\",\n",
    "            \"2bfe358043096fdba9e2a4cf0f5740102b37fd8f\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "python_vulnerability_fixes = python_vulnerability_fixes.filter(\n",
    "    pl.col(\"file\") != \"setup.py\"\n",
    ")\n",
    "print(python_vulnerability_fixes.filter(pl.col(\"commit\").is_null()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>vulnerability_id</th><th>repo</th><th>commit</th><th>pull_request_number</th><th>file</th><th>patch</th><th>patch_time</th><th>commit_source</th><th>file_extension</th><th>language</th><th>cwe_id</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>datetime[Î¼s, UTC]</td><td>str</td><td>str</td><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;2022-29198&quot;</td><td>&quot;tensorflow/tensorflow&quot;</td><td>&quot;ea50a40e84f6bff15a0912728e35b6â€¦</td><td>null</td><td>&quot;tensorflow/core/kernels/sparseâ€¦</td><td>&quot;@@ -67,6 +67,13 @@ class Sparsâ€¦</td><td>2022-05-02 23:16:54 UTC</td><td>&quot;github&quot;</td><td>&quot;cc&quot;</td><td>&quot;C/C++&quot;</td><td>[&quot;CWE-20&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 11)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ vulnerabi â”† repo      â”† commit    â”† pull_requ â”† â€¦ â”† commit_so â”† file_exte â”† language â”† cwe_id    â”‚\n",
       "â”‚ lity_id   â”† ---       â”† ---       â”† est_numbe â”†   â”† urce      â”† nsion     â”† ---      â”† ---       â”‚\n",
       "â”‚ ---       â”† str       â”† str       â”† r         â”†   â”† ---       â”† ---       â”† str      â”† list[str] â”‚\n",
       "â”‚ str       â”†           â”†           â”† ---       â”†   â”† str       â”† str       â”†          â”†           â”‚\n",
       "â”‚           â”†           â”†           â”† i64       â”†   â”†           â”†           â”†          â”†           â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2022-2919 â”† tensorflo â”† ea50a40e8 â”† null      â”† â€¦ â”† github    â”† cc        â”† C/C++    â”† [\"CWE-20\" â”‚\n",
       "â”‚ 8         â”† w/tensorf â”† 4f6bff15a â”†           â”†   â”†           â”†           â”†          â”† ]         â”‚\n",
       "â”‚           â”† low       â”† 0912728e3 â”†           â”†   â”†           â”†           â”†          â”†           â”‚\n",
       "â”‚           â”†           â”† 5b6â€¦      â”†           â”†   â”†           â”†           â”†          â”†           â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_vulnerability_fixes.filter(pl.col(\"vulnerability_id\") == \"2022-29198\").unique(\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading code fixes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1800 [00:00<?, ?it/s]ERROR:src.process_code_changes:Error processing commit c4f81d9616d40c60584e36abb15300853a66e489\n",
      "ERROR:src.process_code_changes:{'repo': 'opengeos/streamlit-geospatial', 'vulnerability_id': '2024-41117', 'commit': 'c4f81d9616d40c60584e36abb15300853a66e489', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': ['@@ -1,4 +1,5 @@\\n import ee\\n+import json\\n import os\\n import warnings\\n import datetime\\n@@ -377,7 +378,9 @@ def app():\\n                         st.write(\\n                             cm.plot_colormap(cmap=palette_options, return_fig=True)\\n                         )\\n-                        st.session_state[\"palette\"] = eval(palette)\\n+                        st.session_state[\"palette\"] = json.loads(\\n+                            palette.replace(\"\\'\", \\'\"\\')\\n+                        )\\n \\n                     if bands:\\n                         vis_params = st.text_area(\\n@@ -392,7 +395,9 @@ def app():\\n                             \"{}\",\\n                         )\\n                     try:\\n-                        st.session_state[\"vis_params\"] = eval(vis_params)\\n+                        st.session_state[\"vis_params\"] = json.loads(\\n+                            vis_params.replace(\"\\'\", \\'\"\\')\\n+                        )\\n                         st.session_state[\"vis_params\"][\"palette\"] = st.session_state[\\n                             \"palette\"\\n                         ]\\n@@ -432,7 +437,7 @@ def app():\\n                 palette_values,\\n             )\\n             st.write(cm.plot_colormap(cmap=palette_options, return_fig=True))\\n-            st.session_state[\"palette\"] = eval(palette)\\n+            st.session_state[\"palette\"] = json.loads(palette.replace(\"\\'\", \\'\"\\'))\\n         elif collection == \"MODIS Ocean Color SMI\":\\n             with st.expander(\"Show dataset details\", False):\\n                 st.markdown(\\n@@ -490,7 +495,7 @@ def app():\\n                 palette_values,\\n             )\\n             st.write(cm.plot_colormap(cmap=palette_options, return_fig=True))\\n-            st.session_state[\"palette\"] = eval(palette)\\n+            st.session_state[\"palette\"] = json.loads(palette.replace(\"\\'\", \\'\"\\'))\\n \\n         sample_roi = st.selectbox(\\n             \"Select a sample ROI or upload a GeoJSON file:\",\\n@@ -1342,7 +1347,9 @@ def app():\\n                                 if vis_params.startswith(\"{\") and vis_params.endswith(\\n                                     \"}\"\\n                                 ):\\n-                                    vis_params = eval(vis_params)\\n+                                    vis_params = json.loads(\\n+                                        vis_params.replace(\"\\'\", \\'\"\\')\\n+                                    )\\n                                 else:\\n                                     vis_params = None\\n                                 out_gif = geemap.modis_ocean_color_timelapse(', '@@ -1,4 +1,5 @@\\n import ee\\n+import json\\n import streamlit as st\\n import geemap.foliumap as geemap\\n \\n@@ -110,6 +111,14 @@ def search_data():\\n                 uid = ee_assets[index][\"uid\"]\\n                 st.markdown(f\"\"\"**Earth Engine Snippet:** `{ee_id}`\"\"\")\\n                 ee_asset = f\"{translate[asset_types[index]]}{ee_id}\\')\"\\n+\\n+                if ee_asset.startswith(\"ee.ImageCollection\"):\\n+                    ee_asset = ee.ImageCollection(ee_id)\\n+                elif ee_asset.startswith(\"ee.Image\"):\\n+                    ee_asset = ee.Image(ee_id)\\n+                elif ee_asset.startswith(\"ee.FeatureCollection\"):\\n+                    ee_asset = ee.FeatureCollection(ee_id)\\n+\\n                 vis_params = st.text_input(\\n                     \"Enter visualization parameters as a dictionary\", {}\\n                 )\\n@@ -121,11 +130,11 @@ def search_data():\\n                         if vis_params.strip() == \"\":\\n                             # st.error(\"Please enter visualization parameters\")\\n                             vis_params = \"{}\"\\n-                        vis = eval(vis_params)\\n+                        vis = json.loads(vis_params.replace(\"\\'\", \\'\"\\'))\\n                         if not isinstance(vis, dict):\\n                             st.error(\"Visualization parameters must be a dictionary\")\\n                         try:\\n-                            Map.addLayer(eval(ee_asset), vis, layer_name)\\n+                            Map.addLayer(ee_asset, vis, layer_name)\\n                         except Exception as e:\\n                             st.error(f\"Error adding layer: {e}\")\\n                     except Exception as e:', '@@ -1,4 +1,5 @@\\n import ast\\n+import json\\n import streamlit as st\\n import leafmap.foliumap as leafmap\\n \\n@@ -19,13 +20,23 @@\\n     \"\"\"\\n )\\n \\n+# Define a whitelist of trusted URLs\\n+trusted_urls = [\\n+    \"https://services.terrascope.be/wms/v2\",\\n+    # Add more trusted URLs here\\n+]\\n+\\n \\n @st.cache_data\\n def get_layers(url):\\n     options = leafmap.get_wms_layers(url)\\n     return options\\n \\n \\n+def is_trusted_url(url):\\n+    return url in trusted_urls\\n+\\n+\\n def app():\\n     st.title(\"Web Map Service (WMS)\")\\n     st.markdown(\\n@@ -50,7 +61,14 @@ def app():\\n         empty = st.empty()\\n \\n         if url:\\n-            options = get_layers(url)\\n+\\n+            if is_trusted_url(url):\\n+                options = get_layers(url)\\n+                # Process options as needed\\n+            else:\\n+                st.error(\\n+                    \"The entered URL is not trusted. Please enter a valid WMS URL.\"\\n+                )\\n \\n             default = None\\n             if url == esa_landcover:\\n@@ -79,7 +97,7 @@ def app():\\n                         url, layers=layer, name=layer, attribution=\" \", transparent=True\\n                     )\\n             if add_legend and legend_text:\\n-                legend_dict = ast.literal_eval(legend_text)\\n+                legend_dict = json.loads(legend_text.replace(\"\\'\", \\'\"\\'))\\n                 m.add_legend(legend_dict=legend_dict)\\n \\n             m.to_streamlit(height=height)', '@@ -21,6 +21,17 @@\\n )\\n \\n \\n+# Define a whitelist of trusted URLs\\n+trusted_urls = [\\n+    \"https://github.com/giswqs/streamlit-geospatial/raw/master/data/us_states.geojson\",\\n+    # Add more trusted URLs here\\n+]\\n+\\n+\\n+def is_trusted_url(url):\\n+    return url in trusted_urls\\n+\\n+\\n def save_uploaded_file(file_content, file_name):\\n     \"\"\"\\n     Save the uploaded file to a temporary directory\\n@@ -71,7 +82,7 @@ def app():\\n \\n         container = st.container()\\n \\n-        if data or url:\\n+        if data or is_trusted_url(url):\\n             if data:\\n                 file_path = save_uploaded_file(data, data.name)\\n                 layer_name = os.path.splitext(data.name)[0]', '@@ -1,3 +1,4 @@\\n+import json\\n import os\\n import leafmap.foliumap as leafmap\\n import leafmap.colormaps as cm\\n@@ -45,6 +46,14 @@ def get_palettes():\\n \"\"\"\\n )\\n \\n+\\n+def is_trusted_url(url):\\n+    if url.startswith(\"https://opendata.digitalglobe.com/events/california-fire-2020/\"):\\n+        return True\\n+    else:\\n+        return False\\n+\\n+\\n row1_col1, row1_col2 = st.columns([2, 1])\\n \\n with row1_col1:\\n@@ -59,7 +68,7 @@ def get_palettes():\\n         cog,\\n     )\\n \\n-    if url:\\n+    if is_trusted_url(url):\\n         try:\\n             options = leafmap.cog_bands(url)\\n         except Exception as e:\\n@@ -74,6 +83,8 @@ def get_palettes():\\n             pass\\n         else:\\n             st.error(\"Please select one or three bands\")\\n+    else:\\n+        st.error(\"Please enter a trusted URL\")\\n \\n     add_params = st.checkbox(\"Add visualization parameters\")\\n     if add_params:\\n@@ -83,7 +94,7 @@ def get_palettes():\\n \\n     if len(vis_params) > 0:\\n         try:\\n-            vis_params = eval(vis_params)\\n+            vis_params = json.loads(vis_params.replace(\"\\'\", \\'\"\\'))\\n         except Exception as e:\\n             st.error(\\n                 f\"Invalid visualization parameters. It should be a dictionary. Error: {e}\"'], 'file': ['pages/1_ğŸ“·_Timelapse.py', 'pages/10_ğŸŒ_Earth_Engine_Datasets.py', 'pages/7_ğŸ“¦_Web_Map_Service.py', 'pages/9_ğŸ”²_Vector_Data_Visualization.py', 'pages/8_ğŸœï¸_Raster_Data_Visualization.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('d1db4613-52a0-4663-b7a5-1835450ee642'), UUID('6b27867f-91a8-4ea8-abbf-b708688e92ae'), UUID('36a85217-6553-4cff-86be-7f85a1aa12a2'), UUID('e349f70e-aea7-41c0-b10b-eea1e35f9ee0'), UUID('b5e80dc5-422a-428d-bf88-41fdbf84c5c6')]}\n",
      "ERROR:root:Error in {'repo': 'opengeos/streamlit-geospatial', 'vulnerability_id': '2024-41117', 'commit': 'c4f81d9616d40c60584e36abb15300853a66e489', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': ['@@ -1,4 +1,5 @@\\n import ee\\n+import json\\n import os\\n import warnings\\n import datetime\\n@@ -377,7 +378,9 @@ def app():\\n                         st.write(\\n                             cm.plot_colormap(cmap=palette_options, return_fig=True)\\n                         )\\n-                        st.session_state[\"palette\"] = eval(palette)\\n+                        st.session_state[\"palette\"] = json.loads(\\n+                            palette.replace(\"\\'\", \\'\"\\')\\n+                        )\\n \\n                     if bands:\\n                         vis_params = st.text_area(\\n@@ -392,7 +395,9 @@ def app():\\n                             \"{}\",\\n                         )\\n                     try:\\n-                        st.session_state[\"vis_params\"] = eval(vis_params)\\n+                        st.session_state[\"vis_params\"] = json.loads(\\n+                            vis_params.replace(\"\\'\", \\'\"\\')\\n+                        )\\n                         st.session_state[\"vis_params\"][\"palette\"] = st.session_state[\\n                             \"palette\"\\n                         ]\\n@@ -432,7 +437,7 @@ def app():\\n                 palette_values,\\n             )\\n             st.write(cm.plot_colormap(cmap=palette_options, return_fig=True))\\n-            st.session_state[\"palette\"] = eval(palette)\\n+            st.session_state[\"palette\"] = json.loads(palette.replace(\"\\'\", \\'\"\\'))\\n         elif collection == \"MODIS Ocean Color SMI\":\\n             with st.expander(\"Show dataset details\", False):\\n                 st.markdown(\\n@@ -490,7 +495,7 @@ def app():\\n                 palette_values,\\n             )\\n             st.write(cm.plot_colormap(cmap=palette_options, return_fig=True))\\n-            st.session_state[\"palette\"] = eval(palette)\\n+            st.session_state[\"palette\"] = json.loads(palette.replace(\"\\'\", \\'\"\\'))\\n \\n         sample_roi = st.selectbox(\\n             \"Select a sample ROI or upload a GeoJSON file:\",\\n@@ -1342,7 +1347,9 @@ def app():\\n                                 if vis_params.startswith(\"{\") and vis_params.endswith(\\n                                     \"}\"\\n                                 ):\\n-                                    vis_params = eval(vis_params)\\n+                                    vis_params = json.loads(\\n+                                        vis_params.replace(\"\\'\", \\'\"\\')\\n+                                    )\\n                                 else:\\n                                     vis_params = None\\n                                 out_gif = geemap.modis_ocean_color_timelapse(', '@@ -1,4 +1,5 @@\\n import ee\\n+import json\\n import streamlit as st\\n import geemap.foliumap as geemap\\n \\n@@ -110,6 +111,14 @@ def search_data():\\n                 uid = ee_assets[index][\"uid\"]\\n                 st.markdown(f\"\"\"**Earth Engine Snippet:** `{ee_id}`\"\"\")\\n                 ee_asset = f\"{translate[asset_types[index]]}{ee_id}\\')\"\\n+\\n+                if ee_asset.startswith(\"ee.ImageCollection\"):\\n+                    ee_asset = ee.ImageCollection(ee_id)\\n+                elif ee_asset.startswith(\"ee.Image\"):\\n+                    ee_asset = ee.Image(ee_id)\\n+                elif ee_asset.startswith(\"ee.FeatureCollection\"):\\n+                    ee_asset = ee.FeatureCollection(ee_id)\\n+\\n                 vis_params = st.text_input(\\n                     \"Enter visualization parameters as a dictionary\", {}\\n                 )\\n@@ -121,11 +130,11 @@ def search_data():\\n                         if vis_params.strip() == \"\":\\n                             # st.error(\"Please enter visualization parameters\")\\n                             vis_params = \"{}\"\\n-                        vis = eval(vis_params)\\n+                        vis = json.loads(vis_params.replace(\"\\'\", \\'\"\\'))\\n                         if not isinstance(vis, dict):\\n                             st.error(\"Visualization parameters must be a dictionary\")\\n                         try:\\n-                            Map.addLayer(eval(ee_asset), vis, layer_name)\\n+                            Map.addLayer(ee_asset, vis, layer_name)\\n                         except Exception as e:\\n                             st.error(f\"Error adding layer: {e}\")\\n                     except Exception as e:', '@@ -1,4 +1,5 @@\\n import ast\\n+import json\\n import streamlit as st\\n import leafmap.foliumap as leafmap\\n \\n@@ -19,13 +20,23 @@\\n     \"\"\"\\n )\\n \\n+# Define a whitelist of trusted URLs\\n+trusted_urls = [\\n+    \"https://services.terrascope.be/wms/v2\",\\n+    # Add more trusted URLs here\\n+]\\n+\\n \\n @st.cache_data\\n def get_layers(url):\\n     options = leafmap.get_wms_layers(url)\\n     return options\\n \\n \\n+def is_trusted_url(url):\\n+    return url in trusted_urls\\n+\\n+\\n def app():\\n     st.title(\"Web Map Service (WMS)\")\\n     st.markdown(\\n@@ -50,7 +61,14 @@ def app():\\n         empty = st.empty()\\n \\n         if url:\\n-            options = get_layers(url)\\n+\\n+            if is_trusted_url(url):\\n+                options = get_layers(url)\\n+                # Process options as needed\\n+            else:\\n+                st.error(\\n+                    \"The entered URL is not trusted. Please enter a valid WMS URL.\"\\n+                )\\n \\n             default = None\\n             if url == esa_landcover:\\n@@ -79,7 +97,7 @@ def app():\\n                         url, layers=layer, name=layer, attribution=\" \", transparent=True\\n                     )\\n             if add_legend and legend_text:\\n-                legend_dict = ast.literal_eval(legend_text)\\n+                legend_dict = json.loads(legend_text.replace(\"\\'\", \\'\"\\'))\\n                 m.add_legend(legend_dict=legend_dict)\\n \\n             m.to_streamlit(height=height)', '@@ -21,6 +21,17 @@\\n )\\n \\n \\n+# Define a whitelist of trusted URLs\\n+trusted_urls = [\\n+    \"https://github.com/giswqs/streamlit-geospatial/raw/master/data/us_states.geojson\",\\n+    # Add more trusted URLs here\\n+]\\n+\\n+\\n+def is_trusted_url(url):\\n+    return url in trusted_urls\\n+\\n+\\n def save_uploaded_file(file_content, file_name):\\n     \"\"\"\\n     Save the uploaded file to a temporary directory\\n@@ -71,7 +82,7 @@ def app():\\n \\n         container = st.container()\\n \\n-        if data or url:\\n+        if data or is_trusted_url(url):\\n             if data:\\n                 file_path = save_uploaded_file(data, data.name)\\n                 layer_name = os.path.splitext(data.name)[0]', '@@ -1,3 +1,4 @@\\n+import json\\n import os\\n import leafmap.foliumap as leafmap\\n import leafmap.colormaps as cm\\n@@ -45,6 +46,14 @@ def get_palettes():\\n \"\"\"\\n )\\n \\n+\\n+def is_trusted_url(url):\\n+    if url.startswith(\"https://opendata.digitalglobe.com/events/california-fire-2020/\"):\\n+        return True\\n+    else:\\n+        return False\\n+\\n+\\n row1_col1, row1_col2 = st.columns([2, 1])\\n \\n with row1_col1:\\n@@ -59,7 +68,7 @@ def get_palettes():\\n         cog,\\n     )\\n \\n-    if url:\\n+    if is_trusted_url(url):\\n         try:\\n             options = leafmap.cog_bands(url)\\n         except Exception as e:\\n@@ -74,6 +83,8 @@ def get_palettes():\\n             pass\\n         else:\\n             st.error(\"Please select one or three bands\")\\n+    else:\\n+        st.error(\"Please enter a trusted URL\")\\n \\n     add_params = st.checkbox(\"Add visualization parameters\")\\n     if add_params:\\n@@ -83,7 +94,7 @@ def get_palettes():\\n \\n     if len(vis_params) > 0:\\n         try:\\n-            vis_params = eval(vis_params)\\n+            vis_params = json.loads(vis_params.replace(\"\\'\", \\'\"\\'))\\n         except Exception as e:\\n             st.error(\\n                 f\"Invalid visualization parameters. It should be a dictionary. Error: {e}\"'], 'file': ['pages/1_ğŸ“·_Timelapse.py', 'pages/10_ğŸŒ_Earth_Engine_Datasets.py', 'pages/7_ğŸ“¦_Web_Map_Service.py', 'pages/9_ğŸ”²_Vector_Data_Visualization.py', 'pages/8_ğŸœï¸_Raster_Data_Visualization.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('d1db4613-52a0-4663-b7a5-1835450ee642'), UUID('6b27867f-91a8-4ea8-abbf-b708688e92ae'), UUID('36a85217-6553-4cff-86be-7f85a1aa12a2'), UUID('e349f70e-aea7-41c0-b10b-eea1e35f9ee0'), UUID('b5e80dc5-422a-428d-bf88-41fdbf84c5c6')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:8:         if len(bands) == 1 or len(bands) == 3:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:8:         if len(bands) == 1 or len(bands) == 3:\n",
      "  0%|          | 7/1800 [00:05<22:50,  1.31it/s]ERROR:src.process_code_changes:Error processing commit 1729d167b4ae4a5835bbc7211b92c6828b1c4125\n",
      "ERROR:src.process_code_changes:{'repo': 'OctoPrint/OctoPrint', 'vulnerability_id': '2024-23637', 'commit': '1729d167b4ae4a5835bbc7211b92c6828b1c4125', 'commit_source': 'github', 'cwe_id': ['CWE-287', 'CWE-287'], 'patch': ['@@ -48,6 +48,10 @@\\n         return this.base.postJson(apiKeyUrl, opts);\\n     };\\n \\n+    OctoPrintSettingsClient.prototype.deleteApiKey = function (opts) {\\n+        return this.base.delete(apiKeyUrl, opts);\\n+    };\\n+\\n     OctoPrintClient.registerComponent(\"settings\", OctoPrintSettingsClient);\\n     return OctoPrintSettingsClient;\\n });', '@@ -127,6 +127,8 @@ $(function () {\\n         self.api_key = ko.observable(undefined);\\n         self.api_allowCrossOrigin = ko.observable(undefined);\\n \\n+        self.apiKeyVisible = ko.observable(false);\\n+\\n         self.appearance_name = ko.observable(undefined);\\n         self.appearance_color = ko.observable(undefined);\\n         self.appearance_colorTransparent = ko.observable();\\n@@ -703,10 +705,36 @@ $(function () {\\n             );\\n         };\\n \\n+        self.deleteApiKey = () => {\\n+            if (!CONFIG_ACCESS_CONTROL) return;\\n+            if (!self.api_key()) return;\\n+\\n+            showConfirmationDialog(\\n+                gettext(\\n+                    \"This will delete the API Key. It will cease to to function immediately.\"\\n+                ),\\n+                function () {\\n+                    OctoPrint.settings.deleteApiKey().done(() => {\\n+                        self.api_key(undefined);\\n+                    });\\n+                }\\n+            );\\n+        };\\n+\\n         self.copyApiKey = function () {\\n             copyToClipboard(self.api_key());\\n         };\\n \\n+        self.revealingApiKey = ko.observable(false);\\n+        self.revealApiKey = () => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                self.revealingApiKey(true);\\n+                self.requestData().always(() => {\\n+                    self.revealingApiKey(false);\\n+                });\\n+            });\\n+        };\\n+\\n         self.showTranslationManager = function () {\\n             self.translationManagerDialog.modal();\\n             return false;\\n@@ -1067,6 +1095,7 @@ $(function () {\\n             return data;\\n         };\\n \\n+        self.reauthenticationTimeout = undefined;\\n         self.fromResponse = function (response, local) {\\n             // server side changes to set\\n             var serverChangedData;\\n@@ -1254,6 +1283,15 @@ $(function () {\\n             mapToObservables(serverChangedData, specialMappings, clientChangedData);\\n \\n             firstRequest.resolve();\\n+\\n+            // special delivery for the API key flag\\n+            self.apiKeyVisible(self.loginState.checkCredentialsSeen());\\n+            if (self.apiKeyVisible()) {\\n+                self.reauthenticationTimeout =\\n+                    self.loginState.afterReauthenticationTimeout(() => {\\n+                        self.requestData();\\n+                    }, self.reauthenticationTimeout);\\n+            }\\n         };\\n \\n         self.cancelData = function () {', '@@ -40,10 +40,10 @@\\n         </thead>\\n         <tbody data-bind=\"foreach: keys.paginatedItems\">\\n         <tr>\\n-            <td class=\"settings_plugin_appkeys_checkbox\"><input type=\"checkbox\" data-bind=\"value: api_key, checked: $root.markedForDeletion\"></td>\\n+            <td class=\"settings_plugin_appkeys_checkbox\"><input type=\"checkbox\" data-bind=\"value: user_id + \\':\\' + app_id, checked: $root.markedForDeletion\"></td>\\n             <td class=\"settings_plugin_appkeys_user\"><span data-bind=\"text: user_id\"></span></td>\\n-            <td class=\"settings_plugin_appkeys_app\"><span data-bind=\"text: app_id\"></span><br /><small class=\"muted\">{{ _(\\'API Key\\') }}: <span data-bind=\"text: api_key\"></span> <a href=\"javascript:void(0)\" title=\"{{ _(\\'Copy API Key to clipboard\\')|edq }}\" data-bind=\"click: function() { copyToClipboard(api_key) }\"><i class=\"fas fa-copy\"></i></a></small></td>\\n-            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"fa fa-trash-o\" data-bind=\"click: function() { $parent.revokeKey($data.api_key, $data.user_id) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.dialog.showDialog(gettext(\\'Details\\'), $data) }\"></a></td>\\n+            <td class=\"settings_plugin_appkeys_app\"><span data-bind=\"text: app_id\"></span></td>\\n+            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"fa fa-trash-o\" data-bind=\"click: function() { $parent.revokeKey($data) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.showKeyDetails($data) }\"></a></td>\\n         </tr>\\n         </tbody>\\n     </table>\\n@@ -71,12 +71,12 @@\\n     <div class=\"control-group\">\\n         <label class=\"control-label\">{{ _(\\'Application identifier\\') }}</label>\\n         <div class=\"controls\">\\n-            <input type=\"text\" data-bind=\"value: editorApp\">\\n+            <input type=\"text\" data-bind=\"value: editorApp, valueUpdate: \\'input\\'\">\\n         </div>\\n     </div>\\n     <div class=\"control-group\">\\n         <div class=\"controls\">\\n-            <button class=\"btn btn-primary\" data-bind=\"click: generateKey\">{{ _(\\'Generate\\') }}</button>\\n+            <button class=\"btn btn-primary\" data-bind=\"click: generateKey, enabled: editorApp, css: {disabled: !editorApp()}\">{{ _(\\'Generate\\') }}</button>\\n         </div>\\n     </div>\\n </form>', '@@ -482,18 +482,20 @@ $(function () {\\n                 self.uploadButton.unbind(\"click\");\\n                 self.uploadButton.bind(\"click\", function () {\\n                     const proceed = () => {\\n-                        self._markWorking(\\n-                            isJsonFile\\n-                                ? gettext(\"Installing plugins...\")\\n-                                : gettext(\"Installing plugin...\"),\\n-                            isJsonFile\\n-                                ? gettext(\"Installing plugins from uploaded file...\")\\n-                                : gettext(\"Installing plugin from uploaded file...\")\\n-                        );\\n-                        data.formData = {\\n-                            dependency_links: self.followDependencyLinks()\\n-                        };\\n-                        data.submit();\\n+                        self.loginState.reauthenticateIfNecessary(() => {\\n+                            self._markWorking(\\n+                                isJsonFile\\n+                                    ? gettext(\"Installing plugins...\")\\n+                                    : gettext(\"Installing plugin...\"),\\n+                                isJsonFile\\n+                                    ? gettext(\"Installing plugins from uploaded file...\")\\n+                                    : gettext(\"Installing plugin from uploaded file...\")\\n+                            );\\n+                            data.formData = {\\n+                                dependency_links: self.followDependencyLinks()\\n+                            };\\n+                            data.submit();\\n+                        });\\n                     };\\n \\n                     if (isJsonFile) {\\n@@ -1160,12 +1162,14 @@ $(function () {\\n             self.selectedPlugins([]);\\n         };\\n \\n-        self.showRepository = function () {\\n-            self.repositoryDialog.modal({\\n-                minHeight: function () {\\n-                    return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                },\\n-                show: true\\n+        self.showRepository = () => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                self.repositoryDialog.modal({\\n+                    minHeight: function () {\\n+                        return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n+                    },\\n+                    show: true\\n+                });\\n             });\\n         };\\n \\n@@ -1174,14 +1178,6 @@ $(function () {\\n         };\\n \\n         self.installFromRepository = function (data) {\\n-            if (\\n-                !self.loginState.hasPermission(\\n-                    self.access.permissions.PLUGIN_PLUGINMANAGER_INSTALL\\n-                )\\n-            ) {\\n-                return;\\n-            }\\n-\\n             self.installPlugin(\\n                 data.archive,\\n                 data.title,\\n@@ -1235,110 +1231,117 @@ $(function () {\\n                 followDependencyLinks = self.followDependencyLinks();\\n             }\\n \\n-            var workTitle, workText;\\n-            if (!reinstall) {\\n-                workTitle = gettext(\"Installing plugin...\");\\n-                if (name) {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                var workTitle, workText;\\n+                if (!reinstall) {\\n+                    workTitle = gettext(\"Installing plugin...\");\\n+                    if (name) {\\n+                        workText = _.sprintf(\\n+                            gettext(\\'Installing plugin \"%(name)s\" from %(url)s...\\'),\\n+                            {url: _.escape(url), name: _.escape(name)}\\n+                        );\\n+                    } else {\\n+                        workText = _.sprintf(\\n+                            gettext(\"Installing plugin from %(url)s...\"),\\n+                            {\\n+                                url: _.escape(url)\\n+                            }\\n+                        );\\n+                    }\\n+                } else {\\n+                    workTitle = gettext(\"Reinstalling plugin...\");\\n                     workText = _.sprintf(\\n-                        gettext(\\'Installing plugin \"%(name)s\" from %(url)s...\\'),\\n+                        gettext(\\'Reinstalling plugin \"%(name)s\" from %(url)s...\\'),\\n                         {url: _.escape(url), name: _.escape(name)}\\n                     );\\n-                } else {\\n-                    workText = _.sprintf(gettext(\"Installing plugin from %(url)s...\"), {\\n-                        url: _.escape(url)\\n-                    });\\n                 }\\n-            } else {\\n-                workTitle = gettext(\"Reinstalling plugin...\");\\n-                workText = _.sprintf(\\n-                    gettext(\\'Reinstalling plugin \"%(name)s\" from %(url)s...\\'),\\n-                    {url: _.escape(url), name: _.escape(name)}\\n-                );\\n-            }\\n \\n-            if (self.multiInstallRunning()) {\\n-                workTitle =\\n-                    _.sprintf(\"[%(index)d/%(total)d] \", {\\n-                        index:\\n-                            this.multiInstallInitialSize() -\\n-                            self.multiInstallQueue().length,\\n-                        total: this.multiInstallInitialSize()\\n-                    }) + workTitle;\\n-            }\\n+                if (self.multiInstallRunning()) {\\n+                    workTitle =\\n+                        _.sprintf(\"[%(index)d/%(total)d] \", {\\n+                            index:\\n+                                this.multiInstallInitialSize() -\\n+                                self.multiInstallQueue().length,\\n+                            total: this.multiInstallInitialSize()\\n+                        }) + workTitle;\\n+                }\\n \\n-            self._markWorking(workTitle, workText);\\n+                self._markWorking(workTitle, workText);\\n \\n-            var onSuccess = function (response) {\\n-                    self.installUrl(\"\");\\n-                    if (response.hasOwnProperty(\"queued_installs\")) {\\n-                        self.queuedInstalls(response.queued_installs);\\n-                        var text =\\n-                            \\'<div class=\"row-fluid\"><p>\\' +\\n-                            gettext(\"The following plugins are queued to be installed.\") +\\n-                            \"</p><ul><li>\" +\\n-                            _.map(response.queued_installs, function (info) {\\n-                                var plugin = ko.utils.arrayFirst(\\n-                                    self.repositoryplugins.paginatedItems(),\\n-                                    function (item) {\\n-                                        return item.archive === info.url;\\n-                                    }\\n-                                );\\n-                                return plugin.title;\\n-                            }).join(\"</li><li>\") +\\n-                            \"</li></ul></div>\";\\n-                        if (typeof self.installQueuePopup !== \"undefined\") {\\n-                            self.installQueuePopup.update({\\n-                                text: text\\n-                            });\\n-                            if (self.installQueuePopup.state === \"closed\") {\\n-                                self.installQueuePopup.open();\\n+                var onSuccess = function (response) {\\n+                        self.installUrl(\"\");\\n+                        if (response.hasOwnProperty(\"queued_installs\")) {\\n+                            self.queuedInstalls(response.queued_installs);\\n+                            var text =\\n+                                \\'<div class=\"row-fluid\"><p>\\' +\\n+                                gettext(\\n+                                    \"The following plugins are queued to be installed.\"\\n+                                ) +\\n+                                \"</p><ul><li>\" +\\n+                                _.map(response.queued_installs, function (info) {\\n+                                    var plugin = ko.utils.arrayFirst(\\n+                                        self.repositoryplugins.paginatedItems(),\\n+                                        function (item) {\\n+                                            return item.archive === info.url;\\n+                                        }\\n+                                    );\\n+                                    return plugin.title;\\n+                                }).join(\"</li><li>\") +\\n+                                \"</li></ul></div>\";\\n+                            if (typeof self.installQueuePopup !== \"undefined\") {\\n+                                self.installQueuePopup.update({\\n+                                    text: text\\n+                                });\\n+                                if (self.installQueuePopup.state === \"closed\") {\\n+                                    self.installQueuePopup.open();\\n+                                }\\n+                            } else {\\n+                                self.installQueuePopup = new PNotify({\\n+                                    title: gettext(\"Plugin installs queued\"),\\n+                                    text: text,\\n+                                    type: \"notice\"\\n+                                });\\n+                            }\\n+                            if (self.multiInstallQueue().length > 0) {\\n+                                self.performMultiInstallJob();\\n+                            } else {\\n+                                self.multiInstallRunning(false);\\n+                                self.workingDialog.modal(\"hide\");\\n+                                self._markDone();\\n                             }\\n-                        } else {\\n-                            self.installQueuePopup = new PNotify({\\n-                                title: gettext(\"Plugin installs queued\"),\\n-                                text: text,\\n-                                type: \"notice\"\\n-                            });\\n                         }\\n-                        if (self.multiInstallQueue().length > 0) {\\n-                            self.performMultiInstallJob();\\n+                    },\\n+                    onError = function (jqXHR) {\\n+                        if (jqXHR.status === 409) {\\n+                            // there\\'s already a plugin being installed\\n+                            self._markDone(\\n+                                \"There\\'s already another plugin install in progress.\"\\n+                            );\\n                         } else {\\n-                            self.multiInstallRunning(false);\\n-                            self.workingDialog.modal(\"hide\");\\n-                            self._markDone();\\n+                            self._markDone(\\n+                                \"Could not install plugin, unknown error, please consult octoprint.log for details\"\\n+                            );\\n+                            new PNotify({\\n+                                title: gettext(\"Something went wrong\"),\\n+                                text: gettext(\"Please consult octoprint.log for details\"),\\n+                                type: \"error\",\\n+                                hide: false\\n+                            });\\n                         }\\n-                    }\\n-                },\\n-                onError = function (jqXHR) {\\n-                    if (jqXHR.status === 409) {\\n-                        // there\\'s already a plugin being installed\\n-                        self._markDone(\\n-                            \"There\\'s already another plugin install in progress.\"\\n-                        );\\n-                    } else {\\n-                        self._markDone(\\n-                            \"Could not install plugin, unknown error, please consult octoprint.log for details\"\\n-                        );\\n-                        new PNotify({\\n-                            title: gettext(\"Something went wrong\"),\\n-                            text: gettext(\"Please consult octoprint.log for details\"),\\n-                            type: \"error\",\\n-                            hide: false\\n-                        });\\n-                    }\\n-                };\\n+                    };\\n \\n-            if (reinstall) {\\n-                OctoPrint.plugins.pluginmanager\\n-                    .reinstall(reinstall, url, followDependencyLinks, fromRepo)\\n-                    .done(onSuccess)\\n-                    .fail(onError);\\n-            } else {\\n-                OctoPrint.plugins.pluginmanager\\n-                    .install(url, followDependencyLinks, fromRepo)\\n-                    .done(onSuccess)\\n-                    .fail(onError);\\n-            }\\n+                if (reinstall) {\\n+                    OctoPrint.plugins.pluginmanager\\n+                        .reinstall(reinstall, url, followDependencyLinks, fromRepo)\\n+                        .done(onSuccess)\\n+                        .fail(onError);\\n+                } else {\\n+                    OctoPrint.plugins.pluginmanager\\n+                        .install(url, followDependencyLinks, fromRepo)\\n+                        .done(onSuccess)\\n+                        .fail(onError);\\n+                }\\n+            });\\n         };\\n \\n         self.uninstallPlugin = function (data) {', '@@ -10,7 +10,11 @@\\n from octoprint.access.permissions import Permissions\\n from octoprint.server import SUCCESS, groupManager, userManager\\n from octoprint.server.api import api, valid_boolean_trues\\n-from octoprint.server.util.flask import no_firstrun_access\\n+from octoprint.server.util.flask import (\\n+    ensure_credentials_checked_recently,\\n+    no_firstrun_access,\\n+    require_credentials_checked_recently,\\n+)\\n \\n # ~~ permission api\\n \\n@@ -32,6 +36,7 @@ def get_groups():\\n \\n @api.route(\"/access/groups\", methods=[\"POST\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def add_group():\\n     data = request.get_json()\\n@@ -77,6 +82,7 @@ def get_group(key):\\n \\n @api.route(\"/access/groups/<key>\", methods=[\"PUT\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def update_group(key):\\n     data = request.get_json()\\n@@ -107,6 +113,7 @@ def update_group(key):\\n \\n @api.route(\"/access/groups/<key>\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def remove_group(key):\\n     try:\\n@@ -130,6 +137,7 @@ def get_users():\\n \\n @api.route(\"/access/users\", methods=[\"POST\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def add_user():\\n     data = request.get_json()\\n@@ -179,6 +187,7 @@ def get_user(username):\\n \\n @api.route(\"/access/users/<username>\", methods=[\"PUT\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def update_user(username):\\n     user = userManager.find_user(username)\\n@@ -208,8 +217,15 @@ def update_user(username):\\n \\n @api.route(\"/access/users/<username>\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def remove_user(username):\\n+    if not userManager.enabled:\\n+        return jsonify(SUCCESS)\\n+\\n+    if current_user.get_name() == username:\\n+        abort(400, description=\"You cannot delete yourself\")\\n+\\n     try:\\n         userManager.remove_user(username)\\n         return get_users()\\n@@ -236,13 +252,20 @@ def change_password_for_user(username):\\n         if \"password\" not in data or not data[\"password\"]:\\n             abort(400, description=\"new password is missing\")\\n \\n-        if not current_user.has_permission(Permissions.ADMIN) or \"current\" in data:\\n+        if current_user.get_name() == username:\\n             if \"current\" not in data or not data[\"current\"]:\\n                 abort(400, description=\"current password is missing\")\\n \\n             if not userManager.check_password(username, data[\"current\"]):\\n                 abort(403, description=\"Invalid current password\")\\n \\n+        elif current_user.has_permission(Permissions.ADMIN):\\n+            ensure_credentials_checked_recently()\\n+\\n+        else:\\n+            # this should never happen\\n+            abort(403, description=\"You are not allowed to change this user\\'s password\")\\n+\\n         try:\\n             userManager.change_user_password(username, data[\"password\"])\\n         except users.UnknownUser:\\n@@ -285,6 +308,10 @@ def change_settings_for_user(username):\\n     ):\\n         abort(403)\\n \\n+    if current_user.get_name() != username:\\n+        # this must be an admin, so we need to ensure credentials were checked\\n+        ensure_credentials_checked_recently()\\n+\\n     data = request.get_json()\\n \\n     try:\\n@@ -305,6 +332,10 @@ def delete_apikey_for_user(username):\\n             or current_user.has_permission(Permissions.ADMIN)\\n         )\\n     ):\\n+        if current_user.get_name() != username:\\n+            # this must be an admin, so we need to ensure credentials were checked\\n+            ensure_credentials_checked_recently()\\n+\\n         try:\\n             userManager.delete_api_key(username)\\n         except users.UnknownUser:\\n@@ -328,6 +359,10 @@ def generate_apikey_for_user(username):\\n             or current_user.has_permission(Permissions.ADMIN)\\n         )\\n     ):\\n+        if current_user.get_name() != username:\\n+            # this must be an admin, so we need to ensure credentials were checked\\n+            ensure_credentials_checked_recently()\\n+\\n         try:\\n             apikey = userManager.generate_api_key(username)\\n         except users.UnknownUser:', '@@ -6,6 +6,10 @@ $(function () {\\n         self.loginPass = ko.observable(\"\");\\n         self.loginRemember = ko.observable(false);\\n \\n+        self.reauthenticateDialog = undefined;\\n+        self.reauthenticatePass = ko.observable(\"\");\\n+        self.reauthenticateFailed = ko.observable(false);\\n+\\n         self.loggedIn = ko.observable(undefined);\\n         self.username = ko.observable(undefined);\\n         self.userneeds = ko.observable(undefined);\\n@@ -17,6 +21,7 @@ $(function () {\\n \\n         self.currentUser = ko.observable(undefined);\\n         self.currentLoginMechanism = ko.observable(undefined);\\n+        self.credentialsSeen = ko.observable(undefined);\\n \\n         self.elementUsernameInput = undefined;\\n         self.elementPasswordInput = undefined;\\n@@ -74,6 +79,86 @@ $(function () {\\n                 .done(self.updateCurrentUserData);\\n         };\\n \\n+        self._reauthenticated = false;\\n+\\n+        self.showReauthenticationDialog = () => {\\n+            const result = $.Deferred();\\n+\\n+            self._reauthenticated = false;\\n+            self.reauthenticateDialog.on(\"shown\", function () {\\n+                $(\"input[type=password]\", self.reauthenticateDialog).focus();\\n+            });\\n+            self.reauthenticateDialog.on(\"hidden\", () => {\\n+                self.reauthenticatePass(\"\");\\n+                self.reauthenticateFailed(false);\\n+                if (self._reauthenticated) {\\n+                    result.resolve();\\n+                } else {\\n+                    result.reject();\\n+                }\\n+            });\\n+            self.reauthenticateDialog.modal(\"show\");\\n+\\n+            return result.promise();\\n+        };\\n+\\n+        self.reauthenticate = () => {\\n+            const user = self.currentUser().name;\\n+            const pass = self.reauthenticatePass();\\n+            return OctoPrint.browser\\n+                .login(user, pass)\\n+                .done((response) => {\\n+                    self.fromResponse(response);\\n+                    self.reauthenticateFailed(false);\\n+                    self._reauthenticated = self.credentialsSeen();\\n+                    $(\"#reauthenticate_dialog\").modal(\"hide\");\\n+                })\\n+                .fail((response) => {\\n+                    self.reauthenticatePass(\"\");\\n+                    self.reauthenticateFailed(true);\\n+                });\\n+        };\\n+\\n+        self.reauthenticateIfNecessary = (callback) => {\\n+            if (!self.checkCredentialsSeen()) {\\n+                self.forceReauthentication(callback);\\n+            } else {\\n+                callback();\\n+            }\\n+        };\\n+\\n+        self.forceReauthentication = (callback) => {\\n+            self.showReauthenticationDialog()\\n+                .done(() => {\\n+                    callback();\\n+                })\\n+                .fail(() => {\\n+                    // Do nothing\\n+                });\\n+        };\\n+\\n+        self.checkCredentialsSeen = () => {\\n+            if (CONFIG_REAUTHENTICATION_TIMEOUT <= 0) return true;\\n+\\n+            const credentialsSeen = self.credentialsSeen();\\n+            if (!credentialsSeen) {\\n+                return false;\\n+            }\\n+\\n+            const now = new Date();\\n+            const seen = new Date(credentialsSeen);\\n+            return now - seen < CONFIG_REAUTHENTICATION_TIMEOUT * 60 * 1000;\\n+        };\\n+\\n+        self.afterReauthenticationTimeout = (callback, timeout) => {\\n+            if (CONFIG_REAUTHENTICATION_TIMEOUT <= 0) return;\\n+            if (timeout) window.clearTimeout(timeout);\\n+            return window.setTimeout(\\n+                callback,\\n+                (CONFIG_REAUTHENTICATION_TIMEOUT * 60 + 10) * 1000\\n+            );\\n+        };\\n+\\n         self.requestData = function () {\\n             return OctoPrint.browser\\n                 .passiveLogin()\\n@@ -91,6 +176,7 @@ $(function () {\\n                 if (response && response.name) {\\n                     self.loggedIn(true);\\n                     self.currentLoginMechanism(response._login_mechanism);\\n+                    self.credentialsSeen(response._credentials_seen);\\n                     self.updateCurrentUserData(response);\\n                     if (!currentLoggedIn || currentLoggedIn === undefined) {\\n                         callViewModels(self.allViewModels, \"onUserLoggedIn\", [response]);\\n@@ -242,6 +328,7 @@ $(function () {\\n                     self.loginUser(\"\");\\n                     self.loginPass(\"\");\\n                     self.loginRemember(false);\\n+                    self.reauthenticatePass(\"\");\\n \\n                     if (history && history.replaceState) {\\n                         history.replaceState(\\n@@ -322,6 +409,8 @@ $(function () {\\n         };\\n \\n         self.onStartup = function () {\\n+            self.reauthenticateDialog = $(\"#reauthenticate_dialog\");\\n+\\n             self.elementUsernameInput = $(\"#login_user\");\\n             self.elementPasswordInput = $(\"#login_pass\");\\n             self.elementLoginButton = $(\"#login_button\");\\n@@ -476,6 +565,7 @@ $(function () {\\n     }\\n \\n     OCTOPRINT_VIEWMODELS.push({\\n-        construct: LoginStateViewModel\\n+        construct: LoginStateViewModel,\\n+        elements: [\"#reauthenticate_dialog\"]\\n     });\\n });', '@@ -15,6 +15,7 @@\\n     var CONFIG_FIRST_RUN = {%  if firstRun -%} true; {% else %} false; {%- endif %}\\n     var CONFIG_TEMPERATURE_GRAPH = {%  if enableTemperatureGraph -%} true; {% else %} false; {%- endif %}\\n     var CONFIG_WIZARD = {% if wizard -%} true; {% else %} false; {%- endif %}\\n+    var CONFIG_REAUTHENTICATION_TIMEOUT = {{ reauthenticationTimeout }};\\n \\n     var SOCKJS_URI = \"{{ url_for(\\'index\\') }}\" + \"sockjs\";\\n     var SOCKJS_DEBUG = CONFIG_DEBUG;', '@@ -41,10 +41,12 @@\\n )\\n from octoprint.server.util import (\\n     has_permissions,\\n+    require_fresh_login_with,\\n     require_login_with,\\n     validate_local_redirect,\\n )\\n from octoprint.server.util.csrf import add_csrf_cookie\\n+from octoprint.server.util.flask import credentials_checked_recently\\n from octoprint.settings import settings\\n from octoprint.util import sv, to_bytes, to_unicode\\n from octoprint.util.version import get_python_version_string\\n@@ -225,9 +227,12 @@ def login():\\n         permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\\n \\n     user_id = request.args.get(\"user_id\", \"\")\\n+    reauthenticate = request.args.get(\"reauthenticate\", \"false\").lower() == \"true\"\\n \\n-    if (not user_id or current_user.get_id() == user_id) and has_permissions(\\n-        *permissions\\n+    if (\\n+        (not user_id or current_user.get_id() == user_id)\\n+        and has_permissions(*permissions)\\n+        and (not reauthenticate or credentials_checked_recently())\\n     ):\\n         return redirect(redirect_url)\\n \\n@@ -237,6 +242,7 @@ def login():\\n         \"permission_names\": map(lambda x: x.get_name(), permissions),\\n         \"user_id\": user_id,\\n         \"logged_in\": not current_user.is_anonymous,\\n+        \"reauthenticate\": reauthenticate,\\n     }\\n \\n     try:\\n@@ -257,7 +263,7 @@ def login():\\n @app.route(\"/recovery\")\\n @app.route(\"/recovery/\")\\n def recovery():\\n-    response = require_login_with(permissions=[Permissions.ADMIN])\\n+    response = require_fresh_login_with(permissions=[Permissions.ADMIN])\\n     if response:\\n         return response\\n \\n@@ -417,6 +423,9 @@ def wizard_active(templates):\\n     enable_webcam = settings().getBoolean([\"webcam\", \"webcamEnabled\"])\\n     enable_temperature_graph = settings().get([\"feature\", \"temperatureGraph\"])\\n     sockjs_connect_timeout = settings().getInt([\"devel\", \"sockJsConnectTimeout\"])\\n+    reauthentication_timeout = settings().getInt(\\n+        [\"accessControl\", \"defaultReauthenticationTimeout\"]\\n+    )\\n \\n     def default_template_filter(template_type, template_key):\\n         if template_type == \"tab\":\\n@@ -431,6 +440,7 @@ def default_template_filter(template_type, template_key):\\n         enable_webcam,\\n         enable_temperature_graph,\\n         sockjs_connect_timeout,\\n+        reauthentication_timeout,\\n         connectivityChecker.online,\\n         wizard_active(_templates.get(locale)),\\n     ] + sorted(\\n@@ -647,6 +657,7 @@ def default_view():\\n                 \"enableLoadingAnimation\": enable_loading_animation,\\n                 \"enableSdSupport\": enable_sd_support,\\n                 \"sockJsConnectTimeout\": sockjs_connect_timeout * 1000,\\n+                \"reauthenticationTimeout\": reauthentication_timeout,\\n                 \"wizard\": wizard,\\n                 \"online\": connectivityChecker.online,\\n                 \"now\": now,', '@@ -27,7 +27,7 @@\\n             </table>\\n         </td>\\n         <td class=\"settings_groups_actions\">\\n-            <a href=\"#\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update group\\')|edq }}\" data-bind=\"enable: changeable, css: {disabled: !changeable}, click: function() { $root.access.groups.showEditGroupDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"#\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete group\\') }}\" data-bind=\"enable: removable, css: {disabled: !removable}, click: function() { $root.access.groups.removeGroup($data); }\"></a>\\n+            <a href=\"javascript:void()\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update group\\')|edq }}\" data-bind=\"enable: changeable, css: {disabled: !changeable}, click: function() { $root.access.groups.showEditGroupDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void()\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete group\\') }}\" data-bind=\"enable: removable, css: {disabled: !removable}, click: function() { $root.access.groups.confirmRemoveGroup($data); }\"></a>\\n         </td>\\n     </tr>\\n     </tbody>', \"@@ -177,6 +177,7 @@\\n             {% include 'dialogs/temperature.jinja2' %}\\n             {% include 'dialogs/timelapse.jinja2' %}\\n             {% include 'dialogs/firmwareerror.jinja2' %}\\n+            {% include 'dialogs/reauthenticate.jinja2' %}\\n             <!-- End of dialogs -->\\n \\n             <!-- Overlays -->\", '@@ -685,6 +685,14 @@ function formatNumberK(num) {\\n     }\\n }\\n \\n+function rsplit(str, sep, limit) {\\n+    // copy of Python\\'s rsplit in JS\\n+    // adapted from https://stackoverflow.com/a/5202185\\n+    // caution: other than split in JS, this limit does limit the splits, NOT the number of parts\\n+    const parts = str.split(sep);\\n+    return limit ? [parts.slice(0, -limit).join(sep)].concat(parts.slice(-limit)) : parts;\\n+}\\n+\\n function pnotifyAdditionalInfo(inner) {\\n     return (\\n         \\'<div class=\"pnotify_additional_info\">\\' +', '@@ -25,14 +25,18 @@\\n         </div>\\n         <div class=\"control-group\">\\n             <label class=\"control-label\" for=\"settings-apikey\">{{ _(\\'Global API Key\\') }}</label>\\n-            <div class=\"controls\">\\n+            <div class=\"controls\" data-bind=\"visible: apiKeyVisible\">\\n                 <div class=\"input-append input-block-level\">\\n                     <input type=\"text\" readonly=\"readonly\" id=\"settings-apikey\" data-bind=\"value: api_key, attr: {placeholder: \\'{{ _(\\'N/A\\')|esq }}\\'}\">\\n                     <a class=\"btn add-on\" title=\"Copy API Key to clipboard\" data-bind=\"click: copyApiKey, css: {\\'disabled\\': !api_key()}\"><i class=\"fas fa-copy\"></i></a>\\n                     <a class=\"btn add-on\" title=\"Generate new API Key\" data-bind=\"click: generateApiKey\"><i class=\"fas fa-sync\"></i></a>\\n+                    <a class=\"btn btn-danger add-on\" title=\"Delete API Key\" data-bind=\"click: deleteApiKey, css: {\\'disabled\\': !api_key()}\"><i class=\"far fa-trash-alt\"></i></a>\\n                 </div>\\n                 <span class=\"help-block\">{{ _(\\'Please note that changes to the API key are applied immediately, without having to \"Save\" first.\\') }}</span>\\n             </div>\\n+            <div class=\"controls\" data-bind=\"visible: !apiKeyVisible()\">\\n+                <button class=\"btn\" data-bind=\"click: revealApiKey, enabled: !revealingApiKey(), css: {disabled: revealingApiKey}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: revealingApiKey\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n+            </div>\\n         </div>\\n         <div class=\"control-group\" data-bind=\"visible: api_key\">\\n             <label class=\"control-label\">{{ _(\\'QR Code\\') }}</label>', '@@ -13,7 +13,11 @@\\n from octoprint.access.permissions import Permissions\\n from octoprint.server import pluginManager, printer, userManager\\n from octoprint.server.api import NO_CONTENT, api\\n-from octoprint.server.util.flask import no_firstrun_access, with_revalidation_checking\\n+from octoprint.server.util.flask import (\\n+    credentials_checked_recently,\\n+    no_firstrun_access,\\n+    with_revalidation_checking,\\n+)\\n from octoprint.settings import settings, valid_boolean_trues\\n from octoprint.timelapse import configure_timelapse\\n from octoprint.webcams import (\\n@@ -91,6 +95,9 @@ def hash_update(value):\\n     # and likewise if the role of the user changes\\n     hash_update(repr(roles))\\n \\n+    # of if the user reauthenticates\\n+    hash_update(repr(credentials_checked_recently()))\\n+\\n     return hash.hexdigest()\\n \\n \\n@@ -118,7 +125,9 @@ def getSettings():\\n \\n     data = {\\n         \"api\": {\\n-            \"key\": s.get([\"api\", \"key\"]) if Permissions.ADMIN.can() else None,\\n+            \"key\": s.get([\"api\", \"key\"])\\n+            if Permissions.ADMIN.can() and credentials_checked_recently()\\n+            else None,\\n             \"allowCrossOrigin\": s.get([\"api\", \"allowCrossOrigin\"]),\\n         },\\n         \"appearance\": {\\n@@ -458,15 +467,15 @@ def setSettings():\\n \\n @api.route(\"/settings/apikey\", methods=[\"POST\"])\\n @no_firstrun_access\\n-@Permissions.SETTINGS.require(403)\\n+@Permissions.ADMIN.require(403)\\n def generateApiKey():\\n     apikey = settings().generateApiKey()\\n     return jsonify(apikey=apikey)\\n \\n \\n @api.route(\"/settings/apikey\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n-@Permissions.SETTINGS.require(403)\\n+@Permissions.ADMIN.require(403)\\n def deleteApiKey():\\n     settings().deleteApiKey()\\n     return NO_CONTENT', '@@ -20,7 +20,7 @@\\n             <div class=\"control-group\">\\n                 <label class=\"control-label\">{{ _(\\'API Key\\') }}</label>\\n                 <div class=\"controls\">\\n-                    <strong id=\"plugin_appkeys_keygenerated_key_text\" class=\"control-text\"></strong>\\n+                    <strong class=\"control-text\"><code id=\"plugin_appkeys_keygenerated_key_text\"></code></strong>\\n                     <a href=\"javascript:void(0)\" id=\"plugin_appkeys_keygenerated_key_copy\" title=\"{{ _(\\'Copy API Key to clipboard\\')|edq }}\" class=\"control-text\"><i class=\"fas fa-copy\"></i></a>\\n                 </div>\\n             </div>', '@@ -11,7 +11,7 @@\\n import os\\n import threading\\n import time\\n-from datetime import datetime, timedelta\\n+from datetime import datetime, timedelta, timezone\\n from typing import Any, Union\\n \\n import flask\\n@@ -732,6 +732,7 @@ def determine_user(u):\\n                             f\"Logging in user {autologin_as} from {remote_address} via autologin\"\\n                         )\\n                         flask.session[\"login_mechanism\"] = \"autologin\"\\n+                        flask.session[\"credentials_seen\"] = False\\n                         return autologin_user\\n             except Exception:\\n                 logger.exception(\\n@@ -753,9 +754,23 @@ def determine_user(u):\\n     )\\n     if flask.session.get(\"login_mechanism\") is not None:\\n         response[\"_login_mechanism\"] = flask.session.get(\"login_mechanism\")\\n+    response[\"_credentials_seen\"] = to_api_credentials_seen(\\n+        flask.session.get(\"credentials_seen\", False)\\n+    )\\n     return flask.jsonify(response)\\n \\n \\n+def to_api_credentials_seen(credentials_seen):\\n+    if not credentials_seen:\\n+        return False\\n+\\n+    return (\\n+        datetime.fromtimestamp(credentials_seen, tz=timezone.utc)\\n+        .replace(microsecond=0)\\n+        .isoformat()\\n+    )\\n+\\n+\\n # ~~ rate limiting helper\\n \\n \\n@@ -1636,6 +1651,47 @@ def decorated_view(*args, **kwargs):\\n     return decorated_view\\n \\n \\n+def credentials_checked_recently():\\n+    minutes = settings().getInt([\"accessControl\", \"defaultReauthenticationTimeout\"])\\n+    if not minutes:\\n+        return True\\n+\\n+    credentials_seen = flask.session.get(\"credentials_seen\")\\n+    now = datetime.now()\\n+\\n+    try:\\n+        if credentials_seen and datetime.fromtimestamp(\\n+            credentials_seen\\n+        ) > now - timedelta(minutes=minutes):\\n+            # credentials seen less than the set minutes ago, proceed\\n+            return True\\n+    except Exception:\\n+        logging.getLogger(__name__).exception(\"Error while checking for seen credentials\")\\n+        pass\\n+\\n+    return False\\n+\\n+\\n+def ensure_credentials_checked_recently():\\n+    if not credentials_checked_recently():\\n+        flask.abort(403, description=\"Please reauthenticate with your credentials\")\\n+\\n+\\n+def require_credentials_checked_recently(func):\\n+    \"\"\"\\n+    If you decorate a view with this, it will ensure that only users who entered their password\\n+    recently in this login session are allowed to proceed. Otherwise it will cause a HTTP 403 status code\\n+    to be returned by the decorated resource.\\n+    \"\"\"\\n+\\n+    @functools.wraps(func)\\n+    def decorated_view(*args, **kwargs):\\n+        ensure_credentials_checked_recently()\\n+        return func(*args, **kwargs)\\n+\\n+    return decorated_view\\n+\\n+\\n def get_remote_address(request):\\n     forwardedFor = request.headers.get(\"X-Forwarded-For\", None)\\n     if forwardedFor is not None:', '@@ -63,24 +63,34 @@\\n \\n     <div id=\"login\" class=\"container\">\\n         <form class=\"form-signin\">\\n-            <h2 class=\"form-signin-heading\" data-test-id=\"login-title\">{{ _(\\'Please log in\\') }}</h2>\\n+            <h2 class=\"form-signin-heading\" data-test-id=\"login-title\">\\n+                {% if reauthenticate %}\\n+                    {{ _(\\'Please reauthenticate\\') }}\\n+                {% else %}\\n+                    {{ _(\\'Please log in\\') }}\\n+                {% endif %}\\n+            </h2>\\n \\n             <div id=\"login-error-credentials\" class=\"alert alert-error\" data-test-id=\"login-error\">{{ _(\\'Incorrect username or password. Hint: Both are case sensitive!\\') }}</div>\\n             <div id=\"login-error-rate\" class=\"alert alert-error\" data-test-id=\"login-error-rate\">{{ _(\\'You have made too many failed login attempts. Please try again later.\\') }}</div>\\n             <div id=\"login-offline\" class=\"alert alert-error\">{{ _(\\'Server is currently offline.\\') }} <a id=\"login-reconnect\" href=\"javascript:void(0)\">{{ _(\\'Reconnect...\\') }}</a></div>\\n \\n-            {% if user_id %}<p>\\n+            {% if reauthenticate %}<p>\\n+                {{ _(\\'You need to reauthenticate before you can continue. Please enter your password below.\\') }}\\n+            </p>{% elif user_id %}<p>\\n                 {{ _(\\'The following account is required:\\') }} {{ user_id|e }}\\n             </p>{% elif logged_in %}<p>\\n                 {{ _(\\'An account with the following permissions is required:\\') }} {{ permission_names|join(\", \") }}\\n             </p>{% endif %}\\n \\n             <input type=\"text\" id=\"login-user\" data-test-id=\"login-username\" class=\"input-block-level\" placeholder=\"{{ _(\\'Username\\')|edq }}\" {% if user_id %}value=\"{{ user_id|edq }}\" disabled{% endif %} autofocus autocapitalize=\"none\">\\n             <input type=\"password\" id=\"login-password\" data-test-id=\"login-password\" class=\"input-block-level\" placeholder=\"{{ _(\\'Password\\')|edq }}\">\\n+            {% if not reauthenticate %}\\n             <span class=\"pull-right\"><small><a href=\"https://faq.octoprint.org/forgotten-password\" id=\"login-forgotpassword\" target=\"_blank\" tabindex=\"-1\">{{ _(\\'Forgot password?\\') }}</a></small></span>\\n             <label class=\"checkbox\">\\n                 <input type=\"checkbox\" id=\"login-remember\" data-test-id=\"login-remember-me\"> {{ _(\\'Remember me\\') }}\\n             </label>\\n+            {% endif %}\\n             <button class=\"btn btn-block btn-large btn-primary\" id=\"login-button\" data-test-id=\"login-submit\" type=\"submit\">{{ _(\\'Log in\\') }}</button>\\n         </form>\\n     </div>', '@@ -28,7 +28,7 @@\\n         </td>\\n         <td class=\"settings_users_active\"><i class=\"far\" data-bind=\"css: { \\'fa-check-square\\': active, \\'fa-square\\': !active }\"></i></td>\\n         <td class=\"settings_users_actions\">\\n-            <a href=\"#\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update User\\')|edq }}\" data-bind=\"click: function() { $root.access.users.showEditUserDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"#\" class=\"fas fa-key\" title=\"{{ _(\\'Change password\\') }}\" data-bind=\"click: function() { $root.access.users.showChangePasswordDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"#\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete user\\') }}\" data-bind=\"click: function() { $root.access.users.removeUser($data); }\"></a>\\n+            <a href=\"javascript:void()\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update User\\')|edq }}\" data-bind=\"click: function() { $root.access.users.showEditUserDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void()\" class=\"fas fa-key\" title=\"{{ _(\\'Change password\\') }}\" data-bind=\"click: function() { $root.access.users.showChangePasswordDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void()\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete user\\') }}\" data-bind=\"css: { disabled: !$root.access.users.isDeleteUserEnabled($data)}, enabled: $root.access.users.isDeleteUserEnabled($data), click: function() { if ($root.access.users.isDeleteUserEnabled($data)) { $root.access.users.confirmRemoveUser($data); } }\"></a>\\n         </td>\\n     </tr>\\n     </tbody>\\n@@ -138,6 +138,12 @@\\n     </div>\\n     <div class=\"modal-body\">\\n         <form class=\"form-horizontal\" onsubmit=\"return false;\">\\n+            <div class=\"control-group\" data-bind=\"visible: $root.access.loginState.username() == $root.access.users.editor.name()\">\\n+                <label class=\"control-label\" for=\"settings-usersDialogChangePasswordPasswordC\">{{ _(\\'Current Password\\') }}</label>\\n+                <div class=\"controls\">\\n+                    <input type=\"password\" class=\"input-block-level\" id=\"settings-usersDialogChangePasswordPasswordC\" data-bind=\"value: $root.access.users.editor.currentPassword\" required>\\n+                </div>\\n+            </div>\\n             <div class=\"control-group\">\\n                 <label class=\"control-label\" for=\"settings-usersDialogChangePasswordPassword1\">{{ _(\\'New Password\\') }}</label>\\n                 <div class=\"controls\">', '@@ -0,0 +1,25 @@\\n+<div id=\"reauthenticate_dialog\" class=\"modal hide fade-in\">\\n+    <div class=\"modal-header\">\\n+        <a href=\"javascript:void()\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">&times;</a>\\n+        <h3>{{ _(\\'Please reauthenticate\\') }}</h3>\\n+    </div>\\n+    <div class=\"modal-body\">\\n+        <p>{% trans %}\\n+            You need to reauthenticate to perform this action.\\n+            Please enter your password below.\\n+        {% endtrans %}</p>\\n+        <form class=\"form-horizontal\" data-bind=\"event: {\\'submit\\': reauthenticate }\" onsubmit=\"return false; // this gets overwritten on view model bind\">\\n+            <div class=\"control-group\">\\n+                <label class=\"control-label\">{{ _(\\'Password\\') }}</label>\\n+                <div class=\"controls\">\\n+                    <input type=\"password\" class=\"input-block-level\" data-bind=\"value: reauthenticatePass\" placeholder=\"{{ _(\"Password\")|edq }}\">\\n+                    <span class=\"help-block text-error\" data-bind=\"visible: reauthenticateFailed\">{{ _(\\'Reauthentication failed. Wrong password?\\') }}</span>\\n+                </div>\\n+            </div>\\n+        </form>\\n+    </div>\\n+    <div class=\"modal-footer\">\\n+        <button class=\"btn\" data-dismiss=\"modal\">{{ _(\\'Abort\\') }}</button>\\n+        <button class=\"btn btn-primary\" data-bind=\"click: function() { reauthenticate(); }\">{{ _(\\'Confirm\\') }}</button>\\n+    </div>\\n+</div>', '@@ -50,3 +50,6 @@ class AccessControlConfig(BaseModel):\\n \\n     addRemoteUsers: bool = False\\n     \"\"\"If a remote user is not found, add them. Use this only if all users from the remote system can use OctoPrint.\"\"\"\\n+\\n+    defaultReauthenticationTimeout: int = 5\\n+    \"\"\"Default timeout after which to require reauthentication by a user for dangerous changes, in minutes. Defaults to 5 minutes. Set to 0 to disable reauthentication requirements (SECURITY IMPACT!).\"\"\"', '@@ -13,8 +13,10 @@\\n         </thead>\\n         <tbody data-bind=\"foreach: keys.paginatedItems\">\\n         <tr>\\n-            <td class=\"settings_plugin_appkeys_app\"><span data-bind=\"text: app_id\"></span><br /><small class=\"muted\">{{ _(\\'API Key\\') }}: <span data-bind=\"text: api_key\"></span> <a href=\"javascript:void(0)\" title=\"{{ _(\\'Copy API Key to clipboard\\')|edq }}\" data-bind=\"click: function() { copyToClipboard(api_key) }\"><i class=\"fas fa-copy\"></i></a></small></td>\\n-            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"far fa-trash-alt\" data-bind=\"click: function() { $parent.revokeKey($data.api_key) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.dialog.showDialog(gettext(\\'Details\\'), $data) }\"></a></td>\\n+            <td class=\"settings_plugin_appkeys_app\">\\n+                <span data-bind=\"text: app_id\"></span>\\n+            </td>\\n+            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"far fa-trash-alt\" data-bind=\"click: function() { $parent.revokeKey($data) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.showKeyDetails($data) }\"></a></td>\\n         </tr>\\n         </tbody>\\n     </table>\\n@@ -37,13 +39,13 @@\\n     <div class=\"control-group\">\\n         <label class=\"control-label\">{{ _(\\'Application identifier\\') }}</label>\\n         <div class=\"controls\">\\n-            <input type=\"text\" data-bind=\"value: editorApp\">\\n+            <input type=\"text\" data-bind=\"value: editorApp, valueUpdate: \\'input\\'\">\\n         </div>\\n     </div>\\n \\n     <div class=\"control-group\">\\n         <div class=\"controls\">\\n-            <button class=\"btn btn-primary\" data-bind=\"click: generateKey\">{{ _(\\'Generate\\') }}</button>\\n+            <button class=\"btn btn-primary\" data-bind=\"click: generateKey, enabled: editorApp, css: {disabled: !editorApp()}\">{{ _(\\'Generate\\') }}</button>\\n         </div>\\n     </div>\\n </form>', '@@ -35,6 +35,14 @@ $(function () {\\n \\n             self.currentUser = ko.observable(self.emptyUser).extend({notify: \"always\"});\\n \\n+            self.isCurrentUser = (user) => {\\n+                return user && user.name && user.name == access.loginState.username();\\n+            };\\n+\\n+            self.isDeleteUserEnabled = (user) => {\\n+                return !self.isCurrentUser(user);\\n+            };\\n+\\n             self.editor = {\\n                 name: ko.observable(undefined),\\n                 groups: ko.observableArray([]),\\n@@ -160,22 +168,28 @@ $(function () {\\n             self.showAddUserDialog = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.currentUser(undefined);\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.currentUser(undefined);\\n \\n-                $(\\'ul.nav-pills a[data-toggle=\"tab\"]:first\\', self.userEditorDialog).tab(\\n-                    \"show\"\\n-                );\\n-                self.userEditorDialog\\n-                    .modal({\\n-                        minHeight: function () {\\n-                            return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                        }\\n-                    })\\n-                    .css({\\n-                        \"margin-left\": function () {\\n-                            return -($(this).width() / 2);\\n-                        }\\n-                    });\\n+                    $(\\n+                        \\'ul.nav-pills a[data-toggle=\"tab\"]:first\\',\\n+                        self.userEditorDialog\\n+                    ).tab(\"show\");\\n+                    self.userEditorDialog\\n+                        .modal({\\n+                            minHeight: function () {\\n+                                return Math.max(\\n+                                    $.fn.modal.defaults.maxHeight() - 80,\\n+                                    250\\n+                                );\\n+                            }\\n+                        })\\n+                        .css({\\n+                            \"margin-left\": function () {\\n+                                return -($(this).width() / 2);\\n+                            }\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmAddUser = function () {\\n@@ -189,10 +203,12 @@ $(function () {\\n                     active: self.editor.active()\\n                 };\\n \\n-                self.addUser(user).done(function () {\\n-                    // close dialog\\n-                    self.currentUser(undefined);\\n-                    self.userEditorDialog.modal(\"hide\");\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.addUser(user).done(function () {\\n+                        // close dialog\\n+                        self.currentUser(undefined);\\n+                        self.userEditorDialog.modal(\"hide\");\\n+                    });\\n                 });\\n             };\\n \\n@@ -222,17 +238,19 @@ $(function () {\\n                         });\\n                 };\\n \\n-                OctoPrint.users\\n-                    .get(user.name)\\n-                    .done(function (data) {\\n-                        process(data);\\n-                    })\\n-                    .fail(function () {\\n-                        log.warn(\\n-                            \"Could not fetch current user data, proceeding with client side data copy\"\\n-                        );\\n-                        process(user);\\n-                    });\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    OctoPrint.users\\n+                        .get(user.name)\\n+                        .done(function (data) {\\n+                            process(data);\\n+                        })\\n+                        .fail(function () {\\n+                            log.warn(\\n+                                \"Could not fetch current user data, proceeding with client side data copy\"\\n+                            );\\n+                            process(user);\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmEditUser = function () {\\n@@ -243,45 +261,97 @@ $(function () {\\n                 user.groups = self.editor.groups();\\n                 user.permissions = self.editor.permissions();\\n \\n-                self.updateUser(user).done(function () {\\n-                    // close dialog\\n-                    self.currentUser(undefined);\\n-                    self.userEditorDialog.modal(\"hide\");\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.updateUser(user).done(function () {\\n+                        // close dialog\\n+                        self.currentUser(undefined);\\n+                        self.userEditorDialog.modal(\"hide\");\\n+                    });\\n+                });\\n+            };\\n+\\n+            self.confirmRemoveUser = (user) => {\\n+                if (!CONFIG_ACCESS_CONTROL) return;\\n+\\n+                if (user.name === access.loginState.username()) {\\n+                    // we do not allow to delete ourselves\\n+                    new PNotify({\\n+                        title: gettext(\"Not possible\"),\\n+                        text: gettext(\"You may not delete your own account.\"),\\n+                        type: \"error\"\\n+                    });\\n+                    return $.Deferred()\\n+                        .reject(\"You may not delete your own account\")\\n+                        .promise();\\n+                }\\n+\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    showConfirmationDialog({\\n+                        title: gettext(\"Are you sure?\"),\\n+                        message: _.sprintf(\\n+                            gettext(\\'You are about to delete the user \"%(name)s\".\\'),\\n+                            {name: _.escape(user.name)}\\n+                        ),\\n+                        proceed: gettext(\"Delete\"),\\n+                        onproceed: () => {\\n+                            self.removeUser(user);\\n+                        }\\n+                    });\\n                 });\\n             };\\n \\n             self.showChangePasswordDialog = function (user) {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.currentUser(user);\\n-                self.changePasswordDialog.modal(\"show\");\\n+                const proceed = () => {\\n+                    self.currentUser(user);\\n+                    self.changePasswordDialog.modal(\"show\");\\n+                };\\n+\\n+                if (self.isCurrentUser(user)) {\\n+                    proceed();\\n+                } else {\\n+                    access.loginState.reauthenticateIfNecessary(proceed);\\n+                }\\n             };\\n \\n             self.confirmChangePassword = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.updatePassword(\\n-                    self.currentUser().name,\\n-                    self.editor.password(),\\n-                    self.editor.currentPassword()\\n-                )\\n-                    .done(function () {\\n-                        // close dialog\\n-                        self.currentUser(undefined);\\n-                        self.changePasswordDialog.modal(\"hide\");\\n-                    })\\n-                    .fail(function (xhr) {\\n-                        if (xhr.status === 403) {\\n-                            self.currentPasswordMismatch(true);\\n-                        }\\n-                    });\\n+                const proceed = () => {\\n+                    self.updatePassword(\\n+                        self.currentUser().name,\\n+                        self.editor.password(),\\n+                        self.editor.currentPassword()\\n+                    )\\n+                        .done(function () {\\n+                            // close dialog\\n+                            self.currentUser(undefined);\\n+                            self.changePasswordDialog.modal(\"hide\");\\n+                        })\\n+                        .fail(function (xhr) {\\n+                            if (xhr.status === 403) {\\n+                                self.currentPasswordMismatch(true);\\n+                            }\\n+                        });\\n+                };\\n+\\n+                if (self.isCurrentUser()) {\\n+                    proceed();\\n+                } else {\\n+                    access.loginState.reauthenticateIfNecessary(proceed);\\n+                }\\n             };\\n \\n             self.confirmGenerateApikey = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.generateApikey(self.currentUser().name).done(function (response) {\\n-                    self._updateApikey(response.apikey);\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.generateApikey(self.currentUser().name).done(function (\\n+                        response\\n+                    ) {\\n+                        self._updateApikey(response.apikey);\\n+                    });\\n                 });\\n             };\\n \\n@@ -297,8 +367,10 @@ $(function () {\\n             self.confirmDeleteApikey = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.deleteApikey(self.currentUser().name).done(function () {\\n-                    self._updateApikey(undefined);\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.deleteApikey(self.currentUser().name).done(function () {\\n+                        self._updateApikey(undefined);\\n+                    });\\n                 });\\n             };\\n \\n@@ -315,10 +387,16 @@ $(function () {\\n                 if (!user) {\\n                     throw OctoPrint.InvalidArgumentError(\"user must be set\");\\n                 }\\n-                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN))\\n+                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN)) {\\n                     return $.Deferred()\\n                         .reject(\"You are not authorized to perform this action\")\\n                         .promise();\\n+                }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.users.add(user).done(self.fromResponse);\\n             };\\n@@ -327,40 +405,29 @@ $(function () {\\n                 if (!user) {\\n                     throw OctoPrint.InvalidArgumentError(\"user must be set\");\\n                 }\\n-                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN))\\n+                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN)) {\\n                     return $.Deferred()\\n                         .reject(\"You are not authorized to perform this action\")\\n                         .promise();\\n-\\n-                if (user.name === access.loginState.username()) {\\n-                    // we do not allow to delete ourselves\\n-                    new PNotify({\\n-                        title: gettext(\"Not possible\"),\\n-                        text: gettext(\"You may not delete your own account.\"),\\n-                        type: \"error\"\\n-                    });\\n+                }\\n+                if (!access.loginState.credentialsSeen()) {\\n                     return $.Deferred()\\n-                        .reject(\"You may not delete your own account\")\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n                         .promise();\\n                 }\\n \\n-                showConfirmationDialog({\\n-                    title: gettext(\"Are you sure?\"),\\n-                    message: _.sprintf(\\n-                        gettext(\\'You are about to delete the user \"%(name)s\".\\'),\\n-                        {name: _.escape(user.name)}\\n-                    ),\\n-                    proceed: gettext(\"Delete\"),\\n-                    onproceed: function () {\\n-                        OctoPrint.access.users.delete(user.name).done(self.fromResponse);\\n-                    }\\n-                });\\n+                return OctoPrint.access.users.delete(user.name).done(self.fromResponse);\\n             };\\n \\n             self.updateUser = function (user) {\\n                 if (!user) {\\n                     throw OctoPrint.InvalidArgumentError(\"user must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.users\\n                     .update(\\n@@ -374,16 +441,31 @@ $(function () {\\n             };\\n \\n             self.updatePassword = function (username, password, current) {\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n                 return OctoPrint.access.users.changePassword(username, password, current);\\n             };\\n \\n             self.generateApikey = function (username) {\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n                 return OctoPrint.access.users.generateApiKey(username).done(function () {\\n                     self.requestData();\\n                 });\\n             };\\n \\n             self.deleteApikey = function (username) {\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n                 return OctoPrint.access.users.resetApiKey(username);\\n             };\\n \\n@@ -576,21 +658,27 @@ $(function () {\\n             };\\n \\n             self.showAddGroupDialog = function () {\\n-                self.currentGroup(undefined);\\n-                $(\\'ul.nav-pills a[data-toggle=\"tab\"]:first\\', self.groupEditorDialog).tab(\\n-                    \"show\"\\n-                );\\n-                self.groupEditorDialog\\n-                    .modal({\\n-                        minHeight: function () {\\n-                            return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                        }\\n-                    })\\n-                    .css({\\n-                        \"margin-left\": function () {\\n-                            return -($(this).width() / 2);\\n-                        }\\n-                    });\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.currentGroup(undefined);\\n+                    $(\\n+                        \\'ul.nav-pills a[data-toggle=\"tab\"]:first\\',\\n+                        self.groupEditorDialog\\n+                    ).tab(\"show\");\\n+                    self.groupEditorDialog\\n+                        .modal({\\n+                            minHeight: function () {\\n+                                return Math.max(\\n+                                    $.fn.modal.defaults.maxHeight() - 80,\\n+                                    250\\n+                                );\\n+                            }\\n+                        })\\n+                        .css({\\n+                            \"margin-left\": function () {\\n+                                return -($(this).width() / 2);\\n+                            }\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmAddGroup = function () {\\n@@ -617,21 +705,27 @@ $(function () {\\n             self.showEditGroupDialog = function (group) {\\n                 if (!group.changeable) return;\\n \\n-                self.currentGroup(group);\\n-                $(\\'ul.nav-pills a[data-toggle=\"tab\"]:first\\', self.groupEditorDialog).tab(\\n-                    \"show\"\\n-                );\\n-                self.groupEditorDialog\\n-                    .modal({\\n-                        minHeight: function () {\\n-                            return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                        }\\n-                    })\\n-                    .css({\\n-                        \"margin-left\": function () {\\n-                            return -($(this).width() / 2);\\n-                        }\\n-                    });\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.currentGroup(group);\\n+                    $(\\n+                        \\'ul.nav-pills a[data-toggle=\"tab\"]:first\\',\\n+                        self.groupEditorDialog\\n+                    ).tab(\"show\");\\n+                    self.groupEditorDialog\\n+                        .modal({\\n+                            minHeight: function () {\\n+                                return Math.max(\\n+                                    $.fn.modal.defaults.maxHeight() - 80,\\n+                                    250\\n+                                );\\n+                            }\\n+                        })\\n+                        .css({\\n+                            \"margin-left\": function () {\\n+                                return -($(this).width() / 2);\\n+                            }\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmEditGroup = function () {\\n@@ -653,6 +747,24 @@ $(function () {\\n                 });\\n             };\\n \\n+            self.confirmRemoveGroup = (group) => {\\n+                if (!group.removable) return;\\n+\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    showConfirmationDialog({\\n+                        title: gettext(\"Are you sure?\"),\\n+                        message: _.sprintf(\\n+                            gettext(\\'You are about to delete the group \"%(name)s\".\\'),\\n+                            {name: _.escape(group.name)}\\n+                        ),\\n+                        proceed: gettext(\"Delete\"),\\n+                        onproceed: () => {\\n+                            self.removeGroup(group);\\n+                        }\\n+                    });\\n+                });\\n+            };\\n+\\n             //~~ Framework\\n \\n             self.onStartup = function () {\\n@@ -665,6 +777,11 @@ $(function () {\\n                 if (!group) {\\n                     throw OctoPrint.InvalidArgumentError(\"group must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.groups.add(group).done(self.fromResponse);\\n             };\\n@@ -673,31 +790,27 @@ $(function () {\\n                 if (!group) {\\n                     throw OctoPrint.InvalidArgumentError(\"group must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n-                if (!group.removable) return;\\n-\\n-                showConfirmationDialog({\\n-                    title: gettext(\"Are you sure?\"),\\n-                    message: _.sprintf(\\n-                        gettext(\\'You are about to delete the group \"%(name)s\".\\'),\\n-                        {name: _.escape(group.name)}\\n-                    ),\\n-                    proceed: gettext(\"Delete\"),\\n-                    onproceed: function () {\\n-                        OctoPrint.access.groups\\n-                            .delete(group.key)\\n-                            .done(function (response) {\\n-                                self.fromResponse(response);\\n-                                access.users.requestData();\\n-                            });\\n-                    }\\n+                OctoPrint.access.groups.delete(group.key).done((response) => {\\n+                    self.fromResponse(response);\\n+                    access.users.requestData();\\n                 });\\n             };\\n \\n             self.updateGroup = function (group) {\\n                 if (!group) {\\n                     throw OctoPrint.InvalidArgumentError(\"group must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.groups.update(group).done(self.fromResponse);\\n             };', '@@ -6,6 +6,9 @@ $(function () {\\n \\n         self.onStartup = function () {\\n             self.dialog = $(\"#plugin_appkeys_keygenerated\");\\n+            self.dialog.on(\"hidden\", () => {\\n+                self.resetDialog();\\n+            });\\n         };\\n \\n         self.showDialog = function (title, data) {\\n@@ -40,6 +43,17 @@ $(function () {\\n \\n             self.dialog.modal(\"show\");\\n         };\\n+\\n+        self.resetDialog = () => {\\n+            if (self.dialog === undefined) return;\\n+\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_title\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_user\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_app\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_key_text\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_key_copy\").off();\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_key_qrcode\").empty();\\n+        };\\n     }\\n \\n     function UserAppKeysViewModel(parameters) {\\n@@ -69,7 +83,7 @@ $(function () {\\n         self.editorApp = ko.observable();\\n \\n         self.requestData = function () {\\n-            OctoPrint.plugins.appkeys.getKeys().done(self.fromResponse);\\n+            return OctoPrint.plugins.appkeys.getKeys().done(self.fromResponse);\\n         };\\n \\n         self.fromResponse = function (response) {\\n@@ -81,26 +95,42 @@ $(function () {\\n         };\\n \\n         self.generateKey = function () {\\n-            return OctoPrint.plugins.appkeys\\n-                .generateKey(self.editorApp())\\n-                .done(self.requestData)\\n-                .done(function () {\\n-                    self.editorApp(\"\");\\n-                });\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys\\n+                    .generateKey(self.editorApp())\\n+                    .done(self.requestData)\\n+                    .done(function () {\\n+                        self.editorApp(\"\");\\n+                    });\\n+            });\\n         };\\n \\n-        self.revokeKey = function (key) {\\n-            var perform = function () {\\n-                OctoPrint.plugins.appkeys.revokeKey(key).done(self.requestData);\\n-            };\\n+        self.revokeKey = (data) => {\\n+            const app = data.app_id;\\n+\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                showConfirmationDialog(\\n+                    _.sprintf(\\n+                        gettext(\\n+                            \"You are about to revoke the application key for %(app)s.\"\\n+                        ),\\n+                        {app: _.escape(app)}\\n+                    ),\\n+                    () => {\\n+                        OctoPrint.plugins.appkeys\\n+                            .revokeKeyForApp(app)\\n+                            .done(self.requestData);\\n+                    }\\n+                );\\n+            });\\n+        };\\n \\n-            showConfirmationDialog(\\n-                _.sprintf(\\n-                    gettext(\\'You are about to revoke the application key \"%(key)s\".\\'),\\n-                    {key: _.escape(key)}\\n-                ),\\n-                perform\\n-            );\\n+        self.showKeyDetails = (data) => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys.getKey(data.app_id).done((response) => {\\n+                    self.dialog.showDialog(gettext(\"Details\"), response.key);\\n+                });\\n+            });\\n         };\\n \\n         self.allowApp = function (token) {\\n@@ -135,8 +165,10 @@ $(function () {\\n                         {\\n                             text: gettext(\"Allow\"),\\n                             click: function (notice) {\\n-                                self.allowApp(token);\\n-                                notice.remove();\\n+                                self.loginState.reauthenticateIfNecessary(() => {\\n+                                    self.allowApp(token);\\n+                                    notice.remove();\\n+                                });\\n                             }\\n                         },\\n                         {\\n@@ -245,7 +277,7 @@ $(function () {\\n         };\\n \\n         self.requestData = function () {\\n-            OctoPrint.plugins.appkeys.getAllKeys().done(self.fromResponse);\\n+            return OctoPrint.plugins.appkeys.getAllKeys().done(self.fromResponse);\\n         };\\n \\n         self.fromResponse = function (response) {\\n@@ -267,46 +299,65 @@ $(function () {\\n             self.apps(apps);\\n         };\\n \\n+        self.showKeyDetails = (data) => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys\\n+                    .getKey(data.app_id, data.user_id)\\n+                    .done((response) => {\\n+                        self.dialog.showDialog(gettext(\"Details\"), response.key);\\n+                    });\\n+            });\\n+        };\\n+\\n         self.generateKey = function () {\\n-            return OctoPrint.plugins.appkeys\\n-                .generateKeyForUser(self.editorUser(), self.editorApp())\\n-                .done(self.requestData)\\n-                .done(function () {\\n-                    self.editorUser(self.loginState.username());\\n-                    self.editorApp(\"\");\\n-                })\\n-                .done(function (data) {\\n-                    self.dialog.showDialog(gettext(\"New key generated!\"), data);\\n-                });\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys\\n+                    .generateKeyForUser(self.editorUser(), self.editorApp())\\n+                    .done(self.requestData)\\n+                    .done(function () {\\n+                        self.editorUser(self.loginState.username());\\n+                        self.editorApp(\"\");\\n+                    })\\n+                    .done(function (data) {\\n+                        self.dialog.showDialog(gettext(\"New key generated!\"), data);\\n+                    });\\n+            });\\n         };\\n \\n-        self.revokeKey = function (key) {\\n-            var perform = function () {\\n-                OctoPrint.plugins.appkeys.revokeKey(key).done(self.requestData);\\n-            };\\n+        self.revokeKey = function (data) {\\n+            const app = data.app_id;\\n+            const user = data.user_id;\\n \\n             showConfirmationDialog(\\n                 _.sprintf(\\n-                    gettext(\\'You are about to revoke the application key \"%(key)s\".\\'),\\n-                    {key: _.escape(key)}\\n+                    gettext(\\n+                        \"You are about to revoke the application key for %(app)s for user %(user)s.\"\\n+                    ),\\n+                    {app: _.escape(app), user: _.escape(user)}\\n                 ),\\n-                perform\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        OctoPrint.plugins.appkeys\\n+                            .revokeKeyForApp(app, user)\\n+                            .done(self.requestData);\\n+                    });\\n+                }\\n             );\\n         };\\n \\n         self.revokeMarked = function () {\\n-            var perform = function () {\\n-                self._bulkRevoke(self.markedForDeletion()).done(function () {\\n-                    self.markedForDeletion.removeAll();\\n-                });\\n-            };\\n-\\n             showConfirmationDialog(\\n                 _.sprintf(\\n                     gettext(\"You are about to revoke %(count)d application keys.\"),\\n                     {count: self.markedForDeletion().length}\\n                 ),\\n-                perform\\n+                () => {\\n+                    self.loginState.forceReauthentication(() => {\\n+                        self._bulkRevoke(self.markedForDeletion()).done(() => {\\n+                            self.markedForDeletion.removeAll();\\n+                        });\\n+                    });\\n+                }\\n             );\\n         };\\n \\n@@ -315,13 +366,22 @@ $(function () {\\n                 _.uniq(\\n                     self\\n                         .markedForDeletion()\\n-                        .concat(_.map(self.keys.paginatedItems(), \"api_key\"))\\n+                        .concat(\\n+                            _.map(\\n+                                self.keys.paginatedItems(),\\n+                                (item) => `${item.user_id}:${item.app_id}`\\n+                            )\\n+                        )\\n                 )\\n             );\\n         };\\n \\n         self.markAllForDeletion = function () {\\n-            self.markedForDeletion(_.uniq(_.map(self.keys.allItems, \"api_key\")));\\n+            self.markedForDeletion(\\n+                _.uniq(\\n+                    _.map(self.keys.allItems, (item) => `${item.user_id}:${item.app_id}`)\\n+                )\\n+            );\\n         };\\n \\n         self.markAllByUserForDeletion = function (user) {\\n@@ -341,7 +401,12 @@ $(function () {\\n                 _.uniq(\\n                     self\\n                         .markedForDeletion()\\n-                        .concat(_.map(_.filter(self.keys.allItems, filter), \"api_key\"))\\n+                        .concat(\\n+                            _.map(\\n+                                _.filter(self.keys.allItems, filter),\\n+                                (item) => `${item.user_id}:${item.app_id}`\\n+                            )\\n+                        )\\n                 )\\n             );\\n         };\\n@@ -351,31 +416,49 @@ $(function () {\\n         };\\n \\n         self._bulkRevoke = function (keys) {\\n+            /*\\n+             * TODO: This still has a risk of running into reauthentication for REALLY large numbers of keys\\n+             * whose bulk removal takes longer than the reauthentication timeout.\\n+             */\\n+\\n             var title, message, handler;\\n \\n             title = gettext(\"Revoking application keys\");\\n             message = _.sprintf(gettext(\"Revoking %(count)d application keys...\"), {\\n                 count: keys.length\\n             });\\n-            handler = function (key) {\\n+            handler = function (id) {\\n+                const [user, app] = rsplit(id, \":\", 1);\\n                 return OctoPrint.plugins.appkeys\\n-                    .revokeKey(key)\\n+                    .revokeKeyForApp(app, user)\\n                     .done(function () {\\n                         deferred.notify(\\n-                            _.sprintf(gettext(\"Revoked %(key)s...\"), {\\n-                                key: _.escape(key)\\n+                            _.sprintf(gettext(\"Revoked %(app)s for %(user)s...\"), {\\n+                                app: _.escape(app),\\n+                                user: _.escape(user)\\n                             }),\\n                             true\\n                         );\\n                     })\\n                     .fail(function (jqXHR) {\\n                         var short = _.sprintf(\\n-                            gettext(\"Revocation of %(key)s failed, continuing...\"),\\n-                            {key: _.escape(key)}\\n+                            gettext(\\n+                                \"Revocation of %(app)s for user %(user)s failed, continuing...\"\\n+                            ),\\n+                            {\\n+                                app: _.escape(app),\\n+                                user: _.escape(user)\\n+                            }\\n                         );\\n                         var long = _.sprintf(\\n-                            gettext(\"Deletion of %(key)s failed: %(error)s\"),\\n-                            {key: _.escape(key), error: _.escape(jqXHR.responseText)}\\n+                            gettext(\\n+                                \"Deletion of %(app)s for user %(user)s failed: %(error)s\"\\n+                            ),\\n+                            {\\n+                                app: _.escape(app),\\n+                                user: _.escape(user),\\n+                                error: _.escape(jqXHR.responseText)\\n+                            }\\n                         );\\n                         deferred.notify(short, long, false);\\n                     });', '@@ -14,8 +14,15 @@\\n     };\\n \\n     OctoPrintAppKeysClient.prototype.getAllKeys = function (opts) {\\n+        return this.base.get(this.base.getSimpleApiUrl(\"appkeys\") + \"?all=true\", opts);\\n+    };\\n+\\n+    OctoPrintAppKeysClient.prototype.getKey = function (app, user, opts) {\\n         return this.base.get(\\n-            OctoPrintClient.prototype.getSimpleApiUrl(\"appkeys\") + \"?all=true\",\\n+            this.base.getSimpleApiUrl(\"appkeys\") +\\n+                \"?app=\" +\\n+                encodeURIComponent(app) +\\n+                (user ? \"&user=\" + encodeURIComponent(user) : \"\"),\\n             opts\\n         );\\n     };\\n@@ -34,9 +41,20 @@\\n     };\\n \\n     OctoPrintAppKeysClient.prototype.revokeKey = function (key, opts) {\\n+        console.log(\\n+            \"revokeKey should be considered deprecated, use revokeKeyForApp instead\"\\n+        );\\n         return this.base.simpleApiCommand(\"appkeys\", \"revoke\", {key: key}, opts);\\n     };\\n \\n+    OctoPrintAppKeysClient.prototype.revokeKeyForApp = function (app, user, opts) {\\n+        const params = {app: app};\\n+        if (user) {\\n+            params.user = user;\\n+        }\\n+        return this.base.simpleApiCommand(\"appkeys\", \"revoke\", params, opts);\\n+    };\\n+\\n     OctoPrintAppKeysClient.prototype.decide = function (token, decision, opts) {\\n         return this.base.postJson(\\n             this.base.getBlueprintUrl(\"appkeys\") + \"decision/\" + token,'], 'file': ['src/octoprint/static/js/app/client/settings.js', 'src/octoprint/static/js/app/viewmodels/settings.js', 'src/octoprint/plugins/appkeys/templates/appkeys_settings.jinja2', 'src/octoprint/plugins/pluginmanager/static/js/pluginmanager.js', 'src/octoprint/server/api/access.py', 'src/octoprint/static/js/app/viewmodels/loginstate.js', 'src/octoprint/templates/initscript.jinja2', 'src/octoprint/server/views.py', 'src/octoprint/templates/snippets/settings/accesscontrol/groups.jinja2', 'src/octoprint/templates/index.jinja2', 'src/octoprint/static/js/app/helpers.js', 'src/octoprint/templates/dialogs/settings/api.jinja2', 'src/octoprint/server/api/settings.py', 'src/octoprint/plugins/appkeys/templates/appkeys.jinja2', 'src/octoprint/server/util/flask.py', 'src/octoprint/templates/login.jinja2', 'src/octoprint/templates/snippets/settings/accesscontrol/users.jinja2', 'src/octoprint/templates/dialogs/reauthenticate.jinja2', 'src/octoprint/schema/config/access_control.py', 'src/octoprint/plugins/appkeys/templates/appkeys_usersettings.jinja2', 'src/octoprint/static/js/app/viewmodels/access.js', 'src/octoprint/plugins/appkeys/static/js/appkeys.js', 'src/octoprint/plugins/appkeys/static/clientjs/appkeys.js'], 'language': ['JavaScript/TypeScript', 'JavaScript/TypeScript', 'Jinja2', 'JavaScript/TypeScript', 'Python', 'JavaScript/TypeScript', 'Jinja2', 'Python', 'Jinja2', 'Jinja2', 'JavaScript/TypeScript', 'Jinja2', 'Python', 'Jinja2', 'Python', 'Jinja2', 'Jinja2', 'Jinja2', 'Python', 'Jinja2', 'JavaScript/TypeScript', 'JavaScript/TypeScript', 'JavaScript/TypeScript'], 'temp_id': [UUID('29b2fc9d-abcf-4f4f-a550-490bf2ae221c'), UUID('388c03e2-e3ec-437e-9d42-0a989f2ba91b'), UUID('685ea5a4-b066-41d3-972d-c21afd44bf62'), UUID('3f137ddf-92d8-4213-8995-79d2485f5fa9'), UUID('c5e332ce-7791-455b-8dd1-985468e65668'), UUID('a5b6a49d-3cf1-4da4-b246-9aacd669b4e4'), UUID('3e97ced5-b09a-4aba-ba6a-8c62155b76ed'), UUID('173b0460-7308-4e07-ad1f-0da6eec20955'), UUID('ec66400c-6f91-452d-9e5a-b4755062028c'), UUID('0276a155-dd99-4203-b0c2-a06104f0e53e'), UUID('ca4c06e4-30f2-47cd-98be-e2e55372335e'), UUID('3747fd5f-8314-4c4a-a02f-7b34fd16b16a'), UUID('a8f689d2-2622-4580-9d1d-29360045dccb'), UUID('cb3945f5-bc43-43d8-a1ee-f6f154330371'), UUID('84ec7cb6-da1d-4e59-bd2a-b3b54c3d568a'), UUID('047a215b-bbd2-468c-8e95-bd08f14ecf3d'), UUID('132ef3a8-7d8d-4330-8b31-21b525938d8c'), UUID('39350f1f-764e-4e43-b103-c2b2ac7d8815'), UUID('145f2be2-a455-463c-a041-42013b62b556'), UUID('473f764f-3069-42e6-8389-a291dcc52072'), UUID('c7633b29-b3b4-4296-8c1f-9f7b75de59d2'), UUID('4b322418-4adb-48fd-a148-c642a9723b5e'), UUID('5e8e14a0-0487-47cf-8a38-82330087b730')]}\n",
      "ERROR:root:Error in {'repo': 'OctoPrint/OctoPrint', 'vulnerability_id': '2024-23637', 'commit': '1729d167b4ae4a5835bbc7211b92c6828b1c4125', 'commit_source': 'github', 'cwe_id': ['CWE-287', 'CWE-287'], 'patch': ['@@ -48,6 +48,10 @@\\n         return this.base.postJson(apiKeyUrl, opts);\\n     };\\n \\n+    OctoPrintSettingsClient.prototype.deleteApiKey = function (opts) {\\n+        return this.base.delete(apiKeyUrl, opts);\\n+    };\\n+\\n     OctoPrintClient.registerComponent(\"settings\", OctoPrintSettingsClient);\\n     return OctoPrintSettingsClient;\\n });', '@@ -127,6 +127,8 @@ $(function () {\\n         self.api_key = ko.observable(undefined);\\n         self.api_allowCrossOrigin = ko.observable(undefined);\\n \\n+        self.apiKeyVisible = ko.observable(false);\\n+\\n         self.appearance_name = ko.observable(undefined);\\n         self.appearance_color = ko.observable(undefined);\\n         self.appearance_colorTransparent = ko.observable();\\n@@ -703,10 +705,36 @@ $(function () {\\n             );\\n         };\\n \\n+        self.deleteApiKey = () => {\\n+            if (!CONFIG_ACCESS_CONTROL) return;\\n+            if (!self.api_key()) return;\\n+\\n+            showConfirmationDialog(\\n+                gettext(\\n+                    \"This will delete the API Key. It will cease to to function immediately.\"\\n+                ),\\n+                function () {\\n+                    OctoPrint.settings.deleteApiKey().done(() => {\\n+                        self.api_key(undefined);\\n+                    });\\n+                }\\n+            );\\n+        };\\n+\\n         self.copyApiKey = function () {\\n             copyToClipboard(self.api_key());\\n         };\\n \\n+        self.revealingApiKey = ko.observable(false);\\n+        self.revealApiKey = () => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                self.revealingApiKey(true);\\n+                self.requestData().always(() => {\\n+                    self.revealingApiKey(false);\\n+                });\\n+            });\\n+        };\\n+\\n         self.showTranslationManager = function () {\\n             self.translationManagerDialog.modal();\\n             return false;\\n@@ -1067,6 +1095,7 @@ $(function () {\\n             return data;\\n         };\\n \\n+        self.reauthenticationTimeout = undefined;\\n         self.fromResponse = function (response, local) {\\n             // server side changes to set\\n             var serverChangedData;\\n@@ -1254,6 +1283,15 @@ $(function () {\\n             mapToObservables(serverChangedData, specialMappings, clientChangedData);\\n \\n             firstRequest.resolve();\\n+\\n+            // special delivery for the API key flag\\n+            self.apiKeyVisible(self.loginState.checkCredentialsSeen());\\n+            if (self.apiKeyVisible()) {\\n+                self.reauthenticationTimeout =\\n+                    self.loginState.afterReauthenticationTimeout(() => {\\n+                        self.requestData();\\n+                    }, self.reauthenticationTimeout);\\n+            }\\n         };\\n \\n         self.cancelData = function () {', '@@ -40,10 +40,10 @@\\n         </thead>\\n         <tbody data-bind=\"foreach: keys.paginatedItems\">\\n         <tr>\\n-            <td class=\"settings_plugin_appkeys_checkbox\"><input type=\"checkbox\" data-bind=\"value: api_key, checked: $root.markedForDeletion\"></td>\\n+            <td class=\"settings_plugin_appkeys_checkbox\"><input type=\"checkbox\" data-bind=\"value: user_id + \\':\\' + app_id, checked: $root.markedForDeletion\"></td>\\n             <td class=\"settings_plugin_appkeys_user\"><span data-bind=\"text: user_id\"></span></td>\\n-            <td class=\"settings_plugin_appkeys_app\"><span data-bind=\"text: app_id\"></span><br /><small class=\"muted\">{{ _(\\'API Key\\') }}: <span data-bind=\"text: api_key\"></span> <a href=\"javascript:void(0)\" title=\"{{ _(\\'Copy API Key to clipboard\\')|edq }}\" data-bind=\"click: function() { copyToClipboard(api_key) }\"><i class=\"fas fa-copy\"></i></a></small></td>\\n-            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"fa fa-trash-o\" data-bind=\"click: function() { $parent.revokeKey($data.api_key, $data.user_id) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.dialog.showDialog(gettext(\\'Details\\'), $data) }\"></a></td>\\n+            <td class=\"settings_plugin_appkeys_app\"><span data-bind=\"text: app_id\"></span></td>\\n+            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"fa fa-trash-o\" data-bind=\"click: function() { $parent.revokeKey($data) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.showKeyDetails($data) }\"></a></td>\\n         </tr>\\n         </tbody>\\n     </table>\\n@@ -71,12 +71,12 @@\\n     <div class=\"control-group\">\\n         <label class=\"control-label\">{{ _(\\'Application identifier\\') }}</label>\\n         <div class=\"controls\">\\n-            <input type=\"text\" data-bind=\"value: editorApp\">\\n+            <input type=\"text\" data-bind=\"value: editorApp, valueUpdate: \\'input\\'\">\\n         </div>\\n     </div>\\n     <div class=\"control-group\">\\n         <div class=\"controls\">\\n-            <button class=\"btn btn-primary\" data-bind=\"click: generateKey\">{{ _(\\'Generate\\') }}</button>\\n+            <button class=\"btn btn-primary\" data-bind=\"click: generateKey, enabled: editorApp, css: {disabled: !editorApp()}\">{{ _(\\'Generate\\') }}</button>\\n         </div>\\n     </div>\\n </form>', '@@ -482,18 +482,20 @@ $(function () {\\n                 self.uploadButton.unbind(\"click\");\\n                 self.uploadButton.bind(\"click\", function () {\\n                     const proceed = () => {\\n-                        self._markWorking(\\n-                            isJsonFile\\n-                                ? gettext(\"Installing plugins...\")\\n-                                : gettext(\"Installing plugin...\"),\\n-                            isJsonFile\\n-                                ? gettext(\"Installing plugins from uploaded file...\")\\n-                                : gettext(\"Installing plugin from uploaded file...\")\\n-                        );\\n-                        data.formData = {\\n-                            dependency_links: self.followDependencyLinks()\\n-                        };\\n-                        data.submit();\\n+                        self.loginState.reauthenticateIfNecessary(() => {\\n+                            self._markWorking(\\n+                                isJsonFile\\n+                                    ? gettext(\"Installing plugins...\")\\n+                                    : gettext(\"Installing plugin...\"),\\n+                                isJsonFile\\n+                                    ? gettext(\"Installing plugins from uploaded file...\")\\n+                                    : gettext(\"Installing plugin from uploaded file...\")\\n+                            );\\n+                            data.formData = {\\n+                                dependency_links: self.followDependencyLinks()\\n+                            };\\n+                            data.submit();\\n+                        });\\n                     };\\n \\n                     if (isJsonFile) {\\n@@ -1160,12 +1162,14 @@ $(function () {\\n             self.selectedPlugins([]);\\n         };\\n \\n-        self.showRepository = function () {\\n-            self.repositoryDialog.modal({\\n-                minHeight: function () {\\n-                    return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                },\\n-                show: true\\n+        self.showRepository = () => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                self.repositoryDialog.modal({\\n+                    minHeight: function () {\\n+                        return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n+                    },\\n+                    show: true\\n+                });\\n             });\\n         };\\n \\n@@ -1174,14 +1178,6 @@ $(function () {\\n         };\\n \\n         self.installFromRepository = function (data) {\\n-            if (\\n-                !self.loginState.hasPermission(\\n-                    self.access.permissions.PLUGIN_PLUGINMANAGER_INSTALL\\n-                )\\n-            ) {\\n-                return;\\n-            }\\n-\\n             self.installPlugin(\\n                 data.archive,\\n                 data.title,\\n@@ -1235,110 +1231,117 @@ $(function () {\\n                 followDependencyLinks = self.followDependencyLinks();\\n             }\\n \\n-            var workTitle, workText;\\n-            if (!reinstall) {\\n-                workTitle = gettext(\"Installing plugin...\");\\n-                if (name) {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                var workTitle, workText;\\n+                if (!reinstall) {\\n+                    workTitle = gettext(\"Installing plugin...\");\\n+                    if (name) {\\n+                        workText = _.sprintf(\\n+                            gettext(\\'Installing plugin \"%(name)s\" from %(url)s...\\'),\\n+                            {url: _.escape(url), name: _.escape(name)}\\n+                        );\\n+                    } else {\\n+                        workText = _.sprintf(\\n+                            gettext(\"Installing plugin from %(url)s...\"),\\n+                            {\\n+                                url: _.escape(url)\\n+                            }\\n+                        );\\n+                    }\\n+                } else {\\n+                    workTitle = gettext(\"Reinstalling plugin...\");\\n                     workText = _.sprintf(\\n-                        gettext(\\'Installing plugin \"%(name)s\" from %(url)s...\\'),\\n+                        gettext(\\'Reinstalling plugin \"%(name)s\" from %(url)s...\\'),\\n                         {url: _.escape(url), name: _.escape(name)}\\n                     );\\n-                } else {\\n-                    workText = _.sprintf(gettext(\"Installing plugin from %(url)s...\"), {\\n-                        url: _.escape(url)\\n-                    });\\n                 }\\n-            } else {\\n-                workTitle = gettext(\"Reinstalling plugin...\");\\n-                workText = _.sprintf(\\n-                    gettext(\\'Reinstalling plugin \"%(name)s\" from %(url)s...\\'),\\n-                    {url: _.escape(url), name: _.escape(name)}\\n-                );\\n-            }\\n \\n-            if (self.multiInstallRunning()) {\\n-                workTitle =\\n-                    _.sprintf(\"[%(index)d/%(total)d] \", {\\n-                        index:\\n-                            this.multiInstallInitialSize() -\\n-                            self.multiInstallQueue().length,\\n-                        total: this.multiInstallInitialSize()\\n-                    }) + workTitle;\\n-            }\\n+                if (self.multiInstallRunning()) {\\n+                    workTitle =\\n+                        _.sprintf(\"[%(index)d/%(total)d] \", {\\n+                            index:\\n+                                this.multiInstallInitialSize() -\\n+                                self.multiInstallQueue().length,\\n+                            total: this.multiInstallInitialSize()\\n+                        }) + workTitle;\\n+                }\\n \\n-            self._markWorking(workTitle, workText);\\n+                self._markWorking(workTitle, workText);\\n \\n-            var onSuccess = function (response) {\\n-                    self.installUrl(\"\");\\n-                    if (response.hasOwnProperty(\"queued_installs\")) {\\n-                        self.queuedInstalls(response.queued_installs);\\n-                        var text =\\n-                            \\'<div class=\"row-fluid\"><p>\\' +\\n-                            gettext(\"The following plugins are queued to be installed.\") +\\n-                            \"</p><ul><li>\" +\\n-                            _.map(response.queued_installs, function (info) {\\n-                                var plugin = ko.utils.arrayFirst(\\n-                                    self.repositoryplugins.paginatedItems(),\\n-                                    function (item) {\\n-                                        return item.archive === info.url;\\n-                                    }\\n-                                );\\n-                                return plugin.title;\\n-                            }).join(\"</li><li>\") +\\n-                            \"</li></ul></div>\";\\n-                        if (typeof self.installQueuePopup !== \"undefined\") {\\n-                            self.installQueuePopup.update({\\n-                                text: text\\n-                            });\\n-                            if (self.installQueuePopup.state === \"closed\") {\\n-                                self.installQueuePopup.open();\\n+                var onSuccess = function (response) {\\n+                        self.installUrl(\"\");\\n+                        if (response.hasOwnProperty(\"queued_installs\")) {\\n+                            self.queuedInstalls(response.queued_installs);\\n+                            var text =\\n+                                \\'<div class=\"row-fluid\"><p>\\' +\\n+                                gettext(\\n+                                    \"The following plugins are queued to be installed.\"\\n+                                ) +\\n+                                \"</p><ul><li>\" +\\n+                                _.map(response.queued_installs, function (info) {\\n+                                    var plugin = ko.utils.arrayFirst(\\n+                                        self.repositoryplugins.paginatedItems(),\\n+                                        function (item) {\\n+                                            return item.archive === info.url;\\n+                                        }\\n+                                    );\\n+                                    return plugin.title;\\n+                                }).join(\"</li><li>\") +\\n+                                \"</li></ul></div>\";\\n+                            if (typeof self.installQueuePopup !== \"undefined\") {\\n+                                self.installQueuePopup.update({\\n+                                    text: text\\n+                                });\\n+                                if (self.installQueuePopup.state === \"closed\") {\\n+                                    self.installQueuePopup.open();\\n+                                }\\n+                            } else {\\n+                                self.installQueuePopup = new PNotify({\\n+                                    title: gettext(\"Plugin installs queued\"),\\n+                                    text: text,\\n+                                    type: \"notice\"\\n+                                });\\n+                            }\\n+                            if (self.multiInstallQueue().length > 0) {\\n+                                self.performMultiInstallJob();\\n+                            } else {\\n+                                self.multiInstallRunning(false);\\n+                                self.workingDialog.modal(\"hide\");\\n+                                self._markDone();\\n                             }\\n-                        } else {\\n-                            self.installQueuePopup = new PNotify({\\n-                                title: gettext(\"Plugin installs queued\"),\\n-                                text: text,\\n-                                type: \"notice\"\\n-                            });\\n                         }\\n-                        if (self.multiInstallQueue().length > 0) {\\n-                            self.performMultiInstallJob();\\n+                    },\\n+                    onError = function (jqXHR) {\\n+                        if (jqXHR.status === 409) {\\n+                            // there\\'s already a plugin being installed\\n+                            self._markDone(\\n+                                \"There\\'s already another plugin install in progress.\"\\n+                            );\\n                         } else {\\n-                            self.multiInstallRunning(false);\\n-                            self.workingDialog.modal(\"hide\");\\n-                            self._markDone();\\n+                            self._markDone(\\n+                                \"Could not install plugin, unknown error, please consult octoprint.log for details\"\\n+                            );\\n+                            new PNotify({\\n+                                title: gettext(\"Something went wrong\"),\\n+                                text: gettext(\"Please consult octoprint.log for details\"),\\n+                                type: \"error\",\\n+                                hide: false\\n+                            });\\n                         }\\n-                    }\\n-                },\\n-                onError = function (jqXHR) {\\n-                    if (jqXHR.status === 409) {\\n-                        // there\\'s already a plugin being installed\\n-                        self._markDone(\\n-                            \"There\\'s already another plugin install in progress.\"\\n-                        );\\n-                    } else {\\n-                        self._markDone(\\n-                            \"Could not install plugin, unknown error, please consult octoprint.log for details\"\\n-                        );\\n-                        new PNotify({\\n-                            title: gettext(\"Something went wrong\"),\\n-                            text: gettext(\"Please consult octoprint.log for details\"),\\n-                            type: \"error\",\\n-                            hide: false\\n-                        });\\n-                    }\\n-                };\\n+                    };\\n \\n-            if (reinstall) {\\n-                OctoPrint.plugins.pluginmanager\\n-                    .reinstall(reinstall, url, followDependencyLinks, fromRepo)\\n-                    .done(onSuccess)\\n-                    .fail(onError);\\n-            } else {\\n-                OctoPrint.plugins.pluginmanager\\n-                    .install(url, followDependencyLinks, fromRepo)\\n-                    .done(onSuccess)\\n-                    .fail(onError);\\n-            }\\n+                if (reinstall) {\\n+                    OctoPrint.plugins.pluginmanager\\n+                        .reinstall(reinstall, url, followDependencyLinks, fromRepo)\\n+                        .done(onSuccess)\\n+                        .fail(onError);\\n+                } else {\\n+                    OctoPrint.plugins.pluginmanager\\n+                        .install(url, followDependencyLinks, fromRepo)\\n+                        .done(onSuccess)\\n+                        .fail(onError);\\n+                }\\n+            });\\n         };\\n \\n         self.uninstallPlugin = function (data) {', '@@ -10,7 +10,11 @@\\n from octoprint.access.permissions import Permissions\\n from octoprint.server import SUCCESS, groupManager, userManager\\n from octoprint.server.api import api, valid_boolean_trues\\n-from octoprint.server.util.flask import no_firstrun_access\\n+from octoprint.server.util.flask import (\\n+    ensure_credentials_checked_recently,\\n+    no_firstrun_access,\\n+    require_credentials_checked_recently,\\n+)\\n \\n # ~~ permission api\\n \\n@@ -32,6 +36,7 @@ def get_groups():\\n \\n @api.route(\"/access/groups\", methods=[\"POST\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def add_group():\\n     data = request.get_json()\\n@@ -77,6 +82,7 @@ def get_group(key):\\n \\n @api.route(\"/access/groups/<key>\", methods=[\"PUT\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def update_group(key):\\n     data = request.get_json()\\n@@ -107,6 +113,7 @@ def update_group(key):\\n \\n @api.route(\"/access/groups/<key>\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def remove_group(key):\\n     try:\\n@@ -130,6 +137,7 @@ def get_users():\\n \\n @api.route(\"/access/users\", methods=[\"POST\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def add_user():\\n     data = request.get_json()\\n@@ -179,6 +187,7 @@ def get_user(username):\\n \\n @api.route(\"/access/users/<username>\", methods=[\"PUT\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def update_user(username):\\n     user = userManager.find_user(username)\\n@@ -208,8 +217,15 @@ def update_user(username):\\n \\n @api.route(\"/access/users/<username>\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n @Permissions.ADMIN.require(403)\\n def remove_user(username):\\n+    if not userManager.enabled:\\n+        return jsonify(SUCCESS)\\n+\\n+    if current_user.get_name() == username:\\n+        abort(400, description=\"You cannot delete yourself\")\\n+\\n     try:\\n         userManager.remove_user(username)\\n         return get_users()\\n@@ -236,13 +252,20 @@ def change_password_for_user(username):\\n         if \"password\" not in data or not data[\"password\"]:\\n             abort(400, description=\"new password is missing\")\\n \\n-        if not current_user.has_permission(Permissions.ADMIN) or \"current\" in data:\\n+        if current_user.get_name() == username:\\n             if \"current\" not in data or not data[\"current\"]:\\n                 abort(400, description=\"current password is missing\")\\n \\n             if not userManager.check_password(username, data[\"current\"]):\\n                 abort(403, description=\"Invalid current password\")\\n \\n+        elif current_user.has_permission(Permissions.ADMIN):\\n+            ensure_credentials_checked_recently()\\n+\\n+        else:\\n+            # this should never happen\\n+            abort(403, description=\"You are not allowed to change this user\\'s password\")\\n+\\n         try:\\n             userManager.change_user_password(username, data[\"password\"])\\n         except users.UnknownUser:\\n@@ -285,6 +308,10 @@ def change_settings_for_user(username):\\n     ):\\n         abort(403)\\n \\n+    if current_user.get_name() != username:\\n+        # this must be an admin, so we need to ensure credentials were checked\\n+        ensure_credentials_checked_recently()\\n+\\n     data = request.get_json()\\n \\n     try:\\n@@ -305,6 +332,10 @@ def delete_apikey_for_user(username):\\n             or current_user.has_permission(Permissions.ADMIN)\\n         )\\n     ):\\n+        if current_user.get_name() != username:\\n+            # this must be an admin, so we need to ensure credentials were checked\\n+            ensure_credentials_checked_recently()\\n+\\n         try:\\n             userManager.delete_api_key(username)\\n         except users.UnknownUser:\\n@@ -328,6 +359,10 @@ def generate_apikey_for_user(username):\\n             or current_user.has_permission(Permissions.ADMIN)\\n         )\\n     ):\\n+        if current_user.get_name() != username:\\n+            # this must be an admin, so we need to ensure credentials were checked\\n+            ensure_credentials_checked_recently()\\n+\\n         try:\\n             apikey = userManager.generate_api_key(username)\\n         except users.UnknownUser:', '@@ -6,6 +6,10 @@ $(function () {\\n         self.loginPass = ko.observable(\"\");\\n         self.loginRemember = ko.observable(false);\\n \\n+        self.reauthenticateDialog = undefined;\\n+        self.reauthenticatePass = ko.observable(\"\");\\n+        self.reauthenticateFailed = ko.observable(false);\\n+\\n         self.loggedIn = ko.observable(undefined);\\n         self.username = ko.observable(undefined);\\n         self.userneeds = ko.observable(undefined);\\n@@ -17,6 +21,7 @@ $(function () {\\n \\n         self.currentUser = ko.observable(undefined);\\n         self.currentLoginMechanism = ko.observable(undefined);\\n+        self.credentialsSeen = ko.observable(undefined);\\n \\n         self.elementUsernameInput = undefined;\\n         self.elementPasswordInput = undefined;\\n@@ -74,6 +79,86 @@ $(function () {\\n                 .done(self.updateCurrentUserData);\\n         };\\n \\n+        self._reauthenticated = false;\\n+\\n+        self.showReauthenticationDialog = () => {\\n+            const result = $.Deferred();\\n+\\n+            self._reauthenticated = false;\\n+            self.reauthenticateDialog.on(\"shown\", function () {\\n+                $(\"input[type=password]\", self.reauthenticateDialog).focus();\\n+            });\\n+            self.reauthenticateDialog.on(\"hidden\", () => {\\n+                self.reauthenticatePass(\"\");\\n+                self.reauthenticateFailed(false);\\n+                if (self._reauthenticated) {\\n+                    result.resolve();\\n+                } else {\\n+                    result.reject();\\n+                }\\n+            });\\n+            self.reauthenticateDialog.modal(\"show\");\\n+\\n+            return result.promise();\\n+        };\\n+\\n+        self.reauthenticate = () => {\\n+            const user = self.currentUser().name;\\n+            const pass = self.reauthenticatePass();\\n+            return OctoPrint.browser\\n+                .login(user, pass)\\n+                .done((response) => {\\n+                    self.fromResponse(response);\\n+                    self.reauthenticateFailed(false);\\n+                    self._reauthenticated = self.credentialsSeen();\\n+                    $(\"#reauthenticate_dialog\").modal(\"hide\");\\n+                })\\n+                .fail((response) => {\\n+                    self.reauthenticatePass(\"\");\\n+                    self.reauthenticateFailed(true);\\n+                });\\n+        };\\n+\\n+        self.reauthenticateIfNecessary = (callback) => {\\n+            if (!self.checkCredentialsSeen()) {\\n+                self.forceReauthentication(callback);\\n+            } else {\\n+                callback();\\n+            }\\n+        };\\n+\\n+        self.forceReauthentication = (callback) => {\\n+            self.showReauthenticationDialog()\\n+                .done(() => {\\n+                    callback();\\n+                })\\n+                .fail(() => {\\n+                    // Do nothing\\n+                });\\n+        };\\n+\\n+        self.checkCredentialsSeen = () => {\\n+            if (CONFIG_REAUTHENTICATION_TIMEOUT <= 0) return true;\\n+\\n+            const credentialsSeen = self.credentialsSeen();\\n+            if (!credentialsSeen) {\\n+                return false;\\n+            }\\n+\\n+            const now = new Date();\\n+            const seen = new Date(credentialsSeen);\\n+            return now - seen < CONFIG_REAUTHENTICATION_TIMEOUT * 60 * 1000;\\n+        };\\n+\\n+        self.afterReauthenticationTimeout = (callback, timeout) => {\\n+            if (CONFIG_REAUTHENTICATION_TIMEOUT <= 0) return;\\n+            if (timeout) window.clearTimeout(timeout);\\n+            return window.setTimeout(\\n+                callback,\\n+                (CONFIG_REAUTHENTICATION_TIMEOUT * 60 + 10) * 1000\\n+            );\\n+        };\\n+\\n         self.requestData = function () {\\n             return OctoPrint.browser\\n                 .passiveLogin()\\n@@ -91,6 +176,7 @@ $(function () {\\n                 if (response && response.name) {\\n                     self.loggedIn(true);\\n                     self.currentLoginMechanism(response._login_mechanism);\\n+                    self.credentialsSeen(response._credentials_seen);\\n                     self.updateCurrentUserData(response);\\n                     if (!currentLoggedIn || currentLoggedIn === undefined) {\\n                         callViewModels(self.allViewModels, \"onUserLoggedIn\", [response]);\\n@@ -242,6 +328,7 @@ $(function () {\\n                     self.loginUser(\"\");\\n                     self.loginPass(\"\");\\n                     self.loginRemember(false);\\n+                    self.reauthenticatePass(\"\");\\n \\n                     if (history && history.replaceState) {\\n                         history.replaceState(\\n@@ -322,6 +409,8 @@ $(function () {\\n         };\\n \\n         self.onStartup = function () {\\n+            self.reauthenticateDialog = $(\"#reauthenticate_dialog\");\\n+\\n             self.elementUsernameInput = $(\"#login_user\");\\n             self.elementPasswordInput = $(\"#login_pass\");\\n             self.elementLoginButton = $(\"#login_button\");\\n@@ -476,6 +565,7 @@ $(function () {\\n     }\\n \\n     OCTOPRINT_VIEWMODELS.push({\\n-        construct: LoginStateViewModel\\n+        construct: LoginStateViewModel,\\n+        elements: [\"#reauthenticate_dialog\"]\\n     });\\n });', '@@ -15,6 +15,7 @@\\n     var CONFIG_FIRST_RUN = {%  if firstRun -%} true; {% else %} false; {%- endif %}\\n     var CONFIG_TEMPERATURE_GRAPH = {%  if enableTemperatureGraph -%} true; {% else %} false; {%- endif %}\\n     var CONFIG_WIZARD = {% if wizard -%} true; {% else %} false; {%- endif %}\\n+    var CONFIG_REAUTHENTICATION_TIMEOUT = {{ reauthenticationTimeout }};\\n \\n     var SOCKJS_URI = \"{{ url_for(\\'index\\') }}\" + \"sockjs\";\\n     var SOCKJS_DEBUG = CONFIG_DEBUG;', '@@ -41,10 +41,12 @@\\n )\\n from octoprint.server.util import (\\n     has_permissions,\\n+    require_fresh_login_with,\\n     require_login_with,\\n     validate_local_redirect,\\n )\\n from octoprint.server.util.csrf import add_csrf_cookie\\n+from octoprint.server.util.flask import credentials_checked_recently\\n from octoprint.settings import settings\\n from octoprint.util import sv, to_bytes, to_unicode\\n from octoprint.util.version import get_python_version_string\\n@@ -225,9 +227,12 @@ def login():\\n         permissions = [Permissions.STATUS, Permissions.SETTINGS_READ]\\n \\n     user_id = request.args.get(\"user_id\", \"\")\\n+    reauthenticate = request.args.get(\"reauthenticate\", \"false\").lower() == \"true\"\\n \\n-    if (not user_id or current_user.get_id() == user_id) and has_permissions(\\n-        *permissions\\n+    if (\\n+        (not user_id or current_user.get_id() == user_id)\\n+        and has_permissions(*permissions)\\n+        and (not reauthenticate or credentials_checked_recently())\\n     ):\\n         return redirect(redirect_url)\\n \\n@@ -237,6 +242,7 @@ def login():\\n         \"permission_names\": map(lambda x: x.get_name(), permissions),\\n         \"user_id\": user_id,\\n         \"logged_in\": not current_user.is_anonymous,\\n+        \"reauthenticate\": reauthenticate,\\n     }\\n \\n     try:\\n@@ -257,7 +263,7 @@ def login():\\n @app.route(\"/recovery\")\\n @app.route(\"/recovery/\")\\n def recovery():\\n-    response = require_login_with(permissions=[Permissions.ADMIN])\\n+    response = require_fresh_login_with(permissions=[Permissions.ADMIN])\\n     if response:\\n         return response\\n \\n@@ -417,6 +423,9 @@ def wizard_active(templates):\\n     enable_webcam = settings().getBoolean([\"webcam\", \"webcamEnabled\"])\\n     enable_temperature_graph = settings().get([\"feature\", \"temperatureGraph\"])\\n     sockjs_connect_timeout = settings().getInt([\"devel\", \"sockJsConnectTimeout\"])\\n+    reauthentication_timeout = settings().getInt(\\n+        [\"accessControl\", \"defaultReauthenticationTimeout\"]\\n+    )\\n \\n     def default_template_filter(template_type, template_key):\\n         if template_type == \"tab\":\\n@@ -431,6 +440,7 @@ def default_template_filter(template_type, template_key):\\n         enable_webcam,\\n         enable_temperature_graph,\\n         sockjs_connect_timeout,\\n+        reauthentication_timeout,\\n         connectivityChecker.online,\\n         wizard_active(_templates.get(locale)),\\n     ] + sorted(\\n@@ -647,6 +657,7 @@ def default_view():\\n                 \"enableLoadingAnimation\": enable_loading_animation,\\n                 \"enableSdSupport\": enable_sd_support,\\n                 \"sockJsConnectTimeout\": sockjs_connect_timeout * 1000,\\n+                \"reauthenticationTimeout\": reauthentication_timeout,\\n                 \"wizard\": wizard,\\n                 \"online\": connectivityChecker.online,\\n                 \"now\": now,', '@@ -27,7 +27,7 @@\\n             </table>\\n         </td>\\n         <td class=\"settings_groups_actions\">\\n-            <a href=\"#\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update group\\')|edq }}\" data-bind=\"enable: changeable, css: {disabled: !changeable}, click: function() { $root.access.groups.showEditGroupDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"#\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete group\\') }}\" data-bind=\"enable: removable, css: {disabled: !removable}, click: function() { $root.access.groups.removeGroup($data); }\"></a>\\n+            <a href=\"javascript:void()\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update group\\')|edq }}\" data-bind=\"enable: changeable, css: {disabled: !changeable}, click: function() { $root.access.groups.showEditGroupDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void()\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete group\\') }}\" data-bind=\"enable: removable, css: {disabled: !removable}, click: function() { $root.access.groups.confirmRemoveGroup($data); }\"></a>\\n         </td>\\n     </tr>\\n     </tbody>', \"@@ -177,6 +177,7 @@\\n             {% include 'dialogs/temperature.jinja2' %}\\n             {% include 'dialogs/timelapse.jinja2' %}\\n             {% include 'dialogs/firmwareerror.jinja2' %}\\n+            {% include 'dialogs/reauthenticate.jinja2' %}\\n             <!-- End of dialogs -->\\n \\n             <!-- Overlays -->\", '@@ -685,6 +685,14 @@ function formatNumberK(num) {\\n     }\\n }\\n \\n+function rsplit(str, sep, limit) {\\n+    // copy of Python\\'s rsplit in JS\\n+    // adapted from https://stackoverflow.com/a/5202185\\n+    // caution: other than split in JS, this limit does limit the splits, NOT the number of parts\\n+    const parts = str.split(sep);\\n+    return limit ? [parts.slice(0, -limit).join(sep)].concat(parts.slice(-limit)) : parts;\\n+}\\n+\\n function pnotifyAdditionalInfo(inner) {\\n     return (\\n         \\'<div class=\"pnotify_additional_info\">\\' +', '@@ -25,14 +25,18 @@\\n         </div>\\n         <div class=\"control-group\">\\n             <label class=\"control-label\" for=\"settings-apikey\">{{ _(\\'Global API Key\\') }}</label>\\n-            <div class=\"controls\">\\n+            <div class=\"controls\" data-bind=\"visible: apiKeyVisible\">\\n                 <div class=\"input-append input-block-level\">\\n                     <input type=\"text\" readonly=\"readonly\" id=\"settings-apikey\" data-bind=\"value: api_key, attr: {placeholder: \\'{{ _(\\'N/A\\')|esq }}\\'}\">\\n                     <a class=\"btn add-on\" title=\"Copy API Key to clipboard\" data-bind=\"click: copyApiKey, css: {\\'disabled\\': !api_key()}\"><i class=\"fas fa-copy\"></i></a>\\n                     <a class=\"btn add-on\" title=\"Generate new API Key\" data-bind=\"click: generateApiKey\"><i class=\"fas fa-sync\"></i></a>\\n+                    <a class=\"btn btn-danger add-on\" title=\"Delete API Key\" data-bind=\"click: deleteApiKey, css: {\\'disabled\\': !api_key()}\"><i class=\"far fa-trash-alt\"></i></a>\\n                 </div>\\n                 <span class=\"help-block\">{{ _(\\'Please note that changes to the API key are applied immediately, without having to \"Save\" first.\\') }}</span>\\n             </div>\\n+            <div class=\"controls\" data-bind=\"visible: !apiKeyVisible()\">\\n+                <button class=\"btn\" data-bind=\"click: revealApiKey, enabled: !revealingApiKey(), css: {disabled: revealingApiKey}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: revealingApiKey\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n+            </div>\\n         </div>\\n         <div class=\"control-group\" data-bind=\"visible: api_key\">\\n             <label class=\"control-label\">{{ _(\\'QR Code\\') }}</label>', '@@ -13,7 +13,11 @@\\n from octoprint.access.permissions import Permissions\\n from octoprint.server import pluginManager, printer, userManager\\n from octoprint.server.api import NO_CONTENT, api\\n-from octoprint.server.util.flask import no_firstrun_access, with_revalidation_checking\\n+from octoprint.server.util.flask import (\\n+    credentials_checked_recently,\\n+    no_firstrun_access,\\n+    with_revalidation_checking,\\n+)\\n from octoprint.settings import settings, valid_boolean_trues\\n from octoprint.timelapse import configure_timelapse\\n from octoprint.webcams import (\\n@@ -91,6 +95,9 @@ def hash_update(value):\\n     # and likewise if the role of the user changes\\n     hash_update(repr(roles))\\n \\n+    # of if the user reauthenticates\\n+    hash_update(repr(credentials_checked_recently()))\\n+\\n     return hash.hexdigest()\\n \\n \\n@@ -118,7 +125,9 @@ def getSettings():\\n \\n     data = {\\n         \"api\": {\\n-            \"key\": s.get([\"api\", \"key\"]) if Permissions.ADMIN.can() else None,\\n+            \"key\": s.get([\"api\", \"key\"])\\n+            if Permissions.ADMIN.can() and credentials_checked_recently()\\n+            else None,\\n             \"allowCrossOrigin\": s.get([\"api\", \"allowCrossOrigin\"]),\\n         },\\n         \"appearance\": {\\n@@ -458,15 +467,15 @@ def setSettings():\\n \\n @api.route(\"/settings/apikey\", methods=[\"POST\"])\\n @no_firstrun_access\\n-@Permissions.SETTINGS.require(403)\\n+@Permissions.ADMIN.require(403)\\n def generateApiKey():\\n     apikey = settings().generateApiKey()\\n     return jsonify(apikey=apikey)\\n \\n \\n @api.route(\"/settings/apikey\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n-@Permissions.SETTINGS.require(403)\\n+@Permissions.ADMIN.require(403)\\n def deleteApiKey():\\n     settings().deleteApiKey()\\n     return NO_CONTENT', '@@ -20,7 +20,7 @@\\n             <div class=\"control-group\">\\n                 <label class=\"control-label\">{{ _(\\'API Key\\') }}</label>\\n                 <div class=\"controls\">\\n-                    <strong id=\"plugin_appkeys_keygenerated_key_text\" class=\"control-text\"></strong>\\n+                    <strong class=\"control-text\"><code id=\"plugin_appkeys_keygenerated_key_text\"></code></strong>\\n                     <a href=\"javascript:void(0)\" id=\"plugin_appkeys_keygenerated_key_copy\" title=\"{{ _(\\'Copy API Key to clipboard\\')|edq }}\" class=\"control-text\"><i class=\"fas fa-copy\"></i></a>\\n                 </div>\\n             </div>', '@@ -11,7 +11,7 @@\\n import os\\n import threading\\n import time\\n-from datetime import datetime, timedelta\\n+from datetime import datetime, timedelta, timezone\\n from typing import Any, Union\\n \\n import flask\\n@@ -732,6 +732,7 @@ def determine_user(u):\\n                             f\"Logging in user {autologin_as} from {remote_address} via autologin\"\\n                         )\\n                         flask.session[\"login_mechanism\"] = \"autologin\"\\n+                        flask.session[\"credentials_seen\"] = False\\n                         return autologin_user\\n             except Exception:\\n                 logger.exception(\\n@@ -753,9 +754,23 @@ def determine_user(u):\\n     )\\n     if flask.session.get(\"login_mechanism\") is not None:\\n         response[\"_login_mechanism\"] = flask.session.get(\"login_mechanism\")\\n+    response[\"_credentials_seen\"] = to_api_credentials_seen(\\n+        flask.session.get(\"credentials_seen\", False)\\n+    )\\n     return flask.jsonify(response)\\n \\n \\n+def to_api_credentials_seen(credentials_seen):\\n+    if not credentials_seen:\\n+        return False\\n+\\n+    return (\\n+        datetime.fromtimestamp(credentials_seen, tz=timezone.utc)\\n+        .replace(microsecond=0)\\n+        .isoformat()\\n+    )\\n+\\n+\\n # ~~ rate limiting helper\\n \\n \\n@@ -1636,6 +1651,47 @@ def decorated_view(*args, **kwargs):\\n     return decorated_view\\n \\n \\n+def credentials_checked_recently():\\n+    minutes = settings().getInt([\"accessControl\", \"defaultReauthenticationTimeout\"])\\n+    if not minutes:\\n+        return True\\n+\\n+    credentials_seen = flask.session.get(\"credentials_seen\")\\n+    now = datetime.now()\\n+\\n+    try:\\n+        if credentials_seen and datetime.fromtimestamp(\\n+            credentials_seen\\n+        ) > now - timedelta(minutes=minutes):\\n+            # credentials seen less than the set minutes ago, proceed\\n+            return True\\n+    except Exception:\\n+        logging.getLogger(__name__).exception(\"Error while checking for seen credentials\")\\n+        pass\\n+\\n+    return False\\n+\\n+\\n+def ensure_credentials_checked_recently():\\n+    if not credentials_checked_recently():\\n+        flask.abort(403, description=\"Please reauthenticate with your credentials\")\\n+\\n+\\n+def require_credentials_checked_recently(func):\\n+    \"\"\"\\n+    If you decorate a view with this, it will ensure that only users who entered their password\\n+    recently in this login session are allowed to proceed. Otherwise it will cause a HTTP 403 status code\\n+    to be returned by the decorated resource.\\n+    \"\"\"\\n+\\n+    @functools.wraps(func)\\n+    def decorated_view(*args, **kwargs):\\n+        ensure_credentials_checked_recently()\\n+        return func(*args, **kwargs)\\n+\\n+    return decorated_view\\n+\\n+\\n def get_remote_address(request):\\n     forwardedFor = request.headers.get(\"X-Forwarded-For\", None)\\n     if forwardedFor is not None:', '@@ -63,24 +63,34 @@\\n \\n     <div id=\"login\" class=\"container\">\\n         <form class=\"form-signin\">\\n-            <h2 class=\"form-signin-heading\" data-test-id=\"login-title\">{{ _(\\'Please log in\\') }}</h2>\\n+            <h2 class=\"form-signin-heading\" data-test-id=\"login-title\">\\n+                {% if reauthenticate %}\\n+                    {{ _(\\'Please reauthenticate\\') }}\\n+                {% else %}\\n+                    {{ _(\\'Please log in\\') }}\\n+                {% endif %}\\n+            </h2>\\n \\n             <div id=\"login-error-credentials\" class=\"alert alert-error\" data-test-id=\"login-error\">{{ _(\\'Incorrect username or password. Hint: Both are case sensitive!\\') }}</div>\\n             <div id=\"login-error-rate\" class=\"alert alert-error\" data-test-id=\"login-error-rate\">{{ _(\\'You have made too many failed login attempts. Please try again later.\\') }}</div>\\n             <div id=\"login-offline\" class=\"alert alert-error\">{{ _(\\'Server is currently offline.\\') }} <a id=\"login-reconnect\" href=\"javascript:void(0)\">{{ _(\\'Reconnect...\\') }}</a></div>\\n \\n-            {% if user_id %}<p>\\n+            {% if reauthenticate %}<p>\\n+                {{ _(\\'You need to reauthenticate before you can continue. Please enter your password below.\\') }}\\n+            </p>{% elif user_id %}<p>\\n                 {{ _(\\'The following account is required:\\') }} {{ user_id|e }}\\n             </p>{% elif logged_in %}<p>\\n                 {{ _(\\'An account with the following permissions is required:\\') }} {{ permission_names|join(\", \") }}\\n             </p>{% endif %}\\n \\n             <input type=\"text\" id=\"login-user\" data-test-id=\"login-username\" class=\"input-block-level\" placeholder=\"{{ _(\\'Username\\')|edq }}\" {% if user_id %}value=\"{{ user_id|edq }}\" disabled{% endif %} autofocus autocapitalize=\"none\">\\n             <input type=\"password\" id=\"login-password\" data-test-id=\"login-password\" class=\"input-block-level\" placeholder=\"{{ _(\\'Password\\')|edq }}\">\\n+            {% if not reauthenticate %}\\n             <span class=\"pull-right\"><small><a href=\"https://faq.octoprint.org/forgotten-password\" id=\"login-forgotpassword\" target=\"_blank\" tabindex=\"-1\">{{ _(\\'Forgot password?\\') }}</a></small></span>\\n             <label class=\"checkbox\">\\n                 <input type=\"checkbox\" id=\"login-remember\" data-test-id=\"login-remember-me\"> {{ _(\\'Remember me\\') }}\\n             </label>\\n+            {% endif %}\\n             <button class=\"btn btn-block btn-large btn-primary\" id=\"login-button\" data-test-id=\"login-submit\" type=\"submit\">{{ _(\\'Log in\\') }}</button>\\n         </form>\\n     </div>', '@@ -28,7 +28,7 @@\\n         </td>\\n         <td class=\"settings_users_active\"><i class=\"far\" data-bind=\"css: { \\'fa-check-square\\': active, \\'fa-square\\': !active }\"></i></td>\\n         <td class=\"settings_users_actions\">\\n-            <a href=\"#\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update User\\')|edq }}\" data-bind=\"click: function() { $root.access.users.showEditUserDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"#\" class=\"fas fa-key\" title=\"{{ _(\\'Change password\\') }}\" data-bind=\"click: function() { $root.access.users.showChangePasswordDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"#\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete user\\') }}\" data-bind=\"click: function() { $root.access.users.removeUser($data); }\"></a>\\n+            <a href=\"javascript:void()\" class=\"fas fa-pencil-alt\" title=\"{{ _(\\'Update User\\')|edq }}\" data-bind=\"click: function() { $root.access.users.showEditUserDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void()\" class=\"fas fa-key\" title=\"{{ _(\\'Change password\\') }}\" data-bind=\"click: function() { $root.access.users.showChangePasswordDialog($data); }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void()\" class=\"far fa-trash-alt\" title=\"{{ _(\\'Delete user\\') }}\" data-bind=\"css: { disabled: !$root.access.users.isDeleteUserEnabled($data)}, enabled: $root.access.users.isDeleteUserEnabled($data), click: function() { if ($root.access.users.isDeleteUserEnabled($data)) { $root.access.users.confirmRemoveUser($data); } }\"></a>\\n         </td>\\n     </tr>\\n     </tbody>\\n@@ -138,6 +138,12 @@\\n     </div>\\n     <div class=\"modal-body\">\\n         <form class=\"form-horizontal\" onsubmit=\"return false;\">\\n+            <div class=\"control-group\" data-bind=\"visible: $root.access.loginState.username() == $root.access.users.editor.name()\">\\n+                <label class=\"control-label\" for=\"settings-usersDialogChangePasswordPasswordC\">{{ _(\\'Current Password\\') }}</label>\\n+                <div class=\"controls\">\\n+                    <input type=\"password\" class=\"input-block-level\" id=\"settings-usersDialogChangePasswordPasswordC\" data-bind=\"value: $root.access.users.editor.currentPassword\" required>\\n+                </div>\\n+            </div>\\n             <div class=\"control-group\">\\n                 <label class=\"control-label\" for=\"settings-usersDialogChangePasswordPassword1\">{{ _(\\'New Password\\') }}</label>\\n                 <div class=\"controls\">', '@@ -0,0 +1,25 @@\\n+<div id=\"reauthenticate_dialog\" class=\"modal hide fade-in\">\\n+    <div class=\"modal-header\">\\n+        <a href=\"javascript:void()\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">&times;</a>\\n+        <h3>{{ _(\\'Please reauthenticate\\') }}</h3>\\n+    </div>\\n+    <div class=\"modal-body\">\\n+        <p>{% trans %}\\n+            You need to reauthenticate to perform this action.\\n+            Please enter your password below.\\n+        {% endtrans %}</p>\\n+        <form class=\"form-horizontal\" data-bind=\"event: {\\'submit\\': reauthenticate }\" onsubmit=\"return false; // this gets overwritten on view model bind\">\\n+            <div class=\"control-group\">\\n+                <label class=\"control-label\">{{ _(\\'Password\\') }}</label>\\n+                <div class=\"controls\">\\n+                    <input type=\"password\" class=\"input-block-level\" data-bind=\"value: reauthenticatePass\" placeholder=\"{{ _(\"Password\")|edq }}\">\\n+                    <span class=\"help-block text-error\" data-bind=\"visible: reauthenticateFailed\">{{ _(\\'Reauthentication failed. Wrong password?\\') }}</span>\\n+                </div>\\n+            </div>\\n+        </form>\\n+    </div>\\n+    <div class=\"modal-footer\">\\n+        <button class=\"btn\" data-dismiss=\"modal\">{{ _(\\'Abort\\') }}</button>\\n+        <button class=\"btn btn-primary\" data-bind=\"click: function() { reauthenticate(); }\">{{ _(\\'Confirm\\') }}</button>\\n+    </div>\\n+</div>', '@@ -50,3 +50,6 @@ class AccessControlConfig(BaseModel):\\n \\n     addRemoteUsers: bool = False\\n     \"\"\"If a remote user is not found, add them. Use this only if all users from the remote system can use OctoPrint.\"\"\"\\n+\\n+    defaultReauthenticationTimeout: int = 5\\n+    \"\"\"Default timeout after which to require reauthentication by a user for dangerous changes, in minutes. Defaults to 5 minutes. Set to 0 to disable reauthentication requirements (SECURITY IMPACT!).\"\"\"', '@@ -13,8 +13,10 @@\\n         </thead>\\n         <tbody data-bind=\"foreach: keys.paginatedItems\">\\n         <tr>\\n-            <td class=\"settings_plugin_appkeys_app\"><span data-bind=\"text: app_id\"></span><br /><small class=\"muted\">{{ _(\\'API Key\\') }}: <span data-bind=\"text: api_key\"></span> <a href=\"javascript:void(0)\" title=\"{{ _(\\'Copy API Key to clipboard\\')|edq }}\" data-bind=\"click: function() { copyToClipboard(api_key) }\"><i class=\"fas fa-copy\"></i></a></small></td>\\n-            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"far fa-trash-alt\" data-bind=\"click: function() { $parent.revokeKey($data.api_key) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.dialog.showDialog(gettext(\\'Details\\'), $data) }\"></a></td>\\n+            <td class=\"settings_plugin_appkeys_app\">\\n+                <span data-bind=\"text: app_id\"></span>\\n+            </td>\\n+            <td class=\"settings_plugin_appkeys_actions\"><a href=\"javascript:void(0)\" title=\"{{ _(\\'Revoke\\')|edq }}\" class=\"far fa-trash-alt\" data-bind=\"click: function() { $parent.revokeKey($data) }\"></a>&nbsp;|&nbsp;<a href=\"javascript:void(0)\" title=\"{{ _(\\'Details\\')|edq }}\" class=\"fas fa-search\" data-bind=\"click: function() { $parent.showKeyDetails($data) }\"></a></td>\\n         </tr>\\n         </tbody>\\n     </table>\\n@@ -37,13 +39,13 @@\\n     <div class=\"control-group\">\\n         <label class=\"control-label\">{{ _(\\'Application identifier\\') }}</label>\\n         <div class=\"controls\">\\n-            <input type=\"text\" data-bind=\"value: editorApp\">\\n+            <input type=\"text\" data-bind=\"value: editorApp, valueUpdate: \\'input\\'\">\\n         </div>\\n     </div>\\n \\n     <div class=\"control-group\">\\n         <div class=\"controls\">\\n-            <button class=\"btn btn-primary\" data-bind=\"click: generateKey\">{{ _(\\'Generate\\') }}</button>\\n+            <button class=\"btn btn-primary\" data-bind=\"click: generateKey, enabled: editorApp, css: {disabled: !editorApp()}\">{{ _(\\'Generate\\') }}</button>\\n         </div>\\n     </div>\\n </form>', '@@ -35,6 +35,14 @@ $(function () {\\n \\n             self.currentUser = ko.observable(self.emptyUser).extend({notify: \"always\"});\\n \\n+            self.isCurrentUser = (user) => {\\n+                return user && user.name && user.name == access.loginState.username();\\n+            };\\n+\\n+            self.isDeleteUserEnabled = (user) => {\\n+                return !self.isCurrentUser(user);\\n+            };\\n+\\n             self.editor = {\\n                 name: ko.observable(undefined),\\n                 groups: ko.observableArray([]),\\n@@ -160,22 +168,28 @@ $(function () {\\n             self.showAddUserDialog = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.currentUser(undefined);\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.currentUser(undefined);\\n \\n-                $(\\'ul.nav-pills a[data-toggle=\"tab\"]:first\\', self.userEditorDialog).tab(\\n-                    \"show\"\\n-                );\\n-                self.userEditorDialog\\n-                    .modal({\\n-                        minHeight: function () {\\n-                            return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                        }\\n-                    })\\n-                    .css({\\n-                        \"margin-left\": function () {\\n-                            return -($(this).width() / 2);\\n-                        }\\n-                    });\\n+                    $(\\n+                        \\'ul.nav-pills a[data-toggle=\"tab\"]:first\\',\\n+                        self.userEditorDialog\\n+                    ).tab(\"show\");\\n+                    self.userEditorDialog\\n+                        .modal({\\n+                            minHeight: function () {\\n+                                return Math.max(\\n+                                    $.fn.modal.defaults.maxHeight() - 80,\\n+                                    250\\n+                                );\\n+                            }\\n+                        })\\n+                        .css({\\n+                            \"margin-left\": function () {\\n+                                return -($(this).width() / 2);\\n+                            }\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmAddUser = function () {\\n@@ -189,10 +203,12 @@ $(function () {\\n                     active: self.editor.active()\\n                 };\\n \\n-                self.addUser(user).done(function () {\\n-                    // close dialog\\n-                    self.currentUser(undefined);\\n-                    self.userEditorDialog.modal(\"hide\");\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.addUser(user).done(function () {\\n+                        // close dialog\\n+                        self.currentUser(undefined);\\n+                        self.userEditorDialog.modal(\"hide\");\\n+                    });\\n                 });\\n             };\\n \\n@@ -222,17 +238,19 @@ $(function () {\\n                         });\\n                 };\\n \\n-                OctoPrint.users\\n-                    .get(user.name)\\n-                    .done(function (data) {\\n-                        process(data);\\n-                    })\\n-                    .fail(function () {\\n-                        log.warn(\\n-                            \"Could not fetch current user data, proceeding with client side data copy\"\\n-                        );\\n-                        process(user);\\n-                    });\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    OctoPrint.users\\n+                        .get(user.name)\\n+                        .done(function (data) {\\n+                            process(data);\\n+                        })\\n+                        .fail(function () {\\n+                            log.warn(\\n+                                \"Could not fetch current user data, proceeding with client side data copy\"\\n+                            );\\n+                            process(user);\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmEditUser = function () {\\n@@ -243,45 +261,97 @@ $(function () {\\n                 user.groups = self.editor.groups();\\n                 user.permissions = self.editor.permissions();\\n \\n-                self.updateUser(user).done(function () {\\n-                    // close dialog\\n-                    self.currentUser(undefined);\\n-                    self.userEditorDialog.modal(\"hide\");\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.updateUser(user).done(function () {\\n+                        // close dialog\\n+                        self.currentUser(undefined);\\n+                        self.userEditorDialog.modal(\"hide\");\\n+                    });\\n+                });\\n+            };\\n+\\n+            self.confirmRemoveUser = (user) => {\\n+                if (!CONFIG_ACCESS_CONTROL) return;\\n+\\n+                if (user.name === access.loginState.username()) {\\n+                    // we do not allow to delete ourselves\\n+                    new PNotify({\\n+                        title: gettext(\"Not possible\"),\\n+                        text: gettext(\"You may not delete your own account.\"),\\n+                        type: \"error\"\\n+                    });\\n+                    return $.Deferred()\\n+                        .reject(\"You may not delete your own account\")\\n+                        .promise();\\n+                }\\n+\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    showConfirmationDialog({\\n+                        title: gettext(\"Are you sure?\"),\\n+                        message: _.sprintf(\\n+                            gettext(\\'You are about to delete the user \"%(name)s\".\\'),\\n+                            {name: _.escape(user.name)}\\n+                        ),\\n+                        proceed: gettext(\"Delete\"),\\n+                        onproceed: () => {\\n+                            self.removeUser(user);\\n+                        }\\n+                    });\\n                 });\\n             };\\n \\n             self.showChangePasswordDialog = function (user) {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.currentUser(user);\\n-                self.changePasswordDialog.modal(\"show\");\\n+                const proceed = () => {\\n+                    self.currentUser(user);\\n+                    self.changePasswordDialog.modal(\"show\");\\n+                };\\n+\\n+                if (self.isCurrentUser(user)) {\\n+                    proceed();\\n+                } else {\\n+                    access.loginState.reauthenticateIfNecessary(proceed);\\n+                }\\n             };\\n \\n             self.confirmChangePassword = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.updatePassword(\\n-                    self.currentUser().name,\\n-                    self.editor.password(),\\n-                    self.editor.currentPassword()\\n-                )\\n-                    .done(function () {\\n-                        // close dialog\\n-                        self.currentUser(undefined);\\n-                        self.changePasswordDialog.modal(\"hide\");\\n-                    })\\n-                    .fail(function (xhr) {\\n-                        if (xhr.status === 403) {\\n-                            self.currentPasswordMismatch(true);\\n-                        }\\n-                    });\\n+                const proceed = () => {\\n+                    self.updatePassword(\\n+                        self.currentUser().name,\\n+                        self.editor.password(),\\n+                        self.editor.currentPassword()\\n+                    )\\n+                        .done(function () {\\n+                            // close dialog\\n+                            self.currentUser(undefined);\\n+                            self.changePasswordDialog.modal(\"hide\");\\n+                        })\\n+                        .fail(function (xhr) {\\n+                            if (xhr.status === 403) {\\n+                                self.currentPasswordMismatch(true);\\n+                            }\\n+                        });\\n+                };\\n+\\n+                if (self.isCurrentUser()) {\\n+                    proceed();\\n+                } else {\\n+                    access.loginState.reauthenticateIfNecessary(proceed);\\n+                }\\n             };\\n \\n             self.confirmGenerateApikey = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.generateApikey(self.currentUser().name).done(function (response) {\\n-                    self._updateApikey(response.apikey);\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.generateApikey(self.currentUser().name).done(function (\\n+                        response\\n+                    ) {\\n+                        self._updateApikey(response.apikey);\\n+                    });\\n                 });\\n             };\\n \\n@@ -297,8 +367,10 @@ $(function () {\\n             self.confirmDeleteApikey = function () {\\n                 if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-                self.deleteApikey(self.currentUser().name).done(function () {\\n-                    self._updateApikey(undefined);\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.deleteApikey(self.currentUser().name).done(function () {\\n+                        self._updateApikey(undefined);\\n+                    });\\n                 });\\n             };\\n \\n@@ -315,10 +387,16 @@ $(function () {\\n                 if (!user) {\\n                     throw OctoPrint.InvalidArgumentError(\"user must be set\");\\n                 }\\n-                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN))\\n+                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN)) {\\n                     return $.Deferred()\\n                         .reject(\"You are not authorized to perform this action\")\\n                         .promise();\\n+                }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.users.add(user).done(self.fromResponse);\\n             };\\n@@ -327,40 +405,29 @@ $(function () {\\n                 if (!user) {\\n                     throw OctoPrint.InvalidArgumentError(\"user must be set\");\\n                 }\\n-                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN))\\n+                if (!access.loginState.hasPermissionKo(access.permissions.ADMIN)) {\\n                     return $.Deferred()\\n                         .reject(\"You are not authorized to perform this action\")\\n                         .promise();\\n-\\n-                if (user.name === access.loginState.username()) {\\n-                    // we do not allow to delete ourselves\\n-                    new PNotify({\\n-                        title: gettext(\"Not possible\"),\\n-                        text: gettext(\"You may not delete your own account.\"),\\n-                        type: \"error\"\\n-                    });\\n+                }\\n+                if (!access.loginState.credentialsSeen()) {\\n                     return $.Deferred()\\n-                        .reject(\"You may not delete your own account\")\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n                         .promise();\\n                 }\\n \\n-                showConfirmationDialog({\\n-                    title: gettext(\"Are you sure?\"),\\n-                    message: _.sprintf(\\n-                        gettext(\\'You are about to delete the user \"%(name)s\".\\'),\\n-                        {name: _.escape(user.name)}\\n-                    ),\\n-                    proceed: gettext(\"Delete\"),\\n-                    onproceed: function () {\\n-                        OctoPrint.access.users.delete(user.name).done(self.fromResponse);\\n-                    }\\n-                });\\n+                return OctoPrint.access.users.delete(user.name).done(self.fromResponse);\\n             };\\n \\n             self.updateUser = function (user) {\\n                 if (!user) {\\n                     throw OctoPrint.InvalidArgumentError(\"user must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.users\\n                     .update(\\n@@ -374,16 +441,31 @@ $(function () {\\n             };\\n \\n             self.updatePassword = function (username, password, current) {\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n                 return OctoPrint.access.users.changePassword(username, password, current);\\n             };\\n \\n             self.generateApikey = function (username) {\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n                 return OctoPrint.access.users.generateApiKey(username).done(function () {\\n                     self.requestData();\\n                 });\\n             };\\n \\n             self.deleteApikey = function (username) {\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n                 return OctoPrint.access.users.resetApiKey(username);\\n             };\\n \\n@@ -576,21 +658,27 @@ $(function () {\\n             };\\n \\n             self.showAddGroupDialog = function () {\\n-                self.currentGroup(undefined);\\n-                $(\\'ul.nav-pills a[data-toggle=\"tab\"]:first\\', self.groupEditorDialog).tab(\\n-                    \"show\"\\n-                );\\n-                self.groupEditorDialog\\n-                    .modal({\\n-                        minHeight: function () {\\n-                            return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                        }\\n-                    })\\n-                    .css({\\n-                        \"margin-left\": function () {\\n-                            return -($(this).width() / 2);\\n-                        }\\n-                    });\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.currentGroup(undefined);\\n+                    $(\\n+                        \\'ul.nav-pills a[data-toggle=\"tab\"]:first\\',\\n+                        self.groupEditorDialog\\n+                    ).tab(\"show\");\\n+                    self.groupEditorDialog\\n+                        .modal({\\n+                            minHeight: function () {\\n+                                return Math.max(\\n+                                    $.fn.modal.defaults.maxHeight() - 80,\\n+                                    250\\n+                                );\\n+                            }\\n+                        })\\n+                        .css({\\n+                            \"margin-left\": function () {\\n+                                return -($(this).width() / 2);\\n+                            }\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmAddGroup = function () {\\n@@ -617,21 +705,27 @@ $(function () {\\n             self.showEditGroupDialog = function (group) {\\n                 if (!group.changeable) return;\\n \\n-                self.currentGroup(group);\\n-                $(\\'ul.nav-pills a[data-toggle=\"tab\"]:first\\', self.groupEditorDialog).tab(\\n-                    \"show\"\\n-                );\\n-                self.groupEditorDialog\\n-                    .modal({\\n-                        minHeight: function () {\\n-                            return Math.max($.fn.modal.defaults.maxHeight() - 80, 250);\\n-                        }\\n-                    })\\n-                    .css({\\n-                        \"margin-left\": function () {\\n-                            return -($(this).width() / 2);\\n-                        }\\n-                    });\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.currentGroup(group);\\n+                    $(\\n+                        \\'ul.nav-pills a[data-toggle=\"tab\"]:first\\',\\n+                        self.groupEditorDialog\\n+                    ).tab(\"show\");\\n+                    self.groupEditorDialog\\n+                        .modal({\\n+                            minHeight: function () {\\n+                                return Math.max(\\n+                                    $.fn.modal.defaults.maxHeight() - 80,\\n+                                    250\\n+                                );\\n+                            }\\n+                        })\\n+                        .css({\\n+                            \"margin-left\": function () {\\n+                                return -($(this).width() / 2);\\n+                            }\\n+                        });\\n+                });\\n             };\\n \\n             self.confirmEditGroup = function () {\\n@@ -653,6 +747,24 @@ $(function () {\\n                 });\\n             };\\n \\n+            self.confirmRemoveGroup = (group) => {\\n+                if (!group.removable) return;\\n+\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    showConfirmationDialog({\\n+                        title: gettext(\"Are you sure?\"),\\n+                        message: _.sprintf(\\n+                            gettext(\\'You are about to delete the group \"%(name)s\".\\'),\\n+                            {name: _.escape(group.name)}\\n+                        ),\\n+                        proceed: gettext(\"Delete\"),\\n+                        onproceed: () => {\\n+                            self.removeGroup(group);\\n+                        }\\n+                    });\\n+                });\\n+            };\\n+\\n             //~~ Framework\\n \\n             self.onStartup = function () {\\n@@ -665,6 +777,11 @@ $(function () {\\n                 if (!group) {\\n                     throw OctoPrint.InvalidArgumentError(\"group must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.groups.add(group).done(self.fromResponse);\\n             };\\n@@ -673,31 +790,27 @@ $(function () {\\n                 if (!group) {\\n                     throw OctoPrint.InvalidArgumentError(\"group must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n-                if (!group.removable) return;\\n-\\n-                showConfirmationDialog({\\n-                    title: gettext(\"Are you sure?\"),\\n-                    message: _.sprintf(\\n-                        gettext(\\'You are about to delete the group \"%(name)s\".\\'),\\n-                        {name: _.escape(group.name)}\\n-                    ),\\n-                    proceed: gettext(\"Delete\"),\\n-                    onproceed: function () {\\n-                        OctoPrint.access.groups\\n-                            .delete(group.key)\\n-                            .done(function (response) {\\n-                                self.fromResponse(response);\\n-                                access.users.requestData();\\n-                            });\\n-                    }\\n+                OctoPrint.access.groups.delete(group.key).done((response) => {\\n+                    self.fromResponse(response);\\n+                    access.users.requestData();\\n                 });\\n             };\\n \\n             self.updateGroup = function (group) {\\n                 if (!group) {\\n                     throw OctoPrint.InvalidArgumentError(\"group must be set\");\\n                 }\\n+                if (!access.loginState.credentialsSeen()) {\\n+                    return $.Deferred()\\n+                        .reject(\"You need to reauthenticate to perform this action\")\\n+                        .promise();\\n+                }\\n \\n                 return OctoPrint.access.groups.update(group).done(self.fromResponse);\\n             };', '@@ -6,6 +6,9 @@ $(function () {\\n \\n         self.onStartup = function () {\\n             self.dialog = $(\"#plugin_appkeys_keygenerated\");\\n+            self.dialog.on(\"hidden\", () => {\\n+                self.resetDialog();\\n+            });\\n         };\\n \\n         self.showDialog = function (title, data) {\\n@@ -40,6 +43,17 @@ $(function () {\\n \\n             self.dialog.modal(\"show\");\\n         };\\n+\\n+        self.resetDialog = () => {\\n+            if (self.dialog === undefined) return;\\n+\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_title\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_user\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_app\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_key_text\").text(\"\");\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_key_copy\").off();\\n+            self.dialog.find(\"#plugin_appkeys_keygenerated_key_qrcode\").empty();\\n+        };\\n     }\\n \\n     function UserAppKeysViewModel(parameters) {\\n@@ -69,7 +83,7 @@ $(function () {\\n         self.editorApp = ko.observable();\\n \\n         self.requestData = function () {\\n-            OctoPrint.plugins.appkeys.getKeys().done(self.fromResponse);\\n+            return OctoPrint.plugins.appkeys.getKeys().done(self.fromResponse);\\n         };\\n \\n         self.fromResponse = function (response) {\\n@@ -81,26 +95,42 @@ $(function () {\\n         };\\n \\n         self.generateKey = function () {\\n-            return OctoPrint.plugins.appkeys\\n-                .generateKey(self.editorApp())\\n-                .done(self.requestData)\\n-                .done(function () {\\n-                    self.editorApp(\"\");\\n-                });\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys\\n+                    .generateKey(self.editorApp())\\n+                    .done(self.requestData)\\n+                    .done(function () {\\n+                        self.editorApp(\"\");\\n+                    });\\n+            });\\n         };\\n \\n-        self.revokeKey = function (key) {\\n-            var perform = function () {\\n-                OctoPrint.plugins.appkeys.revokeKey(key).done(self.requestData);\\n-            };\\n+        self.revokeKey = (data) => {\\n+            const app = data.app_id;\\n+\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                showConfirmationDialog(\\n+                    _.sprintf(\\n+                        gettext(\\n+                            \"You are about to revoke the application key for %(app)s.\"\\n+                        ),\\n+                        {app: _.escape(app)}\\n+                    ),\\n+                    () => {\\n+                        OctoPrint.plugins.appkeys\\n+                            .revokeKeyForApp(app)\\n+                            .done(self.requestData);\\n+                    }\\n+                );\\n+            });\\n+        };\\n \\n-            showConfirmationDialog(\\n-                _.sprintf(\\n-                    gettext(\\'You are about to revoke the application key \"%(key)s\".\\'),\\n-                    {key: _.escape(key)}\\n-                ),\\n-                perform\\n-            );\\n+        self.showKeyDetails = (data) => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys.getKey(data.app_id).done((response) => {\\n+                    self.dialog.showDialog(gettext(\"Details\"), response.key);\\n+                });\\n+            });\\n         };\\n \\n         self.allowApp = function (token) {\\n@@ -135,8 +165,10 @@ $(function () {\\n                         {\\n                             text: gettext(\"Allow\"),\\n                             click: function (notice) {\\n-                                self.allowApp(token);\\n-                                notice.remove();\\n+                                self.loginState.reauthenticateIfNecessary(() => {\\n+                                    self.allowApp(token);\\n+                                    notice.remove();\\n+                                });\\n                             }\\n                         },\\n                         {\\n@@ -245,7 +277,7 @@ $(function () {\\n         };\\n \\n         self.requestData = function () {\\n-            OctoPrint.plugins.appkeys.getAllKeys().done(self.fromResponse);\\n+            return OctoPrint.plugins.appkeys.getAllKeys().done(self.fromResponse);\\n         };\\n \\n         self.fromResponse = function (response) {\\n@@ -267,46 +299,65 @@ $(function () {\\n             self.apps(apps);\\n         };\\n \\n+        self.showKeyDetails = (data) => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys\\n+                    .getKey(data.app_id, data.user_id)\\n+                    .done((response) => {\\n+                        self.dialog.showDialog(gettext(\"Details\"), response.key);\\n+                    });\\n+            });\\n+        };\\n+\\n         self.generateKey = function () {\\n-            return OctoPrint.plugins.appkeys\\n-                .generateKeyForUser(self.editorUser(), self.editorApp())\\n-                .done(self.requestData)\\n-                .done(function () {\\n-                    self.editorUser(self.loginState.username());\\n-                    self.editorApp(\"\");\\n-                })\\n-                .done(function (data) {\\n-                    self.dialog.showDialog(gettext(\"New key generated!\"), data);\\n-                });\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                OctoPrint.plugins.appkeys\\n+                    .generateKeyForUser(self.editorUser(), self.editorApp())\\n+                    .done(self.requestData)\\n+                    .done(function () {\\n+                        self.editorUser(self.loginState.username());\\n+                        self.editorApp(\"\");\\n+                    })\\n+                    .done(function (data) {\\n+                        self.dialog.showDialog(gettext(\"New key generated!\"), data);\\n+                    });\\n+            });\\n         };\\n \\n-        self.revokeKey = function (key) {\\n-            var perform = function () {\\n-                OctoPrint.plugins.appkeys.revokeKey(key).done(self.requestData);\\n-            };\\n+        self.revokeKey = function (data) {\\n+            const app = data.app_id;\\n+            const user = data.user_id;\\n \\n             showConfirmationDialog(\\n                 _.sprintf(\\n-                    gettext(\\'You are about to revoke the application key \"%(key)s\".\\'),\\n-                    {key: _.escape(key)}\\n+                    gettext(\\n+                        \"You are about to revoke the application key for %(app)s for user %(user)s.\"\\n+                    ),\\n+                    {app: _.escape(app), user: _.escape(user)}\\n                 ),\\n-                perform\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        OctoPrint.plugins.appkeys\\n+                            .revokeKeyForApp(app, user)\\n+                            .done(self.requestData);\\n+                    });\\n+                }\\n             );\\n         };\\n \\n         self.revokeMarked = function () {\\n-            var perform = function () {\\n-                self._bulkRevoke(self.markedForDeletion()).done(function () {\\n-                    self.markedForDeletion.removeAll();\\n-                });\\n-            };\\n-\\n             showConfirmationDialog(\\n                 _.sprintf(\\n                     gettext(\"You are about to revoke %(count)d application keys.\"),\\n                     {count: self.markedForDeletion().length}\\n                 ),\\n-                perform\\n+                () => {\\n+                    self.loginState.forceReauthentication(() => {\\n+                        self._bulkRevoke(self.markedForDeletion()).done(() => {\\n+                            self.markedForDeletion.removeAll();\\n+                        });\\n+                    });\\n+                }\\n             );\\n         };\\n \\n@@ -315,13 +366,22 @@ $(function () {\\n                 _.uniq(\\n                     self\\n                         .markedForDeletion()\\n-                        .concat(_.map(self.keys.paginatedItems(), \"api_key\"))\\n+                        .concat(\\n+                            _.map(\\n+                                self.keys.paginatedItems(),\\n+                                (item) => `${item.user_id}:${item.app_id}`\\n+                            )\\n+                        )\\n                 )\\n             );\\n         };\\n \\n         self.markAllForDeletion = function () {\\n-            self.markedForDeletion(_.uniq(_.map(self.keys.allItems, \"api_key\")));\\n+            self.markedForDeletion(\\n+                _.uniq(\\n+                    _.map(self.keys.allItems, (item) => `${item.user_id}:${item.app_id}`)\\n+                )\\n+            );\\n         };\\n \\n         self.markAllByUserForDeletion = function (user) {\\n@@ -341,7 +401,12 @@ $(function () {\\n                 _.uniq(\\n                     self\\n                         .markedForDeletion()\\n-                        .concat(_.map(_.filter(self.keys.allItems, filter), \"api_key\"))\\n+                        .concat(\\n+                            _.map(\\n+                                _.filter(self.keys.allItems, filter),\\n+                                (item) => `${item.user_id}:${item.app_id}`\\n+                            )\\n+                        )\\n                 )\\n             );\\n         };\\n@@ -351,31 +416,49 @@ $(function () {\\n         };\\n \\n         self._bulkRevoke = function (keys) {\\n+            /*\\n+             * TODO: This still has a risk of running into reauthentication for REALLY large numbers of keys\\n+             * whose bulk removal takes longer than the reauthentication timeout.\\n+             */\\n+\\n             var title, message, handler;\\n \\n             title = gettext(\"Revoking application keys\");\\n             message = _.sprintf(gettext(\"Revoking %(count)d application keys...\"), {\\n                 count: keys.length\\n             });\\n-            handler = function (key) {\\n+            handler = function (id) {\\n+                const [user, app] = rsplit(id, \":\", 1);\\n                 return OctoPrint.plugins.appkeys\\n-                    .revokeKey(key)\\n+                    .revokeKeyForApp(app, user)\\n                     .done(function () {\\n                         deferred.notify(\\n-                            _.sprintf(gettext(\"Revoked %(key)s...\"), {\\n-                                key: _.escape(key)\\n+                            _.sprintf(gettext(\"Revoked %(app)s for %(user)s...\"), {\\n+                                app: _.escape(app),\\n+                                user: _.escape(user)\\n                             }),\\n                             true\\n                         );\\n                     })\\n                     .fail(function (jqXHR) {\\n                         var short = _.sprintf(\\n-                            gettext(\"Revocation of %(key)s failed, continuing...\"),\\n-                            {key: _.escape(key)}\\n+                            gettext(\\n+                                \"Revocation of %(app)s for user %(user)s failed, continuing...\"\\n+                            ),\\n+                            {\\n+                                app: _.escape(app),\\n+                                user: _.escape(user)\\n+                            }\\n                         );\\n                         var long = _.sprintf(\\n-                            gettext(\"Deletion of %(key)s failed: %(error)s\"),\\n-                            {key: _.escape(key), error: _.escape(jqXHR.responseText)}\\n+                            gettext(\\n+                                \"Deletion of %(app)s for user %(user)s failed: %(error)s\"\\n+                            ),\\n+                            {\\n+                                app: _.escape(app),\\n+                                user: _.escape(user),\\n+                                error: _.escape(jqXHR.responseText)\\n+                            }\\n                         );\\n                         deferred.notify(short, long, false);\\n                     });', '@@ -14,8 +14,15 @@\\n     };\\n \\n     OctoPrintAppKeysClient.prototype.getAllKeys = function (opts) {\\n+        return this.base.get(this.base.getSimpleApiUrl(\"appkeys\") + \"?all=true\", opts);\\n+    };\\n+\\n+    OctoPrintAppKeysClient.prototype.getKey = function (app, user, opts) {\\n         return this.base.get(\\n-            OctoPrintClient.prototype.getSimpleApiUrl(\"appkeys\") + \"?all=true\",\\n+            this.base.getSimpleApiUrl(\"appkeys\") +\\n+                \"?app=\" +\\n+                encodeURIComponent(app) +\\n+                (user ? \"&user=\" + encodeURIComponent(user) : \"\"),\\n             opts\\n         );\\n     };\\n@@ -34,9 +41,20 @@\\n     };\\n \\n     OctoPrintAppKeysClient.prototype.revokeKey = function (key, opts) {\\n+        console.log(\\n+            \"revokeKey should be considered deprecated, use revokeKeyForApp instead\"\\n+        );\\n         return this.base.simpleApiCommand(\"appkeys\", \"revoke\", {key: key}, opts);\\n     };\\n \\n+    OctoPrintAppKeysClient.prototype.revokeKeyForApp = function (app, user, opts) {\\n+        const params = {app: app};\\n+        if (user) {\\n+            params.user = user;\\n+        }\\n+        return this.base.simpleApiCommand(\"appkeys\", \"revoke\", params, opts);\\n+    };\\n+\\n     OctoPrintAppKeysClient.prototype.decide = function (token, decision, opts) {\\n         return this.base.postJson(\\n             this.base.getBlueprintUrl(\"appkeys\") + \"decision/\" + token,'], 'file': ['src/octoprint/static/js/app/client/settings.js', 'src/octoprint/static/js/app/viewmodels/settings.js', 'src/octoprint/plugins/appkeys/templates/appkeys_settings.jinja2', 'src/octoprint/plugins/pluginmanager/static/js/pluginmanager.js', 'src/octoprint/server/api/access.py', 'src/octoprint/static/js/app/viewmodels/loginstate.js', 'src/octoprint/templates/initscript.jinja2', 'src/octoprint/server/views.py', 'src/octoprint/templates/snippets/settings/accesscontrol/groups.jinja2', 'src/octoprint/templates/index.jinja2', 'src/octoprint/static/js/app/helpers.js', 'src/octoprint/templates/dialogs/settings/api.jinja2', 'src/octoprint/server/api/settings.py', 'src/octoprint/plugins/appkeys/templates/appkeys.jinja2', 'src/octoprint/server/util/flask.py', 'src/octoprint/templates/login.jinja2', 'src/octoprint/templates/snippets/settings/accesscontrol/users.jinja2', 'src/octoprint/templates/dialogs/reauthenticate.jinja2', 'src/octoprint/schema/config/access_control.py', 'src/octoprint/plugins/appkeys/templates/appkeys_usersettings.jinja2', 'src/octoprint/static/js/app/viewmodels/access.js', 'src/octoprint/plugins/appkeys/static/js/appkeys.js', 'src/octoprint/plugins/appkeys/static/clientjs/appkeys.js'], 'language': ['JavaScript/TypeScript', 'JavaScript/TypeScript', 'Jinja2', 'JavaScript/TypeScript', 'Python', 'JavaScript/TypeScript', 'Jinja2', 'Python', 'Jinja2', 'Jinja2', 'JavaScript/TypeScript', 'Jinja2', 'Python', 'Jinja2', 'Python', 'Jinja2', 'Jinja2', 'Jinja2', 'Python', 'Jinja2', 'JavaScript/TypeScript', 'JavaScript/TypeScript', 'JavaScript/TypeScript'], 'temp_id': [UUID('29b2fc9d-abcf-4f4f-a550-490bf2ae221c'), UUID('388c03e2-e3ec-437e-9d42-0a989f2ba91b'), UUID('685ea5a4-b066-41d3-972d-c21afd44bf62'), UUID('3f137ddf-92d8-4213-8995-79d2485f5fa9'), UUID('c5e332ce-7791-455b-8dd1-985468e65668'), UUID('a5b6a49d-3cf1-4da4-b246-9aacd669b4e4'), UUID('3e97ced5-b09a-4aba-ba6a-8c62155b76ed'), UUID('173b0460-7308-4e07-ad1f-0da6eec20955'), UUID('ec66400c-6f91-452d-9e5a-b4755062028c'), UUID('0276a155-dd99-4203-b0c2-a06104f0e53e'), UUID('ca4c06e4-30f2-47cd-98be-e2e55372335e'), UUID('3747fd5f-8314-4c4a-a02f-7b34fd16b16a'), UUID('a8f689d2-2622-4580-9d1d-29360045dccb'), UUID('cb3945f5-bc43-43d8-a1ee-f6f154330371'), UUID('84ec7cb6-da1d-4e59-bd2a-b3b54c3d568a'), UUID('047a215b-bbd2-468c-8e95-bd08f14ecf3d'), UUID('132ef3a8-7d8d-4330-8b31-21b525938d8c'), UUID('39350f1f-764e-4e43-b103-c2b2ac7d8815'), UUID('145f2be2-a455-463c-a041-42013b62b556'), UUID('473f764f-3069-42e6-8389-a291dcc52072'), UUID('c7633b29-b3b4-4296-8c1f-9f7b75de59d2'), UUID('4b322418-4adb-48fd-a148-c642a9723b5e'), UUID('5e8e14a0-0487-47cf-8a38-82330087b730')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 360:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 360:0: <line number missing in source>\n",
      "  0%|          | 9/1800 [00:13<52:49,  1.77s/it]ERROR:src.process_code_changes:Error processing commit c52714b85e6725b1b24516fbdedacb333b939152\n",
      "ERROR:src.process_code_changes:{'repo': 'python-jsonschema/check-jsonschema', 'vulnerability_id': '2024-53848', 'commit': 'c52714b85e6725b1b24516fbdedacb333b939152', 'commit_source': 'github', 'cwe_id': ['CWE-349'], 'patch': ['@@ -1,6 +1,7 @@\\n from __future__ import annotations\\n \\n import contextlib\\n+import hashlib\\n import io\\n import os\\n import platform\\n@@ -33,7 +34,7 @@ def _base_cache_dir() -> str | None:\\n     return cache_dir\\n \\n \\n-def _resolve_cache_dir(dirname: str = \"downloads\") -> str | None:\\n+def _resolve_cache_dir(dirname: str) -> str | None:\\n     cache_dir = _base_cache_dir()\\n     if cache_dir:\\n         cache_dir = os.path.join(cache_dir, \"check_jsonschema\", dirname)\\n@@ -95,18 +96,32 @@ def _cache_hit(cachefile: str, response: requests.Response) -> bool:\\n     return local_mtime >= remote_mtime\\n \\n \\n+def url_to_cache_filename(ref_url: str) -> str:\\n+    \"\"\"\\n+    Given a schema URL, convert it to a filename for caching in a cache dir.\\n+\\n+    Rules are as follows:\\n+    - the base filename is an sha256 hash of the URL\\n+    - if the filename ends in an extension (.json, .yaml, etc) that extension\\n+      is appended to the hash\\n+\\n+    Preserving file extensions preserves the extension-based logic used for parsing, and\\n+    it also helps a local editor (browsing the cache) identify filetypes.\\n+    \"\"\"\\n+    filename = hashlib.sha256(ref_url.encode()).hexdigest()\\n+    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):\\n+        _, _, extension = last_part.rpartition(\".\")\\n+        filename = f\"{filename}.{extension}\"\\n+    return filename\\n+\\n+\\n class FailedDownloadError(Exception):\\n     pass\\n \\n \\n class CacheDownloader:\\n-    def __init__(\\n-        self, cache_dir: str | None = None, disable_cache: bool = False\\n-    ) -> None:\\n-        if cache_dir is None:\\n-            self._cache_dir = _resolve_cache_dir()\\n-        else:\\n-            self._cache_dir = _resolve_cache_dir(cache_dir)\\n+    def __init__(self, cache_dir: str, *, disable_cache: bool = False) -> None:\\n+        self._cache_dir = _resolve_cache_dir(cache_dir)\\n         self._disable_cache = disable_cache\\n \\n     def _download(\\n@@ -160,21 +175,21 @@ def bind(\\n         validation_callback: t.Callable[[bytes], t.Any] | None = None,\\n     ) -> BoundCacheDownloader:\\n         return BoundCacheDownloader(\\n-            file_url, filename, self, validation_callback=validation_callback\\n+            file_url, self, filename=filename, validation_callback=validation_callback\\n         )\\n \\n \\n class BoundCacheDownloader:\\n     def __init__(\\n         self,\\n         file_url: str,\\n-        filename: str | None,\\n         downloader: CacheDownloader,\\n         *,\\n+        filename: str | None = None,\\n         validation_callback: t.Callable[[bytes], t.Any] | None = None,\\n     ) -> None:\\n         self._file_url = file_url\\n-        self._filename = filename or file_url.split(\"/\")[-1]\\n+        self._filename = filename or url_to_cache_filename(file_url)\\n         self._downloader = downloader\\n         self._validation_callback = validation_callback\\n ', '@@ -73,14 +73,13 @@ class HttpSchemaReader:\\n     def __init__(\\n         self,\\n         url: str,\\n-        cache_filename: str | None,\\n         disable_cache: bool,\\n     ) -> None:\\n         self.url = url\\n         self.parsers = ParserSet()\\n-        self.downloader = CacheDownloader(\\n-            disable_cache=disable_cache,\\n-        ).bind(url, cache_filename, validation_callback=self._parse)\\n+        self.downloader = CacheDownloader(\"schemas\", disable_cache=disable_cache).bind(\\n+            url, validation_callback=self._parse\\n+        )\\n         self._parsed_schema: dict | _UnsetType = _UNSET\\n \\n     def _parse(self, schema_bytes: bytes) -> t.Any:', '@@ -1,6 +1,5 @@\\n from __future__ import annotations\\n \\n-import hashlib\\n import typing as t\\n import urllib.parse\\n \\n@@ -12,21 +11,6 @@\\n from ..utils import filename2path\\n \\n \\n-def ref_url_to_cache_filename(ref_url: str) -> str:\\n-    \"\"\"\\n-    Given a $ref URL, convert it to the filename in the refs/ cache dir.\\n-    Rules are as follows:\\n-    - the base filename is an md5 hash of the URL\\n-    - if the filename ends in an extension (.json, .yaml, etc) that extension\\n-      is appended to the hash\\n-    \"\"\"\\n-    filename = hashlib.md5(ref_url.encode()).hexdigest()\\n-    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):\\n-        _, _, extension = last_part.rpartition(\".\")\\n-        filename = f\"{filename}.{extension}\"\\n-    return filename\\n-\\n-\\n def make_reference_registry(\\n     parsers: ParserSet, retrieval_uri: str | None, schema: dict, disable_cache: bool\\n ) -> referencing.Registry:\\n@@ -66,7 +50,7 @@ def create_retrieve_callable(\\n         base_uri = retrieval_uri\\n \\n     cache = ResourceCache()\\n-    downloader = CacheDownloader(\"refs\", disable_cache)\\n+    downloader = CacheDownloader(\"refs\", disable_cache=disable_cache)\\n \\n     def get_local_file(uri: str) -> t.Any:\\n         path = filename2path(uri)\\n@@ -89,9 +73,7 @@ def validation_callback(content: bytes) -> None:\\n                 parser_set.parse_data_with_path(content, full_uri, \"json\")\\n \\n             bound_downloader = downloader.bind(\\n-                full_uri,\\n-                ref_url_to_cache_filename(full_uri),\\n-                validation_callback,\\n+                full_uri, validation_callback=validation_callback\\n             )\\n             with bound_downloader.open() as fp:\\n                 data = fp.read()', '@@ -130,11 +130,7 @@ def pretty_helptext_list(values: list[str] | tuple[str, ...]) -> str:\\n     help=\"Disable schema caching. Always download remote schemas.\",\\n )\\n @click.option(\\n-    \"--cache-filename\",\\n-    help=(\\n-        \"The name to use for caching a remote schema. \"\\n-        \"Defaults to the last slash-delimited part of the URI.\"\\n-    ),\\n+    \"--cache-filename\", help=\"Deprecated. This option no longer has any effect.\"\\n )\\n @click.option(\\n     \"--disable-formats\",\\n@@ -271,8 +267,6 @@ def main(\\n     args.disable_cache = no_cache\\n     args.default_filetype = default_filetype\\n     args.fill_defaults = fill_defaults\\n-    if cache_filename is not None:\\n-        args.cache_filename = cache_filename\\n     if data_transform is not None:\\n         args.data_transform = TRANSFORM_LIBRARY[data_transform]\\n \\n@@ -300,7 +294,6 @@ def build_schema_loader(args: ParseResult) -> SchemaLoaderBase:\\n         assert args.schema_path is not None\\n         return SchemaLoader(\\n             args.schema_path,\\n-            cache_filename=args.cache_filename,\\n             disable_cache=args.disable_cache,\\n             base_uri=args.base_uri,\\n             validator_class=args.validator_class,', '@@ -64,14 +64,12 @@ def __init__(\\n         self,\\n         schemafile: str,\\n         *,\\n-        cache_filename: str | None = None,\\n         base_uri: str | None = None,\\n         validator_class: type[jsonschema.protocols.Validator] | None = None,\\n         disable_cache: bool = True,\\n     ) -> None:\\n         # record input parameters (these are not to be modified)\\n         self.schemafile = schemafile\\n-        self.cache_filename = cache_filename\\n         self.disable_cache = disable_cache\\n         self.base_uri = base_uri\\n         self.validator_class = validator_class\\n@@ -105,11 +103,7 @@ def _get_schema_reader(\\n             return LocalSchemaReader(self.schemafile)\\n \\n         if self.url_info.scheme in (\"http\", \"https\"):\\n-            return HttpSchemaReader(\\n-                self.schemafile,\\n-                self.cache_filename,\\n-                self.disable_cache,\\n-            )\\n+            return HttpSchemaReader(self.schemafile, self.disable_cache)\\n         else:\\n             raise UnsupportedUrlScheme(\\n                 \"check-jsonschema only supports http, https, and local files. \"'], 'file': ['src/check_jsonschema/cachedownloader.py', 'src/check_jsonschema/schema_loader/readers.py', 'src/check_jsonschema/schema_loader/resolver.py', 'src/check_jsonschema/cli/main_command.py', 'src/check_jsonschema/schema_loader/main.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('1087a19c-835e-4e7a-876f-cada44185152'), UUID('a4766faa-0a30-431d-aa68-b4b4c770646a'), UUID('45acfc34-d443-48b0-8c83-3e6cde81522d'), UUID('05a77957-a725-44c1-90b4-60077b835172'), UUID('5c5ad1ae-340f-492c-a453-cacfdc142588')]}\n",
      "ERROR:root:Error in {'repo': 'python-jsonschema/check-jsonschema', 'vulnerability_id': '2024-53848', 'commit': 'c52714b85e6725b1b24516fbdedacb333b939152', 'commit_source': 'github', 'cwe_id': ['CWE-349'], 'patch': ['@@ -1,6 +1,7 @@\\n from __future__ import annotations\\n \\n import contextlib\\n+import hashlib\\n import io\\n import os\\n import platform\\n@@ -33,7 +34,7 @@ def _base_cache_dir() -> str | None:\\n     return cache_dir\\n \\n \\n-def _resolve_cache_dir(dirname: str = \"downloads\") -> str | None:\\n+def _resolve_cache_dir(dirname: str) -> str | None:\\n     cache_dir = _base_cache_dir()\\n     if cache_dir:\\n         cache_dir = os.path.join(cache_dir, \"check_jsonschema\", dirname)\\n@@ -95,18 +96,32 @@ def _cache_hit(cachefile: str, response: requests.Response) -> bool:\\n     return local_mtime >= remote_mtime\\n \\n \\n+def url_to_cache_filename(ref_url: str) -> str:\\n+    \"\"\"\\n+    Given a schema URL, convert it to a filename for caching in a cache dir.\\n+\\n+    Rules are as follows:\\n+    - the base filename is an sha256 hash of the URL\\n+    - if the filename ends in an extension (.json, .yaml, etc) that extension\\n+      is appended to the hash\\n+\\n+    Preserving file extensions preserves the extension-based logic used for parsing, and\\n+    it also helps a local editor (browsing the cache) identify filetypes.\\n+    \"\"\"\\n+    filename = hashlib.sha256(ref_url.encode()).hexdigest()\\n+    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):\\n+        _, _, extension = last_part.rpartition(\".\")\\n+        filename = f\"{filename}.{extension}\"\\n+    return filename\\n+\\n+\\n class FailedDownloadError(Exception):\\n     pass\\n \\n \\n class CacheDownloader:\\n-    def __init__(\\n-        self, cache_dir: str | None = None, disable_cache: bool = False\\n-    ) -> None:\\n-        if cache_dir is None:\\n-            self._cache_dir = _resolve_cache_dir()\\n-        else:\\n-            self._cache_dir = _resolve_cache_dir(cache_dir)\\n+    def __init__(self, cache_dir: str, *, disable_cache: bool = False) -> None:\\n+        self._cache_dir = _resolve_cache_dir(cache_dir)\\n         self._disable_cache = disable_cache\\n \\n     def _download(\\n@@ -160,21 +175,21 @@ def bind(\\n         validation_callback: t.Callable[[bytes], t.Any] | None = None,\\n     ) -> BoundCacheDownloader:\\n         return BoundCacheDownloader(\\n-            file_url, filename, self, validation_callback=validation_callback\\n+            file_url, self, filename=filename, validation_callback=validation_callback\\n         )\\n \\n \\n class BoundCacheDownloader:\\n     def __init__(\\n         self,\\n         file_url: str,\\n-        filename: str | None,\\n         downloader: CacheDownloader,\\n         *,\\n+        filename: str | None = None,\\n         validation_callback: t.Callable[[bytes], t.Any] | None = None,\\n     ) -> None:\\n         self._file_url = file_url\\n-        self._filename = filename or file_url.split(\"/\")[-1]\\n+        self._filename = filename or url_to_cache_filename(file_url)\\n         self._downloader = downloader\\n         self._validation_callback = validation_callback\\n ', '@@ -73,14 +73,13 @@ class HttpSchemaReader:\\n     def __init__(\\n         self,\\n         url: str,\\n-        cache_filename: str | None,\\n         disable_cache: bool,\\n     ) -> None:\\n         self.url = url\\n         self.parsers = ParserSet()\\n-        self.downloader = CacheDownloader(\\n-            disable_cache=disable_cache,\\n-        ).bind(url, cache_filename, validation_callback=self._parse)\\n+        self.downloader = CacheDownloader(\"schemas\", disable_cache=disable_cache).bind(\\n+            url, validation_callback=self._parse\\n+        )\\n         self._parsed_schema: dict | _UnsetType = _UNSET\\n \\n     def _parse(self, schema_bytes: bytes) -> t.Any:', '@@ -1,6 +1,5 @@\\n from __future__ import annotations\\n \\n-import hashlib\\n import typing as t\\n import urllib.parse\\n \\n@@ -12,21 +11,6 @@\\n from ..utils import filename2path\\n \\n \\n-def ref_url_to_cache_filename(ref_url: str) -> str:\\n-    \"\"\"\\n-    Given a $ref URL, convert it to the filename in the refs/ cache dir.\\n-    Rules are as follows:\\n-    - the base filename is an md5 hash of the URL\\n-    - if the filename ends in an extension (.json, .yaml, etc) that extension\\n-      is appended to the hash\\n-    \"\"\"\\n-    filename = hashlib.md5(ref_url.encode()).hexdigest()\\n-    if \".\" in (last_part := ref_url.rpartition(\"/\")[-1]):\\n-        _, _, extension = last_part.rpartition(\".\")\\n-        filename = f\"{filename}.{extension}\"\\n-    return filename\\n-\\n-\\n def make_reference_registry(\\n     parsers: ParserSet, retrieval_uri: str | None, schema: dict, disable_cache: bool\\n ) -> referencing.Registry:\\n@@ -66,7 +50,7 @@ def create_retrieve_callable(\\n         base_uri = retrieval_uri\\n \\n     cache = ResourceCache()\\n-    downloader = CacheDownloader(\"refs\", disable_cache)\\n+    downloader = CacheDownloader(\"refs\", disable_cache=disable_cache)\\n \\n     def get_local_file(uri: str) -> t.Any:\\n         path = filename2path(uri)\\n@@ -89,9 +73,7 @@ def validation_callback(content: bytes) -> None:\\n                 parser_set.parse_data_with_path(content, full_uri, \"json\")\\n \\n             bound_downloader = downloader.bind(\\n-                full_uri,\\n-                ref_url_to_cache_filename(full_uri),\\n-                validation_callback,\\n+                full_uri, validation_callback=validation_callback\\n             )\\n             with bound_downloader.open() as fp:\\n                 data = fp.read()', '@@ -130,11 +130,7 @@ def pretty_helptext_list(values: list[str] | tuple[str, ...]) -> str:\\n     help=\"Disable schema caching. Always download remote schemas.\",\\n )\\n @click.option(\\n-    \"--cache-filename\",\\n-    help=(\\n-        \"The name to use for caching a remote schema. \"\\n-        \"Defaults to the last slash-delimited part of the URI.\"\\n-    ),\\n+    \"--cache-filename\", help=\"Deprecated. This option no longer has any effect.\"\\n )\\n @click.option(\\n     \"--disable-formats\",\\n@@ -271,8 +267,6 @@ def main(\\n     args.disable_cache = no_cache\\n     args.default_filetype = default_filetype\\n     args.fill_defaults = fill_defaults\\n-    if cache_filename is not None:\\n-        args.cache_filename = cache_filename\\n     if data_transform is not None:\\n         args.data_transform = TRANSFORM_LIBRARY[data_transform]\\n \\n@@ -300,7 +294,6 @@ def build_schema_loader(args: ParseResult) -> SchemaLoaderBase:\\n         assert args.schema_path is not None\\n         return SchemaLoader(\\n             args.schema_path,\\n-            cache_filename=args.cache_filename,\\n             disable_cache=args.disable_cache,\\n             base_uri=args.base_uri,\\n             validator_class=args.validator_class,', '@@ -64,14 +64,12 @@ def __init__(\\n         self,\\n         schemafile: str,\\n         *,\\n-        cache_filename: str | None = None,\\n         base_uri: str | None = None,\\n         validator_class: type[jsonschema.protocols.Validator] | None = None,\\n         disable_cache: bool = True,\\n     ) -> None:\\n         # record input parameters (these are not to be modified)\\n         self.schemafile = schemafile\\n-        self.cache_filename = cache_filename\\n         self.disable_cache = disable_cache\\n         self.base_uri = base_uri\\n         self.validator_class = validator_class\\n@@ -105,11 +103,7 @@ def _get_schema_reader(\\n             return LocalSchemaReader(self.schemafile)\\n \\n         if self.url_info.scheme in (\"http\", \"https\"):\\n-            return HttpSchemaReader(\\n-                self.schemafile,\\n-                self.cache_filename,\\n-                self.disable_cache,\\n-            )\\n+            return HttpSchemaReader(self.schemafile, self.disable_cache)\\n         else:\\n             raise UnsupportedUrlScheme(\\n                 \"check-jsonschema only supports http, https, and local files. \"'], 'file': ['src/check_jsonschema/cachedownloader.py', 'src/check_jsonschema/schema_loader/readers.py', 'src/check_jsonschema/schema_loader/resolver.py', 'src/check_jsonschema/cli/main_command.py', 'src/check_jsonschema/schema_loader/main.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('1087a19c-835e-4e7a-876f-cada44185152'), UUID('a4766faa-0a30-431d-aa68-b4b4c770646a'), UUID('45acfc34-d443-48b0-8c83-3e6cde81522d'), UUID('05a77957-a725-44c1-90b4-60077b835172'), UUID('5c5ad1ae-340f-492c-a453-cacfdc142588')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 166:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 166:0: <line number missing in source>\n",
      "  2%|â–         | 37/1800 [00:16<09:21,  3.14it/s]ERROR:src.process_code_changes:Error processing commit 22abeb9f6cc555591bf8e92b5e328e43aa07ff6c\n",
      "ERROR:src.process_code_changes:{'repo': 'langchain-ai/langchain', 'vulnerability_id': '2023-36281', 'commit': '22abeb9f6cc555591bf8e92b5e328e43aa07ff6c', 'commit_source': 'github', 'cwe_id': ['CWE-94'], 'patch': ['@@ -12,7 +12,12 @@\\n \\n \\n def jinja2_formatter(template: str, **kwargs: Any) -> str:\\n-    \"\"\"Format a template using jinja2.\"\"\"\\n+    \"\"\"Format a template using jinja2.\\n+\\n+    *Security warning*: jinja2 templates are not sandboxed and may lead\\n+    to arbitrary Python code execution. Do not expand jinja2 templates\\n+    using unverified or user-controlled inputs!\\n+    \"\"\"\\n     try:\\n         from jinja2 import Template\\n     except ImportError:', '@@ -22,6 +22,11 @@ class PromptTemplate(StringPromptTemplate):\\n \\n     The template can be formatted using either f-strings (default) or jinja2 syntax.\\n \\n+    *Security warning*: Prefer using `template_format=\"f-string\"` instead of\\n+    `template_format=\"jinja2\"`, since jinja2 templates are not sandboxed and may\\n+    lead to arbitrary Python code execution. Do not construct a jinja2 `PromptTemplate`\\n+    from unverified or user-controlled inputs!\\n+\\n     Example:\\n \\n         .. code-block:: python', '@@ -113,6 +113,17 @@ def _load_prompt(config: dict) -> PromptTemplate:\\n     # Load the template from disk if necessary.\\n     config = _load_template(\"template\", config)\\n     config = _load_output_parser(config)\\n+\\n+    template_format = config.get(\"template_format\", \"f-string\")\\n+    if template_format == \"jinja2\":\\n+        # Disabled due to:\\n+        # https://github.com/langchain-ai/langchain/issues/4394\\n+        raise ValueError(\\n+            f\"Loading templates with \\'{template_format}\\' format is no longer supported \"\\n+            f\"since it can lead to arbitrary code execution. Please migrate to using \"\\n+            f\"the \\'f-string\\' template format, which does not suffer from this issue.\"\\n+        )\\n+\\n     return PromptTemplate(**config)\\n \\n '], 'file': ['libs/langchain/langchain/prompts/base.py', 'libs/langchain/langchain/prompts/prompt.py', 'libs/langchain/langchain/prompts/loading.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('c74d3797-7019-4591-b916-d5395fbea688'), UUID('e0a6bbd3-d2c9-4af2-9ab6-b52771e3c779'), UUID('99629dc0-292a-422f-8f56-0818e6e6a002')]}\n",
      "ERROR:root:Error in {'repo': 'langchain-ai/langchain', 'vulnerability_id': '2023-36281', 'commit': '22abeb9f6cc555591bf8e92b5e328e43aa07ff6c', 'commit_source': 'github', 'cwe_id': ['CWE-94'], 'patch': ['@@ -12,7 +12,12 @@\\n \\n \\n def jinja2_formatter(template: str, **kwargs: Any) -> str:\\n-    \"\"\"Format a template using jinja2.\"\"\"\\n+    \"\"\"Format a template using jinja2.\\n+\\n+    *Security warning*: jinja2 templates are not sandboxed and may lead\\n+    to arbitrary Python code execution. Do not expand jinja2 templates\\n+    using unverified or user-controlled inputs!\\n+    \"\"\"\\n     try:\\n         from jinja2 import Template\\n     except ImportError:', '@@ -22,6 +22,11 @@ class PromptTemplate(StringPromptTemplate):\\n \\n     The template can be formatted using either f-strings (default) or jinja2 syntax.\\n \\n+    *Security warning*: Prefer using `template_format=\"f-string\"` instead of\\n+    `template_format=\"jinja2\"`, since jinja2 templates are not sandboxed and may\\n+    lead to arbitrary Python code execution. Do not construct a jinja2 `PromptTemplate`\\n+    from unverified or user-controlled inputs!\\n+\\n     Example:\\n \\n         .. code-block:: python', '@@ -113,6 +113,17 @@ def _load_prompt(config: dict) -> PromptTemplate:\\n     # Load the template from disk if necessary.\\n     config = _load_template(\"template\", config)\\n     config = _load_output_parser(config)\\n+\\n+    template_format = config.get(\"template_format\", \"f-string\")\\n+    if template_format == \"jinja2\":\\n+        # Disabled due to:\\n+        # https://github.com/langchain-ai/langchain/issues/4394\\n+        raise ValueError(\\n+            f\"Loading templates with \\'{template_format}\\' format is no longer supported \"\\n+            f\"since it can lead to arbitrary code execution. Please migrate to using \"\\n+            f\"the \\'f-string\\' template format, which does not suffer from this issue.\"\\n+        )\\n+\\n     return PromptTemplate(**config)\\n \\n '], 'file': ['libs/langchain/langchain/prompts/base.py', 'libs/langchain/langchain/prompts/prompt.py', 'libs/langchain/langchain/prompts/loading.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('c74d3797-7019-4591-b916-d5395fbea688'), UUID('e0a6bbd3-d2c9-4af2-9ab6-b52771e3c779'), UUID('99629dc0-292a-422f-8f56-0818e6e6a002')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 29:0: <line number missing in source>\n",
      "  3%|â–         | 54/1800 [00:17<05:17,  5.51it/s]ERROR:src.process_code_changes:Error processing commit 359b343f51524342a5ca03828e7c975a1d654b11\n",
      "ERROR:src.process_code_changes:{'repo': 'goauthentik/authentik', 'vulnerability_id': '2024-42490', 'commit': '359b343f51524342a5ca03828e7c975a1d654b11', 'commit_source': 'github', 'cwe_id': ['CWE-285', 'CWE-285'], 'patch': ['@@ -37,6 +37,7 @@\\n )\\n from authentik.lib.views import bad_request_message\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -281,7 +282,7 @@ def set_background_url(self, request: Request, slug: str):\\n             400: OpenApiResponse(description=\"Flow not applicable\"),\\n         },\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def execute(self, request: Request, slug: str):\\n         \"\"\"Execute flow for current user\"\"\"\\n         # Because we pre-plan the flow here, and not in the planner, we need to manually clear', '@@ -511,6 +511,7 @@ const docsSidebar = {\\n             items: [\\n                 \"security/security-hardening\",\\n                 \"security/policy\",\\n+                \"security/CVE-2024-42490\",\\n                 \"security/CVE-2024-38371\",\\n                 \"security/CVE-2024-37905\",\\n                 \"security/CVE-2024-23647\",', '@@ -35,6 +35,7 @@\\n from authentik.crypto.models import CertificateKeyPair\\n from authentik.events.models import Event, EventAction\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -265,7 +266,7 @@ def generate(self, request: Request) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_certificate(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs certificate and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()\\n@@ -295,7 +296,7 @@ def view_certificate(self, request: Request, pk: str) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_private_key(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs private key and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()', '@@ -26,6 +26,7 @@\\n     KubernetesServiceConnection,\\n     OutpostServiceConnection,\\n )\\n+from authentik.rbac.filters import ObjectFilter\\n \\n \\n class ServiceConnectionSerializer(ModelSerializer, MetaNameSerializer):\\n@@ -75,7 +76,7 @@ class ServiceConnectionViewSet(\\n     filterset_fields = [\"name\"]\\n \\n     @extend_schema(responses={200: ServiceConnectionStateSerializer(many=False)})\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def state(self, request: Request, pk: str) -> Response:\\n         \"\"\"Get the service connection\\'s state\"\"\"\\n         connection = self.get_object()', '@@ -14,6 +14,7 @@\\n from rest_framework.response import Response\\n \\n from authentik.core.api.utils import PassiveSerializer\\n+from authentik.rbac.filters import ObjectFilter\\n \\n \\n class DeleteAction(Enum):\\n@@ -53,7 +54,7 @@ class UsedByMixin:\\n     @extend_schema(\\n         responses={200: UsedBySerializer(many=True)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def used_by(self, request: Request, *args, **kwargs) -> Response:\\n         \"\"\"Get a list of all objects that use this object\"\"\"\\n         model: Model = self.get_object()'], 'file': ['authentik/flows/api/flows.py', 'website/sidebars.js', 'authentik/crypto/api.py', 'authentik/outposts/api/service_connections.py', 'authentik/core/api/used_by.py'], 'language': ['Python', 'JavaScript/TypeScript', 'Python', 'Python', 'Python'], 'temp_id': [UUID('1b238dd0-ea58-40f3-bee9-a97bbf9d93f8'), UUID('5460c30d-8543-4a83-bac1-33e3d436f8a8'), UUID('3c5250e0-53a3-41e5-a022-a4846cf858a5'), UUID('c49cf8a4-7149-4c66-9f23-00aaffe3fa0d'), UUID('61f60d5e-e1ac-491e-a1db-77cab6b5f660')]}\n",
      "ERROR:root:Error in {'repo': 'goauthentik/authentik', 'vulnerability_id': '2024-42490', 'commit': '359b343f51524342a5ca03828e7c975a1d654b11', 'commit_source': 'github', 'cwe_id': ['CWE-285', 'CWE-285'], 'patch': ['@@ -37,6 +37,7 @@\\n )\\n from authentik.lib.views import bad_request_message\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -281,7 +282,7 @@ def set_background_url(self, request: Request, slug: str):\\n             400: OpenApiResponse(description=\"Flow not applicable\"),\\n         },\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def execute(self, request: Request, slug: str):\\n         \"\"\"Execute flow for current user\"\"\"\\n         # Because we pre-plan the flow here, and not in the planner, we need to manually clear', '@@ -511,6 +511,7 @@ const docsSidebar = {\\n             items: [\\n                 \"security/security-hardening\",\\n                 \"security/policy\",\\n+                \"security/CVE-2024-42490\",\\n                 \"security/CVE-2024-38371\",\\n                 \"security/CVE-2024-37905\",\\n                 \"security/CVE-2024-23647\",', '@@ -35,6 +35,7 @@\\n from authentik.crypto.models import CertificateKeyPair\\n from authentik.events.models import Event, EventAction\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -265,7 +266,7 @@ def generate(self, request: Request) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_certificate(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs certificate and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()\\n@@ -295,7 +296,7 @@ def view_certificate(self, request: Request, pk: str) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_private_key(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs private key and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()', '@@ -26,6 +26,7 @@\\n     KubernetesServiceConnection,\\n     OutpostServiceConnection,\\n )\\n+from authentik.rbac.filters import ObjectFilter\\n \\n \\n class ServiceConnectionSerializer(ModelSerializer, MetaNameSerializer):\\n@@ -75,7 +76,7 @@ class ServiceConnectionViewSet(\\n     filterset_fields = [\"name\"]\\n \\n     @extend_schema(responses={200: ServiceConnectionStateSerializer(many=False)})\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def state(self, request: Request, pk: str) -> Response:\\n         \"\"\"Get the service connection\\'s state\"\"\"\\n         connection = self.get_object()', '@@ -14,6 +14,7 @@\\n from rest_framework.response import Response\\n \\n from authentik.core.api.utils import PassiveSerializer\\n+from authentik.rbac.filters import ObjectFilter\\n \\n \\n class DeleteAction(Enum):\\n@@ -53,7 +54,7 @@ class UsedByMixin:\\n     @extend_schema(\\n         responses={200: UsedBySerializer(many=True)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def used_by(self, request: Request, *args, **kwargs) -> Response:\\n         \"\"\"Get a list of all objects that use this object\"\"\"\\n         model: Model = self.get_object()'], 'file': ['authentik/flows/api/flows.py', 'website/sidebars.js', 'authentik/crypto/api.py', 'authentik/outposts/api/service_connections.py', 'authentik/core/api/used_by.py'], 'language': ['Python', 'JavaScript/TypeScript', 'Python', 'Python', 'Python'], 'temp_id': [UUID('1b238dd0-ea58-40f3-bee9-a97bbf9d93f8'), UUID('5460c30d-8543-4a83-bac1-33e3d436f8a8'), UUID('3c5250e0-53a3-41e5-a022-a4846cf858a5'), UUID('c49cf8a4-7149-4c66-9f23-00aaffe3fa0d'), UUID('61f60d5e-e1ac-491e-a1db-77cab6b5f660')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\n",
      "  5%|â–Œ         | 90/1800 [00:20<03:45,  7.57it/s]ERROR:src.process_code_changes:Error processing commit e808840f957c810b8e3944cba808716dc722581b\n",
      "ERROR:src.process_code_changes:{'repo': 'CIRCL/AIL-framework', 'vulnerability_id': '2020-8545', 'commit': 'e808840f957c810b8e3944cba808716dc722581b', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -45,8 +45,10 @@ def rreplace(s, old, new, occurrence):\\n \\n     p = Process(config_section)\\n \\n+    # get and sanityze PASTE DIRECTORY\\n     PASTES_FOLDER = os.path.join(os.environ[\\'AIL_HOME\\'], p.config.get(\"Directories\", \"pastes\"))\\n     PASTES_FOLDERS = PASTES_FOLDER + \\'/\\'\\n+    PASTES_FOLDERS = os.path.join(os.path.realpath(PASTES_FOLDERS), \\'\\')\\n \\n     # LOGGING #\\n     publisher.info(\"Feed Script started to receive & publish.\")\\n@@ -75,40 +77,46 @@ def rreplace(s, old, new, occurrence):\\n             time.sleep(1)\\n             continue\\n \\n+        # remove PASTES_FOLDER from item path (crawled item + submited)\\n+        if PASTES_FOLDERS in paste:\\n+            paste = paste.replace(PASTES_FOLDERS, \\'\\', 1)\\n+\\n         file_name_paste = paste.split(\\'/\\')[-1]\\n         if len(file_name_paste)>255:\\n             new_file_name_paste = \\'{}{}.gz\\'.format(file_name_paste[:215], str(uuid.uuid4()))\\n             paste = rreplace(paste, file_name_paste, new_file_name_paste, 1)\\n \\n         # Creating the full filepath\\n         filename = os.path.join(PASTES_FOLDER, paste)\\n+        filename = os.path.realpath(filename)\\n \\n-        dirname = os.path.dirname(filename)\\n-        if not os.path.exists(dirname):\\n-            os.makedirs(dirname)\\n-\\n-        decoded = base64.standard_b64decode(gzip64encoded)\\n+        # incorrect filename\\n+        if not os.path.commonprefix([filename, PASTES_FOLDER]) == PASTES_FOLDER:\\n+            print(\\'Path traversal detected {}\\'.format(filename))\\n+            publisher.warning(\\'Global; Path traversal detected\\')\\n+        else:\\n+            dirname = os.path.dirname(filename)\\n+            if not os.path.exists(dirname):\\n+                os.makedirs(dirname)\\n \\n-        with open(filename, \\'wb\\') as f:\\n-            f.write(decoded)\\n-        \\'\\'\\'try:\\n-            decoded2 = gunzip_bytes_obj(decoded)\\n-        except:\\n-            decoded2 =\\'\\'\\n+            decoded = base64.standard_b64decode(gzip64encoded)\\n \\n-        type = magic.from_buffer(decoded2, mime=True)\\n+            with open(filename, \\'wb\\') as f:\\n+                f.write(decoded)\\n+            \\'\\'\\'try:\\n+                decoded2 = gunzip_bytes_obj(decoded)\\n+            except:\\n+                decoded2 =\\'\\'\\n \\n-        if type!= \\'text/x-c++\\' and type!= \\'text/html\\' and type!= \\'text/x-c\\' and type!= \\'text/x-python\\' and type!= \\'text/x-php\\' and type!= \\'application/xml\\' and type!= \\'text/x-shellscript\\' and type!= \\'text/plain\\' and type!= \\'text/x-diff\\' and type!= \\'text/x-ruby\\':\\n+            type = magic.from_buffer(decoded2, mime=True)\\n \\n-            print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n-            print(filename)\\n-            print(type)\\n-            print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n-        \\'\\'\\'\\n+            if type!= \\'text/x-c++\\' and type!= \\'text/html\\' and type!= \\'text/x-c\\' and type!= \\'text/x-python\\' and type!= \\'text/x-php\\' and type!= \\'application/xml\\' and type!= \\'text/x-shellscript\\' and type!= \\'text/plain\\' and type!= \\'text/x-diff\\' and type!= \\'text/x-ruby\\':\\n \\n-        # remove PASTES_FOLDER from item path (crawled item + submited)\\n-        if PASTES_FOLDERS in paste:\\n-            paste = paste.replace(PASTES_FOLDERS, \\'\\', 1)\\n+                print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n+                print(filename)\\n+                print(type)\\n+                print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n+            \\'\\'\\'\\n \\n-        p.populate_set_out(paste)\\n-        processed_paste+=1\\n+            p.populate_set_out(paste)\\n+            processed_paste+=1'], 'file': ['bin/Global.py'], 'language': ['Python'], 'temp_id': [UUID('b146c458-fe7e-4fcb-8a01-610631604f93')]}\n",
      "ERROR:root:Error in {'repo': 'CIRCL/AIL-framework', 'vulnerability_id': '2020-8545', 'commit': 'e808840f957c810b8e3944cba808716dc722581b', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -45,8 +45,10 @@ def rreplace(s, old, new, occurrence):\\n \\n     p = Process(config_section)\\n \\n+    # get and sanityze PASTE DIRECTORY\\n     PASTES_FOLDER = os.path.join(os.environ[\\'AIL_HOME\\'], p.config.get(\"Directories\", \"pastes\"))\\n     PASTES_FOLDERS = PASTES_FOLDER + \\'/\\'\\n+    PASTES_FOLDERS = os.path.join(os.path.realpath(PASTES_FOLDERS), \\'\\')\\n \\n     # LOGGING #\\n     publisher.info(\"Feed Script started to receive & publish.\")\\n@@ -75,40 +77,46 @@ def rreplace(s, old, new, occurrence):\\n             time.sleep(1)\\n             continue\\n \\n+        # remove PASTES_FOLDER from item path (crawled item + submited)\\n+        if PASTES_FOLDERS in paste:\\n+            paste = paste.replace(PASTES_FOLDERS, \\'\\', 1)\\n+\\n         file_name_paste = paste.split(\\'/\\')[-1]\\n         if len(file_name_paste)>255:\\n             new_file_name_paste = \\'{}{}.gz\\'.format(file_name_paste[:215], str(uuid.uuid4()))\\n             paste = rreplace(paste, file_name_paste, new_file_name_paste, 1)\\n \\n         # Creating the full filepath\\n         filename = os.path.join(PASTES_FOLDER, paste)\\n+        filename = os.path.realpath(filename)\\n \\n-        dirname = os.path.dirname(filename)\\n-        if not os.path.exists(dirname):\\n-            os.makedirs(dirname)\\n-\\n-        decoded = base64.standard_b64decode(gzip64encoded)\\n+        # incorrect filename\\n+        if not os.path.commonprefix([filename, PASTES_FOLDER]) == PASTES_FOLDER:\\n+            print(\\'Path traversal detected {}\\'.format(filename))\\n+            publisher.warning(\\'Global; Path traversal detected\\')\\n+        else:\\n+            dirname = os.path.dirname(filename)\\n+            if not os.path.exists(dirname):\\n+                os.makedirs(dirname)\\n \\n-        with open(filename, \\'wb\\') as f:\\n-            f.write(decoded)\\n-        \\'\\'\\'try:\\n-            decoded2 = gunzip_bytes_obj(decoded)\\n-        except:\\n-            decoded2 =\\'\\'\\n+            decoded = base64.standard_b64decode(gzip64encoded)\\n \\n-        type = magic.from_buffer(decoded2, mime=True)\\n+            with open(filename, \\'wb\\') as f:\\n+                f.write(decoded)\\n+            \\'\\'\\'try:\\n+                decoded2 = gunzip_bytes_obj(decoded)\\n+            except:\\n+                decoded2 =\\'\\'\\n \\n-        if type!= \\'text/x-c++\\' and type!= \\'text/html\\' and type!= \\'text/x-c\\' and type!= \\'text/x-python\\' and type!= \\'text/x-php\\' and type!= \\'application/xml\\' and type!= \\'text/x-shellscript\\' and type!= \\'text/plain\\' and type!= \\'text/x-diff\\' and type!= \\'text/x-ruby\\':\\n+            type = magic.from_buffer(decoded2, mime=True)\\n \\n-            print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n-            print(filename)\\n-            print(type)\\n-            print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n-        \\'\\'\\'\\n+            if type!= \\'text/x-c++\\' and type!= \\'text/html\\' and type!= \\'text/x-c\\' and type!= \\'text/x-python\\' and type!= \\'text/x-php\\' and type!= \\'application/xml\\' and type!= \\'text/x-shellscript\\' and type!= \\'text/plain\\' and type!= \\'text/x-diff\\' and type!= \\'text/x-ruby\\':\\n \\n-        # remove PASTES_FOLDER from item path (crawled item + submited)\\n-        if PASTES_FOLDERS in paste:\\n-            paste = paste.replace(PASTES_FOLDERS, \\'\\', 1)\\n+                print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n+                print(filename)\\n+                print(type)\\n+                print(\\'-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\\')\\n+            \\'\\'\\'\\n \\n-        p.populate_set_out(paste)\\n-        processed_paste+=1\\n+            p.populate_set_out(paste)\\n+            processed_paste+=1'], 'file': ['bin/Global.py'], 'language': ['Python'], 'temp_id': [UUID('b146c458-fe7e-4fcb-8a01-610631604f93')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 101, in clear_file_content\n",
      "    new_content = remove_comments(content)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 71, in remove_comments\n",
      "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 541, in _generate_tokens_from_c_tokenizer\n",
      "    raise e from None\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 537, in _generate_tokens_from_c_tokenizer\n",
      "    for info in it:\n",
      "  File \"<string>\", line 6\n",
      "    if PASTES_FOLDERS in paste:\n",
      "                               ^\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      "  6%|â–‹         | 117/1800 [00:26<06:27,  4.35it/s]ERROR:src.process_code_changes:Error processing commit f4761f55f7cf6d56d6c5129f921393b0b47fd976\n",
      "ERROR:src.process_code_changes:{'repo': 'pgadmin-org/pgadmin4', 'vulnerability_id': '2024-4215', 'commit': 'f4761f55f7cf6d56d6c5129f921393b0b47fd976', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -0,0 +1,24 @@\\n+##########################################################################\\n+#\\n+# pgAdmin 4 - PostgreSQL Tools\\n+#\\n+# Copyright (C) 2013 - 2024, The pgAdmin Development Team\\n+# This software is released under the PostgreSQL Licence\\n+#\\n+##########################################################################\\n+\\n+\"\"\"Implements pgAdmin4 User validity.\"\"\"\\n+\\n+from functools import wraps\\n+from flask_security import login_required\\n+\\n+\\n+def pga_login_required(func):\\n+    import pgadmin.authenticate.mfa.utils as mfa_utils\\n+\\n+    @wraps(func)\\n+    @mfa_utils.mfa_required\\n+    def wrapper(*args, **kwargs):\\n+        return func(*args, **kwargs)\\n+\\n+    return wrapper', '@@ -18,7 +18,7 @@\\n     current_app, render_template, flash, url_for\\n from flask_security.views import _security\\n from flask_security.utils import logout_user\\n-from flask_security import login_required\\n+from pgadmin.user_login_check import pga_login_required\\n \\n import config\\n from pgadmin.model import User\\n@@ -97,7 +97,7 @@ def kerberos_logout():\\n     @blueprint.route(\"/update_ticket\",\\n                      endpoint=\"update_ticket\", methods=[\"GET\"])\\n     @pgCSRFProtect.exempt\\n-    @login_required\\n+    @pga_login_required\\n     def kerberos_update_ticket():\\n         \"\"\"\\n         Update the kerberos ticket.\\n@@ -127,7 +127,7 @@ def kerberos_update_ticket():\\n     @blueprint.route(\"/validate_ticket\",\\n                      endpoint=\"validate_ticket\", methods=[\"GET\"])\\n     @pgCSRFProtect.exempt\\n-    @login_required\\n+    @pga_login_required\\n     def kerberos_validate_ticket():\\n         \"\"\"\\n         Return the kerberos ticket lifetime left after getting the', '@@ -12,7 +12,8 @@\\n import json\\n from flask_babel import gettext\\n from flask import current_app, request\\n-from flask_security import login_required, current_user\\n+from pgadmin.user_login_check import pga_login_required\\n+from flask_security import current_user\\n from pgadmin.utils.ajax import make_response as ajax_response,\\\\\\n     make_json_response\\n from pgadmin.model import db, Macros, UserMacros', '@@ -13,7 +13,7 @@\\n \\n from flask import url_for, session, request, redirect\\n from flask_login.utils import login_url\\n-from flask_security import current_user\\n+from flask_security import current_user, login_required\\n \\n import config\\n from pgadmin.model import UserMFA, db\\n@@ -279,7 +279,7 @@ def get_next_url():\\n         registration_url = url_for(\\'mfa.register\\')\\n \\n         if next_url.startswith(registration_url):\\n-            return url(\\'browser.index\\')\\n+            return url_for(\\'browser.index\\')\\n \\n         return next_url\\n \\n@@ -290,8 +290,8 @@ def redirect_to_mfa_registration():\\n         return redirect(login_url(\"mfa.register\", next_url=get_next_url()))\\n \\n     @wraps(wrapped)\\n+    @login_required\\n     def inner(*args, **kwargs):\\n-\\n         def execute_func():\\n             session[\\'mfa_authenticated\\'] = True\\n             return wrapped(*args, **kwargs)'], 'file': ['web/pgadmin/user_login_check.py', 'web/pgadmin/authenticate/kerberos.py', 'web/pgadmin/tools/sqleditor/utils/macros.py', 'web/pgadmin/authenticate/mfa/utils.py'], 'language': ['Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('b3a1c8bf-b5ba-4b39-a3d1-90168051d3d5'), UUID('23f2efcc-64c5-4631-b2d8-5984e51cc15b'), UUID('8d56f850-4d6b-495a-a898-cf3393c9b722'), UUID('f2781609-489d-4aad-89b0-b8be77b79145')]}\n",
      "ERROR:root:Error in {'repo': 'pgadmin-org/pgadmin4', 'vulnerability_id': '2024-4215', 'commit': 'f4761f55f7cf6d56d6c5129f921393b0b47fd976', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -0,0 +1,24 @@\\n+##########################################################################\\n+#\\n+# pgAdmin 4 - PostgreSQL Tools\\n+#\\n+# Copyright (C) 2013 - 2024, The pgAdmin Development Team\\n+# This software is released under the PostgreSQL Licence\\n+#\\n+##########################################################################\\n+\\n+\"\"\"Implements pgAdmin4 User validity.\"\"\"\\n+\\n+from functools import wraps\\n+from flask_security import login_required\\n+\\n+\\n+def pga_login_required(func):\\n+    import pgadmin.authenticate.mfa.utils as mfa_utils\\n+\\n+    @wraps(func)\\n+    @mfa_utils.mfa_required\\n+    def wrapper(*args, **kwargs):\\n+        return func(*args, **kwargs)\\n+\\n+    return wrapper', '@@ -18,7 +18,7 @@\\n     current_app, render_template, flash, url_for\\n from flask_security.views import _security\\n from flask_security.utils import logout_user\\n-from flask_security import login_required\\n+from pgadmin.user_login_check import pga_login_required\\n \\n import config\\n from pgadmin.model import User\\n@@ -97,7 +97,7 @@ def kerberos_logout():\\n     @blueprint.route(\"/update_ticket\",\\n                      endpoint=\"update_ticket\", methods=[\"GET\"])\\n     @pgCSRFProtect.exempt\\n-    @login_required\\n+    @pga_login_required\\n     def kerberos_update_ticket():\\n         \"\"\"\\n         Update the kerberos ticket.\\n@@ -127,7 +127,7 @@ def kerberos_update_ticket():\\n     @blueprint.route(\"/validate_ticket\",\\n                      endpoint=\"validate_ticket\", methods=[\"GET\"])\\n     @pgCSRFProtect.exempt\\n-    @login_required\\n+    @pga_login_required\\n     def kerberos_validate_ticket():\\n         \"\"\"\\n         Return the kerberos ticket lifetime left after getting the', '@@ -12,7 +12,8 @@\\n import json\\n from flask_babel import gettext\\n from flask import current_app, request\\n-from flask_security import login_required, current_user\\n+from pgadmin.user_login_check import pga_login_required\\n+from flask_security import current_user\\n from pgadmin.utils.ajax import make_response as ajax_response,\\\\\\n     make_json_response\\n from pgadmin.model import db, Macros, UserMacros', '@@ -13,7 +13,7 @@\\n \\n from flask import url_for, session, request, redirect\\n from flask_login.utils import login_url\\n-from flask_security import current_user\\n+from flask_security import current_user, login_required\\n \\n import config\\n from pgadmin.model import UserMFA, db\\n@@ -279,7 +279,7 @@ def get_next_url():\\n         registration_url = url_for(\\'mfa.register\\')\\n \\n         if next_url.startswith(registration_url):\\n-            return url(\\'browser.index\\')\\n+            return url_for(\\'browser.index\\')\\n \\n         return next_url\\n \\n@@ -290,8 +290,8 @@ def redirect_to_mfa_registration():\\n         return redirect(login_url(\"mfa.register\", next_url=get_next_url()))\\n \\n     @wraps(wrapped)\\n+    @login_required\\n     def inner(*args, **kwargs):\\n-\\n         def execute_func():\\n             session[\\'mfa_authenticated\\'] = True\\n             return wrapped(*args, **kwargs)'], 'file': ['web/pgadmin/user_login_check.py', 'web/pgadmin/authenticate/kerberos.py', 'web/pgadmin/tools/sqleditor/utils/macros.py', 'web/pgadmin/authenticate/mfa/utils.py'], 'language': ['Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('b3a1c8bf-b5ba-4b39-a3d1-90168051d3d5'), UUID('23f2efcc-64c5-4631-b2d8-5984e51cc15b'), UUID('8d56f850-4d6b-495a-a898-cf3393c9b722'), UUID('f2781609-489d-4aad-89b0-b8be77b79145')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @pga_login_required\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @pga_login_required\n",
      "  7%|â–‹         | 118/1800 [00:27<07:09,  3.92it/s]ERROR:src.process_code_changes:Error processing commit 87a6c2c8f45b49ea83fbb5fe8fff7ab5365a60c9\n",
      "ERROR:src.process_code_changes:{'repo': 'zenml-io/zenml', 'vulnerability_id': '2024-4311', 'commit': '87a6c2c8f45b49ea83fbb5fe8fff7ab5365a60c9', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': ['@@ -16,11 +16,13 @@\\n import inspect\\n import time\\n from collections import defaultdict\\n+from contextlib import contextmanager\\n from functools import wraps\\n from typing import (\\n     Any,\\n     Callable,\\n     Dict,\\n+    Generator,\\n     List,\\n     Optional,\\n     TypeVar,\\n@@ -133,6 +135,25 @@ def _get_ipaddr(self, request: Request) -> str:\\n \\n             return request.client.host\\n \\n+    @contextmanager\\n+    def limit_failed_requests(\\n+        self, request: Request\\n+    ) -> Generator[None, Any, Any]:\\n+        \"\"\"Limits the number of failed requests.\\n+\\n+        Args:\\n+            request: Request object.\\n+\\n+        Yields:\\n+            None\\n+        \"\"\"\\n+        self.hit_limiter(request)\\n+\\n+        yield\\n+\\n+        # if request was successful - reset limiter\\n+        self.reset_limiter(request)\\n+\\n \\n def rate_limit_requests(\\n     day_limit: Optional[int] = None,\\n@@ -171,13 +192,8 @@ def decorated(\\n                 request = kwargs[request_kwarg]\\n             else:\\n                 request = args[request_arg]\\n-            limiter.hit_limiter(request)\\n-\\n-            ret = func(*args, **kwargs)\\n-\\n-            # if request was successful - reset limiter\\n-            limiter.reset_limiter(request)\\n-            return ret\\n+            with limiter.limit_failed_requests(request):\\n+                return func(*args, **kwargs)\\n \\n         return cast(F, decorated)\\n ', '@@ -17,6 +17,7 @@\\n from uuid import UUID\\n \\n from fastapi import APIRouter, Depends, Security\\n+from starlette.requests import Request\\n \\n from zenml.analytics.utils import email_opt_int\\n from zenml.constants import (\\n@@ -44,6 +45,7 @@\\n     authorize,\\n )\\n from zenml.zen_server.exceptions import error_response\\n+from zenml.zen_server.rate_limit import RequestLimiter\\n from zenml.zen_server.rbac.endpoint_utils import (\\n     verify_permissions_and_create_entity,\\n )\\n@@ -226,6 +228,10 @@ def get_user(\\n # When the auth scheme is set to EXTERNAL, users cannot be updated via the\\n # API.\\n if server_config().auth_scheme != AuthScheme.EXTERNAL:\\n+    pass_change_limiter = RequestLimiter(\\n+        day_limit=server_config().login_rate_limit_day,\\n+        minute_limit=server_config().login_rate_limit_minute,\\n+    )\\n \\n     @router.put(\\n         \"/{user_name_or_id}\",\\n@@ -240,13 +246,15 @@ def get_user(\\n     def update_user(\\n         user_name_or_id: Union[str, UUID],\\n         user_update: UserUpdate,\\n+        request: Request,\\n         auth_context: AuthContext = Security(authorize),\\n     ) -> UserResponse:\\n         \"\"\"Updates a specific user.\\n \\n         Args:\\n             user_name_or_id: Name or ID of the user.\\n             user_update: the user to use for the update.\\n+            request: The request object.\\n             auth_context: Authentication context.\\n \\n         Returns:\\n@@ -283,13 +291,15 @@ def update_user(\\n                     \"The current password must be supplied when changing the \"\\n                     \"password.\"\\n                 )\\n-            auth_user = zen_store().get_auth_user(user_name_or_id)\\n-            if not UserAuthModel.verify_password(\\n-                user_update.old_password, auth_user\\n-            ):\\n-                raise IllegalOperationError(\\n-                    \"The current password is incorrect.\"\\n-                )\\n+\\n+            with pass_change_limiter.limit_failed_requests(request):\\n+                auth_user = zen_store().get_auth_user(user_name_or_id)\\n+                if not UserAuthModel.verify_password(\\n+                    user_update.old_password, auth_user\\n+                ):\\n+                    raise IllegalOperationError(\\n+                        \"The current password is incorrect.\"\\n+                    )\\n \\n         if (\\n             user_update.is_admin is not None\\n@@ -529,12 +539,14 @@ def get_current_user(\\n     @handle_exceptions\\n     def update_myself(\\n         user: UserUpdate,\\n+        request: Request,\\n         auth_context: AuthContext = Security(authorize),\\n     ) -> UserResponse:\\n         \"\"\"Updates a specific user.\\n \\n         Args:\\n             user: the user to use for the update.\\n+            request: The request object.\\n             auth_context: The authentication context.\\n \\n         Returns:\\n@@ -554,11 +566,14 @@ def update_myself(\\n                     \"The current password must be supplied when changing the \"\\n                     \"password.\"\\n                 )\\n-            auth_user = zen_store().get_auth_user(auth_context.user.id)\\n-            if not UserAuthModel.verify_password(user.old_password, auth_user):\\n-                raise IllegalOperationError(\\n-                    \"The current password is incorrect.\"\\n-                )\\n+            with pass_change_limiter.limit_failed_requests(request):\\n+                auth_user = zen_store().get_auth_user(auth_context.user.id)\\n+                if not UserAuthModel.verify_password(\\n+                    user.old_password, auth_user\\n+                ):\\n+                    raise IllegalOperationError(\\n+                        \"The current password is incorrect.\"\\n+                    )\\n \\n         user.activation_token = current_user.activation_token\\n         user.active = current_user.active'], 'file': ['src/zenml/zen_server/rate_limit.py', 'src/zenml/zen_server/routers/users_endpoints.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('c8276e36-3502-493d-8b64-dc5e8636d42a'), UUID('93cb4305-12e3-4960-88e1-fd6409d5d21e')]}\n",
      "ERROR:root:Error in {'repo': 'zenml-io/zenml', 'vulnerability_id': '2024-4311', 'commit': '87a6c2c8f45b49ea83fbb5fe8fff7ab5365a60c9', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': ['@@ -16,11 +16,13 @@\\n import inspect\\n import time\\n from collections import defaultdict\\n+from contextlib import contextmanager\\n from functools import wraps\\n from typing import (\\n     Any,\\n     Callable,\\n     Dict,\\n+    Generator,\\n     List,\\n     Optional,\\n     TypeVar,\\n@@ -133,6 +135,25 @@ def _get_ipaddr(self, request: Request) -> str:\\n \\n             return request.client.host\\n \\n+    @contextmanager\\n+    def limit_failed_requests(\\n+        self, request: Request\\n+    ) -> Generator[None, Any, Any]:\\n+        \"\"\"Limits the number of failed requests.\\n+\\n+        Args:\\n+            request: Request object.\\n+\\n+        Yields:\\n+            None\\n+        \"\"\"\\n+        self.hit_limiter(request)\\n+\\n+        yield\\n+\\n+        # if request was successful - reset limiter\\n+        self.reset_limiter(request)\\n+\\n \\n def rate_limit_requests(\\n     day_limit: Optional[int] = None,\\n@@ -171,13 +192,8 @@ def decorated(\\n                 request = kwargs[request_kwarg]\\n             else:\\n                 request = args[request_arg]\\n-            limiter.hit_limiter(request)\\n-\\n-            ret = func(*args, **kwargs)\\n-\\n-            # if request was successful - reset limiter\\n-            limiter.reset_limiter(request)\\n-            return ret\\n+            with limiter.limit_failed_requests(request):\\n+                return func(*args, **kwargs)\\n \\n         return cast(F, decorated)\\n ', '@@ -17,6 +17,7 @@\\n from uuid import UUID\\n \\n from fastapi import APIRouter, Depends, Security\\n+from starlette.requests import Request\\n \\n from zenml.analytics.utils import email_opt_int\\n from zenml.constants import (\\n@@ -44,6 +45,7 @@\\n     authorize,\\n )\\n from zenml.zen_server.exceptions import error_response\\n+from zenml.zen_server.rate_limit import RequestLimiter\\n from zenml.zen_server.rbac.endpoint_utils import (\\n     verify_permissions_and_create_entity,\\n )\\n@@ -226,6 +228,10 @@ def get_user(\\n # When the auth scheme is set to EXTERNAL, users cannot be updated via the\\n # API.\\n if server_config().auth_scheme != AuthScheme.EXTERNAL:\\n+    pass_change_limiter = RequestLimiter(\\n+        day_limit=server_config().login_rate_limit_day,\\n+        minute_limit=server_config().login_rate_limit_minute,\\n+    )\\n \\n     @router.put(\\n         \"/{user_name_or_id}\",\\n@@ -240,13 +246,15 @@ def get_user(\\n     def update_user(\\n         user_name_or_id: Union[str, UUID],\\n         user_update: UserUpdate,\\n+        request: Request,\\n         auth_context: AuthContext = Security(authorize),\\n     ) -> UserResponse:\\n         \"\"\"Updates a specific user.\\n \\n         Args:\\n             user_name_or_id: Name or ID of the user.\\n             user_update: the user to use for the update.\\n+            request: The request object.\\n             auth_context: Authentication context.\\n \\n         Returns:\\n@@ -283,13 +291,15 @@ def update_user(\\n                     \"The current password must be supplied when changing the \"\\n                     \"password.\"\\n                 )\\n-            auth_user = zen_store().get_auth_user(user_name_or_id)\\n-            if not UserAuthModel.verify_password(\\n-                user_update.old_password, auth_user\\n-            ):\\n-                raise IllegalOperationError(\\n-                    \"The current password is incorrect.\"\\n-                )\\n+\\n+            with pass_change_limiter.limit_failed_requests(request):\\n+                auth_user = zen_store().get_auth_user(user_name_or_id)\\n+                if not UserAuthModel.verify_password(\\n+                    user_update.old_password, auth_user\\n+                ):\\n+                    raise IllegalOperationError(\\n+                        \"The current password is incorrect.\"\\n+                    )\\n \\n         if (\\n             user_update.is_admin is not None\\n@@ -529,12 +539,14 @@ def get_current_user(\\n     @handle_exceptions\\n     def update_myself(\\n         user: UserUpdate,\\n+        request: Request,\\n         auth_context: AuthContext = Security(authorize),\\n     ) -> UserResponse:\\n         \"\"\"Updates a specific user.\\n \\n         Args:\\n             user: the user to use for the update.\\n+            request: The request object.\\n             auth_context: The authentication context.\\n \\n         Returns:\\n@@ -554,11 +566,14 @@ def update_myself(\\n                     \"The current password must be supplied when changing the \"\\n                     \"password.\"\\n                 )\\n-            auth_user = zen_store().get_auth_user(auth_context.user.id)\\n-            if not UserAuthModel.verify_password(user.old_password, auth_user):\\n-                raise IllegalOperationError(\\n-                    \"The current password is incorrect.\"\\n-                )\\n+            with pass_change_limiter.limit_failed_requests(request):\\n+                auth_user = zen_store().get_auth_user(auth_context.user.id)\\n+                if not UserAuthModel.verify_password(\\n+                    user.old_password, auth_user\\n+                ):\\n+                    raise IllegalOperationError(\\n+                        \"The current password is incorrect.\"\\n+                    )\\n \\n         user.activation_token = current_user.activation_token\\n         user.active = current_user.active'], 'file': ['src/zenml/zen_server/rate_limit.py', 'src/zenml/zen_server/routers/users_endpoints.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('c8276e36-3502-493d-8b64-dc5e8636d42a'), UUID('93cb4305-12e3-4960-88e1-fd6409d5d21e')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0:     pass_change_limiter = RequestLimiter(\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0:     pass_change_limiter = RequestLimiter(\n",
      "  8%|â–Š         | 138/1800 [00:35<11:35,  2.39it/s]ERROR:src.process_code_changes:Error processing commit 785726deee13b4d56f6c3503dd57c1e3eb7d6f30\n",
      "ERROR:src.process_code_changes:{'repo': 'janeczku/calibre-web', 'vulnerability_id': '2021-4164', 'commit': '785726deee13b4d56f6c3503dd57c1e3eb7d6f30', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -62,6 +62,7 @@\\n from binascii import hexlify\\n from datetime import datetime\\n from os import urandom\\n+from functools import wraps\\n \\n from flask import g, Blueprint, url_for, abort, request\\n from flask_login import login_user, current_user, login_required\\n@@ -70,11 +71,6 @@\\n from . import logger, config, calibre_db, db, helper, ub, lm\\n from .render_template import render_title_template\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We\\'re not using Python 3\\n-\\n \\n log = logger.create()\\n \\n@@ -167,7 +163,7 @@ def generate_auth_token(user_id):\\n         )\\n \\n \\n-@kobo_auth.route(\"/deleteauthtoken/<int:user_id>\")\\n+@kobo_auth.route(\"/deleteauthtoken/<int:user_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_auth_token(user_id):\\n     # Invalidate any prevously generated Kobo Auth token for this user.', '@@ -129,11 +129,11 @@ def admin_forbidden():\\n     abort(403)\\n \\n \\n-@admi.route(\"/shutdown\")\\n+@admi.route(\"/shutdown\", methods=[\"POST\"])\\n @login_required\\n @admin_required\\n def shutdown():\\n-    task = int(request.args.get(\"parameter\").strip())\\n+    task = request.get_json().get(\\'parameter\\', -1)\\n     showtext = {}\\n     if task in (0, 1):  # valid commandos received\\n         # close all database connections\\n@@ -906,7 +906,7 @@ def list_restriction(res_type, user_id):\\n     response.headers[\"Content-Type\"] = \"application/json; charset=utf-8\"\\n     return response\\n \\n-@admi.route(\"/ajax/fullsync\")\\n+@admi.route(\"/ajax/fullsync\", methods=[\"POST\"])\\n @login_required\\n def ajax_fullsync():\\n     count = ub.session.query(ub.KoboSyncedBooks).filter(current_user.id == ub.KoboSyncedBooks.user_id).delete()\\n@@ -1626,7 +1626,7 @@ def edit_user(user_id):\\n                                  page=\"edituser\")\\n \\n \\n-@admi.route(\"/admin/resetpassword/<int:user_id>\")\\n+@admi.route(\"/admin/resetpassword/<int:user_id>\", methods=[\"POST\"])\\n @login_required\\n @admin_required\\n def reset_user_password(user_id):\\n@@ -1802,7 +1802,7 @@ def ldap_import_create_user(user, user_data):\\n         return 0, message\\n \\n \\n-@admi.route(\\'/import_ldap_users\\')\\n+@admi.route(\\'/import_ldap_users\\', methods=[\"POST\"])\\n @login_required\\n @admin_required\\n def import_ldap_users():', '@@ -179,7 +179,7 @@ $(\"#delete_confirm\").click(function() {\\n         if (ajaxResponse) {\\n             path = getPath() + \"/ajax/delete/\" + deleteId;\\n             $.ajax({\\n-                method:\"get\",\\n+                method:\"post\",\\n                 url: path,\\n                 timeout: 900,\\n                 success:function(data) {\\n@@ -376,9 +376,11 @@ $(function() {\\n \\n     $(\"#restart\").click(function() {\\n         $.ajax({\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n-            url: window.location.pathname + \"/../../shutdown\",\\n-            data: {\"parameter\":0},\\n+            url: getPath() + \"/shutdown\",\\n+            data: JSON.stringify({\"parameter\":0}),\\n             success: function success() {\\n                 $(\"#spinner\").show();\\n                 setTimeout(restartTimer, 3000);\\n@@ -387,9 +389,11 @@ $(function() {\\n     });\\n     $(\"#shutdown\").click(function() {\\n         $.ajax({\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n-            url: window.location.pathname + \"/../../shutdown\",\\n-            data: {\"parameter\":1},\\n+            url: getPath() + \"/shutdown\",\\n+            data: JSON.stringify({\"parameter\":1}),\\n             success: function success(data) {\\n                 return alert(data.text);\\n             }\\n@@ -447,9 +451,11 @@ $(function() {\\n         $(\"#DialogContent\").html(\"\");\\n         $(\"#spinner2\").show();\\n         $.ajax({\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n             url: getPath() + \"/shutdown\",\\n-            data: {\"parameter\":2},\\n+            data: JSON.stringify({\"parameter\":2}),\\n             success: function success(data) {\\n                 $(\"#spinner2\").hide();\\n                 $(\"#DialogContent\").html(data.text);\\n@@ -527,7 +533,7 @@ $(function() {\\n             $(this).data(\\'value\\'),\\n             function (value) {\\n                 $.ajax({\\n-                    method: \"get\",\\n+                    method: \"post\",\\n                     url: getPath() + \"/kobo_auth/deleteauthtoken/\" + value,\\n                 });\\n                 $(\"#config_delete_kobo_token\").hide();\\n@@ -574,7 +580,7 @@ $(function() {\\n             function(value){\\n                 path = getPath() + \"/ajax/fullsync\"\\n                 $.ajax({\\n-                    method:\"get\",\\n+                    method:\"post\",\\n                     url: path,\\n                     timeout: 900,\\n                     success:function(data) {\\n@@ -638,7 +644,7 @@ $(function() {\\n                     else {\\n                         $(\"#InvalidDialog\").modal(\\'show\\');\\n                     }\\n-                } else {                \\t\\n+                } else {\\n                     changeDbSettings();\\n                 }\\n             }\\n@@ -685,7 +691,7 @@ $(function() {\\n             \"GeneralDeleteModal\",\\n             $(this).data(\\'value\\'),\\n             function(value){\\n-                window.location.href = window.location.pathname + \"/../../shelf/delete/\" + value\\n+                $(\"#delete_shelf\").closest(\"form\").submit()\\n             }\\n         );\\n \\n@@ -734,7 +740,8 @@ $(function() {\\n         $(\"#DialogContent\").html(\"\");\\n         $(\"#spinner2\").show();\\n         $.ajax({\\n-            method:\"get\",\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n             url: getPath() + \"/import_ldap_users\",\\n             success: function success(data) {', '@@ -2,14 +2,16 @@\\n {% block body %}\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n+      <form action=\"{{url_for(\\'shelf.delete_shelf\\', shelf_id=shelf.id)}}\" method=\"post\">\\n   {% if g.user.role_download() %}\\n   <a id=\"shelf_down\" href=\"{{ url_for(\\'shelf.show_simpleshelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-primary\">{{ _(\\'Download\\') }} </a>\\n       {% endif %}\\n   {% if g.user.is_authenticated %}\\n     {% if (g.user.role_edit_shelfs() and shelf.is_public ) or not shelf.is_public  %}\\n-      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n-      <div class=\"btn btn-danger\" id=\"delete_shelf\" data-value=\"{{ shelf.id }}\">{{ _(\\'Delete this Shelf\\') }}</div>\\n+        <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n+        <div class=\"btn btn-danger\" id=\"delete_shelf\" data-value=\"{{ shelf.id }}\">{{ _(\\'Delete this Shelf\\') }}</div>\\n       <a id=\"edit_shelf\" href=\"{{ url_for(\\'shelf.edit_shelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-primary\">{{ _(\\'Edit Shelf Properties\\') }} </a>\\n+      </form>\\n       {% if entries.__len__() %}\\n       <a id=\"order_shelf\" href=\"{{ url_for(\\'shelf.order_shelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-primary\">{{ _(\\'Arrange books manually\\') }} </a>\\n       <button id=\"toggle_order_shelf\" type=\"button\" data-alt-text=\"{{ _(\\'Disable Change order\\') }}\" class=\"btn btn-primary\">{{ _(\\'Enable Change order\\') }}</button>\\n@@ -84,22 +86,6 @@ <h2>{{title}}</h2>\\n     {% endfor %}\\n   </div>\\n </div>\\n-<!--div id=\"DeleteShelfDialog\" class=\"modal fade\" role=\"dialog\">\\n-  <div class=\"modal-dialog modal-sm\">\\n-    <div class=\"modal-content\">\\n-      <div class=\"modal-header bg-danger text-center\">\\n-      <span>{{_(\\'Are you sure you want to delete this shelf?\\')}}</span>\\n-      </div>\\n-      <div class=\"modal-body text-center\">\\n-        <span>{{_(\\'Shelf will be deleted for all users\\')}}</span>\\n-          <p></p>\\n-        <a id=\"confirm\" href=\"{{ url_for(\\'shelf.delete_shelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-danger\">{{_(\\'OK\\')}}</a>\\n-        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">{{_(\\'Cancel\\')}}</button>\\n-      </div>\\n-    </div>\\n-  </div>\\n-</div-->\\n-\\n {% endblock %}\\n {% block modal %}\\n {{ delete_confirm_modal() }}', '@@ -1055,7 +1055,8 @@ def get_tasks_status():\\n     return render_title_template(\\'tasks.html\\', entries=answer, title=_(u\"Tasks\"), page=\"tasks\")\\n \\n \\n-@app.route(\"/reconnect\")\\n+# method is available without login and not protected by CSRF to make it easy reachable\\n+@app.route(\"/reconnect\", methods=[\\'GET\\'])\\n def reconnect():\\n     calibre_db.reconnect_db(config, ub.app_DB_path)\\n     return json.dumps({})\\n@@ -1435,7 +1436,7 @@ def download_link(book_id, book_format, anyname):\\n     return get_download_link(book_id, book_format, client)\\n \\n \\n-@web.route(\\'/send/<int:book_id>/<book_format>/<int:convert>\\')\\n+@web.route(\\'/send/<int:book_id>/<book_format>/<int:convert>\\', methods=[\"POST\"])\\n @login_required\\n @download_required\\n def send_to_kindle(book_id, book_format, convert):', '@@ -56,7 +56,7 @@ def check_shelf_view_permissions(cur_shelf):\\n     return True\\n \\n \\n-@shelf.route(\"/shelf/add/<int:shelf_id>/<int:book_id>\")\\n+@shelf.route(\"/shelf/add/<int:shelf_id>/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def add_to_shelf(shelf_id, book_id):\\n     xhr = request.headers.get(\\'X-Requested-With\\') == \\'XMLHttpRequest\\'\\n@@ -112,7 +112,7 @@ def add_to_shelf(shelf_id, book_id):\\n     return \"\", 204\\n \\n \\n-@shelf.route(\"/shelf/massadd/<int:shelf_id>\")\\n+@shelf.route(\"/shelf/massadd/<int:shelf_id>\", methods=[\"POST\"])\\n @login_required\\n def search_to_shelf(shelf_id):\\n     shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\\n@@ -164,7 +164,7 @@ def search_to_shelf(shelf_id):\\n     return redirect(url_for(\\'web.index\\'))\\n \\n \\n-@shelf.route(\"/shelf/remove/<int:shelf_id>/<int:book_id>\")\\n+@shelf.route(\"/shelf/remove/<int:shelf_id>/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def remove_from_shelf(shelf_id, book_id):\\n     xhr = request.headers.get(\\'X-Requested-With\\') == \\'XMLHttpRequest\\'\\n@@ -323,12 +323,13 @@ def delete_shelf_helper(cur_shelf):\\n     ub.session_commit(\"successfully deleted Shelf {}\".format(cur_shelf.name))\\n \\n \\n-@shelf.route(\"/shelf/delete/<int:shelf_id>\")\\n+@shelf.route(\"/shelf/delete/<int:shelf_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_shelf(shelf_id):\\n     cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\\n     try:\\n         delete_shelf_helper(cur_shelf)\\n+        flash(_(\"Shelf successfully deleted\"), category=\"success\")\\n     except InvalidRequestError:\\n         ub.session.rollback()\\n         log.error(\"Settings DB is not Writeable\")', '@@ -26,6 +26,8 @@\\n from shutil import copyfile\\n from uuid import uuid4\\n from markupsafe import escape\\n+from functools import wraps\\n+\\n try:\\n     from lxml.html.clean import clean_html\\n except ImportError:\\n@@ -51,13 +53,6 @@\\n from .render_template import render_title_template\\n from .usermanagement import login_required_if_no_ano\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We\\'re not using Python 3\\n-\\n-\\n-\\n \\n editbook = Blueprint(\\'editbook\\', __name__)\\n log = logger.create()\\n@@ -237,14 +232,14 @@ def modify_identifiers(input_identifiers, db_identifiers, db_session):\\n             changed = True\\n     return changed, error\\n \\n-@editbook.route(\"/ajax/delete/<int:book_id>\")\\n+@editbook.route(\"/ajax/delete/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_from_details(book_id):\\n     return Response(delete_book_from_table(book_id, \"\", True), mimetype=\\'application/json\\')\\n \\n \\n-@editbook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"})\\n-@editbook.route(\"/delete/<int:book_id>/<string:book_format>\")\\n+@editbook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"}, methods=[\"POST\"])\\n+@editbook.route(\"/delete/<int:book_id>/<string:book_format>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_ajax(book_id, book_format):\\n     return delete_book_from_table(book_id, book_format, False)\\n@@ -1014,7 +1009,7 @@ def move_coverfile(meta, db_book):\\n               category=\"error\")\\n \\n \\n-@editbook.route(\"/upload\", methods=[\"GET\", \"POST\"])\\n+@editbook.route(\"/upload\", methods=[\"POST\"])\\n @login_required_if_no_ano\\n @upload_required\\n def upload():'], 'file': ['cps/kobo_auth.py', 'cps/admin.py', 'cps/static/js/main.js', 'cps/templates/shelf.html', 'cps/web.py', 'cps/shelf.py', 'cps/editbooks.py'], 'language': ['Python', 'Python', 'JavaScript/TypeScript', 'HTML', 'Python', 'Python', 'Python'], 'temp_id': [UUID('ac4d35d3-74ae-4d5d-84b6-ae1a8fe1e09c'), UUID('0f6bd739-cd38-4d48-86ef-67248e76ac18'), UUID('1d50a26f-f1ab-4d5a-a0e0-4146a721a63c'), UUID('8927679a-e492-4784-92d2-5b7897f73e4b'), UUID('e0a14e4c-704d-407c-8182-619ced1c409e'), UUID('c729f638-00ce-438e-a84c-db5a64b0ce42'), UUID('91b4f5ed-0540-49b5-bce4-eacdd07704de')]}\n",
      "ERROR:root:Error in {'repo': 'janeczku/calibre-web', 'vulnerability_id': '2021-4164', 'commit': '785726deee13b4d56f6c3503dd57c1e3eb7d6f30', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -62,6 +62,7 @@\\n from binascii import hexlify\\n from datetime import datetime\\n from os import urandom\\n+from functools import wraps\\n \\n from flask import g, Blueprint, url_for, abort, request\\n from flask_login import login_user, current_user, login_required\\n@@ -70,11 +71,6 @@\\n from . import logger, config, calibre_db, db, helper, ub, lm\\n from .render_template import render_title_template\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We\\'re not using Python 3\\n-\\n \\n log = logger.create()\\n \\n@@ -167,7 +163,7 @@ def generate_auth_token(user_id):\\n         )\\n \\n \\n-@kobo_auth.route(\"/deleteauthtoken/<int:user_id>\")\\n+@kobo_auth.route(\"/deleteauthtoken/<int:user_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_auth_token(user_id):\\n     # Invalidate any prevously generated Kobo Auth token for this user.', '@@ -129,11 +129,11 @@ def admin_forbidden():\\n     abort(403)\\n \\n \\n-@admi.route(\"/shutdown\")\\n+@admi.route(\"/shutdown\", methods=[\"POST\"])\\n @login_required\\n @admin_required\\n def shutdown():\\n-    task = int(request.args.get(\"parameter\").strip())\\n+    task = request.get_json().get(\\'parameter\\', -1)\\n     showtext = {}\\n     if task in (0, 1):  # valid commandos received\\n         # close all database connections\\n@@ -906,7 +906,7 @@ def list_restriction(res_type, user_id):\\n     response.headers[\"Content-Type\"] = \"application/json; charset=utf-8\"\\n     return response\\n \\n-@admi.route(\"/ajax/fullsync\")\\n+@admi.route(\"/ajax/fullsync\", methods=[\"POST\"])\\n @login_required\\n def ajax_fullsync():\\n     count = ub.session.query(ub.KoboSyncedBooks).filter(current_user.id == ub.KoboSyncedBooks.user_id).delete()\\n@@ -1626,7 +1626,7 @@ def edit_user(user_id):\\n                                  page=\"edituser\")\\n \\n \\n-@admi.route(\"/admin/resetpassword/<int:user_id>\")\\n+@admi.route(\"/admin/resetpassword/<int:user_id>\", methods=[\"POST\"])\\n @login_required\\n @admin_required\\n def reset_user_password(user_id):\\n@@ -1802,7 +1802,7 @@ def ldap_import_create_user(user, user_data):\\n         return 0, message\\n \\n \\n-@admi.route(\\'/import_ldap_users\\')\\n+@admi.route(\\'/import_ldap_users\\', methods=[\"POST\"])\\n @login_required\\n @admin_required\\n def import_ldap_users():', '@@ -179,7 +179,7 @@ $(\"#delete_confirm\").click(function() {\\n         if (ajaxResponse) {\\n             path = getPath() + \"/ajax/delete/\" + deleteId;\\n             $.ajax({\\n-                method:\"get\",\\n+                method:\"post\",\\n                 url: path,\\n                 timeout: 900,\\n                 success:function(data) {\\n@@ -376,9 +376,11 @@ $(function() {\\n \\n     $(\"#restart\").click(function() {\\n         $.ajax({\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n-            url: window.location.pathname + \"/../../shutdown\",\\n-            data: {\"parameter\":0},\\n+            url: getPath() + \"/shutdown\",\\n+            data: JSON.stringify({\"parameter\":0}),\\n             success: function success() {\\n                 $(\"#spinner\").show();\\n                 setTimeout(restartTimer, 3000);\\n@@ -387,9 +389,11 @@ $(function() {\\n     });\\n     $(\"#shutdown\").click(function() {\\n         $.ajax({\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n-            url: window.location.pathname + \"/../../shutdown\",\\n-            data: {\"parameter\":1},\\n+            url: getPath() + \"/shutdown\",\\n+            data: JSON.stringify({\"parameter\":1}),\\n             success: function success(data) {\\n                 return alert(data.text);\\n             }\\n@@ -447,9 +451,11 @@ $(function() {\\n         $(\"#DialogContent\").html(\"\");\\n         $(\"#spinner2\").show();\\n         $.ajax({\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n             url: getPath() + \"/shutdown\",\\n-            data: {\"parameter\":2},\\n+            data: JSON.stringify({\"parameter\":2}),\\n             success: function success(data) {\\n                 $(\"#spinner2\").hide();\\n                 $(\"#DialogContent\").html(data.text);\\n@@ -527,7 +533,7 @@ $(function() {\\n             $(this).data(\\'value\\'),\\n             function (value) {\\n                 $.ajax({\\n-                    method: \"get\",\\n+                    method: \"post\",\\n                     url: getPath() + \"/kobo_auth/deleteauthtoken/\" + value,\\n                 });\\n                 $(\"#config_delete_kobo_token\").hide();\\n@@ -574,7 +580,7 @@ $(function() {\\n             function(value){\\n                 path = getPath() + \"/ajax/fullsync\"\\n                 $.ajax({\\n-                    method:\"get\",\\n+                    method:\"post\",\\n                     url: path,\\n                     timeout: 900,\\n                     success:function(data) {\\n@@ -638,7 +644,7 @@ $(function() {\\n                     else {\\n                         $(\"#InvalidDialog\").modal(\\'show\\');\\n                     }\\n-                } else {                \\t\\n+                } else {\\n                     changeDbSettings();\\n                 }\\n             }\\n@@ -685,7 +691,7 @@ $(function() {\\n             \"GeneralDeleteModal\",\\n             $(this).data(\\'value\\'),\\n             function(value){\\n-                window.location.href = window.location.pathname + \"/../../shelf/delete/\" + value\\n+                $(\"#delete_shelf\").closest(\"form\").submit()\\n             }\\n         );\\n \\n@@ -734,7 +740,8 @@ $(function() {\\n         $(\"#DialogContent\").html(\"\");\\n         $(\"#spinner2\").show();\\n         $.ajax({\\n-            method:\"get\",\\n+            method:\"post\",\\n+            contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n             url: getPath() + \"/import_ldap_users\",\\n             success: function success(data) {', '@@ -2,14 +2,16 @@\\n {% block body %}\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n+      <form action=\"{{url_for(\\'shelf.delete_shelf\\', shelf_id=shelf.id)}}\" method=\"post\">\\n   {% if g.user.role_download() %}\\n   <a id=\"shelf_down\" href=\"{{ url_for(\\'shelf.show_simpleshelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-primary\">{{ _(\\'Download\\') }} </a>\\n       {% endif %}\\n   {% if g.user.is_authenticated %}\\n     {% if (g.user.role_edit_shelfs() and shelf.is_public ) or not shelf.is_public  %}\\n-      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n-      <div class=\"btn btn-danger\" id=\"delete_shelf\" data-value=\"{{ shelf.id }}\">{{ _(\\'Delete this Shelf\\') }}</div>\\n+        <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n+        <div class=\"btn btn-danger\" id=\"delete_shelf\" data-value=\"{{ shelf.id }}\">{{ _(\\'Delete this Shelf\\') }}</div>\\n       <a id=\"edit_shelf\" href=\"{{ url_for(\\'shelf.edit_shelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-primary\">{{ _(\\'Edit Shelf Properties\\') }} </a>\\n+      </form>\\n       {% if entries.__len__() %}\\n       <a id=\"order_shelf\" href=\"{{ url_for(\\'shelf.order_shelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-primary\">{{ _(\\'Arrange books manually\\') }} </a>\\n       <button id=\"toggle_order_shelf\" type=\"button\" data-alt-text=\"{{ _(\\'Disable Change order\\') }}\" class=\"btn btn-primary\">{{ _(\\'Enable Change order\\') }}</button>\\n@@ -84,22 +86,6 @@ <h2>{{title}}</h2>\\n     {% endfor %}\\n   </div>\\n </div>\\n-<!--div id=\"DeleteShelfDialog\" class=\"modal fade\" role=\"dialog\">\\n-  <div class=\"modal-dialog modal-sm\">\\n-    <div class=\"modal-content\">\\n-      <div class=\"modal-header bg-danger text-center\">\\n-      <span>{{_(\\'Are you sure you want to delete this shelf?\\')}}</span>\\n-      </div>\\n-      <div class=\"modal-body text-center\">\\n-        <span>{{_(\\'Shelf will be deleted for all users\\')}}</span>\\n-          <p></p>\\n-        <a id=\"confirm\" href=\"{{ url_for(\\'shelf.delete_shelf\\', shelf_id=shelf.id) }}\" class=\"btn btn-danger\">{{_(\\'OK\\')}}</a>\\n-        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">{{_(\\'Cancel\\')}}</button>\\n-      </div>\\n-    </div>\\n-  </div>\\n-</div-->\\n-\\n {% endblock %}\\n {% block modal %}\\n {{ delete_confirm_modal() }}', '@@ -1055,7 +1055,8 @@ def get_tasks_status():\\n     return render_title_template(\\'tasks.html\\', entries=answer, title=_(u\"Tasks\"), page=\"tasks\")\\n \\n \\n-@app.route(\"/reconnect\")\\n+# method is available without login and not protected by CSRF to make it easy reachable\\n+@app.route(\"/reconnect\", methods=[\\'GET\\'])\\n def reconnect():\\n     calibre_db.reconnect_db(config, ub.app_DB_path)\\n     return json.dumps({})\\n@@ -1435,7 +1436,7 @@ def download_link(book_id, book_format, anyname):\\n     return get_download_link(book_id, book_format, client)\\n \\n \\n-@web.route(\\'/send/<int:book_id>/<book_format>/<int:convert>\\')\\n+@web.route(\\'/send/<int:book_id>/<book_format>/<int:convert>\\', methods=[\"POST\"])\\n @login_required\\n @download_required\\n def send_to_kindle(book_id, book_format, convert):', '@@ -56,7 +56,7 @@ def check_shelf_view_permissions(cur_shelf):\\n     return True\\n \\n \\n-@shelf.route(\"/shelf/add/<int:shelf_id>/<int:book_id>\")\\n+@shelf.route(\"/shelf/add/<int:shelf_id>/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def add_to_shelf(shelf_id, book_id):\\n     xhr = request.headers.get(\\'X-Requested-With\\') == \\'XMLHttpRequest\\'\\n@@ -112,7 +112,7 @@ def add_to_shelf(shelf_id, book_id):\\n     return \"\", 204\\n \\n \\n-@shelf.route(\"/shelf/massadd/<int:shelf_id>\")\\n+@shelf.route(\"/shelf/massadd/<int:shelf_id>\", methods=[\"POST\"])\\n @login_required\\n def search_to_shelf(shelf_id):\\n     shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\\n@@ -164,7 +164,7 @@ def search_to_shelf(shelf_id):\\n     return redirect(url_for(\\'web.index\\'))\\n \\n \\n-@shelf.route(\"/shelf/remove/<int:shelf_id>/<int:book_id>\")\\n+@shelf.route(\"/shelf/remove/<int:shelf_id>/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def remove_from_shelf(shelf_id, book_id):\\n     xhr = request.headers.get(\\'X-Requested-With\\') == \\'XMLHttpRequest\\'\\n@@ -323,12 +323,13 @@ def delete_shelf_helper(cur_shelf):\\n     ub.session_commit(\"successfully deleted Shelf {}\".format(cur_shelf.name))\\n \\n \\n-@shelf.route(\"/shelf/delete/<int:shelf_id>\")\\n+@shelf.route(\"/shelf/delete/<int:shelf_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_shelf(shelf_id):\\n     cur_shelf = ub.session.query(ub.Shelf).filter(ub.Shelf.id == shelf_id).first()\\n     try:\\n         delete_shelf_helper(cur_shelf)\\n+        flash(_(\"Shelf successfully deleted\"), category=\"success\")\\n     except InvalidRequestError:\\n         ub.session.rollback()\\n         log.error(\"Settings DB is not Writeable\")', '@@ -26,6 +26,8 @@\\n from shutil import copyfile\\n from uuid import uuid4\\n from markupsafe import escape\\n+from functools import wraps\\n+\\n try:\\n     from lxml.html.clean import clean_html\\n except ImportError:\\n@@ -51,13 +53,6 @@\\n from .render_template import render_title_template\\n from .usermanagement import login_required_if_no_ano\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We\\'re not using Python 3\\n-\\n-\\n-\\n \\n editbook = Blueprint(\\'editbook\\', __name__)\\n log = logger.create()\\n@@ -237,14 +232,14 @@ def modify_identifiers(input_identifiers, db_identifiers, db_session):\\n             changed = True\\n     return changed, error\\n \\n-@editbook.route(\"/ajax/delete/<int:book_id>\")\\n+@editbook.route(\"/ajax/delete/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_from_details(book_id):\\n     return Response(delete_book_from_table(book_id, \"\", True), mimetype=\\'application/json\\')\\n \\n \\n-@editbook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"})\\n-@editbook.route(\"/delete/<int:book_id>/<string:book_format>\")\\n+@editbook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"}, methods=[\"POST\"])\\n+@editbook.route(\"/delete/<int:book_id>/<string:book_format>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_ajax(book_id, book_format):\\n     return delete_book_from_table(book_id, book_format, False)\\n@@ -1014,7 +1009,7 @@ def move_coverfile(meta, db_book):\\n               category=\"error\")\\n \\n \\n-@editbook.route(\"/upload\", methods=[\"GET\", \"POST\"])\\n+@editbook.route(\"/upload\", methods=[\"POST\"])\\n @login_required_if_no_ano\\n @upload_required\\n def upload():'], 'file': ['cps/kobo_auth.py', 'cps/admin.py', 'cps/static/js/main.js', 'cps/templates/shelf.html', 'cps/web.py', 'cps/shelf.py', 'cps/editbooks.py'], 'language': ['Python', 'Python', 'JavaScript/TypeScript', 'HTML', 'Python', 'Python', 'Python'], 'temp_id': [UUID('ac4d35d3-74ae-4d5d-84b6-ae1a8fe1e09c'), UUID('0f6bd739-cd38-4d48-86ef-67248e76ac18'), UUID('1d50a26f-f1ab-4d5a-a0e0-4146a721a63c'), UUID('8927679a-e492-4784-92d2-5b7897f73e4b'), UUID('e0a14e4c-704d-407c-8182-619ced1c409e'), UUID('c729f638-00ce-438e-a84c-db5a64b0ce42'), UUID('91b4f5ed-0540-49b5-bce4-eacdd07704de')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      "  8%|â–Š         | 145/1800 [00:36<07:48,  3.53it/s]ERROR:src.process_code_changes:Error processing commit 5b047b645f5f93900d5e2fc31230848c25eb1f5f\n",
      "ERROR:src.process_code_changes:{'repo': 'urllib3/urllib3', 'vulnerability_id': '2021-33503', 'commit': '5b047b645f5f93900d5e2fc31230848c25eb1f5f', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -1,262 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module for the regular expressions crafted from ABNF.\"\"\"\\n-\\n-import sys\\n-\\n-# https://tools.ietf.org/html/rfc3986#page-13\\n-GEN_DELIMS = GENERIC_DELIMITERS = \":/?#[]@\"\\n-GENERIC_DELIMITERS_SET = set(GENERIC_DELIMITERS)\\n-# https://tools.ietf.org/html/rfc3986#page-13\\n-SUB_DELIMS = SUB_DELIMITERS = \"!$&\\'()*+,;=\"\\n-SUB_DELIMITERS_SET = set(SUB_DELIMITERS)\\n-# Escape the \\'*\\' for use in regular expressions\\n-SUB_DELIMITERS_RE = r\"!$&\\'()\\\\*+,;=\"\\n-RESERVED_CHARS_SET = GENERIC_DELIMITERS_SET.union(SUB_DELIMITERS_SET)\\n-ALPHA = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\\n-DIGIT = \"0123456789\"\\n-# https://tools.ietf.org/html/rfc3986#section-2.3\\n-UNRESERVED = UNRESERVED_CHARS = ALPHA + DIGIT + r\"._!-\"\\n-UNRESERVED_CHARS_SET = set(UNRESERVED_CHARS)\\n-NON_PCT_ENCODED_SET = RESERVED_CHARS_SET.union(UNRESERVED_CHARS_SET)\\n-# We need to escape the \\'-\\' in this case:\\n-UNRESERVED_RE = r\"A-Za-z0-9._~\\\\-\"\\n-\\n-# Percent encoded character values\\n-PERCENT_ENCODED = PCT_ENCODED = \"%[A-Fa-f0-9]{2}\"\\n-PCHAR = \"([\" + UNRESERVED_RE + SUB_DELIMITERS_RE + \":@]|%s)\" % PCT_ENCODED\\n-\\n-# NOTE(sigmavirus24): We\\'re going to use more strict regular expressions\\n-# than appear in Appendix B for scheme. This will prevent over-eager\\n-# consuming of items that aren\\'t schemes.\\n-SCHEME_RE = \"[a-zA-Z][a-zA-Z0-9+.-]*\"\\n-_AUTHORITY_RE = \"[^/?#]*\"\\n-_PATH_RE = \"[^?#]*\"\\n-_QUERY_RE = \"[^#]*\"\\n-_FRAGMENT_RE = \".*\"\\n-\\n-# Extracted from http://tools.ietf.org/html/rfc3986#appendix-B\\n-COMPONENT_PATTERN_DICT = {\\n-    \"scheme\": SCHEME_RE,\\n-    \"authority\": _AUTHORITY_RE,\\n-    \"path\": _PATH_RE,\\n-    \"query\": _QUERY_RE,\\n-    \"fragment\": _FRAGMENT_RE,\\n-}\\n-\\n-# See http://tools.ietf.org/html/rfc3986#appendix-B\\n-# In this case, we name each of the important matches so we can use\\n-# SRE_Match#groupdict to parse the values out if we so choose. This is also\\n-# modified to ignore other matches that are not important to the parsing of\\n-# the reference so we can also simply use SRE_Match#groups.\\n-URL_PARSING_RE = (\\n-    r\"(?:(?P<scheme>{scheme}):)?(?://(?P<authority>{authority}))?\"\\n-    r\"(?P<path>{path})(?:\\\\?(?P<query>{query}))?\"\\n-    r\"(?:#(?P<fragment>{fragment}))?\"\\n-).format(**COMPONENT_PATTERN_DICT)\\n-\\n-\\n-# #########################\\n-# Authority Matcher Section\\n-# #########################\\n-\\n-# Host patterns, see: http://tools.ietf.org/html/rfc3986#section-3.2.2\\n-# The pattern for a regular name, e.g.,  www.google.com, api.github.com\\n-REGULAR_NAME_RE = REG_NAME = \"((?:{0}|[{1}])*)\".format(\\n-    \"%[0-9A-Fa-f]{2}\", SUB_DELIMITERS_RE + UNRESERVED_RE\\n-)\\n-# The pattern for an IPv4 address, e.g., 192.168.255.255, 127.0.0.1,\\n-IPv4_RE = r\"([0-9]{1,3}\\\\.){3}[0-9]{1,3}\"\\n-# Hexadecimal characters used in each piece of an IPv6 address\\n-HEXDIG_RE = \"[0-9A-Fa-f]{1,4}\"\\n-# Least-significant 32 bits of an IPv6 address\\n-LS32_RE = \"({hex}:{hex}|{ipv4})\".format(hex=HEXDIG_RE, ipv4=IPv4_RE)\\n-# Substitutions into the following patterns for IPv6 patterns defined\\n-# http://tools.ietf.org/html/rfc3986#page-20\\n-_subs = {\"hex\": HEXDIG_RE, \"ls32\": LS32_RE}\\n-\\n-# Below: h16 = hexdig, see: https://tools.ietf.org/html/rfc5234 for details\\n-# about ABNF (Augmented Backus-Naur Form) use in the comments\\n-variations = [\\n-    #                            6( h16 \":\" ) ls32\\n-    \"(%(hex)s:){6}%(ls32)s\" % _subs,\\n-    #                       \"::\" 5( h16 \":\" ) ls32\\n-    \"::(%(hex)s:){5}%(ls32)s\" % _subs,\\n-    # [               h16 ] \"::\" 4( h16 \":\" ) ls32\\n-    \"(%(hex)s)?::(%(hex)s:){4}%(ls32)s\" % _subs,\\n-    # [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\\n-    \"((%(hex)s:)?%(hex)s)?::(%(hex)s:){3}%(ls32)s\" % _subs,\\n-    # [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\\n-    \"((%(hex)s:){0,2}%(hex)s)?::(%(hex)s:){2}%(ls32)s\" % _subs,\\n-    # [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\\n-    \"((%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s\" % _subs,\\n-    # [ *4( h16 \":\" ) h16 ] \"::\"              ls32\\n-    \"((%(hex)s:){0,4}%(hex)s)?::%(ls32)s\" % _subs,\\n-    # [ *5( h16 \":\" ) h16 ] \"::\"              h16\\n-    \"((%(hex)s:){0,5}%(hex)s)?::%(hex)s\" % _subs,\\n-    # [ *6( h16 \":\" ) h16 ] \"::\"\\n-    \"((%(hex)s:){0,6}%(hex)s)?::\" % _subs,\\n-]\\n-\\n-IPv6_RE = \"(({0})|({1})|({2})|({3})|({4})|({5})|({6})|({7})|({8}))\".format(*variations)\\n-\\n-IPv_FUTURE_RE = r\"v[0-9A-Fa-f]+\\\\.[%s]+\" % (UNRESERVED_RE + SUB_DELIMITERS_RE + \":\")\\n-\\n-# RFC 6874 Zone ID ABNF\\n-ZONE_ID = \"(?:[\" + UNRESERVED_RE + \"]|\" + PCT_ENCODED + \")+\"\\n-\\n-IPv6_ADDRZ_RFC4007_RE = IPv6_RE + \"(?:(?:%25|%)\" + ZONE_ID + \")?\"\\n-IPv6_ADDRZ_RE = IPv6_RE + \"(?:%25\" + ZONE_ID + \")?\"\\n-\\n-IP_LITERAL_RE = r\"\\\\[({0}|{1})\\\\]\".format(IPv6_ADDRZ_RFC4007_RE, IPv_FUTURE_RE)\\n-\\n-# Pattern for matching the host piece of the authority\\n-HOST_RE = HOST_PATTERN = \"({0}|{1}|{2})\".format(REG_NAME, IPv4_RE, IP_LITERAL_RE)\\n-USERINFO_RE = \"^([\" + UNRESERVED_RE + SUB_DELIMITERS_RE + \":]|%s)+\" % (PCT_ENCODED)\\n-PORT_RE = \"[0-9]{1,5}\"\\n-\\n-# ####################\\n-# Path Matcher Section\\n-# ####################\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-3.3 for more information\\n-# about the path patterns defined below.\\n-segments = {\\n-    \"segment\": PCHAR + \"*\",\\n-    # Non-zero length segment\\n-    \"segment-nz\": PCHAR + \"+\",\\n-    # Non-zero length segment without \":\"\\n-    \"segment-nz-nc\": PCHAR.replace(\":\", \"\") + \"+\",\\n-}\\n-\\n-# Path types taken from Section 3.3 (linked above)\\n-PATH_EMPTY = \"^$\"\\n-PATH_ROOTLESS = \"%(segment-nz)s(/%(segment)s)*\" % segments\\n-PATH_NOSCHEME = \"%(segment-nz-nc)s(/%(segment)s)*\" % segments\\n-PATH_ABSOLUTE = \"/(%s)?\" % PATH_ROOTLESS\\n-PATH_ABEMPTY = \"(/%(segment)s)*\" % segments\\n-PATH_RE = \"^(%s|%s|%s|%s|%s)$\" % (\\n-    PATH_ABEMPTY,\\n-    PATH_ABSOLUTE,\\n-    PATH_NOSCHEME,\\n-    PATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)\\n-\\n-FRAGMENT_RE = QUERY_RE = (\\n-    \"^([/?:@\" + UNRESERVED_RE + SUB_DELIMITERS_RE + \"]|%s)*$\" % PCT_ENCODED\\n-)\\n-\\n-# ##########################\\n-# Relative reference matcher\\n-# ##########################\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-4.2 for details\\n-RELATIVE_PART_RE = \"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    PATH_ABEMPTY,\\n-    PATH_ABSOLUTE,\\n-    PATH_NOSCHEME,\\n-    PATH_EMPTY,\\n-)\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-3 for definition\\n-HIER_PART_RE = \"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    PATH_ABEMPTY,\\n-    PATH_ABSOLUTE,\\n-    PATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)\\n-\\n-# ###############\\n-# IRIs / RFC 3987\\n-# ###############\\n-\\n-# Only wide-unicode gets the high-ranges of UCSCHAR\\n-if sys.maxunicode > 0xFFFF:  # pragma: no cover\\n-    IPRIVATE = u\"\\\\uE000-\\\\uF8FF\\\\U000F0000-\\\\U000FFFFD\\\\U00100000-\\\\U0010FFFD\"\\n-    UCSCHAR_RE = (\\n-        u\"\\\\u00A0-\\\\uD7FF\\\\uF900-\\\\uFDCF\\\\uFDF0-\\\\uFFEF\"\\n-        u\"\\\\U00010000-\\\\U0001FFFD\\\\U00020000-\\\\U0002FFFD\"\\n-        u\"\\\\U00030000-\\\\U0003FFFD\\\\U00040000-\\\\U0004FFFD\"\\n-        u\"\\\\U00050000-\\\\U0005FFFD\\\\U00060000-\\\\U0006FFFD\"\\n-        u\"\\\\U00070000-\\\\U0007FFFD\\\\U00080000-\\\\U0008FFFD\"\\n-        u\"\\\\U00090000-\\\\U0009FFFD\\\\U000A0000-\\\\U000AFFFD\"\\n-        u\"\\\\U000B0000-\\\\U000BFFFD\\\\U000C0000-\\\\U000CFFFD\"\\n-        u\"\\\\U000D0000-\\\\U000DFFFD\\\\U000E1000-\\\\U000EFFFD\"\\n-    )\\n-else:  # pragma: no cover\\n-    IPRIVATE = u\"\\\\uE000-\\\\uF8FF\"\\n-    UCSCHAR_RE = u\"\\\\u00A0-\\\\uD7FF\\\\uF900-\\\\uFDCF\\\\uFDF0-\\\\uFFEF\"\\n-\\n-IUNRESERVED_RE = u\"A-Za-z0-9\\\\\\\\._~\\\\\\\\-\" + UCSCHAR_RE\\n-IPCHAR = u\"([\" + IUNRESERVED_RE + SUB_DELIMITERS_RE + u\":@]|%s)\" % PCT_ENCODED\\n-\\n-isegments = {\\n-    \"isegment\": IPCHAR + u\"*\",\\n-    # Non-zero length segment\\n-    \"isegment-nz\": IPCHAR + u\"+\",\\n-    # Non-zero length segment without \":\"\\n-    \"isegment-nz-nc\": IPCHAR.replace(\":\", \"\") + u\"+\",\\n-}\\n-\\n-IPATH_ROOTLESS = u\"%(isegment-nz)s(/%(isegment)s)*\" % isegments\\n-IPATH_NOSCHEME = u\"%(isegment-nz-nc)s(/%(isegment)s)*\" % isegments\\n-IPATH_ABSOLUTE = u\"/(?:%s)?\" % IPATH_ROOTLESS\\n-IPATH_ABEMPTY = u\"(?:/%(isegment)s)*\" % isegments\\n-IPATH_RE = u\"^(?:%s|%s|%s|%s|%s)$\" % (\\n-    IPATH_ABEMPTY,\\n-    IPATH_ABSOLUTE,\\n-    IPATH_NOSCHEME,\\n-    IPATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)\\n-\\n-IREGULAR_NAME_RE = IREG_NAME = u\"(?:{0}|[{1}])*\".format(\\n-    u\"%[0-9A-Fa-f]{2}\", SUB_DELIMITERS_RE + IUNRESERVED_RE\\n-)\\n-\\n-IHOST_RE = IHOST_PATTERN = u\"({0}|{1}|{2})\".format(IREG_NAME, IPv4_RE, IP_LITERAL_RE)\\n-\\n-IUSERINFO_RE = (\\n-    u\"^(?:[\" + IUNRESERVED_RE + SUB_DELIMITERS_RE + u\":]|%s)+\" % (PCT_ENCODED)\\n-)\\n-\\n-IFRAGMENT_RE = (\\n-    u\"^(?:[/?:@\" + IUNRESERVED_RE + SUB_DELIMITERS_RE + u\"]|%s)*$\" % PCT_ENCODED\\n-)\\n-IQUERY_RE = (\\n-    u\"^(?:[/?:@\"\\n-    + IUNRESERVED_RE\\n-    + SUB_DELIMITERS_RE\\n-    + IPRIVATE\\n-    + u\"]|%s)*$\" % PCT_ENCODED\\n-)\\n-\\n-IRELATIVE_PART_RE = u\"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    IPATH_ABEMPTY,\\n-    IPATH_ABSOLUTE,\\n-    IPATH_NOSCHEME,\\n-    PATH_EMPTY,\\n-)\\n-\\n-IHIER_PART_RE = u\"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    IPATH_ABEMPTY,\\n-    IPATH_ABSOLUTE,\\n-    IPATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)', '@@ -1,172 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module with functions to normalize components.\"\"\"\\n-import re\\n-\\n-from . import compat\\n-from . import misc\\n-\\n-\\n-def normalize_scheme(scheme):\\n-    \"\"\"Normalize the scheme component.\"\"\"\\n-    return scheme.lower()\\n-\\n-\\n-def normalize_authority(authority):\\n-    \"\"\"Normalize an authority tuple to a string.\"\"\"\\n-    userinfo, host, port = authority\\n-    result = \"\"\\n-    if userinfo:\\n-        result += normalize_percent_characters(userinfo) + \"@\"\\n-    if host:\\n-        result += normalize_host(host)\\n-    if port:\\n-        result += \":\" + port\\n-    return result\\n-\\n-\\n-def normalize_username(username):\\n-    \"\"\"Normalize a username to make it safe to include in userinfo.\"\"\"\\n-    return compat.urlquote(username)\\n-\\n-\\n-def normalize_password(password):\\n-    \"\"\"Normalize a password to make safe for userinfo.\"\"\"\\n-    return compat.urlquote(password)\\n-\\n-\\n-def normalize_host(host):\\n-    \"\"\"Normalize a host string.\"\"\"\\n-    if misc.IPv6_MATCHER.match(host):\\n-        percent = host.find(\"%\")\\n-        if percent != -1:\\n-            percent_25 = host.find(\"%25\")\\n-\\n-            # Replace RFC 4007 IPv6 Zone ID delimiter \\'%\\' with \\'%25\\'\\n-            # from RFC 6874. If the host is \\'[<IPv6 addr>%25]\\' then we\\n-            # assume RFC 4007 and normalize to \\'[<IPV6 addr>%2525]\\'\\n-            if (\\n-                percent_25 == -1\\n-                or percent < percent_25\\n-                or (percent == percent_25 and percent_25 == len(host) - 4)\\n-            ):\\n-                host = host.replace(\"%\", \"%25\", 1)\\n-\\n-            # Don\\'t normalize the casing of the Zone ID\\n-            return host[:percent].lower() + host[percent:]\\n-\\n-    return host.lower()\\n-\\n-\\n-def normalize_path(path):\\n-    \"\"\"Normalize the path string.\"\"\"\\n-    if not path:\\n-        return path\\n-\\n-    path = normalize_percent_characters(path)\\n-    return remove_dot_segments(path)\\n-\\n-\\n-def normalize_query(query):\\n-    \"\"\"Normalize the query string.\"\"\"\\n-    if not query:\\n-        return query\\n-    return normalize_percent_characters(query)\\n-\\n-\\n-def normalize_fragment(fragment):\\n-    \"\"\"Normalize the fragment string.\"\"\"\\n-    if not fragment:\\n-        return fragment\\n-    return normalize_percent_characters(fragment)\\n-\\n-\\n-PERCENT_MATCHER = re.compile(\"%[A-Fa-f0-9]{2}\")\\n-\\n-\\n-def normalize_percent_characters(s):\\n-    \"\"\"All percent characters should be upper-cased.\\n-\\n-    For example, ``\"%3afoo%DF%ab\"`` should be turned into ``\"%3Afoo%DF%AB\"``.\\n-    \"\"\"\\n-    matches = set(PERCENT_MATCHER.findall(s))\\n-    for m in matches:\\n-        if not m.isupper():\\n-            s = s.replace(m, m.upper())\\n-    return s\\n-\\n-\\n-def remove_dot_segments(s):\\n-    \"\"\"Remove dot segments from the string.\\n-\\n-    See also Section 5.2.4 of :rfc:`3986`.\\n-    \"\"\"\\n-    # See http://tools.ietf.org/html/rfc3986#section-5.2.4 for pseudo-code\\n-    segments = s.split(\"/\")  # Turn the path into a list of segments\\n-    output = []  # Initialize the variable to use to store output\\n-\\n-    for segment in segments:\\n-        # \\'.\\' is the current directory, so ignore it, it is superfluous\\n-        if segment == \".\":\\n-            continue\\n-        # Anything other than \\'..\\', should be appended to the output\\n-        elif segment != \"..\":\\n-            output.append(segment)\\n-        # In this case segment == \\'..\\', if we can, we should pop the last\\n-        # element\\n-        elif output:\\n-            output.pop()\\n-\\n-    # If the path starts with \\'/\\' and the output is empty or the first string\\n-    # is non-empty\\n-    if s.startswith(\"/\") and (not output or output[0]):\\n-        output.insert(0, \"\")\\n-\\n-    # If the path starts with \\'/.\\' or \\'/..\\' ensure we add one more empty\\n-    # string to add a trailing \\'/\\'\\n-    if s.endswith((\"/.\", \"/..\")):\\n-        output.append(\"\")\\n-\\n-    return \"/\".join(output)\\n-\\n-\\n-def encode_component(uri_component, encoding):\\n-    \"\"\"Encode the specific component in the provided encoding.\"\"\"\\n-    if uri_component is None:\\n-        return uri_component\\n-\\n-    # Try to see if the component we\\'re encoding is already percent-encoded\\n-    # so we can skip all \\'%\\' characters but still encode all others.\\n-    percent_encodings = len(\\n-        PERCENT_MATCHER.findall(compat.to_str(uri_component, encoding))\\n-    )\\n-\\n-    uri_bytes = compat.to_bytes(uri_component, encoding)\\n-    is_percent_encoded = percent_encodings == uri_bytes.count(b\"%\")\\n-\\n-    encoded_uri = bytearray()\\n-\\n-    for i in range(0, len(uri_bytes)):\\n-        # Will return a single character bytestring on both Python 2 & 3\\n-        byte = uri_bytes[i : i + 1]\\n-        byte_ord = ord(byte)\\n-        if (is_percent_encoded and byte == b\"%\") or (\\n-            byte_ord < 128 and byte.decode() in misc.NON_PCT_ENCODED\\n-        ):\\n-            encoded_uri.extend(byte)\\n-            continue\\n-        encoded_uri.extend(\"%{0:02x}\".format(byte_ord).encode().upper())\\n-\\n-    return encoded_uri.decode(encoding)', '@@ -1,457 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2015 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module containing the urlparse compatibility logic.\"\"\"\\n-from collections import namedtuple\\n-\\n-from . import compat\\n-from . import exceptions\\n-from . import misc\\n-from . import normalizers\\n-from . import uri\\n-\\n-__all__ = (\"ParseResult\", \"ParseResultBytes\")\\n-\\n-PARSED_COMPONENTS = (\"scheme\", \"userinfo\", \"host\", \"port\", \"path\", \"query\", \"fragment\")\\n-\\n-\\n-class ParseResultMixin(object):\\n-    def _generate_authority(self, attributes):\\n-        # I swear I did not align the comparisons below. That\\'s just how they\\n-        # happened to align based on pep8 and attribute lengths.\\n-        userinfo, host, port = (attributes[p] for p in (\"userinfo\", \"host\", \"port\"))\\n-        if self.userinfo != userinfo or self.host != host or self.port != port:\\n-            if port:\\n-                port = \"{0}\".format(port)\\n-            return normalizers.normalize_authority(\\n-                (\\n-                    compat.to_str(userinfo, self.encoding),\\n-                    compat.to_str(host, self.encoding),\\n-                    port,\\n-                )\\n-            )\\n-        return self.authority\\n-\\n-    def geturl(self):\\n-        \"\"\"Shim to match the standard library method.\"\"\"\\n-        return self.unsplit()\\n-\\n-    @property\\n-    def hostname(self):\\n-        \"\"\"Shim to match the standard library.\"\"\"\\n-        return self.host\\n-\\n-    @property\\n-    def netloc(self):\\n-        \"\"\"Shim to match the standard library.\"\"\"\\n-        return self.authority\\n-\\n-    @property\\n-    def params(self):\\n-        \"\"\"Shim to match the standard library.\"\"\"\\n-        return self.query\\n-\\n-\\n-class ParseResult(namedtuple(\"ParseResult\", PARSED_COMPONENTS), ParseResultMixin):\\n-    \"\"\"Implementation of urlparse compatibility class.\\n-\\n-    This uses the URIReference logic to handle compatibility with the\\n-    urlparse.ParseResult class.\\n-    \"\"\"\\n-\\n-    slots = ()\\n-\\n-    def __new__(\\n-        cls,\\n-        scheme,\\n-        userinfo,\\n-        host,\\n-        port,\\n-        path,\\n-        query,\\n-        fragment,\\n-        uri_ref,\\n-        encoding=\"utf-8\",\\n-    ):\\n-        \"\"\"Create a new ParseResult.\"\"\"\\n-        parse_result = super(ParseResult, cls).__new__(\\n-            cls,\\n-            scheme or None,\\n-            userinfo or None,\\n-            host,\\n-            port or None,\\n-            path or None,\\n-            query,\\n-            fragment,\\n-        )\\n-        parse_result.encoding = encoding\\n-        parse_result.reference = uri_ref\\n-        return parse_result\\n-\\n-    @classmethod\\n-    def from_parts(\\n-        cls,\\n-        scheme=None,\\n-        userinfo=None,\\n-        host=None,\\n-        port=None,\\n-        path=None,\\n-        query=None,\\n-        fragment=None,\\n-        encoding=\"utf-8\",\\n-    ):\\n-        \"\"\"Create a ParseResult instance from its parts.\"\"\"\\n-        authority = \"\"\\n-        if userinfo is not None:\\n-            authority += userinfo + \"@\"\\n-        if host is not None:\\n-            authority += host\\n-        if port is not None:\\n-            authority += \":{0}\".format(port)\\n-        uri_ref = uri.URIReference(\\n-            scheme=scheme,\\n-            authority=authority,\\n-            path=path,\\n-            query=query,\\n-            fragment=fragment,\\n-            encoding=encoding,\\n-        ).normalize()\\n-        userinfo, host, port = authority_from(uri_ref, strict=True)\\n-        return cls(\\n-            scheme=uri_ref.scheme,\\n-            userinfo=userinfo,\\n-            host=host,\\n-            port=port,\\n-            path=uri_ref.path,\\n-            query=uri_ref.query,\\n-            fragment=uri_ref.fragment,\\n-            uri_ref=uri_ref,\\n-            encoding=encoding,\\n-        )\\n-\\n-    @classmethod\\n-    def from_string(\\n-        cls, uri_string, encoding=\"utf-8\", strict=True, lazy_normalize=True\\n-    ):\\n-        \"\"\"Parse a URI from the given unicode URI string.\\n-\\n-        :param str uri_string: Unicode URI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :param bool strict: Parse strictly according to :rfc:`3986` if True.\\n-            If False, parse similarly to the standard library\\'s urlparse\\n-            function.\\n-        :returns: :class:`ParseResult` or subclass thereof\\n-        \"\"\"\\n-        reference = uri.URIReference.from_string(uri_string, encoding)\\n-        if not lazy_normalize:\\n-            reference = reference.normalize()\\n-        userinfo, host, port = authority_from(reference, strict)\\n-\\n-        return cls(\\n-            scheme=reference.scheme,\\n-            userinfo=userinfo,\\n-            host=host,\\n-            port=port,\\n-            path=reference.path,\\n-            query=reference.query,\\n-            fragment=reference.fragment,\\n-            uri_ref=reference,\\n-            encoding=encoding,\\n-        )\\n-\\n-    @property\\n-    def authority(self):\\n-        \"\"\"Return the normalized authority.\"\"\"\\n-        return self.reference.authority\\n-\\n-    def copy_with(\\n-        self,\\n-        scheme=misc.UseExisting,\\n-        userinfo=misc.UseExisting,\\n-        host=misc.UseExisting,\\n-        port=misc.UseExisting,\\n-        path=misc.UseExisting,\\n-        query=misc.UseExisting,\\n-        fragment=misc.UseExisting,\\n-    ):\\n-        \"\"\"Create a copy of this instance replacing with specified parts.\"\"\"\\n-        attributes = zip(\\n-            PARSED_COMPONENTS, (scheme, userinfo, host, port, path, query, fragment)\\n-        )\\n-        attrs_dict = {}\\n-        for name, value in attributes:\\n-            if value is misc.UseExisting:\\n-                value = getattr(self, name)\\n-            attrs_dict[name] = value\\n-        authority = self._generate_authority(attrs_dict)\\n-        ref = self.reference.copy_with(\\n-            scheme=attrs_dict[\"scheme\"],\\n-            authority=authority,\\n-            path=attrs_dict[\"path\"],\\n-            query=attrs_dict[\"query\"],\\n-            fragment=attrs_dict[\"fragment\"],\\n-        )\\n-        return ParseResult(uri_ref=ref, encoding=self.encoding, **attrs_dict)\\n-\\n-    def encode(self, encoding=None):\\n-        \"\"\"Convert to an instance of ParseResultBytes.\"\"\"\\n-        encoding = encoding or self.encoding\\n-        attrs = dict(\\n-            zip(\\n-                PARSED_COMPONENTS,\\n-                (\\n-                    attr.encode(encoding) if hasattr(attr, \"encode\") else attr\\n-                    for attr in self\\n-                ),\\n-            )\\n-        )\\n-        return ParseResultBytes(uri_ref=self.reference, encoding=encoding, **attrs)\\n-\\n-    def unsplit(self, use_idna=False):\\n-        \"\"\"Create a URI string from the components.\\n-\\n-        :returns: The parsed URI reconstituted as a string.\\n-        :rtype: str\\n-        \"\"\"\\n-        parse_result = self\\n-        if use_idna and self.host:\\n-            hostbytes = self.host.encode(\"idna\")\\n-            host = hostbytes.decode(self.encoding)\\n-            parse_result = self.copy_with(host=host)\\n-        return parse_result.reference.unsplit()\\n-\\n-\\n-class ParseResultBytes(\\n-    namedtuple(\"ParseResultBytes\", PARSED_COMPONENTS), ParseResultMixin\\n-):\\n-    \"\"\"Compatibility shim for the urlparse.ParseResultBytes object.\"\"\"\\n-\\n-    def __new__(\\n-        cls,\\n-        scheme,\\n-        userinfo,\\n-        host,\\n-        port,\\n-        path,\\n-        query,\\n-        fragment,\\n-        uri_ref,\\n-        encoding=\"utf-8\",\\n-        lazy_normalize=True,\\n-    ):\\n-        \"\"\"Create a new ParseResultBytes instance.\"\"\"\\n-        parse_result = super(ParseResultBytes, cls).__new__(\\n-            cls,\\n-            scheme or None,\\n-            userinfo or None,\\n-            host,\\n-            port or None,\\n-            path or None,\\n-            query or None,\\n-            fragment or None,\\n-        )\\n-        parse_result.encoding = encoding\\n-        parse_result.reference = uri_ref\\n-        parse_result.lazy_normalize = lazy_normalize\\n-        return parse_result\\n-\\n-    @classmethod\\n-    def from_parts(\\n-        cls,\\n-        scheme=None,\\n-        userinfo=None,\\n-        host=None,\\n-        port=None,\\n-        path=None,\\n-        query=None,\\n-        fragment=None,\\n-        encoding=\"utf-8\",\\n-        lazy_normalize=True,\\n-    ):\\n-        \"\"\"Create a ParseResult instance from its parts.\"\"\"\\n-        authority = \"\"\\n-        if userinfo is not None:\\n-            authority += userinfo + \"@\"\\n-        if host is not None:\\n-            authority += host\\n-        if port is not None:\\n-            authority += \":{0}\".format(int(port))\\n-        uri_ref = uri.URIReference(\\n-            scheme=scheme,\\n-            authority=authority,\\n-            path=path,\\n-            query=query,\\n-            fragment=fragment,\\n-            encoding=encoding,\\n-        )\\n-        if not lazy_normalize:\\n-            uri_ref = uri_ref.normalize()\\n-        to_bytes = compat.to_bytes\\n-        userinfo, host, port = authority_from(uri_ref, strict=True)\\n-        return cls(\\n-            scheme=to_bytes(scheme, encoding),\\n-            userinfo=to_bytes(userinfo, encoding),\\n-            host=to_bytes(host, encoding),\\n-            port=port,\\n-            path=to_bytes(path, encoding),\\n-            query=to_bytes(query, encoding),\\n-            fragment=to_bytes(fragment, encoding),\\n-            uri_ref=uri_ref,\\n-            encoding=encoding,\\n-            lazy_normalize=lazy_normalize,\\n-        )\\n-\\n-    @classmethod\\n-    def from_string(\\n-        cls, uri_string, encoding=\"utf-8\", strict=True, lazy_normalize=True\\n-    ):\\n-        \"\"\"Parse a URI from the given unicode URI string.\\n-\\n-        :param str uri_string: Unicode URI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :param bool strict: Parse strictly according to :rfc:`3986` if True.\\n-            If False, parse similarly to the standard library\\'s urlparse\\n-            function.\\n-        :returns: :class:`ParseResultBytes` or subclass thereof\\n-        \"\"\"\\n-        reference = uri.URIReference.from_string(uri_string, encoding)\\n-        if not lazy_normalize:\\n-            reference = reference.normalize()\\n-        userinfo, host, port = authority_from(reference, strict)\\n-\\n-        to_bytes = compat.to_bytes\\n-        return cls(\\n-            scheme=to_bytes(reference.scheme, encoding),\\n-            userinfo=to_bytes(userinfo, encoding),\\n-            host=to_bytes(host, encoding),\\n-            port=port,\\n-            path=to_bytes(reference.path, encoding),\\n-            query=to_bytes(reference.query, encoding),\\n-            fragment=to_bytes(reference.fragment, encoding),\\n-            uri_ref=reference,\\n-            encoding=encoding,\\n-            lazy_normalize=lazy_normalize,\\n-        )\\n-\\n-    @property\\n-    def authority(self):\\n-        \"\"\"Return the normalized authority.\"\"\"\\n-        return self.reference.authority.encode(self.encoding)\\n-\\n-    def copy_with(\\n-        self,\\n-        scheme=misc.UseExisting,\\n-        userinfo=misc.UseExisting,\\n-        host=misc.UseExisting,\\n-        port=misc.UseExisting,\\n-        path=misc.UseExisting,\\n-        query=misc.UseExisting,\\n-        fragment=misc.UseExisting,\\n-        lazy_normalize=True,\\n-    ):\\n-        \"\"\"Create a copy of this instance replacing with specified parts.\"\"\"\\n-        attributes = zip(\\n-            PARSED_COMPONENTS, (scheme, userinfo, host, port, path, query, fragment)\\n-        )\\n-        attrs_dict = {}\\n-        for name, value in attributes:\\n-            if value is misc.UseExisting:\\n-                value = getattr(self, name)\\n-            if not isinstance(value, bytes) and hasattr(value, \"encode\"):\\n-                value = value.encode(self.encoding)\\n-            attrs_dict[name] = value\\n-        authority = self._generate_authority(attrs_dict)\\n-        to_str = compat.to_str\\n-        ref = self.reference.copy_with(\\n-            scheme=to_str(attrs_dict[\"scheme\"], self.encoding),\\n-            authority=to_str(authority, self.encoding),\\n-            path=to_str(attrs_dict[\"path\"], self.encoding),\\n-            query=to_str(attrs_dict[\"query\"], self.encoding),\\n-            fragment=to_str(attrs_dict[\"fragment\"], self.encoding),\\n-        )\\n-        if not lazy_normalize:\\n-            ref = ref.normalize()\\n-        return ParseResultBytes(\\n-            uri_ref=ref,\\n-            encoding=self.encoding,\\n-            lazy_normalize=lazy_normalize,\\n-            **attrs_dict\\n-        )\\n-\\n-    def unsplit(self, use_idna=False):\\n-        \"\"\"Create a URI bytes object from the components.\\n-\\n-        :returns: The parsed URI reconstituted as a string.\\n-        :rtype: bytes\\n-        \"\"\"\\n-        parse_result = self\\n-        if use_idna and self.host:\\n-            # self.host is bytes, to encode to idna, we need to decode it\\n-            # first\\n-            host = self.host.decode(self.encoding)\\n-            hostbytes = host.encode(\"idna\")\\n-            parse_result = self.copy_with(host=hostbytes)\\n-        if self.lazy_normalize:\\n-            parse_result = parse_result.copy_with(lazy_normalize=False)\\n-        uri = parse_result.reference.unsplit()\\n-        return uri.encode(self.encoding)\\n-\\n-\\n-def split_authority(authority):\\n-    # Initialize our expected return values\\n-    userinfo = host = port = None\\n-    # Initialize an extra var we may need to use\\n-    extra_host = None\\n-    # Set-up rest in case there is no userinfo portion\\n-    rest = authority\\n-\\n-    if \"@\" in authority:\\n-        userinfo, rest = authority.rsplit(\"@\", 1)\\n-\\n-    # Handle IPv6 host addresses\\n-    if rest.startswith(\"[\"):\\n-        host, rest = rest.split(\"]\", 1)\\n-        host += \"]\"\\n-\\n-    if \":\" in rest:\\n-        extra_host, port = rest.split(\":\", 1)\\n-    elif not host and rest:\\n-        host = rest\\n-\\n-    if extra_host and not host:\\n-        host = extra_host\\n-\\n-    return userinfo, host, port\\n-\\n-\\n-def authority_from(reference, strict):\\n-    try:\\n-        subauthority = reference.authority_info()\\n-    except exceptions.InvalidAuthority:\\n-        if strict:\\n-            raise\\n-        userinfo, host, port = split_authority(reference.authority)\\n-    else:\\n-        # Thanks to Richard Barrell for this idea:\\n-        # https://twitter.com/0x2ba22e11/status/617338811975139328\\n-        userinfo, host, port = (\\n-            subauthority.get(p) for p in (\"userinfo\", \"host\", \"port\")\\n-        )\\n-\\n-    if port:\\n-        try:\\n-            port = int(port)\\n-        except ValueError:\\n-            raise exceptions.InvalidPort(port)\\n-    return userinfo, host, port', '@@ -77,6 +77,8 @@ def blacken(session):\\n     session.install(\"black\")\\r\\n     session.run(\"black\", \"src\", \"dummyserver\", \"test\", \"noxfile.py\", \"setup.py\")\\r\\n \\r\\n+    lint(session)\\r\\n+\\r\\n \\r\\n @nox.session\\r\\n def lint(session):\\r', '@@ -3,10 +3,7 @@\\n from collections import namedtuple\\n \\n from ..exceptions import LocationParseError\\n-from ..packages import six, rfc3986\\n-from ..packages.rfc3986.exceptions import RFC3986Exception, ValidationError\\n-from ..packages.rfc3986.validators import Validator\\n-from ..packages.rfc3986 import abnf_regexp, normalizers, compat, misc\\n+from ..packages import six\\n \\n \\n url_attrs = [\"scheme\", \"auth\", \"host\", \"port\", \"path\", \"query\", \"fragment\"]\\n@@ -15,12 +12,68 @@\\n # urllib3 infers URLs without a scheme (None) to be http.\\n NORMALIZABLE_SCHEMES = (\"http\", \"https\", None)\\n \\n-# Regex for detecting URLs with schemes. RFC 3986 Section 3.1\\n-SCHEME_REGEX = re.compile(r\"^(?:[a-zA-Z][a-zA-Z0-9+\\\\-]*:|/)\")\\n+# Almost all of these patterns were derived from the\\n+# \\'rfc3986\\' module: https://github.com/python-hyper/rfc3986\\n+PERCENT_RE = re.compile(r\"%[a-fA-F0-9]{2}\")\\n+SCHEME_RE = re.compile(r\"^(?:[a-zA-Z][a-zA-Z0-9+-]*:|/)\")\\n+URI_RE = re.compile(\\n+    r\"^(?:([a-zA-Z][a-zA-Z0-9+.-]*):)?\"\\n+    r\"(?://([^/?#]*))?\"\\n+    r\"([^?#]*)\"\\n+    r\"(?:\\\\?([^#]*))?\"\\n+    r\"(?:#(.*))?$\",\\n+    re.UNICODE | re.DOTALL,\\n+)\\n+\\n+IPV4_PAT = r\"(?:[0-9]{1,3}\\\\.){3}[0-9]{1,3}\"\\n+HEX_PAT = \"[0-9A-Fa-f]{1,4}\"\\n+LS32_PAT = \"(?:{hex}:{hex}|{ipv4})\".format(hex=HEX_PAT, ipv4=IPV4_PAT)\\n+_subs = {\"hex\": HEX_PAT, \"ls32\": LS32_PAT}\\n+_variations = [\\n+    #                            6( h16 \":\" ) ls32\\n+    \"(?:%(hex)s:){6}%(ls32)s\",\\n+    #                       \"::\" 5( h16 \":\" ) ls32\\n+    \"::(?:%(hex)s:){5}%(ls32)s\",\\n+    # [               h16 ] \"::\" 4( h16 \":\" ) ls32\\n+    \"(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)s\",\\n+    # [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\\n+    \"(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)s\",\\n+    # [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\\n+    \"(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)s\",\\n+    # [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\\n+    \"(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s\",\\n+    # [ *4( h16 \":\" ) h16 ] \"::\"              ls32\\n+    \"(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)s\",\\n+    # [ *5( h16 \":\" ) h16 ] \"::\"              h16\\n+    \"(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)s\",\\n+    # [ *6( h16 \":\" ) h16 ] \"::\"\\n+    \"(?:(?:%(hex)s:){0,6}%(hex)s)?::\",\\n+]\\n+\\n+UNRESERVED_PAT = r\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._!\\\\-\"\\n+IPV6_PAT = \"(?:\" + \"|\".join([x % _subs for x in _variations]) + \")\"\\n+ZONE_ID_PAT = \"(?:%25|%)(?:[\" + UNRESERVED_PAT + \"]|%[a-fA-F0-9]{2})+\"\\n+IPV6_ADDRZ_PAT = r\"\\\\[\" + IPV6_PAT + r\"(?:\" + ZONE_ID_PAT + r\")?\\\\]\"\\n+REG_NAME_PAT = r\"(?:[^\\\\[\\\\]%:/?#]|%[a-fA-F0-9]{2})*\"\\n+\\n+IPV4_RE = re.compile(\"^\" + IPV4_PAT + \"$\")\\n+IPV6_RE = re.compile(\"^\" + IPV6_PAT + \"$\")\\n+IPV6_ADDRZ_RE = re.compile(\"^\" + IPV6_ADDRZ_PAT + \"$\")\\n+BRACELESS_IPV6_ADDRZ_RE = re.compile(\"^\" + IPV6_ADDRZ_PAT[2:-2] + \"$\")\\n+ZONE_ID_RE = re.compile(\"(\" + ZONE_ID_PAT + r\")\\\\]$\")\\n+\\n+SUBAUTHORITY_PAT = (u\"^(?:(.*)@)?\" u\"(%s|%s|%s)\" u\"(?::([0-9]{0,5}))?$\") % (\\n+    REG_NAME_PAT,\\n+    IPV4_PAT,\\n+    IPV6_ADDRZ_PAT,\\n+)\\n+SUBAUTHORITY_RE = re.compile(SUBAUTHORITY_PAT, re.UNICODE | re.DOTALL)\\n \\n-PATH_CHARS = (\\n-    abnf_regexp.UNRESERVED_CHARS_SET | abnf_regexp.SUB_DELIMITERS_SET | {\":\", \"@\", \"/\"}\\n+ZONE_ID_CHARS = set(\\n+    \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"abcdefghijklmnopqrstuvwxyz\" \"0123456789._!-\"\\n )\\n+USERINFO_CHARS = ZONE_ID_CHARS | set(\"$&\\'()*+,;=:\")\\n+PATH_CHARS = USERINFO_CHARS | {\"@\", \"/\"}\\n QUERY_CHARS = FRAGMENT_CHARS = PATH_CHARS | {\"?\"}\\n \\n \\n@@ -154,20 +207,24 @@ def split_first(s, delims):\\n \\n def _encode_invalid_chars(component, allowed_chars, encoding=\"utf-8\"):\\n     \"\"\"Percent-encodes a URI component without reapplying\\n-    onto an already percent-encoded component. Based on\\n-    rfc3986.normalizers.encode_component()\\n+    onto an already percent-encoded component.\\n     \"\"\"\\n     if component is None:\\n         return component\\n \\n+    component = six.ensure_text(component)\\n+\\n     # Try to see if the component we\\'re encoding is already percent-encoded\\n     # so we can skip all \\'%\\' characters but still encode all others.\\n-    percent_encodings = len(\\n-        normalizers.PERCENT_MATCHER.findall(compat.to_str(component, encoding))\\n-    )\\n+    percent_encodings = PERCENT_RE.findall(component)\\n+\\n+    # Normalize existing percent-encoded bytes.\\n+    for enc in percent_encodings:\\n+        if not enc.isupper():\\n+            component = component.replace(enc, enc.upper())\\n \\n     uri_bytes = component.encode(\"utf-8\", \"surrogatepass\")\\n-    is_percent_encoded = percent_encodings == uri_bytes.count(b\"%\")\\n+    is_percent_encoded = len(percent_encodings) == uri_bytes.count(b\"%\")\\n \\n     encoded_component = bytearray()\\n \\n@@ -180,17 +237,96 @@ def _encode_invalid_chars(component, allowed_chars, encoding=\"utf-8\"):\\n         ):\\n             encoded_component.extend(byte)\\n             continue\\n-        encoded_component.extend(\"%{0:02x}\".format(byte_ord).encode().upper())\\n+        encoded_component.extend(b\"%\" + (hex(byte_ord)[2:].encode().zfill(2).upper()))\\n \\n     return encoded_component.decode(encoding)\\n \\n \\n+def _remove_path_dot_segments(path):\\n+    # See http://tools.ietf.org/html/rfc3986#section-5.2.4 for pseudo-code\\n+    segments = path.split(\"/\")  # Turn the path into a list of segments\\n+    output = []  # Initialize the variable to use to store output\\n+\\n+    for segment in segments:\\n+        # \\'.\\' is the current directory, so ignore it, it is superfluous\\n+        if segment == \".\":\\n+            continue\\n+        # Anything other than \\'..\\', should be appended to the output\\n+        elif segment != \"..\":\\n+            output.append(segment)\\n+        # In this case segment == \\'..\\', if we can, we should pop the last\\n+        # element\\n+        elif output:\\n+            output.pop()\\n+\\n+    # If the path starts with \\'/\\' and the output is empty or the first string\\n+    # is non-empty\\n+    if path.startswith(\"/\") and (not output or output[0]):\\n+        output.insert(0, \"\")\\n+\\n+    # If the path starts with \\'/.\\' or \\'/..\\' ensure we add one more empty\\n+    # string to add a trailing \\'/\\'\\n+    if path.endswith((\"/.\", \"/..\")):\\n+        output.append(\"\")\\n+\\n+    return \"/\".join(output)\\n+\\n+\\n+def _normalize_host(host, scheme):\\n+    if host:\\n+        if isinstance(host, six.binary_type):\\n+            host = six.ensure_str(host)\\n+\\n+        if scheme in NORMALIZABLE_SCHEMES:\\n+            is_ipv6 = IPV6_ADDRZ_RE.match(host)\\n+            if is_ipv6:\\n+                match = ZONE_ID_RE.search(host)\\n+                if match:\\n+                    start, end = match.span(1)\\n+                    zone_id = host[start:end]\\n+\\n+                    if zone_id.startswith(\"%25\") and zone_id != \"%25\":\\n+                        zone_id = zone_id[3:]\\n+                    else:\\n+                        zone_id = zone_id[1:]\\n+                    zone_id = \"%\" + _encode_invalid_chars(zone_id, ZONE_ID_CHARS)\\n+                    return host[:start].lower() + zone_id + host[end:]\\n+                else:\\n+                    return host.lower()\\n+            elif not IPV4_RE.match(host):\\n+                return six.ensure_str(\\n+                    b\".\".join([_idna_encode(label) for label in host.split(\".\")])\\n+                )\\n+    return host\\n+\\n+\\n+def _idna_encode(name):\\n+    if name and any([ord(x) > 128 for x in name]):\\n+        try:\\n+            import idna\\n+        except ImportError:\\n+            six.raise_from(\\n+                LocationParseError(\"Unable to parse URL without the \\'idna\\' module\"),\\n+                None,\\n+            )\\n+        try:\\n+            return idna.encode(name.lower(), strict=True, std3_rules=True)\\n+        except idna.IDNAError:\\n+            six.raise_from(\\n+                LocationParseError(u\"Name \\'%s\\' is not a valid IDNA label\" % name), None\\n+            )\\n+    return name.lower().encode(\"ascii\")\\n+\\n+\\n def parse_url(url):\\n     \"\"\"\\n     Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\\n     performed to parse incomplete urls. Fields not provided will be None.\\n     This parser is RFC 3986 compliant.\\n \\n+    The parser logic and helper functions are based heavily on\\n+    work done in the ``rfc3986`` module.\\n+\\n     :param str url: URL to parse into a :class:`.Url` namedtuple.\\n \\n     Partly backwards-compatible with :mod:`urlparse`.\\n@@ -208,90 +344,72 @@ def parse_url(url):\\n         # Empty\\n         return Url()\\n \\n-    is_string = not isinstance(url, six.binary_type)\\n-\\n-    # RFC 3986 doesn\\'t like URLs that have a host but don\\'t start\\n-    # with a scheme and we support URLs like that so we need to\\n-    # detect that problem and add an empty scheme indication.\\n-    # We don\\'t get hurt on path-only URLs here as it\\'s stripped\\n-    # off and given an empty scheme anyways.\\n-    if not SCHEME_REGEX.search(url):\\n+    source_url = url\\n+    if not SCHEME_RE.search(url):\\n         url = \"//\" + url\\n \\n-    def idna_encode(name):\\n-        if name and any([ord(x) > 128 for x in name]):\\n-            try:\\n-                import idna\\n-            except ImportError:\\n-                raise LocationParseError(\\n-                    \"Unable to parse URL without the \\'idna\\' module\"\\n-                )\\n-            try:\\n-                return idna.encode(name.lower(), strict=True, std3_rules=True)\\n-            except idna.IDNAError:\\n-                raise LocationParseError(u\"Name \\'%s\\' is not a valid IDNA label\" % name)\\n-        return name\\n-\\n-    try:\\n-        split_iri = misc.IRI_MATCHER.match(compat.to_str(url)).groupdict()\\n-        iri_ref = rfc3986.IRIReference(\\n-            split_iri[\"scheme\"],\\n-            split_iri[\"authority\"],\\n-            _encode_invalid_chars(split_iri[\"path\"], PATH_CHARS),\\n-            _encode_invalid_chars(split_iri[\"query\"], QUERY_CHARS),\\n-            _encode_invalid_chars(split_iri[\"fragment\"], FRAGMENT_CHARS),\\n-        )\\n-        has_authority = iri_ref.authority is not None\\n-        uri_ref = iri_ref.encode(idna_encoder=idna_encode)\\n-    except (ValueError, RFC3986Exception):\\n-        return six.raise_from(LocationParseError(url), None)\\n-\\n-    # rfc3986 strips the authority if it\\'s invalid\\n-    if has_authority and uri_ref.authority is None:\\n-        raise LocationParseError(url)\\n-\\n-    # Only normalize schemes we understand to not break http+unix\\n-    # or other schemes that don\\'t follow RFC 3986.\\n-    if uri_ref.scheme is None or uri_ref.scheme.lower() in NORMALIZABLE_SCHEMES:\\n-        uri_ref = uri_ref.normalize()\\n-\\n-    # Validate all URIReference components and ensure that all\\n-    # components that were set before are still set after\\n-    # normalization has completed.\\n-    validator = Validator()\\n     try:\\n-        validator.check_validity_of(*validator.COMPONENT_NAMES).validate(uri_ref)\\n-    except ValidationError:\\n-        return six.raise_from(LocationParseError(url), None)\\n+        scheme, authority, path, query, fragment = URI_RE.match(url).groups()\\n+        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES\\n+\\n+        if scheme:\\n+            scheme = scheme.lower()\\n+\\n+        if authority:\\n+            auth, host, port = SUBAUTHORITY_RE.match(authority).groups()\\n+            if auth and normalize_uri:\\n+                auth = _encode_invalid_chars(auth, USERINFO_CHARS)\\n+            if port == \"\":\\n+                port = None\\n+        else:\\n+            auth, host, port = None, None, None\\n+\\n+        if port is not None:\\n+            port = int(port)\\n+            if not (0 <= port <= 65535):\\n+                raise LocationParseError(url)\\n+\\n+        host = _normalize_host(host, scheme)\\n+\\n+        if normalize_uri and path:\\n+            path = _remove_path_dot_segments(path)\\n+            path = _encode_invalid_chars(path, PATH_CHARS)\\n+        if normalize_uri and query:\\n+            query = _encode_invalid_chars(query, QUERY_CHARS)\\n+        if normalize_uri and fragment:\\n+            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)\\n+\\n+    except (ValueError, AttributeError):\\n+        return six.raise_from(LocationParseError(source_url), None)\\n \\n     # For the sake of backwards compatibility we put empty\\n     # string values for path if there are any defined values\\n     # beyond the path in the URL.\\n     # TODO: Remove this when we break backwards compatibility.\\n-    path = uri_ref.path\\n     if not path:\\n-        if uri_ref.query is not None or uri_ref.fragment is not None:\\n+        if query is not None or fragment is not None:\\n             path = \"\"\\n         else:\\n             path = None\\n \\n     # Ensure that each part of the URL is a `str` for\\n     # backwards compatibility.\\n-    def to_input_type(x):\\n-        if x is None:\\n-            return None\\n-        elif not is_string and not isinstance(x, six.binary_type):\\n-            return x.encode(\"utf-8\")\\n-        return x\\n+    if isinstance(url, six.text_type):\\n+        ensure_func = six.ensure_text\\n+    else:\\n+        ensure_func = six.ensure_str\\n+\\n+    def ensure_type(x):\\n+        return x if x is None else ensure_func(x)\\n \\n     return Url(\\n-        scheme=to_input_type(uri_ref.scheme),\\n-        auth=to_input_type(uri_ref.userinfo),\\n-        host=to_input_type(uri_ref.host),\\n-        port=int(uri_ref.port) if uri_ref.port is not None else None,\\n-        path=to_input_type(path),\\n-        query=to_input_type(uri_ref.query),\\n-        fragment=to_input_type(uri_ref.fragment),\\n+        scheme=ensure_type(scheme),\\n+        auth=ensure_type(auth),\\n+        host=ensure_type(host),\\n+        port=port,\\n+        path=ensure_type(path),\\n+        query=ensure_type(query),\\n+        fragment=ensure_type(fragment),\\n     )\\n \\n ', '@@ -2,14 +2,13 @@\\n import errno\\n import warnings\\n import hmac\\n-import re\\n \\n from binascii import hexlify, unhexlify\\n from hashlib import md5, sha1, sha256\\n \\n+from .url import IPV4_RE, BRACELESS_IPV6_ADDRZ_RE\\n from ..exceptions import SSLError, InsecurePlatformWarning, SNIMissingWarning\\n from ..packages import six\\n-from ..packages.rfc3986 import abnf_regexp\\n \\n \\n SSLContext = None\\n@@ -36,13 +35,6 @@ def _const_compare_digest_backport(a, b):\\n \\n _const_compare_digest = getattr(hmac, \"compare_digest\", _const_compare_digest_backport)\\n \\n-# Borrow rfc3986\\'s regular expressions for IPv4\\n-# and IPv6 addresses for use in is_ipaddress()\\n-_IP_ADDRESS_REGEX = re.compile(\\n-    r\"^(?:%s|%s|%s)$\"\\n-    % (abnf_regexp.IPv4_RE, abnf_regexp.IPv6_RE, abnf_regexp.IPv6_ADDRZ_RFC4007_RE)\\n-)\\n-\\n try:  # Test for SSL features\\n     import ssl\\n     from ssl import wrap_socket, CERT_REQUIRED\\n@@ -389,7 +381,7 @@ def is_ipaddress(hostname):\\n     if six.PY3 and isinstance(hostname, bytes):\\n         # IDN A-label bytes are ASCII compatible.\\n         hostname = hostname.decode(\"ascii\")\\n-    return _IP_ADDRESS_REGEX.match(hostname) is not None\\n+    return bool(IPV4_RE.match(hostname) or BRACELESS_IPV6_ADDRZ_RE.match(hostname))\\n \\n \\n def _is_key_file_encrypted(key_file):', '@@ -3,6 +3,6 @@\\n set -exo pipefail\\n \\n if [[ -e .coverage ]]; then\\n-    python3.6 -m pip install codecov\\n-    python3.6 -m codecov --env TRAVIS_OS_NAME,NOX_SESSION\\n+    python3 -m pip install codecov\\n+    python3 -m codecov --env TRAVIS_OS_NAME,NOX_SESSION\\n fi', '@@ -1,49 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Compatibility module for Python 2 and 3 support.\"\"\"\\n-import sys\\n-\\n-try:\\n-    from urllib.parse import quote as urlquote\\n-except ImportError:  # Python 2.x\\n-    from urllib import quote as urlquote\\n-\\n-try:\\n-    from urllib.parse import urlencode\\n-except ImportError:  # Python 2.x\\n-    from urllib import urlencode\\n-\\n-__all__ = (\"to_bytes\", \"to_str\", \"urlquote\", \"urlencode\")\\n-\\n-PY3 = (3, 0) <= sys.version_info < (4, 0)\\n-PY2 = (2, 6) <= sys.version_info < (2, 8)\\n-\\n-\\n-if PY3:\\n-    unicode = str  # Python 3.x\\n-\\n-\\n-def to_str(b, encoding=\"utf-8\"):\\n-    \"\"\"Ensure that b is text in the specified encoding.\"\"\"\\n-    if hasattr(b, \"decode\") and not isinstance(b, unicode):\\n-        b = b.decode(encoding)\\n-    return b\\n-\\n-\\n-def to_bytes(s, encoding=\"utf-8\"):\\n-    \"\"\"Ensure that s is converted to bytes from the encoding.\"\"\"\\n-    if hasattr(s, \"encode\") and not isinstance(s, bytes):\\n-        s = s.encode(encoding)\\n-    return s', '@@ -1,150 +0,0 @@\\n-\"\"\"Module containing the implementation of the IRIReference class.\"\"\"\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Copyright (c) 2015 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-from collections import namedtuple\\n-\\n-from . import compat\\n-from . import exceptions\\n-from . import misc\\n-from . import normalizers\\n-from . import uri\\n-\\n-\\n-try:\\n-    import idna\\n-except ImportError:  # pragma: no cover\\n-    idna = None\\n-\\n-\\n-class IRIReference(namedtuple(\"IRIReference\", misc.URI_COMPONENTS), uri.URIMixin):\\n-    \"\"\"Immutable object representing a parsed IRI Reference.\\n-\\n-    Can be encoded into an URIReference object via the procedure\\n-    specified in RFC 3987 Section 3.1\\n-\\n-     .. note::\\n-        The IRI submodule is a new interface and may possibly change in\\n-        the future. Check for changes to the interface when upgrading.\\n-    \"\"\"\\n-\\n-    slots = ()\\n-\\n-    def __new__(cls, scheme, authority, path, query, fragment, encoding=\"utf-8\"):\\n-        \"\"\"Create a new IRIReference.\"\"\"\\n-        ref = super(IRIReference, cls).__new__(\\n-            cls, scheme or None, authority or None, path or None, query, fragment\\n-        )\\n-        ref.encoding = encoding\\n-        return ref\\n-\\n-    def __eq__(self, other):\\n-        \"\"\"Compare this reference to another.\"\"\"\\n-        other_ref = other\\n-        if isinstance(other, tuple):\\n-            other_ref = self.__class__(*other)\\n-        elif not isinstance(other, IRIReference):\\n-            try:\\n-                other_ref = self.__class__.from_string(other)\\n-            except TypeError:\\n-                raise TypeError(\\n-                    \"Unable to compare {0}() to {1}()\".format(\\n-                        type(self).__name__, type(other).__name__\\n-                    )\\n-                )\\n-\\n-        # See http://tools.ietf.org/html/rfc3986#section-6.2\\n-        return tuple(self) == tuple(other_ref)\\n-\\n-    def _match_subauthority(self):\\n-        return misc.ISUBAUTHORITY_MATCHER.match(self.authority)\\n-\\n-    @classmethod\\n-    def from_string(cls, iri_string, encoding=\"utf-8\"):\\n-        \"\"\"Parse a IRI reference from the given unicode IRI string.\\n-\\n-        :param str iri_string: Unicode IRI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :returns: :class:`IRIReference` or subclass thereof\\n-        \"\"\"\\n-        iri_string = compat.to_str(iri_string, encoding)\\n-\\n-        split_iri = misc.IRI_MATCHER.match(iri_string).groupdict()\\n-        return cls(\\n-            split_iri[\"scheme\"],\\n-            split_iri[\"authority\"],\\n-            normalizers.encode_component(split_iri[\"path\"], encoding),\\n-            normalizers.encode_component(split_iri[\"query\"], encoding),\\n-            normalizers.encode_component(split_iri[\"fragment\"], encoding),\\n-            encoding,\\n-        )\\n-\\n-    def encode(self, idna_encoder=None):  # noqa: C901\\n-        \"\"\"Encode an IRIReference into a URIReference instance.\\n-\\n-        If the ``idna`` module is installed or the ``rfc3986[idna]``\\n-        extra is used then unicode characters in the IRI host\\n-        component will be encoded with IDNA2008.\\n-\\n-        :param idna_encoder:\\n-            Function that encodes each part of the host component\\n-            If not given will raise an exception if the IRI\\n-            contains a host component.\\n-        :rtype: uri.URIReference\\n-        :returns: A URI reference\\n-        \"\"\"\\n-        authority = self.authority\\n-        if authority:\\n-            if idna_encoder is None:\\n-                if idna is None:  # pragma: no cover\\n-                    raise exceptions.MissingDependencyError(\\n-                        \"Could not import the \\'idna\\' module \"\\n-                        \"and the IRI hostname requires encoding\"\\n-                    )\\n-\\n-                def idna_encoder(name):\\n-                    if any(ord(c) > 128 for c in name):\\n-                        try:\\n-                            return idna.encode(\\n-                                name.lower(), strict=True, std3_rules=True\\n-                            )\\n-                        except idna.IDNAError:\\n-                            raise exceptions.InvalidAuthority(self.authority)\\n-                    return name\\n-\\n-            authority = \"\"\\n-            if self.host:\\n-                authority = \".\".join(\\n-                    [compat.to_str(idna_encoder(part)) for part in self.host.split(\".\")]\\n-                )\\n-\\n-            if self.userinfo is not None:\\n-                authority = (\\n-                    normalizers.encode_component(self.userinfo, self.encoding)\\n-                    + \"@\"\\n-                    + authority\\n-                )\\n-\\n-            if self.port is not None:\\n-                authority += \":\" + str(self.port)\\n-\\n-        return uri.URIReference(\\n-            self.scheme,\\n-            authority,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-            encoding=self.encoding,\\n-        )', '@@ -1,125 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"\\n-Module containing compiled regular expressions and constants.\\n-\\n-This module contains important constants, patterns, and compiled regular\\n-expressions for parsing and validating URIs and their components.\\n-\"\"\"\\n-\\n-import re\\n-\\n-from . import abnf_regexp\\n-\\n-# These are enumerated for the named tuple used as a superclass of\\n-# URIReference\\n-URI_COMPONENTS = [\"scheme\", \"authority\", \"path\", \"query\", \"fragment\"]\\n-\\n-important_characters = {\\n-    \"generic_delimiters\": abnf_regexp.GENERIC_DELIMITERS,\\n-    \"sub_delimiters\": abnf_regexp.SUB_DELIMITERS,\\n-    # We need to escape the \\'*\\' in this case\\n-    \"re_sub_delimiters\": abnf_regexp.SUB_DELIMITERS_RE,\\n-    \"unreserved_chars\": abnf_regexp.UNRESERVED_CHARS,\\n-    # We need to escape the \\'-\\' in this case:\\n-    \"re_unreserved\": abnf_regexp.UNRESERVED_RE,\\n-}\\n-\\n-# For details about delimiters and reserved characters, see:\\n-# http://tools.ietf.org/html/rfc3986#section-2.2\\n-GENERIC_DELIMITERS = abnf_regexp.GENERIC_DELIMITERS_SET\\n-SUB_DELIMITERS = abnf_regexp.SUB_DELIMITERS_SET\\n-RESERVED_CHARS = abnf_regexp.RESERVED_CHARS_SET\\n-# For details about unreserved characters, see:\\n-# http://tools.ietf.org/html/rfc3986#section-2.3\\n-UNRESERVED_CHARS = abnf_regexp.UNRESERVED_CHARS_SET\\n-NON_PCT_ENCODED = abnf_regexp.NON_PCT_ENCODED_SET\\n-\\n-URI_MATCHER = re.compile(abnf_regexp.URL_PARSING_RE)\\n-\\n-SUBAUTHORITY_MATCHER = re.compile(\\n-    (\\n-        \"^(?:(?P<userinfo>{0})@)?\"  # userinfo\\n-        \"(?P<host>{1})\"  # host\\n-        \":?(?P<port>{2})?$\"  # port\\n-    ).format(abnf_regexp.USERINFO_RE, abnf_regexp.HOST_PATTERN, abnf_regexp.PORT_RE)\\n-)\\n-\\n-\\n-HOST_MATCHER = re.compile(\"^\" + abnf_regexp.HOST_RE + \"$\")\\n-IPv4_MATCHER = re.compile(\"^\" + abnf_regexp.IPv4_RE + \"$\")\\n-IPv6_MATCHER = re.compile(r\"^\\\\[\" + abnf_regexp.IPv6_ADDRZ_RFC4007_RE + r\"\\\\]$\")\\n-\\n-# Used by host validator\\n-IPv6_NO_RFC4007_MATCHER = re.compile(r\"^\\\\[%s\\\\]$\" % (abnf_regexp.IPv6_ADDRZ_RE))\\n-\\n-# Matcher used to validate path components\\n-PATH_MATCHER = re.compile(abnf_regexp.PATH_RE)\\n-\\n-\\n-# ##################################\\n-# Query and Fragment Matcher Section\\n-# ##################################\\n-\\n-QUERY_MATCHER = re.compile(abnf_regexp.QUERY_RE)\\n-\\n-FRAGMENT_MATCHER = QUERY_MATCHER\\n-\\n-# Scheme validation, see: http://tools.ietf.org/html/rfc3986#section-3.1\\n-SCHEME_MATCHER = re.compile(\"^{0}$\".format(abnf_regexp.SCHEME_RE))\\n-\\n-RELATIVE_REF_MATCHER = re.compile(\\n-    r\"^%s(\\\\?%s)?(#%s)?$\"\\n-    % (abnf_regexp.RELATIVE_PART_RE, abnf_regexp.QUERY_RE, abnf_regexp.FRAGMENT_RE)\\n-)\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-4.3\\n-ABSOLUTE_URI_MATCHER = re.compile(\\n-    r\"^%s:%s(\\\\?%s)?$\"\\n-    % (\\n-        abnf_regexp.COMPONENT_PATTERN_DICT[\"scheme\"],\\n-        abnf_regexp.HIER_PART_RE,\\n-        abnf_regexp.QUERY_RE[1:-1],\\n-    )\\n-)\\n-\\n-# ###############\\n-# IRIs / RFC 3987\\n-# ###############\\n-\\n-IRI_MATCHER = re.compile(abnf_regexp.URL_PARSING_RE, re.UNICODE)\\n-\\n-ISUBAUTHORITY_MATCHER = re.compile(\\n-    (\\n-        u\"^(?:(?P<userinfo>{0})@)?\"  # iuserinfo\\n-        u\"(?P<host>{1})\"  # ihost\\n-        u\":?(?P<port>{2})?$\"  # port\\n-    ).format(abnf_regexp.IUSERINFO_RE, abnf_regexp.IHOST_RE, abnf_regexp.PORT_RE),\\n-    re.UNICODE,\\n-)\\n-\\n-\\n-# Path merger as defined in http://tools.ietf.org/html/rfc3986#section-5.2.3\\n-def merge_paths(base_uri, relative_path):\\n-    \"\"\"Merge a base URI\\'s path with a relative URI\\'s path.\"\"\"\\n-    if base_uri.path is None and base_uri.authority is not None:\\n-        return \"/\" + relative_path\\n-    else:\\n-        path = base_uri.path or \"\"\\n-        index = path.rfind(\"/\")\\n-        return path[:index] + \"/\" + relative_path\\n-\\n-\\n-UseExisting = object()', '@@ -1,6 +1,4 @@\\n-\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\\n-\\n-# Copyright (c) 2010-2015 Benjamin Peterson\\n+# Copyright (c) 2010-2019 Benjamin Peterson\\n #\\n # Permission is hereby granted, free of charge, to any person obtaining a copy\\n # of this software and associated documentation files (the \"Software\"), to deal\\n@@ -20,6 +18,8 @@\\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n # SOFTWARE.\\n \\n+\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\\n+\\n from __future__ import absolute_import\\n \\n import functools\\n@@ -29,7 +29,7 @@\\n import types\\n \\n __author__ = \"Benjamin Peterson <benjamin@python.org>\"\\n-__version__ = \"1.10.0\"\\n+__version__ = \"1.12.0\"\\n \\n \\n # Useful for very coarse version differentiation.\\n@@ -242,6 +242,7 @@ class _MovedItems(_LazyModule):\\n     MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\\n     MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\\n     MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\\n+    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\\n     MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\\n     MovedAttribute(\\n         \"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"\\n@@ -267,12 +268,13 @@ class _MovedItems(_LazyModule):\\n     MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\\n     MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\\n     MovedModule(\"http_client\", \"httplib\", \"http.client\"),\\n+    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\\n+    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\\n     MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\\n     MovedModule(\\n         \"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"\\n     ),\\n     MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\\n-    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\\n     MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\\n     MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\\n     MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\\n@@ -339,10 +341,14 @@ class Module_six_moves_urllib_parse(_LazyModule):\\n     MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\\n+    MovedAttribute(\\n+        \"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"\\n+    ),\\n     MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\\n+    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\\n     MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\\n     MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\\n@@ -424,6 +430,8 @@ class Module_six_moves_urllib_request(_LazyModule):\\n     MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\\n     MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\\n     MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\\n+    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\\n+    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\\n ]\\n for attr in _urllib_request_moved_attributes:\\n     setattr(Module_six_moves_urllib_request, attr.name, attr)\\n@@ -665,6 +673,7 @@ def u(s):\\n \\n     StringIO = io.StringIO\\n     BytesIO = io.BytesIO\\n+    del io\\n     _assertCountEqual = \"assertCountEqual\"\\n     if sys.version_info[1] <= 1:\\n         _assertRaisesRegex = \"assertRaisesRegexp\"\\n@@ -718,11 +727,15 @@ def assertRegex(self, *args, **kwargs):\\n     exec_ = getattr(moves.builtins, \"exec\")\\n \\n     def reraise(tp, value, tb=None):\\n-        if value is None:\\n-            value = tp()\\n-        if value.__traceback__ is not tb:\\n-            raise value.with_traceback(tb)\\n-        raise value\\n+        try:\\n+            if value is None:\\n+                value = tp()\\n+            if value.__traceback__ is not tb:\\n+                raise value.with_traceback(tb)\\n+            raise value\\n+        finally:\\n+            value = None\\n+            tb = None\\n \\n \\n else:\\n@@ -741,23 +754,32 @@ def exec_(_code_, _globs_=None, _locs_=None):\\n \\n     exec_(\\n         \"\"\"def reraise(tp, value, tb=None):\\n-    raise tp, value, tb\\n+    try:\\n+        raise tp, value, tb\\n+    finally:\\n+        tb = None\\n \"\"\"\\n     )\\n \\n \\n if sys.version_info[:2] == (3, 2):\\n     exec_(\\n         \"\"\"def raise_from(value, from_value):\\n-    if from_value is None:\\n-        raise value\\n-    raise value from from_value\\n+    try:\\n+        if from_value is None:\\n+            raise value\\n+        raise value from from_value\\n+    finally:\\n+        value = None\\n \"\"\"\\n     )\\n elif sys.version_info[:2] > (3, 2):\\n     exec_(\\n         \"\"\"def raise_from(value, from_value):\\n-    raise value from from_value\\n+    try:\\n+        raise value from from_value\\n+    finally:\\n+        value = None\\n \"\"\"\\n     )\\n else:\\n@@ -864,10 +886,14 @@ def with_metaclass(meta, *bases):\\n     # This requires a bit of explanation: the basic idea is to make a dummy\\n     # metaclass for one level of class instantiation that replaces itself with\\n     # the actual metaclass.\\n-    class metaclass(meta):\\n+    class metaclass(type):\\n         def __new__(cls, name, this_bases, d):\\n             return meta(name, bases, d)\\n \\n+        @classmethod\\n+        def __prepare__(cls, name, this_bases):\\n+            return meta.__prepare__(name, bases)\\n+\\n     return type.__new__(metaclass, \"temporary_class\", (), {})\\n \\n \\n@@ -884,11 +910,71 @@ def wrapper(cls):\\n                 orig_vars.pop(slots_var)\\n         orig_vars.pop(\"__dict__\", None)\\n         orig_vars.pop(\"__weakref__\", None)\\n+        if hasattr(cls, \"__qualname__\"):\\n+            orig_vars[\"__qualname__\"] = cls.__qualname__\\n         return metaclass(cls.__name__, cls.__bases__, orig_vars)\\n \\n     return wrapper\\n \\n \\n+def ensure_binary(s, encoding=\"utf-8\", errors=\"strict\"):\\n+    \"\"\"Coerce **s** to six.binary_type.\\n+\\n+    For Python 2:\\n+      - `unicode` -> encoded to `str`\\n+      - `str` -> `str`\\n+\\n+    For Python 3:\\n+      - `str` -> encoded to `bytes`\\n+      - `bytes` -> `bytes`\\n+    \"\"\"\\n+    if isinstance(s, text_type):\\n+        return s.encode(encoding, errors)\\n+    elif isinstance(s, binary_type):\\n+        return s\\n+    else:\\n+        raise TypeError(\"not expecting type \\'%s\\'\" % type(s))\\n+\\n+\\n+def ensure_str(s, encoding=\"utf-8\", errors=\"strict\"):\\n+    \"\"\"Coerce *s* to `str`.\\n+\\n+    For Python 2:\\n+      - `unicode` -> encoded to `str`\\n+      - `str` -> `str`\\n+\\n+    For Python 3:\\n+      - `str` -> `str`\\n+      - `bytes` -> decoded to `str`\\n+    \"\"\"\\n+    if not isinstance(s, (text_type, binary_type)):\\n+        raise TypeError(\"not expecting type \\'%s\\'\" % type(s))\\n+    if PY2 and isinstance(s, text_type):\\n+        s = s.encode(encoding, errors)\\n+    elif PY3 and isinstance(s, binary_type):\\n+        s = s.decode(encoding, errors)\\n+    return s\\n+\\n+\\n+def ensure_text(s, encoding=\"utf-8\", errors=\"strict\"):\\n+    \"\"\"Coerce *s* to six.text_type.\\n+\\n+    For Python 2:\\n+      - `unicode` -> `unicode`\\n+      - `str` -> `unicode`\\n+\\n+    For Python 3:\\n+      - `str` -> `str`\\n+      - `bytes` -> decoded to `str`\\n+    \"\"\"\\n+    if isinstance(s, binary_type):\\n+        return s.decode(encoding, errors)\\n+    elif isinstance(s, text_type):\\n+        return s\\n+    else:\\n+        raise TypeError(\"not expecting type \\'%s\\'\" % type(s))\\n+\\n+\\n def python_2_unicode_compatible(klass):\\n     \"\"\"\\n     A decorator that defines __unicode__ and __str__ methods under Python 2.', '@@ -26,7 +26,6 @@\\n from .packages.ssl_match_hostname import CertificateError\\n from .packages import six\\n from .packages.six.moves import queue\\n-from .packages.rfc3986.normalizers import normalize_host\\n from .connection import (\\n     port_by_scheme,\\n     DummyConnection,\\n@@ -44,7 +43,7 @@\\n from .util.response import assert_header_parsing\\n from .util.retry import Retry\\n from .util.timeout import Timeout\\n-from .util.url import get_host, Url, NORMALIZABLE_SCHEMES\\n+from .util.url import get_host, Url, _normalize_host as normalize_host\\n from .util.queue import LifoQueue\\n \\n \\n@@ -1027,14 +1026,14 @@ def _normalize_host(host, scheme):\\n     Normalize hosts for comparisons and use with sockets.\\n     \"\"\"\\n \\n+    host = normalize_host(host, scheme)\\n+\\n     # httplib doesn\\'t like it when we include brackets in IPv6 addresses\\n     # Specifically, if we include brackets but also pass the port then\\n     # httplib crazily doubles up the square brackets on the Host header.\\n     # Instead, we need to make sure we never pass ``None`` as the port.\\n     # However, for backward compatibility reasons we can\\'t actually\\n     # *assert* that.  See http://bugs.python.org/issue28539\\n     if host.startswith(\"[\") and host.endswith(\"]\"):\\n-        host = host.strip(\"[]\")\\n-    if scheme in NORMALIZABLE_SCHEMES:\\n-        host = normalize_host(host)\\n+        host = host[1:-1]\\n     return host', '@@ -1,112 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-\"\"\"Exceptions module for rfc3986.\"\"\"\\n-\\n-from . import compat\\n-\\n-\\n-class RFC3986Exception(Exception):\\n-    \"\"\"Base class for all rfc3986 exception classes.\"\"\"\\n-\\n-    pass\\n-\\n-\\n-class InvalidAuthority(RFC3986Exception):\\n-    \"\"\"Exception when the authority string is invalid.\"\"\"\\n-\\n-    def __init__(self, authority):\\n-        \"\"\"Initialize the exception with the invalid authority.\"\"\"\\n-        super(InvalidAuthority, self).__init__(\\n-            u\"The authority ({0}) is not valid.\".format(compat.to_str(authority))\\n-        )\\n-\\n-\\n-class InvalidPort(RFC3986Exception):\\n-    \"\"\"Exception when the port is invalid.\"\"\"\\n-\\n-    def __init__(self, port):\\n-        \"\"\"Initialize the exception with the invalid port.\"\"\"\\n-        super(InvalidPort, self).__init__(\\'The port (\"{0}\") is not valid.\\'.format(port))\\n-\\n-\\n-class ResolutionError(RFC3986Exception):\\n-    \"\"\"Exception to indicate a failure to resolve a URI.\"\"\"\\n-\\n-    def __init__(self, uri):\\n-        \"\"\"Initialize the error with the failed URI.\"\"\"\\n-        super(ResolutionError, self).__init__(\\n-            \"{0} is not an absolute URI.\".format(uri.unsplit())\\n-        )\\n-\\n-\\n-class ValidationError(RFC3986Exception):\\n-    \"\"\"Exception raised during Validation of a URI.\"\"\"\\n-\\n-    pass\\n-\\n-\\n-class MissingComponentError(ValidationError):\\n-    \"\"\"Exception raised when a required component is missing.\"\"\"\\n-\\n-    def __init__(self, uri, *component_names):\\n-        \"\"\"Initialize the error with the missing component name.\"\"\"\\n-        verb = \"was\"\\n-        if len(component_names) > 1:\\n-            verb = \"were\"\\n-\\n-        self.uri = uri\\n-        self.components = sorted(component_names)\\n-        components = \", \".join(self.components)\\n-        super(MissingComponentError, self).__init__(\\n-            \"{} {} required but missing\".format(components, verb), uri, self.components\\n-        )\\n-\\n-\\n-class UnpermittedComponentError(ValidationError):\\n-    \"\"\"Exception raised when a component has an unpermitted value.\"\"\"\\n-\\n-    def __init__(self, component_name, component_value, allowed_values):\\n-        \"\"\"Initialize the error with the unpermitted component.\"\"\"\\n-        super(UnpermittedComponentError, self).__init__(\\n-            \"{} was required to be one of {!r} but was {!r}\".format(\\n-                component_name, list(sorted(allowed_values)), component_value\\n-            ),\\n-            component_name,\\n-            component_value,\\n-            allowed_values,\\n-        )\\n-        self.component_name = component_name\\n-        self.component_value = component_value\\n-        self.allowed_values = allowed_values\\n-\\n-\\n-class PasswordForbidden(ValidationError):\\n-    \"\"\"Exception raised when a URL has a password in the userinfo section.\"\"\"\\n-\\n-    def __init__(self, uri):\\n-        \"\"\"Initialize the error with the URI that failed validation.\"\"\"\\n-        unsplit = getattr(uri, \"unsplit\", lambda: uri)\\n-        super(PasswordForbidden, self).__init__(\\n-            \\'\"{}\" contained a password when validation forbade it\\'.format(unsplit())\\n-        )\\n-        self.uri = uri\\n-\\n-\\n-class InvalidComponentsError(ValidationError):\\n-    \"\"\"Exception raised when one or more components are invalid.\"\"\"\\n-\\n-    def __init__(self, uri, *component_names):\\n-        \"\"\"Initialize the error with the invalid component name(s).\"\"\"\\n-        verb = \"was\"\\n-        if len(component_names) > 1:\\n-            verb = \"were\"\\n-\\n-        self.uri = uri\\n-        self.components = sorted(component_names)\\n-        components = \", \".join(self.components)\\n-        super(InvalidComponentsError, self).__init__(\\n-            \"{} {} found to be invalid\".format(components, verb), uri, self.components\\n-        )\\n-\\n-\\n-class MissingDependencyError(RFC3986Exception):\\n-    \"\"\"Exception raised when an IRI is encoded without the \\'idna\\' module.\"\"\"', '@@ -1,371 +0,0 @@\\n-\"\"\"Module containing the implementation of the URIMixin class.\"\"\"\\n-import warnings\\n-\\n-from . import exceptions as exc\\n-from . import misc\\n-from . import normalizers\\n-from . import validators\\n-\\n-\\n-class URIMixin(object):\\n-    \"\"\"Mixin with all shared methods for URIs and IRIs.\"\"\"\\n-\\n-    __hash__ = tuple.__hash__\\n-\\n-    def authority_info(self):\\n-        \"\"\"Return a dictionary with the ``userinfo``, ``host``, and ``port``.\\n-\\n-        If the authority is not valid, it will raise a\\n-        :class:`~rfc3986.exceptions.InvalidAuthority` Exception.\\n-\\n-        :returns:\\n-            ``{\\'userinfo\\': \\'username:password\\', \\'host\\': \\'www.example.com\\',\\n-            \\'port\\': \\'80\\'}``\\n-        :rtype: dict\\n-        :raises rfc3986.exceptions.InvalidAuthority:\\n-            If the authority is not ``None`` and can not be parsed.\\n-        \"\"\"\\n-        if not self.authority:\\n-            return {\"userinfo\": None, \"host\": None, \"port\": None}\\n-\\n-        match = self._match_subauthority()\\n-\\n-        if match is None:\\n-            # In this case, we have an authority that was parsed from the URI\\n-            # Reference, but it cannot be further parsed by our\\n-            # misc.SUBAUTHORITY_MATCHER. In this case it must not be a valid\\n-            # authority.\\n-            raise exc.InvalidAuthority(self.authority.encode(self.encoding))\\n-\\n-        # We had a match, now let\\'s ensure that it is actually a valid host\\n-        # address if it is IPv4\\n-        matches = match.groupdict()\\n-        host = matches.get(\"host\")\\n-\\n-        if (\\n-            host\\n-            and misc.IPv4_MATCHER.match(host)\\n-            and not validators.valid_ipv4_host_address(host)\\n-        ):\\n-            # If we have a host, it appears to be IPv4 and it does not have\\n-            # valid bytes, it is an InvalidAuthority.\\n-            raise exc.InvalidAuthority(self.authority.encode(self.encoding))\\n-\\n-        return matches\\n-\\n-    def _match_subauthority(self):\\n-        return misc.SUBAUTHORITY_MATCHER.match(self.authority)\\n-\\n-    @property\\n-    def host(self):\\n-        \"\"\"If present, a string representing the host.\"\"\"\\n-        try:\\n-            authority = self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return None\\n-        return authority[\"host\"]\\n-\\n-    @property\\n-    def port(self):\\n-        \"\"\"If present, the port extracted from the authority.\"\"\"\\n-        try:\\n-            authority = self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return None\\n-        return authority[\"port\"]\\n-\\n-    @property\\n-    def userinfo(self):\\n-        \"\"\"If present, the userinfo extracted from the authority.\"\"\"\\n-        try:\\n-            authority = self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return None\\n-        return authority[\"userinfo\"]\\n-\\n-    def is_absolute(self):\\n-        \"\"\"Determine if this URI Reference is an absolute URI.\\n-\\n-        See http://tools.ietf.org/html/rfc3986#section-4.3 for explanation.\\n-\\n-        :returns: ``True`` if it is an absolute URI, ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        return bool(misc.ABSOLUTE_URI_MATCHER.match(self.unsplit()))\\n-\\n-    def is_valid(self, **kwargs):\\n-        \"\"\"Determine if the URI is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param bool require_scheme: Set to ``True`` if you wish to require the\\n-            presence of the scheme component.\\n-        :param bool require_authority: Set to ``True`` if you wish to require\\n-            the presence of the authority component.\\n-        :param bool require_path: Set to ``True`` if you wish to require the\\n-            presence of the path component.\\n-        :param bool require_query: Set to ``True`` if you wish to require the\\n-            presence of the query component.\\n-        :param bool require_fragment: Set to ``True`` if you wish to require\\n-            the presence of the fragment component.\\n-        :returns: ``True`` if the URI is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        validators = [\\n-            (self.scheme_is_valid, kwargs.get(\"require_scheme\", False)),\\n-            (self.authority_is_valid, kwargs.get(\"require_authority\", False)),\\n-            (self.path_is_valid, kwargs.get(\"require_path\", False)),\\n-            (self.query_is_valid, kwargs.get(\"require_query\", False)),\\n-            (self.fragment_is_valid, kwargs.get(\"require_fragment\", False)),\\n-        ]\\n-        return all(v(r) for v, r in validators)\\n-\\n-    def authority_is_valid(self, require=False):\\n-        \"\"\"Determine if the authority component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param bool require:\\n-            Set to ``True`` to require the presence of this component.\\n-        :returns:\\n-            ``True`` if the authority is valid. ``False`` otherwise.\\n-        :rtype:\\n-            bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        try:\\n-            self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return False\\n-\\n-        return validators.authority_is_valid(\\n-            self.authority, host=self.host, require=require\\n-        )\\n-\\n-    def scheme_is_valid(self, require=False):\\n-        \"\"\"Determine if the scheme component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the scheme is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.scheme_is_valid(self.scheme, require)\\n-\\n-    def path_is_valid(self, require=False):\\n-        \"\"\"Determine if the path component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the path is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.path_is_valid(self.path, require)\\n-\\n-    def query_is_valid(self, require=False):\\n-        \"\"\"Determine if the query component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the query is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.query_is_valid(self.query, require)\\n-\\n-    def fragment_is_valid(self, require=False):\\n-        \"\"\"Determine if the fragment component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the Validator object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the fragment is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.fragment_is_valid(self.fragment, require)\\n-\\n-    def normalized_equality(self, other_ref):\\n-        \"\"\"Compare this URIReference to another URIReference.\\n-\\n-        :param URIReference other_ref: (required), The reference with which\\n-            we\\'re comparing.\\n-        :returns: ``True`` if the references are equal, ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        return tuple(self.normalize()) == tuple(other_ref.normalize())\\n-\\n-    def resolve_with(self, base_uri, strict=False):\\n-        \"\"\"Use an absolute URI Reference to resolve this relative reference.\\n-\\n-        Assuming this is a relative reference that you would like to resolve,\\n-        use the provided base URI to resolve it.\\n-\\n-        See http://tools.ietf.org/html/rfc3986#section-5 for more information.\\n-\\n-        :param base_uri: Either a string or URIReference. It must be an\\n-            absolute URI or it will raise an exception.\\n-        :returns: A new URIReference which is the result of resolving this\\n-            reference using ``base_uri``.\\n-        :rtype: :class:`URIReference`\\n-        :raises rfc3986.exceptions.ResolutionError:\\n-            If the ``base_uri`` is not an absolute URI.\\n-        \"\"\"\\n-        if not isinstance(base_uri, URIMixin):\\n-            base_uri = type(self).from_string(base_uri)\\n-\\n-        if not base_uri.is_absolute():\\n-            raise exc.ResolutionError(base_uri)\\n-\\n-        # This is optional per\\n-        # http://tools.ietf.org/html/rfc3986#section-5.2.1\\n-        base_uri = base_uri.normalize()\\n-\\n-        # The reference we\\'re resolving\\n-        resolving = self\\n-\\n-        if not strict and resolving.scheme == base_uri.scheme:\\n-            resolving = resolving.copy_with(scheme=None)\\n-\\n-        # http://tools.ietf.org/html/rfc3986#page-32\\n-        if resolving.scheme is not None:\\n-            target = resolving.copy_with(\\n-                path=normalizers.normalize_path(resolving.path)\\n-            )\\n-        else:\\n-            if resolving.authority is not None:\\n-                target = resolving.copy_with(\\n-                    scheme=base_uri.scheme,\\n-                    path=normalizers.normalize_path(resolving.path),\\n-                )\\n-            else:\\n-                if resolving.path is None:\\n-                    if resolving.query is not None:\\n-                        query = resolving.query\\n-                    else:\\n-                        query = base_uri.query\\n-                    target = resolving.copy_with(\\n-                        scheme=base_uri.scheme,\\n-                        authority=base_uri.authority,\\n-                        path=base_uri.path,\\n-                        query=query,\\n-                    )\\n-                else:\\n-                    if resolving.path.startswith(\"/\"):\\n-                        path = normalizers.normalize_path(resolving.path)\\n-                    else:\\n-                        path = normalizers.normalize_path(\\n-                            misc.merge_paths(base_uri, resolving.path)\\n-                        )\\n-                    target = resolving.copy_with(\\n-                        scheme=base_uri.scheme,\\n-                        authority=base_uri.authority,\\n-                        path=path,\\n-                        query=resolving.query,\\n-                    )\\n-        return target\\n-\\n-    def unsplit(self):\\n-        \"\"\"Create a URI string from the components.\\n-\\n-        :returns: The URI Reference reconstituted as a string.\\n-        :rtype: str\\n-        \"\"\"\\n-        # See http://tools.ietf.org/html/rfc3986#section-5.3\\n-        result_list = []\\n-        if self.scheme:\\n-            result_list.extend([self.scheme, \":\"])\\n-        if self.authority:\\n-            result_list.extend([\"//\", self.authority])\\n-        if self.path:\\n-            result_list.append(self.path)\\n-        if self.query is not None:\\n-            result_list.extend([\"?\", self.query])\\n-        if self.fragment is not None:\\n-            result_list.extend([\"#\", self.fragment])\\n-        return \"\".join(result_list)\\n-\\n-    def copy_with(\\n-        self,\\n-        scheme=misc.UseExisting,\\n-        authority=misc.UseExisting,\\n-        path=misc.UseExisting,\\n-        query=misc.UseExisting,\\n-        fragment=misc.UseExisting,\\n-    ):\\n-        \"\"\"Create a copy of this reference with the new components.\\n-\\n-        :param str scheme:\\n-            (optional) The scheme to use for the new reference.\\n-        :param str authority:\\n-            (optional) The authority to use for the new reference.\\n-        :param str path:\\n-            (optional) The path to use for the new reference.\\n-        :param str query:\\n-            (optional) The query to use for the new reference.\\n-        :param str fragment:\\n-            (optional) The fragment to use for the new reference.\\n-        :returns:\\n-            New URIReference with provided components.\\n-        :rtype:\\n-            URIReference\\n-        \"\"\"\\n-        attributes = {\\n-            \"scheme\": scheme,\\n-            \"authority\": authority,\\n-            \"path\": path,\\n-            \"query\": query,\\n-            \"fragment\": fragment,\\n-        }\\n-        for key, value in list(attributes.items()):\\n-            if value is misc.UseExisting:\\n-                del attributes[key]\\n-        uri = self._replace(**attributes)\\n-        uri.encoding = self.encoding\\n-        return uri', '@@ -1,106 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"\\n-Module containing the simple and functional API for rfc3986.\\n-\\n-This module defines functions and provides access to the public attributes\\n-and classes of rfc3986.\\n-\"\"\"\\n-\\n-from .iri import IRIReference\\n-from .parseresult import ParseResult\\n-from .uri import URIReference\\n-\\n-\\n-def uri_reference(uri, encoding=\"utf-8\"):\\n-    \"\"\"Parse a URI string into a URIReference.\\n-\\n-    This is a convenience function. You could achieve the same end by using\\n-    ``URIReference.from_string(uri)``.\\n-\\n-    :param str uri: The URI which needs to be parsed into a reference.\\n-    :param str encoding: The encoding of the string provided\\n-    :returns: A parsed URI\\n-    :rtype: :class:`URIReference`\\n-    \"\"\"\\n-    return URIReference.from_string(uri, encoding)\\n-\\n-\\n-def iri_reference(iri, encoding=\"utf-8\"):\\n-    \"\"\"Parse a IRI string into an IRIReference.\\n-\\n-    This is a convenience function. You could achieve the same end by using\\n-    ``IRIReference.from_string(iri)``.\\n-\\n-    :param str iri: The IRI which needs to be parsed into a reference.\\n-    :param str encoding: The encoding of the string provided\\n-    :returns: A parsed IRI\\n-    :rtype: :class:`IRIReference`\\n-    \"\"\"\\n-    return IRIReference.from_string(iri, encoding)\\n-\\n-\\n-def is_valid_uri(uri, encoding=\"utf-8\", **kwargs):\\n-    \"\"\"Determine if the URI given is valid.\\n-\\n-    This is a convenience function. You could use either\\n-    ``uri_reference(uri).is_valid()`` or\\n-    ``URIReference.from_string(uri).is_valid()`` to achieve the same result.\\n-\\n-    :param str uri: The URI to be validated.\\n-    :param str encoding: The encoding of the string provided\\n-    :param bool require_scheme: Set to ``True`` if you wish to require the\\n-        presence of the scheme component.\\n-    :param bool require_authority: Set to ``True`` if you wish to require the\\n-        presence of the authority component.\\n-    :param bool require_path: Set to ``True`` if you wish to require the\\n-        presence of the path component.\\n-    :param bool require_query: Set to ``True`` if you wish to require the\\n-        presence of the query component.\\n-    :param bool require_fragment: Set to ``True`` if you wish to require the\\n-        presence of the fragment component.\\n-    :returns: ``True`` if the URI is valid, ``False`` otherwise.\\n-    :rtype: bool\\n-    \"\"\"\\n-    return URIReference.from_string(uri, encoding).is_valid(**kwargs)\\n-\\n-\\n-def normalize_uri(uri, encoding=\"utf-8\"):\\n-    \"\"\"Normalize the given URI.\\n-\\n-    This is a convenience function. You could use either\\n-    ``uri_reference(uri).normalize().unsplit()`` or\\n-    ``URIReference.from_string(uri).normalize().unsplit()`` instead.\\n-\\n-    :param str uri: The URI to be normalized.\\n-    :param str encoding: The encoding of the string provided\\n-    :returns: The normalized URI.\\n-    :rtype: str\\n-    \"\"\"\\n-    normalized_reference = URIReference.from_string(uri, encoding).normalize()\\n-    return normalized_reference.unsplit()\\n-\\n-\\n-def urlparse(uri, encoding=\"utf-8\"):\\n-    \"\"\"Parse a given URI and return a ParseResult.\\n-\\n-    This is a partial replacement of the standard library\\'s urlparse function.\\n-\\n-    :param str uri: The URI to be parsed.\\n-    :param str encoding: The encoding of the string provided.\\n-    :returns: A parsed URI\\n-    :rtype: :class:`~rfc3986.parseresult.ParseResult`\\n-    \"\"\"\\n-    return ParseResult.from_string(uri, encoding, strict=False)', '@@ -1,301 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2017 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module containing the logic for the URIBuilder object.\"\"\"\\n-from . import compat\\n-from . import normalizers\\n-from . import uri\\n-\\n-\\n-class URIBuilder(object):\\n-    \"\"\"Object to aid in building up a URI Reference from parts.\\n-\\n-    .. note::\\n-\\n-        This object should be instantiated by the user, but it\\'s recommended\\n-        that it is not provided with arguments. Instead, use the available\\n-        method to populate the fields.\\n-\\n-    \"\"\"\\n-\\n-    def __init__(\\n-        self,\\n-        scheme=None,\\n-        userinfo=None,\\n-        host=None,\\n-        port=None,\\n-        path=None,\\n-        query=None,\\n-        fragment=None,\\n-    ):\\n-        \"\"\"Initialize our URI builder.\\n-\\n-        :param str scheme:\\n-            (optional)\\n-        :param str userinfo:\\n-            (optional)\\n-        :param str host:\\n-            (optional)\\n-        :param int port:\\n-            (optional)\\n-        :param str path:\\n-            (optional)\\n-        :param str query:\\n-            (optional)\\n-        :param str fragment:\\n-            (optional)\\n-        \"\"\"\\n-        self.scheme = scheme\\n-        self.userinfo = userinfo\\n-        self.host = host\\n-        self.port = port\\n-        self.path = path\\n-        self.query = query\\n-        self.fragment = fragment\\n-\\n-    def __repr__(self):\\n-        \"\"\"Provide a convenient view of our builder object.\"\"\"\\n-        formatstr = (\\n-            \"URIBuilder(scheme={b.scheme}, userinfo={b.userinfo}, \"\\n-            \"host={b.host}, port={b.port}, path={b.path}, \"\\n-            \"query={b.query}, fragment={b.fragment})\"\\n-        )\\n-        return formatstr.format(b=self)\\n-\\n-    def add_scheme(self, scheme):\\n-        \"\"\"Add a scheme to our builder object.\\n-\\n-        After normalizing, this will generate a new URIBuilder instance with\\n-        the specified scheme and all other attributes the same.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_scheme(\\'HTTPS\\')\\n-            URIBuilder(scheme=\\'https\\', userinfo=None, host=None, port=None,\\n-                    path=None, query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        scheme = normalizers.normalize_scheme(scheme)\\n-        return URIBuilder(\\n-            scheme=scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_credentials(self, username, password):\\n-        \"\"\"Add credentials as the userinfo portion of the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_credentials(\\'root\\', \\'s3crete\\')\\n-            URIBuilder(scheme=None, userinfo=\\'root:s3crete\\', host=None,\\n-                    port=None, path=None, query=None, fragment=None)\\n-\\n-            >>> URIBuilder().add_credentials(\\'root\\', None)\\n-            URIBuilder(scheme=None, userinfo=\\'root\\', host=None,\\n-                    port=None, path=None, query=None, fragment=None)\\n-        \"\"\"\\n-        if username is None:\\n-            raise ValueError(\"Username cannot be None\")\\n-        userinfo = normalizers.normalize_username(username)\\n-\\n-        if password is not None:\\n-            userinfo = \"{}:{}\".format(\\n-                userinfo, normalizers.normalize_password(password)\\n-            )\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_host(self, host):\\n-        \"\"\"Add hostname to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_host(\\'google.com\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=\\'google.com\\',\\n-                    port=None, path=None, query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=normalizers.normalize_host(host),\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_port(self, port):\\n-        \"\"\"Add port to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_port(80)\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=\\'80\\',\\n-                    path=None, query=None, fragment=None)\\n-\\n-            >>> URIBuilder().add_port(443)\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=\\'443\\',\\n-                    path=None, query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        port_int = int(port)\\n-        if port_int < 0:\\n-            raise ValueError(\\n-                \"ports are not allowed to be negative. You provided {}\".format(port_int)\\n-            )\\n-        if port_int > 65535:\\n-            raise ValueError(\\n-                \"ports are not allowed to be larger than 65535. \"\\n-                \"You provided {}\".format(port_int)\\n-            )\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=\"{}\".format(port_int),\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_path(self, path):\\n-        \"\"\"Add a path to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_path(\\'sigmavirus24/rfc3985\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=\\'/sigmavirus24/rfc3986\\', query=None, fragment=None)\\n-\\n-            >>> URIBuilder().add_path(\\'/checkout.php\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=\\'/checkout.php\\', query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        if not path.startswith(\"/\"):\\n-            path = \"/{}\".format(path)\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=normalizers.normalize_path(path),\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_query_from(self, query_items):\\n-        \"\"\"Generate and add a query a dictionary or list of tuples.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_query_from({\\'a\\': \\'b c\\'})\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=\\'a=b+c\\', fragment=None)\\n-\\n-            >>> URIBuilder().add_query_from([(\\'a\\', \\'b c\\')])\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=\\'a=b+c\\', fragment=None)\\n-\\n-        \"\"\"\\n-        query = normalizers.normalize_query(compat.urlencode(query_items))\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_query(self, query):\\n-        \"\"\"Add a pre-formated query string to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_query(\\'a=b&c=d\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=\\'a=b&c=d\\', fragment=None)\\n-\\n-        \"\"\"\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=normalizers.normalize_query(query),\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_fragment(self, fragment):\\n-        \"\"\"Add a fragment to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_fragment(\\'section-2.6.1\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=None, fragment=\\'section-2.6.1\\')\\n-\\n-        \"\"\"\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=normalizers.normalize_fragment(fragment),\\n-        )\\n-\\n-    def finalize(self):\\n-        \"\"\"Create a URIReference from our builder.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_scheme(\\'https\\').add_host(\\'github.com\\'\\n-            ...     ).add_path(\\'sigmavirus24/rfc3986\\').finalize().unsplit()\\n-            \\'https://github.com/sigmavirus24/rfc3986\\'\\n-\\n-            >>> URIBuilder().add_scheme(\\'https\\').add_host(\\'github.com\\'\\n-            ...     ).add_path(\\'sigmavirus24/rfc3986\\').add_credentials(\\n-            ...     \\'sigmavirus24\\', \\'not-re@l\\').finalize().unsplit()\\n-            \\'https://sigmavirus24:not-re%40l@github.com/sigmavirus24/rfc3986\\'\\n-\\n-        \"\"\"\\n-        return uri.URIReference(\\n-            self.scheme,\\n-            normalizers.normalize_authority((self.userinfo, self.host, self.port)),\\n-            self.path,\\n-            self.query,\\n-            self.fragment,\\n-        )', '@@ -1,152 +0,0 @@\\n-\"\"\"Module containing the implementation of the URIReference class.\"\"\"\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Copyright (c) 2015 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-from collections import namedtuple\\n-\\n-from . import compat\\n-from . import misc\\n-from . import normalizers\\n-from ._mixin import URIMixin\\n-\\n-\\n-class URIReference(namedtuple(\"URIReference\", misc.URI_COMPONENTS), URIMixin):\\n-    \"\"\"Immutable object representing a parsed URI Reference.\\n-\\n-    .. note::\\n-\\n-        This class is not intended to be directly instantiated by the user.\\n-\\n-    This object exposes attributes for the following components of a\\n-    URI:\\n-\\n-    - scheme\\n-    - authority\\n-    - path\\n-    - query\\n-    - fragment\\n-\\n-    .. attribute:: scheme\\n-\\n-        The scheme that was parsed for the URI Reference. For example,\\n-        ``http``, ``https``, ``smtp``, ``imap``, etc.\\n-\\n-    .. attribute:: authority\\n-\\n-        Component of the URI that contains the user information, host,\\n-        and port sub-components. For example,\\n-        ``google.com``, ``127.0.0.1:5000``, ``username@[::1]``,\\n-        ``username:password@example.com:443``, etc.\\n-\\n-    .. attribute:: path\\n-\\n-        The path that was parsed for the given URI Reference. For example,\\n-        ``/``, ``/index.php``, etc.\\n-\\n-    .. attribute:: query\\n-\\n-        The query component for a given URI Reference. For example, ``a=b``,\\n-        ``a=b%20c``, ``a=b+c``, ``a=b,c=d,e=%20f``, etc.\\n-\\n-    .. attribute:: fragment\\n-\\n-        The fragment component of a URI. For example, ``section-3.1``.\\n-\\n-    This class also provides extra attributes for easier access to information\\n-    like the subcomponents of the authority component.\\n-\\n-    .. attribute:: userinfo\\n-\\n-        The user information parsed from the authority.\\n-\\n-    .. attribute:: host\\n-\\n-        The hostname, IPv4, or IPv6 adddres parsed from the authority.\\n-\\n-    .. attribute:: port\\n-\\n-        The port parsed from the authority.\\n-    \"\"\"\\n-\\n-    slots = ()\\n-\\n-    def __new__(cls, scheme, authority, path, query, fragment, encoding=\"utf-8\"):\\n-        \"\"\"Create a new URIReference.\"\"\"\\n-        ref = super(URIReference, cls).__new__(\\n-            cls, scheme or None, authority or None, path or None, query, fragment\\n-        )\\n-        ref.encoding = encoding\\n-        return ref\\n-\\n-    __hash__ = tuple.__hash__\\n-\\n-    def __eq__(self, other):\\n-        \"\"\"Compare this reference to another.\"\"\"\\n-        other_ref = other\\n-        if isinstance(other, tuple):\\n-            other_ref = URIReference(*other)\\n-        elif not isinstance(other, URIReference):\\n-            try:\\n-                other_ref = URIReference.from_string(other)\\n-            except TypeError:\\n-                raise TypeError(\\n-                    \"Unable to compare URIReference() to {0}()\".format(\\n-                        type(other).__name__\\n-                    )\\n-                )\\n-\\n-        # See http://tools.ietf.org/html/rfc3986#section-6.2\\n-        naive_equality = tuple(self) == tuple(other_ref)\\n-        return naive_equality or self.normalized_equality(other_ref)\\n-\\n-    def normalize(self):\\n-        \"\"\"Normalize this reference as described in Section 6.2.2.\\n-\\n-        This is not an in-place normalization. Instead this creates a new\\n-        URIReference.\\n-\\n-        :returns: A new reference object with normalized components.\\n-        :rtype: URIReference\\n-        \"\"\"\\n-        # See http://tools.ietf.org/html/rfc3986#section-6.2.2 for logic in\\n-        # this method.\\n-        return URIReference(\\n-            normalizers.normalize_scheme(self.scheme or \"\"),\\n-            normalizers.normalize_authority((self.userinfo, self.host, self.port)),\\n-            normalizers.normalize_path(self.path or \"\"),\\n-            normalizers.normalize_query(self.query),\\n-            normalizers.normalize_fragment(self.fragment),\\n-            self.encoding,\\n-        )\\n-\\n-    @classmethod\\n-    def from_string(cls, uri_string, encoding=\"utf-8\"):\\n-        \"\"\"Parse a URI reference from the given unicode URI string.\\n-\\n-        :param str uri_string: Unicode URI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :returns: :class:`URIReference` or subclass thereof\\n-        \"\"\"\\n-        uri_string = compat.to_str(uri_string, encoding)\\n-\\n-        split_uri = misc.URI_MATCHER.match(uri_string).groupdict()\\n-        return cls(\\n-            split_uri[\"scheme\"],\\n-            split_uri[\"authority\"],\\n-            normalizers.encode_component(split_uri[\"path\"], encoding),\\n-            normalizers.encode_component(split_uri[\"query\"], encoding),\\n-            normalizers.encode_component(split_uri[\"fragment\"], encoding),\\n-            encoding,\\n-        )', '@@ -1,435 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2017 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module containing the validation logic for rfc3986.\"\"\"\\n-from . import exceptions\\n-from . import misc\\n-from . import normalizers\\n-\\n-\\n-class Validator(object):\\n-    \"\"\"Object used to configure validation of all objects in rfc3986.\\n-\\n-    .. versionadded:: 1.0\\n-\\n-    Example usage::\\n-\\n-         >>> from rfc3986 import api, validators\\n-         >>> uri = api.uri_reference(\\'https://github.com/\\')\\n-         >>> validator = validators.Validator().require_presence_of(\\n-         ...    \\'scheme\\', \\'host\\', \\'path\\',\\n-         ... ).allow_schemes(\\n-         ...    \\'http\\', \\'https\\',\\n-         ... ).allow_hosts(\\n-         ...    \\'127.0.0.1\\', \\'github.com\\',\\n-         ... )\\n-         >>> validator.validate(uri)\\n-         >>> invalid_uri = rfc3986.uri_reference(\\'imap://mail.google.com\\')\\n-         >>> validator.validate(invalid_uri)\\n-         Traceback (most recent call last):\\n-         ...\\n-         rfc3986.exceptions.MissingComponentError: (\\'path was required but\\n-         missing\\', URIReference(scheme=u\\'imap\\', authority=u\\'mail.google.com\\',\\n-         path=None, query=None, fragment=None), [\\'path\\'])\\n-\\n-    \"\"\"\\n-\\n-    COMPONENT_NAMES = frozenset(\\n-        [\"scheme\", \"userinfo\", \"host\", \"port\", \"path\", \"query\", \"fragment\"]\\n-    )\\n-\\n-    def __init__(self):\\n-        \"\"\"Initialize our default validations.\"\"\"\\n-        self.allowed_schemes = set()\\n-        self.allowed_hosts = set()\\n-        self.allowed_ports = set()\\n-        self.allow_password = True\\n-        self.required_components = {\\n-            \"scheme\": False,\\n-            \"userinfo\": False,\\n-            \"host\": False,\\n-            \"port\": False,\\n-            \"path\": False,\\n-            \"query\": False,\\n-            \"fragment\": False,\\n-        }\\n-        self.validated_components = self.required_components.copy()\\n-\\n-    def allow_schemes(self, *schemes):\\n-        \"\"\"Require the scheme to be one of the provided schemes.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param schemes:\\n-            Schemes, without ``://`` that are allowed.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        for scheme in schemes:\\n-            self.allowed_schemes.add(normalizers.normalize_scheme(scheme))\\n-        return self\\n-\\n-    def allow_hosts(self, *hosts):\\n-        \"\"\"Require the host to be one of the provided hosts.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param hosts:\\n-            Hosts that are allowed.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        for host in hosts:\\n-            self.allowed_hosts.add(normalizers.normalize_host(host))\\n-        return self\\n-\\n-    def allow_ports(self, *ports):\\n-        \"\"\"Require the port to be one of the provided ports.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param ports:\\n-            Ports that are allowed.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        for port in ports:\\n-            port_int = int(port, base=10)\\n-            if 0 <= port_int <= 65535:\\n-                self.allowed_ports.add(port)\\n-        return self\\n-\\n-    def allow_use_of_password(self):\\n-        \"\"\"Allow passwords to be present in the URI.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        self.allow_password = True\\n-        return self\\n-\\n-    def forbid_use_of_password(self):\\n-        \"\"\"Prevent passwords from being included in the URI.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        self.allow_password = False\\n-        return self\\n-\\n-    def check_validity_of(self, *components):\\n-        \"\"\"Check the validity of the components provided.\\n-\\n-        This can be specified repeatedly.\\n-\\n-        .. versionadded:: 1.1\\n-\\n-        :param components:\\n-            Names of components from :attr:`Validator.COMPONENT_NAMES`.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        components = [c.lower() for c in components]\\n-        for component in components:\\n-            if component not in self.COMPONENT_NAMES:\\n-                raise ValueError(\\'\"{}\" is not a valid component\\'.format(component))\\n-        self.validated_components.update({component: True for component in components})\\n-        return self\\n-\\n-    def require_presence_of(self, *components):\\n-        \"\"\"Require the components provided.\\n-\\n-        This can be specified repeatedly.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param components:\\n-            Names of components from :attr:`Validator.COMPONENT_NAMES`.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        components = [c.lower() for c in components]\\n-        for component in components:\\n-            if component not in self.COMPONENT_NAMES:\\n-                raise ValueError(\\'\"{}\" is not a valid component\\'.format(component))\\n-        self.required_components.update({component: True for component in components})\\n-        return self\\n-\\n-    def validate(self, uri):\\n-        \"\"\"Check a URI for conditions specified on this validator.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param uri:\\n-            Parsed URI to validate.\\n-        :type uri:\\n-            rfc3986.uri.URIReference\\n-        :raises MissingComponentError:\\n-            When a required component is missing.\\n-        :raises UnpermittedComponentError:\\n-            When a component is not one of those allowed.\\n-        :raises PasswordForbidden:\\n-            When a password is present in the userinfo component but is\\n-            not permitted by configuration.\\n-        :raises InvalidComponentsError:\\n-            When a component was found to be invalid.\\n-        \"\"\"\\n-        if not self.allow_password:\\n-            check_password(uri)\\n-\\n-        required_components = [\\n-            component\\n-            for component, required in self.required_components.items()\\n-            if required\\n-        ]\\n-        validated_components = [\\n-            component\\n-            for component, required in self.validated_components.items()\\n-            if required\\n-        ]\\n-        if required_components:\\n-            ensure_required_components_exist(uri, required_components)\\n-        if validated_components:\\n-            ensure_components_are_valid(uri, validated_components)\\n-\\n-        ensure_one_of(self.allowed_schemes, uri, \"scheme\")\\n-        ensure_one_of(self.allowed_hosts, uri, \"host\")\\n-        ensure_one_of(self.allowed_ports, uri, \"port\")\\n-\\n-\\n-def check_password(uri):\\n-    \"\"\"Assert that there is no password present in the uri.\"\"\"\\n-    userinfo = uri.userinfo\\n-    if not userinfo:\\n-        return\\n-    credentials = userinfo.split(\":\", 1)\\n-    if len(credentials) <= 1:\\n-        return\\n-    raise exceptions.PasswordForbidden(uri)\\n-\\n-\\n-def ensure_one_of(allowed_values, uri, attribute):\\n-    \"\"\"Assert that the uri\\'s attribute is one of the allowed values.\"\"\"\\n-    value = getattr(uri, attribute)\\n-    if value is not None and allowed_values and value not in allowed_values:\\n-        raise exceptions.UnpermittedComponentError(attribute, value, allowed_values)\\n-\\n-\\n-def ensure_required_components_exist(uri, required_components):\\n-    \"\"\"Assert that all required components are present in the URI.\"\"\"\\n-    missing_components = sorted(\\n-        [\\n-            component\\n-            for component in required_components\\n-            if getattr(uri, component) is None\\n-        ]\\n-    )\\n-    if missing_components:\\n-        raise exceptions.MissingComponentError(uri, *missing_components)\\n-\\n-\\n-def is_valid(value, matcher, require):\\n-    \"\"\"Determine if a value is valid based on the provided matcher.\\n-\\n-    :param str value:\\n-        Value to validate.\\n-    :param matcher:\\n-        Compiled regular expression to use to validate the value.\\n-    :param require:\\n-        Whether or not the value is required.\\n-    \"\"\"\\n-    if require:\\n-        return value is not None and matcher.match(value)\\n-\\n-    # require is False and value is not None\\n-    return value is None or matcher.match(value)\\n-\\n-\\n-def authority_is_valid(authority, host=None, require=False):\\n-    \"\"\"Determine if the authority string is valid.\\n-\\n-    :param str authority:\\n-        The authority to validate.\\n-    :param str host:\\n-        (optional) The host portion of the authority to validate.\\n-    :param bool require:\\n-        (optional) Specify if authority must not be None.\\n-    :returns:\\n-        ``True`` if valid, ``False`` otherwise\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    validated = is_valid(authority, misc.SUBAUTHORITY_MATCHER, require)\\n-    if validated and host is not None:\\n-        return host_is_valid(host, require)\\n-    return validated\\n-\\n-\\n-def host_is_valid(host, require=False):\\n-    \"\"\"Determine if the host string is valid.\\n-\\n-    :param str host:\\n-        The host to validate.\\n-    :param bool require:\\n-        (optional) Specify if host must not be None.\\n-    :returns:\\n-        ``True`` if valid, ``False`` otherwise\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    validated = is_valid(host, misc.HOST_MATCHER, require)\\n-    if validated and host is not None and misc.IPv4_MATCHER.match(host):\\n-        return valid_ipv4_host_address(host)\\n-    elif validated and host is not None and misc.IPv6_MATCHER.match(host):\\n-        return misc.IPv6_NO_RFC4007_MATCHER.match(host) is not None\\n-    return validated\\n-\\n-\\n-def scheme_is_valid(scheme, require=False):\\n-    \"\"\"Determine if the scheme is valid.\\n-\\n-    :param str scheme:\\n-        The scheme string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a scheme.\\n-    :returns:\\n-        ``True`` if the scheme is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(scheme, misc.SCHEME_MATCHER, require)\\n-\\n-\\n-def path_is_valid(path, require=False):\\n-    \"\"\"Determine if the path component is valid.\\n-\\n-    :param str path:\\n-        The path string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a path.\\n-    :returns:\\n-        ``True`` if the path is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(path, misc.PATH_MATCHER, require)\\n-\\n-\\n-def query_is_valid(query, require=False):\\n-    \"\"\"Determine if the query component is valid.\\n-\\n-    :param str query:\\n-        The query string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a query.\\n-    :returns:\\n-        ``True`` if the query is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(query, misc.QUERY_MATCHER, require)\\n-\\n-\\n-def fragment_is_valid(fragment, require=False):\\n-    \"\"\"Determine if the fragment component is valid.\\n-\\n-    :param str fragment:\\n-        The fragment string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a fragment.\\n-    :returns:\\n-        ``True`` if the fragment is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(fragment, misc.FRAGMENT_MATCHER, require)\\n-\\n-\\n-def valid_ipv4_host_address(host):\\n-    \"\"\"Determine if the given host is a valid IPv4 address.\"\"\"\\n-    # If the host exists, and it might be IPv4, check each byte in the\\n-    # address.\\n-    return all([0 <= int(byte, base=10) <= 255 for byte in host.split(\".\")])\\n-\\n-\\n-_COMPONENT_VALIDATORS = {\\n-    \"scheme\": scheme_is_valid,\\n-    \"path\": path_is_valid,\\n-    \"query\": query_is_valid,\\n-    \"fragment\": fragment_is_valid,\\n-}\\n-\\n-_SUBAUTHORITY_VALIDATORS = set([\"userinfo\", \"host\", \"port\"])\\n-\\n-\\n-def subauthority_component_is_valid(uri, component):\\n-    \"\"\"Determine if the userinfo, host, and port are valid.\"\"\"\\n-    try:\\n-        subauthority_dict = uri.authority_info()\\n-    except exceptions.InvalidAuthority:\\n-        return False\\n-\\n-    # If we can parse the authority into sub-components and we\\'re not\\n-    # validating the port, we can assume it\\'s valid.\\n-    if component == \"host\":\\n-        return host_is_valid(subauthority_dict[\"host\"])\\n-    elif component != \"port\":\\n-        return True\\n-\\n-    try:\\n-        port = int(subauthority_dict[\"port\"])\\n-    except TypeError:\\n-        # If the port wasn\\'t provided it\\'ll be None and int(None) raises a\\n-        # TypeError\\n-        return True\\n-\\n-    return 0 <= port <= 65535\\n-\\n-\\n-def ensure_components_are_valid(uri, validated_components):\\n-    \"\"\"Assert that all components are valid in the URI.\"\"\"\\n-    invalid_components = set([])\\n-    for component in validated_components:\\n-        if component in _SUBAUTHORITY_VALIDATORS:\\n-            if not subauthority_component_is_valid(uri, component):\\n-                invalid_components.add(component)\\n-            # Python\\'s peephole optimizer means that while this continue *is*\\n-            # actually executed, coverage.py cannot detect that. See also,\\n-            # https://bitbucket.org/ned/coveragepy/issues/198/continue-marked-as-not-covered\\n-            continue  # nocov: Python 2.7, 3.3, 3.4\\n-\\n-        validator = _COMPONENT_VALIDATORS[component]\\n-        if not validator(getattr(uri, component)):\\n-            invalid_components.add(component)\\n-\\n-    if invalid_components:\\n-        raise exceptions.InvalidComponentsError(uri, *invalid_components)'], 'file': ['src/urllib3/packages/rfc3986/abnf_regexp.py', 'src/urllib3/packages/rfc3986/normalizers.py', 'src/urllib3/packages/rfc3986/parseresult.py', 'noxfile.py', 'src/urllib3/util/url.py', 'src/urllib3/util/ssl_.py', '_travis/upload_coverage.sh', 'src/urllib3/packages/rfc3986/compat.py', 'src/urllib3/packages/rfc3986/iri.py', 'src/urllib3/packages/rfc3986/misc.py', 'src/urllib3/packages/six.py', 'src/urllib3/connectionpool.py', 'src/urllib3/packages/rfc3986/exceptions.py', 'src/urllib3/packages/rfc3986/_mixin.py', 'src/urllib3/packages/rfc3986/api.py', 'src/urllib3/packages/rfc3986/builder.py', 'src/urllib3/packages/rfc3986/uri.py', 'src/urllib3/packages/rfc3986/validators.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Shell', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('db4061bb-83a6-4331-8469-492ee8498d36'), UUID('4a696d85-be72-4abe-8e37-ddbc0a1fdbfc'), UUID('b7851a07-64e1-4e94-8001-7269619bec0d'), UUID('c3641038-97f3-4ecd-9e0a-ca2bb89432e3'), UUID('36013554-bfe0-42e5-b589-85d2d52067dc'), UUID('400bbab3-c7be-4d04-a16b-6a963afc059a'), UUID('bc5a78a3-0732-4dbb-929c-5e016fd432ff'), UUID('3e2c7474-c146-4ead-9209-e57dee7399cd'), UUID('c3ffe60f-6b6d-4e40-b09f-60c7d03bdd26'), UUID('4d90630d-232f-4c50-abf3-e66e3cfb66b0'), UUID('c62adbd8-c8c4-4b19-bd4c-7c1dd50dc59f'), UUID('27a13a68-ae1d-45bc-a1a4-ac9b375c72d9'), UUID('76ac5fb7-5a67-42da-a4f3-8620a0011b28'), UUID('2929b282-7973-4348-abc6-3b199fc614cb'), UUID('69ce87be-7481-4b52-94e4-480735f05efd'), UUID('612495e9-c4aa-49f4-bbcf-a3c36b8b46a6'), UUID('dd248d1d-e62d-4c4b-90b5-3b7c9c9abefc'), UUID('2b1c83a0-4317-4abd-8c12-4ef7cc325247')]}\n",
      "ERROR:root:Error in {'repo': 'urllib3/urllib3', 'vulnerability_id': '2021-33503', 'commit': '5b047b645f5f93900d5e2fc31230848c25eb1f5f', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -1,262 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module for the regular expressions crafted from ABNF.\"\"\"\\n-\\n-import sys\\n-\\n-# https://tools.ietf.org/html/rfc3986#page-13\\n-GEN_DELIMS = GENERIC_DELIMITERS = \":/?#[]@\"\\n-GENERIC_DELIMITERS_SET = set(GENERIC_DELIMITERS)\\n-# https://tools.ietf.org/html/rfc3986#page-13\\n-SUB_DELIMS = SUB_DELIMITERS = \"!$&\\'()*+,;=\"\\n-SUB_DELIMITERS_SET = set(SUB_DELIMITERS)\\n-# Escape the \\'*\\' for use in regular expressions\\n-SUB_DELIMITERS_RE = r\"!$&\\'()\\\\*+,;=\"\\n-RESERVED_CHARS_SET = GENERIC_DELIMITERS_SET.union(SUB_DELIMITERS_SET)\\n-ALPHA = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\\n-DIGIT = \"0123456789\"\\n-# https://tools.ietf.org/html/rfc3986#section-2.3\\n-UNRESERVED = UNRESERVED_CHARS = ALPHA + DIGIT + r\"._!-\"\\n-UNRESERVED_CHARS_SET = set(UNRESERVED_CHARS)\\n-NON_PCT_ENCODED_SET = RESERVED_CHARS_SET.union(UNRESERVED_CHARS_SET)\\n-# We need to escape the \\'-\\' in this case:\\n-UNRESERVED_RE = r\"A-Za-z0-9._~\\\\-\"\\n-\\n-# Percent encoded character values\\n-PERCENT_ENCODED = PCT_ENCODED = \"%[A-Fa-f0-9]{2}\"\\n-PCHAR = \"([\" + UNRESERVED_RE + SUB_DELIMITERS_RE + \":@]|%s)\" % PCT_ENCODED\\n-\\n-# NOTE(sigmavirus24): We\\'re going to use more strict regular expressions\\n-# than appear in Appendix B for scheme. This will prevent over-eager\\n-# consuming of items that aren\\'t schemes.\\n-SCHEME_RE = \"[a-zA-Z][a-zA-Z0-9+.-]*\"\\n-_AUTHORITY_RE = \"[^/?#]*\"\\n-_PATH_RE = \"[^?#]*\"\\n-_QUERY_RE = \"[^#]*\"\\n-_FRAGMENT_RE = \".*\"\\n-\\n-# Extracted from http://tools.ietf.org/html/rfc3986#appendix-B\\n-COMPONENT_PATTERN_DICT = {\\n-    \"scheme\": SCHEME_RE,\\n-    \"authority\": _AUTHORITY_RE,\\n-    \"path\": _PATH_RE,\\n-    \"query\": _QUERY_RE,\\n-    \"fragment\": _FRAGMENT_RE,\\n-}\\n-\\n-# See http://tools.ietf.org/html/rfc3986#appendix-B\\n-# In this case, we name each of the important matches so we can use\\n-# SRE_Match#groupdict to parse the values out if we so choose. This is also\\n-# modified to ignore other matches that are not important to the parsing of\\n-# the reference so we can also simply use SRE_Match#groups.\\n-URL_PARSING_RE = (\\n-    r\"(?:(?P<scheme>{scheme}):)?(?://(?P<authority>{authority}))?\"\\n-    r\"(?P<path>{path})(?:\\\\?(?P<query>{query}))?\"\\n-    r\"(?:#(?P<fragment>{fragment}))?\"\\n-).format(**COMPONENT_PATTERN_DICT)\\n-\\n-\\n-# #########################\\n-# Authority Matcher Section\\n-# #########################\\n-\\n-# Host patterns, see: http://tools.ietf.org/html/rfc3986#section-3.2.2\\n-# The pattern for a regular name, e.g.,  www.google.com, api.github.com\\n-REGULAR_NAME_RE = REG_NAME = \"((?:{0}|[{1}])*)\".format(\\n-    \"%[0-9A-Fa-f]{2}\", SUB_DELIMITERS_RE + UNRESERVED_RE\\n-)\\n-# The pattern for an IPv4 address, e.g., 192.168.255.255, 127.0.0.1,\\n-IPv4_RE = r\"([0-9]{1,3}\\\\.){3}[0-9]{1,3}\"\\n-# Hexadecimal characters used in each piece of an IPv6 address\\n-HEXDIG_RE = \"[0-9A-Fa-f]{1,4}\"\\n-# Least-significant 32 bits of an IPv6 address\\n-LS32_RE = \"({hex}:{hex}|{ipv4})\".format(hex=HEXDIG_RE, ipv4=IPv4_RE)\\n-# Substitutions into the following patterns for IPv6 patterns defined\\n-# http://tools.ietf.org/html/rfc3986#page-20\\n-_subs = {\"hex\": HEXDIG_RE, \"ls32\": LS32_RE}\\n-\\n-# Below: h16 = hexdig, see: https://tools.ietf.org/html/rfc5234 for details\\n-# about ABNF (Augmented Backus-Naur Form) use in the comments\\n-variations = [\\n-    #                            6( h16 \":\" ) ls32\\n-    \"(%(hex)s:){6}%(ls32)s\" % _subs,\\n-    #                       \"::\" 5( h16 \":\" ) ls32\\n-    \"::(%(hex)s:){5}%(ls32)s\" % _subs,\\n-    # [               h16 ] \"::\" 4( h16 \":\" ) ls32\\n-    \"(%(hex)s)?::(%(hex)s:){4}%(ls32)s\" % _subs,\\n-    # [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\\n-    \"((%(hex)s:)?%(hex)s)?::(%(hex)s:){3}%(ls32)s\" % _subs,\\n-    # [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\\n-    \"((%(hex)s:){0,2}%(hex)s)?::(%(hex)s:){2}%(ls32)s\" % _subs,\\n-    # [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\\n-    \"((%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s\" % _subs,\\n-    # [ *4( h16 \":\" ) h16 ] \"::\"              ls32\\n-    \"((%(hex)s:){0,4}%(hex)s)?::%(ls32)s\" % _subs,\\n-    # [ *5( h16 \":\" ) h16 ] \"::\"              h16\\n-    \"((%(hex)s:){0,5}%(hex)s)?::%(hex)s\" % _subs,\\n-    # [ *6( h16 \":\" ) h16 ] \"::\"\\n-    \"((%(hex)s:){0,6}%(hex)s)?::\" % _subs,\\n-]\\n-\\n-IPv6_RE = \"(({0})|({1})|({2})|({3})|({4})|({5})|({6})|({7})|({8}))\".format(*variations)\\n-\\n-IPv_FUTURE_RE = r\"v[0-9A-Fa-f]+\\\\.[%s]+\" % (UNRESERVED_RE + SUB_DELIMITERS_RE + \":\")\\n-\\n-# RFC 6874 Zone ID ABNF\\n-ZONE_ID = \"(?:[\" + UNRESERVED_RE + \"]|\" + PCT_ENCODED + \")+\"\\n-\\n-IPv6_ADDRZ_RFC4007_RE = IPv6_RE + \"(?:(?:%25|%)\" + ZONE_ID + \")?\"\\n-IPv6_ADDRZ_RE = IPv6_RE + \"(?:%25\" + ZONE_ID + \")?\"\\n-\\n-IP_LITERAL_RE = r\"\\\\[({0}|{1})\\\\]\".format(IPv6_ADDRZ_RFC4007_RE, IPv_FUTURE_RE)\\n-\\n-# Pattern for matching the host piece of the authority\\n-HOST_RE = HOST_PATTERN = \"({0}|{1}|{2})\".format(REG_NAME, IPv4_RE, IP_LITERAL_RE)\\n-USERINFO_RE = \"^([\" + UNRESERVED_RE + SUB_DELIMITERS_RE + \":]|%s)+\" % (PCT_ENCODED)\\n-PORT_RE = \"[0-9]{1,5}\"\\n-\\n-# ####################\\n-# Path Matcher Section\\n-# ####################\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-3.3 for more information\\n-# about the path patterns defined below.\\n-segments = {\\n-    \"segment\": PCHAR + \"*\",\\n-    # Non-zero length segment\\n-    \"segment-nz\": PCHAR + \"+\",\\n-    # Non-zero length segment without \":\"\\n-    \"segment-nz-nc\": PCHAR.replace(\":\", \"\") + \"+\",\\n-}\\n-\\n-# Path types taken from Section 3.3 (linked above)\\n-PATH_EMPTY = \"^$\"\\n-PATH_ROOTLESS = \"%(segment-nz)s(/%(segment)s)*\" % segments\\n-PATH_NOSCHEME = \"%(segment-nz-nc)s(/%(segment)s)*\" % segments\\n-PATH_ABSOLUTE = \"/(%s)?\" % PATH_ROOTLESS\\n-PATH_ABEMPTY = \"(/%(segment)s)*\" % segments\\n-PATH_RE = \"^(%s|%s|%s|%s|%s)$\" % (\\n-    PATH_ABEMPTY,\\n-    PATH_ABSOLUTE,\\n-    PATH_NOSCHEME,\\n-    PATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)\\n-\\n-FRAGMENT_RE = QUERY_RE = (\\n-    \"^([/?:@\" + UNRESERVED_RE + SUB_DELIMITERS_RE + \"]|%s)*$\" % PCT_ENCODED\\n-)\\n-\\n-# ##########################\\n-# Relative reference matcher\\n-# ##########################\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-4.2 for details\\n-RELATIVE_PART_RE = \"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    PATH_ABEMPTY,\\n-    PATH_ABSOLUTE,\\n-    PATH_NOSCHEME,\\n-    PATH_EMPTY,\\n-)\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-3 for definition\\n-HIER_PART_RE = \"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    PATH_ABEMPTY,\\n-    PATH_ABSOLUTE,\\n-    PATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)\\n-\\n-# ###############\\n-# IRIs / RFC 3987\\n-# ###############\\n-\\n-# Only wide-unicode gets the high-ranges of UCSCHAR\\n-if sys.maxunicode > 0xFFFF:  # pragma: no cover\\n-    IPRIVATE = u\"\\\\uE000-\\\\uF8FF\\\\U000F0000-\\\\U000FFFFD\\\\U00100000-\\\\U0010FFFD\"\\n-    UCSCHAR_RE = (\\n-        u\"\\\\u00A0-\\\\uD7FF\\\\uF900-\\\\uFDCF\\\\uFDF0-\\\\uFFEF\"\\n-        u\"\\\\U00010000-\\\\U0001FFFD\\\\U00020000-\\\\U0002FFFD\"\\n-        u\"\\\\U00030000-\\\\U0003FFFD\\\\U00040000-\\\\U0004FFFD\"\\n-        u\"\\\\U00050000-\\\\U0005FFFD\\\\U00060000-\\\\U0006FFFD\"\\n-        u\"\\\\U00070000-\\\\U0007FFFD\\\\U00080000-\\\\U0008FFFD\"\\n-        u\"\\\\U00090000-\\\\U0009FFFD\\\\U000A0000-\\\\U000AFFFD\"\\n-        u\"\\\\U000B0000-\\\\U000BFFFD\\\\U000C0000-\\\\U000CFFFD\"\\n-        u\"\\\\U000D0000-\\\\U000DFFFD\\\\U000E1000-\\\\U000EFFFD\"\\n-    )\\n-else:  # pragma: no cover\\n-    IPRIVATE = u\"\\\\uE000-\\\\uF8FF\"\\n-    UCSCHAR_RE = u\"\\\\u00A0-\\\\uD7FF\\\\uF900-\\\\uFDCF\\\\uFDF0-\\\\uFFEF\"\\n-\\n-IUNRESERVED_RE = u\"A-Za-z0-9\\\\\\\\._~\\\\\\\\-\" + UCSCHAR_RE\\n-IPCHAR = u\"([\" + IUNRESERVED_RE + SUB_DELIMITERS_RE + u\":@]|%s)\" % PCT_ENCODED\\n-\\n-isegments = {\\n-    \"isegment\": IPCHAR + u\"*\",\\n-    # Non-zero length segment\\n-    \"isegment-nz\": IPCHAR + u\"+\",\\n-    # Non-zero length segment without \":\"\\n-    \"isegment-nz-nc\": IPCHAR.replace(\":\", \"\") + u\"+\",\\n-}\\n-\\n-IPATH_ROOTLESS = u\"%(isegment-nz)s(/%(isegment)s)*\" % isegments\\n-IPATH_NOSCHEME = u\"%(isegment-nz-nc)s(/%(isegment)s)*\" % isegments\\n-IPATH_ABSOLUTE = u\"/(?:%s)?\" % IPATH_ROOTLESS\\n-IPATH_ABEMPTY = u\"(?:/%(isegment)s)*\" % isegments\\n-IPATH_RE = u\"^(?:%s|%s|%s|%s|%s)$\" % (\\n-    IPATH_ABEMPTY,\\n-    IPATH_ABSOLUTE,\\n-    IPATH_NOSCHEME,\\n-    IPATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)\\n-\\n-IREGULAR_NAME_RE = IREG_NAME = u\"(?:{0}|[{1}])*\".format(\\n-    u\"%[0-9A-Fa-f]{2}\", SUB_DELIMITERS_RE + IUNRESERVED_RE\\n-)\\n-\\n-IHOST_RE = IHOST_PATTERN = u\"({0}|{1}|{2})\".format(IREG_NAME, IPv4_RE, IP_LITERAL_RE)\\n-\\n-IUSERINFO_RE = (\\n-    u\"^(?:[\" + IUNRESERVED_RE + SUB_DELIMITERS_RE + u\":]|%s)+\" % (PCT_ENCODED)\\n-)\\n-\\n-IFRAGMENT_RE = (\\n-    u\"^(?:[/?:@\" + IUNRESERVED_RE + SUB_DELIMITERS_RE + u\"]|%s)*$\" % PCT_ENCODED\\n-)\\n-IQUERY_RE = (\\n-    u\"^(?:[/?:@\"\\n-    + IUNRESERVED_RE\\n-    + SUB_DELIMITERS_RE\\n-    + IPRIVATE\\n-    + u\"]|%s)*$\" % PCT_ENCODED\\n-)\\n-\\n-IRELATIVE_PART_RE = u\"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    IPATH_ABEMPTY,\\n-    IPATH_ABSOLUTE,\\n-    IPATH_NOSCHEME,\\n-    PATH_EMPTY,\\n-)\\n-\\n-IHIER_PART_RE = u\"(//%s%s|%s|%s|%s)\" % (\\n-    COMPONENT_PATTERN_DICT[\"authority\"],\\n-    IPATH_ABEMPTY,\\n-    IPATH_ABSOLUTE,\\n-    IPATH_ROOTLESS,\\n-    PATH_EMPTY,\\n-)', '@@ -1,172 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module with functions to normalize components.\"\"\"\\n-import re\\n-\\n-from . import compat\\n-from . import misc\\n-\\n-\\n-def normalize_scheme(scheme):\\n-    \"\"\"Normalize the scheme component.\"\"\"\\n-    return scheme.lower()\\n-\\n-\\n-def normalize_authority(authority):\\n-    \"\"\"Normalize an authority tuple to a string.\"\"\"\\n-    userinfo, host, port = authority\\n-    result = \"\"\\n-    if userinfo:\\n-        result += normalize_percent_characters(userinfo) + \"@\"\\n-    if host:\\n-        result += normalize_host(host)\\n-    if port:\\n-        result += \":\" + port\\n-    return result\\n-\\n-\\n-def normalize_username(username):\\n-    \"\"\"Normalize a username to make it safe to include in userinfo.\"\"\"\\n-    return compat.urlquote(username)\\n-\\n-\\n-def normalize_password(password):\\n-    \"\"\"Normalize a password to make safe for userinfo.\"\"\"\\n-    return compat.urlquote(password)\\n-\\n-\\n-def normalize_host(host):\\n-    \"\"\"Normalize a host string.\"\"\"\\n-    if misc.IPv6_MATCHER.match(host):\\n-        percent = host.find(\"%\")\\n-        if percent != -1:\\n-            percent_25 = host.find(\"%25\")\\n-\\n-            # Replace RFC 4007 IPv6 Zone ID delimiter \\'%\\' with \\'%25\\'\\n-            # from RFC 6874. If the host is \\'[<IPv6 addr>%25]\\' then we\\n-            # assume RFC 4007 and normalize to \\'[<IPV6 addr>%2525]\\'\\n-            if (\\n-                percent_25 == -1\\n-                or percent < percent_25\\n-                or (percent == percent_25 and percent_25 == len(host) - 4)\\n-            ):\\n-                host = host.replace(\"%\", \"%25\", 1)\\n-\\n-            # Don\\'t normalize the casing of the Zone ID\\n-            return host[:percent].lower() + host[percent:]\\n-\\n-    return host.lower()\\n-\\n-\\n-def normalize_path(path):\\n-    \"\"\"Normalize the path string.\"\"\"\\n-    if not path:\\n-        return path\\n-\\n-    path = normalize_percent_characters(path)\\n-    return remove_dot_segments(path)\\n-\\n-\\n-def normalize_query(query):\\n-    \"\"\"Normalize the query string.\"\"\"\\n-    if not query:\\n-        return query\\n-    return normalize_percent_characters(query)\\n-\\n-\\n-def normalize_fragment(fragment):\\n-    \"\"\"Normalize the fragment string.\"\"\"\\n-    if not fragment:\\n-        return fragment\\n-    return normalize_percent_characters(fragment)\\n-\\n-\\n-PERCENT_MATCHER = re.compile(\"%[A-Fa-f0-9]{2}\")\\n-\\n-\\n-def normalize_percent_characters(s):\\n-    \"\"\"All percent characters should be upper-cased.\\n-\\n-    For example, ``\"%3afoo%DF%ab\"`` should be turned into ``\"%3Afoo%DF%AB\"``.\\n-    \"\"\"\\n-    matches = set(PERCENT_MATCHER.findall(s))\\n-    for m in matches:\\n-        if not m.isupper():\\n-            s = s.replace(m, m.upper())\\n-    return s\\n-\\n-\\n-def remove_dot_segments(s):\\n-    \"\"\"Remove dot segments from the string.\\n-\\n-    See also Section 5.2.4 of :rfc:`3986`.\\n-    \"\"\"\\n-    # See http://tools.ietf.org/html/rfc3986#section-5.2.4 for pseudo-code\\n-    segments = s.split(\"/\")  # Turn the path into a list of segments\\n-    output = []  # Initialize the variable to use to store output\\n-\\n-    for segment in segments:\\n-        # \\'.\\' is the current directory, so ignore it, it is superfluous\\n-        if segment == \".\":\\n-            continue\\n-        # Anything other than \\'..\\', should be appended to the output\\n-        elif segment != \"..\":\\n-            output.append(segment)\\n-        # In this case segment == \\'..\\', if we can, we should pop the last\\n-        # element\\n-        elif output:\\n-            output.pop()\\n-\\n-    # If the path starts with \\'/\\' and the output is empty or the first string\\n-    # is non-empty\\n-    if s.startswith(\"/\") and (not output or output[0]):\\n-        output.insert(0, \"\")\\n-\\n-    # If the path starts with \\'/.\\' or \\'/..\\' ensure we add one more empty\\n-    # string to add a trailing \\'/\\'\\n-    if s.endswith((\"/.\", \"/..\")):\\n-        output.append(\"\")\\n-\\n-    return \"/\".join(output)\\n-\\n-\\n-def encode_component(uri_component, encoding):\\n-    \"\"\"Encode the specific component in the provided encoding.\"\"\"\\n-    if uri_component is None:\\n-        return uri_component\\n-\\n-    # Try to see if the component we\\'re encoding is already percent-encoded\\n-    # so we can skip all \\'%\\' characters but still encode all others.\\n-    percent_encodings = len(\\n-        PERCENT_MATCHER.findall(compat.to_str(uri_component, encoding))\\n-    )\\n-\\n-    uri_bytes = compat.to_bytes(uri_component, encoding)\\n-    is_percent_encoded = percent_encodings == uri_bytes.count(b\"%\")\\n-\\n-    encoded_uri = bytearray()\\n-\\n-    for i in range(0, len(uri_bytes)):\\n-        # Will return a single character bytestring on both Python 2 & 3\\n-        byte = uri_bytes[i : i + 1]\\n-        byte_ord = ord(byte)\\n-        if (is_percent_encoded and byte == b\"%\") or (\\n-            byte_ord < 128 and byte.decode() in misc.NON_PCT_ENCODED\\n-        ):\\n-            encoded_uri.extend(byte)\\n-            continue\\n-        encoded_uri.extend(\"%{0:02x}\".format(byte_ord).encode().upper())\\n-\\n-    return encoded_uri.decode(encoding)', '@@ -1,457 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2015 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module containing the urlparse compatibility logic.\"\"\"\\n-from collections import namedtuple\\n-\\n-from . import compat\\n-from . import exceptions\\n-from . import misc\\n-from . import normalizers\\n-from . import uri\\n-\\n-__all__ = (\"ParseResult\", \"ParseResultBytes\")\\n-\\n-PARSED_COMPONENTS = (\"scheme\", \"userinfo\", \"host\", \"port\", \"path\", \"query\", \"fragment\")\\n-\\n-\\n-class ParseResultMixin(object):\\n-    def _generate_authority(self, attributes):\\n-        # I swear I did not align the comparisons below. That\\'s just how they\\n-        # happened to align based on pep8 and attribute lengths.\\n-        userinfo, host, port = (attributes[p] for p in (\"userinfo\", \"host\", \"port\"))\\n-        if self.userinfo != userinfo or self.host != host or self.port != port:\\n-            if port:\\n-                port = \"{0}\".format(port)\\n-            return normalizers.normalize_authority(\\n-                (\\n-                    compat.to_str(userinfo, self.encoding),\\n-                    compat.to_str(host, self.encoding),\\n-                    port,\\n-                )\\n-            )\\n-        return self.authority\\n-\\n-    def geturl(self):\\n-        \"\"\"Shim to match the standard library method.\"\"\"\\n-        return self.unsplit()\\n-\\n-    @property\\n-    def hostname(self):\\n-        \"\"\"Shim to match the standard library.\"\"\"\\n-        return self.host\\n-\\n-    @property\\n-    def netloc(self):\\n-        \"\"\"Shim to match the standard library.\"\"\"\\n-        return self.authority\\n-\\n-    @property\\n-    def params(self):\\n-        \"\"\"Shim to match the standard library.\"\"\"\\n-        return self.query\\n-\\n-\\n-class ParseResult(namedtuple(\"ParseResult\", PARSED_COMPONENTS), ParseResultMixin):\\n-    \"\"\"Implementation of urlparse compatibility class.\\n-\\n-    This uses the URIReference logic to handle compatibility with the\\n-    urlparse.ParseResult class.\\n-    \"\"\"\\n-\\n-    slots = ()\\n-\\n-    def __new__(\\n-        cls,\\n-        scheme,\\n-        userinfo,\\n-        host,\\n-        port,\\n-        path,\\n-        query,\\n-        fragment,\\n-        uri_ref,\\n-        encoding=\"utf-8\",\\n-    ):\\n-        \"\"\"Create a new ParseResult.\"\"\"\\n-        parse_result = super(ParseResult, cls).__new__(\\n-            cls,\\n-            scheme or None,\\n-            userinfo or None,\\n-            host,\\n-            port or None,\\n-            path or None,\\n-            query,\\n-            fragment,\\n-        )\\n-        parse_result.encoding = encoding\\n-        parse_result.reference = uri_ref\\n-        return parse_result\\n-\\n-    @classmethod\\n-    def from_parts(\\n-        cls,\\n-        scheme=None,\\n-        userinfo=None,\\n-        host=None,\\n-        port=None,\\n-        path=None,\\n-        query=None,\\n-        fragment=None,\\n-        encoding=\"utf-8\",\\n-    ):\\n-        \"\"\"Create a ParseResult instance from its parts.\"\"\"\\n-        authority = \"\"\\n-        if userinfo is not None:\\n-            authority += userinfo + \"@\"\\n-        if host is not None:\\n-            authority += host\\n-        if port is not None:\\n-            authority += \":{0}\".format(port)\\n-        uri_ref = uri.URIReference(\\n-            scheme=scheme,\\n-            authority=authority,\\n-            path=path,\\n-            query=query,\\n-            fragment=fragment,\\n-            encoding=encoding,\\n-        ).normalize()\\n-        userinfo, host, port = authority_from(uri_ref, strict=True)\\n-        return cls(\\n-            scheme=uri_ref.scheme,\\n-            userinfo=userinfo,\\n-            host=host,\\n-            port=port,\\n-            path=uri_ref.path,\\n-            query=uri_ref.query,\\n-            fragment=uri_ref.fragment,\\n-            uri_ref=uri_ref,\\n-            encoding=encoding,\\n-        )\\n-\\n-    @classmethod\\n-    def from_string(\\n-        cls, uri_string, encoding=\"utf-8\", strict=True, lazy_normalize=True\\n-    ):\\n-        \"\"\"Parse a URI from the given unicode URI string.\\n-\\n-        :param str uri_string: Unicode URI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :param bool strict: Parse strictly according to :rfc:`3986` if True.\\n-            If False, parse similarly to the standard library\\'s urlparse\\n-            function.\\n-        :returns: :class:`ParseResult` or subclass thereof\\n-        \"\"\"\\n-        reference = uri.URIReference.from_string(uri_string, encoding)\\n-        if not lazy_normalize:\\n-            reference = reference.normalize()\\n-        userinfo, host, port = authority_from(reference, strict)\\n-\\n-        return cls(\\n-            scheme=reference.scheme,\\n-            userinfo=userinfo,\\n-            host=host,\\n-            port=port,\\n-            path=reference.path,\\n-            query=reference.query,\\n-            fragment=reference.fragment,\\n-            uri_ref=reference,\\n-            encoding=encoding,\\n-        )\\n-\\n-    @property\\n-    def authority(self):\\n-        \"\"\"Return the normalized authority.\"\"\"\\n-        return self.reference.authority\\n-\\n-    def copy_with(\\n-        self,\\n-        scheme=misc.UseExisting,\\n-        userinfo=misc.UseExisting,\\n-        host=misc.UseExisting,\\n-        port=misc.UseExisting,\\n-        path=misc.UseExisting,\\n-        query=misc.UseExisting,\\n-        fragment=misc.UseExisting,\\n-    ):\\n-        \"\"\"Create a copy of this instance replacing with specified parts.\"\"\"\\n-        attributes = zip(\\n-            PARSED_COMPONENTS, (scheme, userinfo, host, port, path, query, fragment)\\n-        )\\n-        attrs_dict = {}\\n-        for name, value in attributes:\\n-            if value is misc.UseExisting:\\n-                value = getattr(self, name)\\n-            attrs_dict[name] = value\\n-        authority = self._generate_authority(attrs_dict)\\n-        ref = self.reference.copy_with(\\n-            scheme=attrs_dict[\"scheme\"],\\n-            authority=authority,\\n-            path=attrs_dict[\"path\"],\\n-            query=attrs_dict[\"query\"],\\n-            fragment=attrs_dict[\"fragment\"],\\n-        )\\n-        return ParseResult(uri_ref=ref, encoding=self.encoding, **attrs_dict)\\n-\\n-    def encode(self, encoding=None):\\n-        \"\"\"Convert to an instance of ParseResultBytes.\"\"\"\\n-        encoding = encoding or self.encoding\\n-        attrs = dict(\\n-            zip(\\n-                PARSED_COMPONENTS,\\n-                (\\n-                    attr.encode(encoding) if hasattr(attr, \"encode\") else attr\\n-                    for attr in self\\n-                ),\\n-            )\\n-        )\\n-        return ParseResultBytes(uri_ref=self.reference, encoding=encoding, **attrs)\\n-\\n-    def unsplit(self, use_idna=False):\\n-        \"\"\"Create a URI string from the components.\\n-\\n-        :returns: The parsed URI reconstituted as a string.\\n-        :rtype: str\\n-        \"\"\"\\n-        parse_result = self\\n-        if use_idna and self.host:\\n-            hostbytes = self.host.encode(\"idna\")\\n-            host = hostbytes.decode(self.encoding)\\n-            parse_result = self.copy_with(host=host)\\n-        return parse_result.reference.unsplit()\\n-\\n-\\n-class ParseResultBytes(\\n-    namedtuple(\"ParseResultBytes\", PARSED_COMPONENTS), ParseResultMixin\\n-):\\n-    \"\"\"Compatibility shim for the urlparse.ParseResultBytes object.\"\"\"\\n-\\n-    def __new__(\\n-        cls,\\n-        scheme,\\n-        userinfo,\\n-        host,\\n-        port,\\n-        path,\\n-        query,\\n-        fragment,\\n-        uri_ref,\\n-        encoding=\"utf-8\",\\n-        lazy_normalize=True,\\n-    ):\\n-        \"\"\"Create a new ParseResultBytes instance.\"\"\"\\n-        parse_result = super(ParseResultBytes, cls).__new__(\\n-            cls,\\n-            scheme or None,\\n-            userinfo or None,\\n-            host,\\n-            port or None,\\n-            path or None,\\n-            query or None,\\n-            fragment or None,\\n-        )\\n-        parse_result.encoding = encoding\\n-        parse_result.reference = uri_ref\\n-        parse_result.lazy_normalize = lazy_normalize\\n-        return parse_result\\n-\\n-    @classmethod\\n-    def from_parts(\\n-        cls,\\n-        scheme=None,\\n-        userinfo=None,\\n-        host=None,\\n-        port=None,\\n-        path=None,\\n-        query=None,\\n-        fragment=None,\\n-        encoding=\"utf-8\",\\n-        lazy_normalize=True,\\n-    ):\\n-        \"\"\"Create a ParseResult instance from its parts.\"\"\"\\n-        authority = \"\"\\n-        if userinfo is not None:\\n-            authority += userinfo + \"@\"\\n-        if host is not None:\\n-            authority += host\\n-        if port is not None:\\n-            authority += \":{0}\".format(int(port))\\n-        uri_ref = uri.URIReference(\\n-            scheme=scheme,\\n-            authority=authority,\\n-            path=path,\\n-            query=query,\\n-            fragment=fragment,\\n-            encoding=encoding,\\n-        )\\n-        if not lazy_normalize:\\n-            uri_ref = uri_ref.normalize()\\n-        to_bytes = compat.to_bytes\\n-        userinfo, host, port = authority_from(uri_ref, strict=True)\\n-        return cls(\\n-            scheme=to_bytes(scheme, encoding),\\n-            userinfo=to_bytes(userinfo, encoding),\\n-            host=to_bytes(host, encoding),\\n-            port=port,\\n-            path=to_bytes(path, encoding),\\n-            query=to_bytes(query, encoding),\\n-            fragment=to_bytes(fragment, encoding),\\n-            uri_ref=uri_ref,\\n-            encoding=encoding,\\n-            lazy_normalize=lazy_normalize,\\n-        )\\n-\\n-    @classmethod\\n-    def from_string(\\n-        cls, uri_string, encoding=\"utf-8\", strict=True, lazy_normalize=True\\n-    ):\\n-        \"\"\"Parse a URI from the given unicode URI string.\\n-\\n-        :param str uri_string: Unicode URI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :param bool strict: Parse strictly according to :rfc:`3986` if True.\\n-            If False, parse similarly to the standard library\\'s urlparse\\n-            function.\\n-        :returns: :class:`ParseResultBytes` or subclass thereof\\n-        \"\"\"\\n-        reference = uri.URIReference.from_string(uri_string, encoding)\\n-        if not lazy_normalize:\\n-            reference = reference.normalize()\\n-        userinfo, host, port = authority_from(reference, strict)\\n-\\n-        to_bytes = compat.to_bytes\\n-        return cls(\\n-            scheme=to_bytes(reference.scheme, encoding),\\n-            userinfo=to_bytes(userinfo, encoding),\\n-            host=to_bytes(host, encoding),\\n-            port=port,\\n-            path=to_bytes(reference.path, encoding),\\n-            query=to_bytes(reference.query, encoding),\\n-            fragment=to_bytes(reference.fragment, encoding),\\n-            uri_ref=reference,\\n-            encoding=encoding,\\n-            lazy_normalize=lazy_normalize,\\n-        )\\n-\\n-    @property\\n-    def authority(self):\\n-        \"\"\"Return the normalized authority.\"\"\"\\n-        return self.reference.authority.encode(self.encoding)\\n-\\n-    def copy_with(\\n-        self,\\n-        scheme=misc.UseExisting,\\n-        userinfo=misc.UseExisting,\\n-        host=misc.UseExisting,\\n-        port=misc.UseExisting,\\n-        path=misc.UseExisting,\\n-        query=misc.UseExisting,\\n-        fragment=misc.UseExisting,\\n-        lazy_normalize=True,\\n-    ):\\n-        \"\"\"Create a copy of this instance replacing with specified parts.\"\"\"\\n-        attributes = zip(\\n-            PARSED_COMPONENTS, (scheme, userinfo, host, port, path, query, fragment)\\n-        )\\n-        attrs_dict = {}\\n-        for name, value in attributes:\\n-            if value is misc.UseExisting:\\n-                value = getattr(self, name)\\n-            if not isinstance(value, bytes) and hasattr(value, \"encode\"):\\n-                value = value.encode(self.encoding)\\n-            attrs_dict[name] = value\\n-        authority = self._generate_authority(attrs_dict)\\n-        to_str = compat.to_str\\n-        ref = self.reference.copy_with(\\n-            scheme=to_str(attrs_dict[\"scheme\"], self.encoding),\\n-            authority=to_str(authority, self.encoding),\\n-            path=to_str(attrs_dict[\"path\"], self.encoding),\\n-            query=to_str(attrs_dict[\"query\"], self.encoding),\\n-            fragment=to_str(attrs_dict[\"fragment\"], self.encoding),\\n-        )\\n-        if not lazy_normalize:\\n-            ref = ref.normalize()\\n-        return ParseResultBytes(\\n-            uri_ref=ref,\\n-            encoding=self.encoding,\\n-            lazy_normalize=lazy_normalize,\\n-            **attrs_dict\\n-        )\\n-\\n-    def unsplit(self, use_idna=False):\\n-        \"\"\"Create a URI bytes object from the components.\\n-\\n-        :returns: The parsed URI reconstituted as a string.\\n-        :rtype: bytes\\n-        \"\"\"\\n-        parse_result = self\\n-        if use_idna and self.host:\\n-            # self.host is bytes, to encode to idna, we need to decode it\\n-            # first\\n-            host = self.host.decode(self.encoding)\\n-            hostbytes = host.encode(\"idna\")\\n-            parse_result = self.copy_with(host=hostbytes)\\n-        if self.lazy_normalize:\\n-            parse_result = parse_result.copy_with(lazy_normalize=False)\\n-        uri = parse_result.reference.unsplit()\\n-        return uri.encode(self.encoding)\\n-\\n-\\n-def split_authority(authority):\\n-    # Initialize our expected return values\\n-    userinfo = host = port = None\\n-    # Initialize an extra var we may need to use\\n-    extra_host = None\\n-    # Set-up rest in case there is no userinfo portion\\n-    rest = authority\\n-\\n-    if \"@\" in authority:\\n-        userinfo, rest = authority.rsplit(\"@\", 1)\\n-\\n-    # Handle IPv6 host addresses\\n-    if rest.startswith(\"[\"):\\n-        host, rest = rest.split(\"]\", 1)\\n-        host += \"]\"\\n-\\n-    if \":\" in rest:\\n-        extra_host, port = rest.split(\":\", 1)\\n-    elif not host and rest:\\n-        host = rest\\n-\\n-    if extra_host and not host:\\n-        host = extra_host\\n-\\n-    return userinfo, host, port\\n-\\n-\\n-def authority_from(reference, strict):\\n-    try:\\n-        subauthority = reference.authority_info()\\n-    except exceptions.InvalidAuthority:\\n-        if strict:\\n-            raise\\n-        userinfo, host, port = split_authority(reference.authority)\\n-    else:\\n-        # Thanks to Richard Barrell for this idea:\\n-        # https://twitter.com/0x2ba22e11/status/617338811975139328\\n-        userinfo, host, port = (\\n-            subauthority.get(p) for p in (\"userinfo\", \"host\", \"port\")\\n-        )\\n-\\n-    if port:\\n-        try:\\n-            port = int(port)\\n-        except ValueError:\\n-            raise exceptions.InvalidPort(port)\\n-    return userinfo, host, port', '@@ -77,6 +77,8 @@ def blacken(session):\\n     session.install(\"black\")\\r\\n     session.run(\"black\", \"src\", \"dummyserver\", \"test\", \"noxfile.py\", \"setup.py\")\\r\\n \\r\\n+    lint(session)\\r\\n+\\r\\n \\r\\n @nox.session\\r\\n def lint(session):\\r', '@@ -3,10 +3,7 @@\\n from collections import namedtuple\\n \\n from ..exceptions import LocationParseError\\n-from ..packages import six, rfc3986\\n-from ..packages.rfc3986.exceptions import RFC3986Exception, ValidationError\\n-from ..packages.rfc3986.validators import Validator\\n-from ..packages.rfc3986 import abnf_regexp, normalizers, compat, misc\\n+from ..packages import six\\n \\n \\n url_attrs = [\"scheme\", \"auth\", \"host\", \"port\", \"path\", \"query\", \"fragment\"]\\n@@ -15,12 +12,68 @@\\n # urllib3 infers URLs without a scheme (None) to be http.\\n NORMALIZABLE_SCHEMES = (\"http\", \"https\", None)\\n \\n-# Regex for detecting URLs with schemes. RFC 3986 Section 3.1\\n-SCHEME_REGEX = re.compile(r\"^(?:[a-zA-Z][a-zA-Z0-9+\\\\-]*:|/)\")\\n+# Almost all of these patterns were derived from the\\n+# \\'rfc3986\\' module: https://github.com/python-hyper/rfc3986\\n+PERCENT_RE = re.compile(r\"%[a-fA-F0-9]{2}\")\\n+SCHEME_RE = re.compile(r\"^(?:[a-zA-Z][a-zA-Z0-9+-]*:|/)\")\\n+URI_RE = re.compile(\\n+    r\"^(?:([a-zA-Z][a-zA-Z0-9+.-]*):)?\"\\n+    r\"(?://([^/?#]*))?\"\\n+    r\"([^?#]*)\"\\n+    r\"(?:\\\\?([^#]*))?\"\\n+    r\"(?:#(.*))?$\",\\n+    re.UNICODE | re.DOTALL,\\n+)\\n+\\n+IPV4_PAT = r\"(?:[0-9]{1,3}\\\\.){3}[0-9]{1,3}\"\\n+HEX_PAT = \"[0-9A-Fa-f]{1,4}\"\\n+LS32_PAT = \"(?:{hex}:{hex}|{ipv4})\".format(hex=HEX_PAT, ipv4=IPV4_PAT)\\n+_subs = {\"hex\": HEX_PAT, \"ls32\": LS32_PAT}\\n+_variations = [\\n+    #                            6( h16 \":\" ) ls32\\n+    \"(?:%(hex)s:){6}%(ls32)s\",\\n+    #                       \"::\" 5( h16 \":\" ) ls32\\n+    \"::(?:%(hex)s:){5}%(ls32)s\",\\n+    # [               h16 ] \"::\" 4( h16 \":\" ) ls32\\n+    \"(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)s\",\\n+    # [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\\n+    \"(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)s\",\\n+    # [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\\n+    \"(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)s\",\\n+    # [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\\n+    \"(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s\",\\n+    # [ *4( h16 \":\" ) h16 ] \"::\"              ls32\\n+    \"(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)s\",\\n+    # [ *5( h16 \":\" ) h16 ] \"::\"              h16\\n+    \"(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)s\",\\n+    # [ *6( h16 \":\" ) h16 ] \"::\"\\n+    \"(?:(?:%(hex)s:){0,6}%(hex)s)?::\",\\n+]\\n+\\n+UNRESERVED_PAT = r\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._!\\\\-\"\\n+IPV6_PAT = \"(?:\" + \"|\".join([x % _subs for x in _variations]) + \")\"\\n+ZONE_ID_PAT = \"(?:%25|%)(?:[\" + UNRESERVED_PAT + \"]|%[a-fA-F0-9]{2})+\"\\n+IPV6_ADDRZ_PAT = r\"\\\\[\" + IPV6_PAT + r\"(?:\" + ZONE_ID_PAT + r\")?\\\\]\"\\n+REG_NAME_PAT = r\"(?:[^\\\\[\\\\]%:/?#]|%[a-fA-F0-9]{2})*\"\\n+\\n+IPV4_RE = re.compile(\"^\" + IPV4_PAT + \"$\")\\n+IPV6_RE = re.compile(\"^\" + IPV6_PAT + \"$\")\\n+IPV6_ADDRZ_RE = re.compile(\"^\" + IPV6_ADDRZ_PAT + \"$\")\\n+BRACELESS_IPV6_ADDRZ_RE = re.compile(\"^\" + IPV6_ADDRZ_PAT[2:-2] + \"$\")\\n+ZONE_ID_RE = re.compile(\"(\" + ZONE_ID_PAT + r\")\\\\]$\")\\n+\\n+SUBAUTHORITY_PAT = (u\"^(?:(.*)@)?\" u\"(%s|%s|%s)\" u\"(?::([0-9]{0,5}))?$\") % (\\n+    REG_NAME_PAT,\\n+    IPV4_PAT,\\n+    IPV6_ADDRZ_PAT,\\n+)\\n+SUBAUTHORITY_RE = re.compile(SUBAUTHORITY_PAT, re.UNICODE | re.DOTALL)\\n \\n-PATH_CHARS = (\\n-    abnf_regexp.UNRESERVED_CHARS_SET | abnf_regexp.SUB_DELIMITERS_SET | {\":\", \"@\", \"/\"}\\n+ZONE_ID_CHARS = set(\\n+    \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \"abcdefghijklmnopqrstuvwxyz\" \"0123456789._!-\"\\n )\\n+USERINFO_CHARS = ZONE_ID_CHARS | set(\"$&\\'()*+,;=:\")\\n+PATH_CHARS = USERINFO_CHARS | {\"@\", \"/\"}\\n QUERY_CHARS = FRAGMENT_CHARS = PATH_CHARS | {\"?\"}\\n \\n \\n@@ -154,20 +207,24 @@ def split_first(s, delims):\\n \\n def _encode_invalid_chars(component, allowed_chars, encoding=\"utf-8\"):\\n     \"\"\"Percent-encodes a URI component without reapplying\\n-    onto an already percent-encoded component. Based on\\n-    rfc3986.normalizers.encode_component()\\n+    onto an already percent-encoded component.\\n     \"\"\"\\n     if component is None:\\n         return component\\n \\n+    component = six.ensure_text(component)\\n+\\n     # Try to see if the component we\\'re encoding is already percent-encoded\\n     # so we can skip all \\'%\\' characters but still encode all others.\\n-    percent_encodings = len(\\n-        normalizers.PERCENT_MATCHER.findall(compat.to_str(component, encoding))\\n-    )\\n+    percent_encodings = PERCENT_RE.findall(component)\\n+\\n+    # Normalize existing percent-encoded bytes.\\n+    for enc in percent_encodings:\\n+        if not enc.isupper():\\n+            component = component.replace(enc, enc.upper())\\n \\n     uri_bytes = component.encode(\"utf-8\", \"surrogatepass\")\\n-    is_percent_encoded = percent_encodings == uri_bytes.count(b\"%\")\\n+    is_percent_encoded = len(percent_encodings) == uri_bytes.count(b\"%\")\\n \\n     encoded_component = bytearray()\\n \\n@@ -180,17 +237,96 @@ def _encode_invalid_chars(component, allowed_chars, encoding=\"utf-8\"):\\n         ):\\n             encoded_component.extend(byte)\\n             continue\\n-        encoded_component.extend(\"%{0:02x}\".format(byte_ord).encode().upper())\\n+        encoded_component.extend(b\"%\" + (hex(byte_ord)[2:].encode().zfill(2).upper()))\\n \\n     return encoded_component.decode(encoding)\\n \\n \\n+def _remove_path_dot_segments(path):\\n+    # See http://tools.ietf.org/html/rfc3986#section-5.2.4 for pseudo-code\\n+    segments = path.split(\"/\")  # Turn the path into a list of segments\\n+    output = []  # Initialize the variable to use to store output\\n+\\n+    for segment in segments:\\n+        # \\'.\\' is the current directory, so ignore it, it is superfluous\\n+        if segment == \".\":\\n+            continue\\n+        # Anything other than \\'..\\', should be appended to the output\\n+        elif segment != \"..\":\\n+            output.append(segment)\\n+        # In this case segment == \\'..\\', if we can, we should pop the last\\n+        # element\\n+        elif output:\\n+            output.pop()\\n+\\n+    # If the path starts with \\'/\\' and the output is empty or the first string\\n+    # is non-empty\\n+    if path.startswith(\"/\") and (not output or output[0]):\\n+        output.insert(0, \"\")\\n+\\n+    # If the path starts with \\'/.\\' or \\'/..\\' ensure we add one more empty\\n+    # string to add a trailing \\'/\\'\\n+    if path.endswith((\"/.\", \"/..\")):\\n+        output.append(\"\")\\n+\\n+    return \"/\".join(output)\\n+\\n+\\n+def _normalize_host(host, scheme):\\n+    if host:\\n+        if isinstance(host, six.binary_type):\\n+            host = six.ensure_str(host)\\n+\\n+        if scheme in NORMALIZABLE_SCHEMES:\\n+            is_ipv6 = IPV6_ADDRZ_RE.match(host)\\n+            if is_ipv6:\\n+                match = ZONE_ID_RE.search(host)\\n+                if match:\\n+                    start, end = match.span(1)\\n+                    zone_id = host[start:end]\\n+\\n+                    if zone_id.startswith(\"%25\") and zone_id != \"%25\":\\n+                        zone_id = zone_id[3:]\\n+                    else:\\n+                        zone_id = zone_id[1:]\\n+                    zone_id = \"%\" + _encode_invalid_chars(zone_id, ZONE_ID_CHARS)\\n+                    return host[:start].lower() + zone_id + host[end:]\\n+                else:\\n+                    return host.lower()\\n+            elif not IPV4_RE.match(host):\\n+                return six.ensure_str(\\n+                    b\".\".join([_idna_encode(label) for label in host.split(\".\")])\\n+                )\\n+    return host\\n+\\n+\\n+def _idna_encode(name):\\n+    if name and any([ord(x) > 128 for x in name]):\\n+        try:\\n+            import idna\\n+        except ImportError:\\n+            six.raise_from(\\n+                LocationParseError(\"Unable to parse URL without the \\'idna\\' module\"),\\n+                None,\\n+            )\\n+        try:\\n+            return idna.encode(name.lower(), strict=True, std3_rules=True)\\n+        except idna.IDNAError:\\n+            six.raise_from(\\n+                LocationParseError(u\"Name \\'%s\\' is not a valid IDNA label\" % name), None\\n+            )\\n+    return name.lower().encode(\"ascii\")\\n+\\n+\\n def parse_url(url):\\n     \"\"\"\\n     Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is\\n     performed to parse incomplete urls. Fields not provided will be None.\\n     This parser is RFC 3986 compliant.\\n \\n+    The parser logic and helper functions are based heavily on\\n+    work done in the ``rfc3986`` module.\\n+\\n     :param str url: URL to parse into a :class:`.Url` namedtuple.\\n \\n     Partly backwards-compatible with :mod:`urlparse`.\\n@@ -208,90 +344,72 @@ def parse_url(url):\\n         # Empty\\n         return Url()\\n \\n-    is_string = not isinstance(url, six.binary_type)\\n-\\n-    # RFC 3986 doesn\\'t like URLs that have a host but don\\'t start\\n-    # with a scheme and we support URLs like that so we need to\\n-    # detect that problem and add an empty scheme indication.\\n-    # We don\\'t get hurt on path-only URLs here as it\\'s stripped\\n-    # off and given an empty scheme anyways.\\n-    if not SCHEME_REGEX.search(url):\\n+    source_url = url\\n+    if not SCHEME_RE.search(url):\\n         url = \"//\" + url\\n \\n-    def idna_encode(name):\\n-        if name and any([ord(x) > 128 for x in name]):\\n-            try:\\n-                import idna\\n-            except ImportError:\\n-                raise LocationParseError(\\n-                    \"Unable to parse URL without the \\'idna\\' module\"\\n-                )\\n-            try:\\n-                return idna.encode(name.lower(), strict=True, std3_rules=True)\\n-            except idna.IDNAError:\\n-                raise LocationParseError(u\"Name \\'%s\\' is not a valid IDNA label\" % name)\\n-        return name\\n-\\n-    try:\\n-        split_iri = misc.IRI_MATCHER.match(compat.to_str(url)).groupdict()\\n-        iri_ref = rfc3986.IRIReference(\\n-            split_iri[\"scheme\"],\\n-            split_iri[\"authority\"],\\n-            _encode_invalid_chars(split_iri[\"path\"], PATH_CHARS),\\n-            _encode_invalid_chars(split_iri[\"query\"], QUERY_CHARS),\\n-            _encode_invalid_chars(split_iri[\"fragment\"], FRAGMENT_CHARS),\\n-        )\\n-        has_authority = iri_ref.authority is not None\\n-        uri_ref = iri_ref.encode(idna_encoder=idna_encode)\\n-    except (ValueError, RFC3986Exception):\\n-        return six.raise_from(LocationParseError(url), None)\\n-\\n-    # rfc3986 strips the authority if it\\'s invalid\\n-    if has_authority and uri_ref.authority is None:\\n-        raise LocationParseError(url)\\n-\\n-    # Only normalize schemes we understand to not break http+unix\\n-    # or other schemes that don\\'t follow RFC 3986.\\n-    if uri_ref.scheme is None or uri_ref.scheme.lower() in NORMALIZABLE_SCHEMES:\\n-        uri_ref = uri_ref.normalize()\\n-\\n-    # Validate all URIReference components and ensure that all\\n-    # components that were set before are still set after\\n-    # normalization has completed.\\n-    validator = Validator()\\n     try:\\n-        validator.check_validity_of(*validator.COMPONENT_NAMES).validate(uri_ref)\\n-    except ValidationError:\\n-        return six.raise_from(LocationParseError(url), None)\\n+        scheme, authority, path, query, fragment = URI_RE.match(url).groups()\\n+        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES\\n+\\n+        if scheme:\\n+            scheme = scheme.lower()\\n+\\n+        if authority:\\n+            auth, host, port = SUBAUTHORITY_RE.match(authority).groups()\\n+            if auth and normalize_uri:\\n+                auth = _encode_invalid_chars(auth, USERINFO_CHARS)\\n+            if port == \"\":\\n+                port = None\\n+        else:\\n+            auth, host, port = None, None, None\\n+\\n+        if port is not None:\\n+            port = int(port)\\n+            if not (0 <= port <= 65535):\\n+                raise LocationParseError(url)\\n+\\n+        host = _normalize_host(host, scheme)\\n+\\n+        if normalize_uri and path:\\n+            path = _remove_path_dot_segments(path)\\n+            path = _encode_invalid_chars(path, PATH_CHARS)\\n+        if normalize_uri and query:\\n+            query = _encode_invalid_chars(query, QUERY_CHARS)\\n+        if normalize_uri and fragment:\\n+            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)\\n+\\n+    except (ValueError, AttributeError):\\n+        return six.raise_from(LocationParseError(source_url), None)\\n \\n     # For the sake of backwards compatibility we put empty\\n     # string values for path if there are any defined values\\n     # beyond the path in the URL.\\n     # TODO: Remove this when we break backwards compatibility.\\n-    path = uri_ref.path\\n     if not path:\\n-        if uri_ref.query is not None or uri_ref.fragment is not None:\\n+        if query is not None or fragment is not None:\\n             path = \"\"\\n         else:\\n             path = None\\n \\n     # Ensure that each part of the URL is a `str` for\\n     # backwards compatibility.\\n-    def to_input_type(x):\\n-        if x is None:\\n-            return None\\n-        elif not is_string and not isinstance(x, six.binary_type):\\n-            return x.encode(\"utf-8\")\\n-        return x\\n+    if isinstance(url, six.text_type):\\n+        ensure_func = six.ensure_text\\n+    else:\\n+        ensure_func = six.ensure_str\\n+\\n+    def ensure_type(x):\\n+        return x if x is None else ensure_func(x)\\n \\n     return Url(\\n-        scheme=to_input_type(uri_ref.scheme),\\n-        auth=to_input_type(uri_ref.userinfo),\\n-        host=to_input_type(uri_ref.host),\\n-        port=int(uri_ref.port) if uri_ref.port is not None else None,\\n-        path=to_input_type(path),\\n-        query=to_input_type(uri_ref.query),\\n-        fragment=to_input_type(uri_ref.fragment),\\n+        scheme=ensure_type(scheme),\\n+        auth=ensure_type(auth),\\n+        host=ensure_type(host),\\n+        port=port,\\n+        path=ensure_type(path),\\n+        query=ensure_type(query),\\n+        fragment=ensure_type(fragment),\\n     )\\n \\n ', '@@ -2,14 +2,13 @@\\n import errno\\n import warnings\\n import hmac\\n-import re\\n \\n from binascii import hexlify, unhexlify\\n from hashlib import md5, sha1, sha256\\n \\n+from .url import IPV4_RE, BRACELESS_IPV6_ADDRZ_RE\\n from ..exceptions import SSLError, InsecurePlatformWarning, SNIMissingWarning\\n from ..packages import six\\n-from ..packages.rfc3986 import abnf_regexp\\n \\n \\n SSLContext = None\\n@@ -36,13 +35,6 @@ def _const_compare_digest_backport(a, b):\\n \\n _const_compare_digest = getattr(hmac, \"compare_digest\", _const_compare_digest_backport)\\n \\n-# Borrow rfc3986\\'s regular expressions for IPv4\\n-# and IPv6 addresses for use in is_ipaddress()\\n-_IP_ADDRESS_REGEX = re.compile(\\n-    r\"^(?:%s|%s|%s)$\"\\n-    % (abnf_regexp.IPv4_RE, abnf_regexp.IPv6_RE, abnf_regexp.IPv6_ADDRZ_RFC4007_RE)\\n-)\\n-\\n try:  # Test for SSL features\\n     import ssl\\n     from ssl import wrap_socket, CERT_REQUIRED\\n@@ -389,7 +381,7 @@ def is_ipaddress(hostname):\\n     if six.PY3 and isinstance(hostname, bytes):\\n         # IDN A-label bytes are ASCII compatible.\\n         hostname = hostname.decode(\"ascii\")\\n-    return _IP_ADDRESS_REGEX.match(hostname) is not None\\n+    return bool(IPV4_RE.match(hostname) or BRACELESS_IPV6_ADDRZ_RE.match(hostname))\\n \\n \\n def _is_key_file_encrypted(key_file):', '@@ -3,6 +3,6 @@\\n set -exo pipefail\\n \\n if [[ -e .coverage ]]; then\\n-    python3.6 -m pip install codecov\\n-    python3.6 -m codecov --env TRAVIS_OS_NAME,NOX_SESSION\\n+    python3 -m pip install codecov\\n+    python3 -m codecov --env TRAVIS_OS_NAME,NOX_SESSION\\n fi', '@@ -1,49 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Compatibility module for Python 2 and 3 support.\"\"\"\\n-import sys\\n-\\n-try:\\n-    from urllib.parse import quote as urlquote\\n-except ImportError:  # Python 2.x\\n-    from urllib import quote as urlquote\\n-\\n-try:\\n-    from urllib.parse import urlencode\\n-except ImportError:  # Python 2.x\\n-    from urllib import urlencode\\n-\\n-__all__ = (\"to_bytes\", \"to_str\", \"urlquote\", \"urlencode\")\\n-\\n-PY3 = (3, 0) <= sys.version_info < (4, 0)\\n-PY2 = (2, 6) <= sys.version_info < (2, 8)\\n-\\n-\\n-if PY3:\\n-    unicode = str  # Python 3.x\\n-\\n-\\n-def to_str(b, encoding=\"utf-8\"):\\n-    \"\"\"Ensure that b is text in the specified encoding.\"\"\"\\n-    if hasattr(b, \"decode\") and not isinstance(b, unicode):\\n-        b = b.decode(encoding)\\n-    return b\\n-\\n-\\n-def to_bytes(s, encoding=\"utf-8\"):\\n-    \"\"\"Ensure that s is converted to bytes from the encoding.\"\"\"\\n-    if hasattr(s, \"encode\") and not isinstance(s, bytes):\\n-        s = s.encode(encoding)\\n-    return s', '@@ -1,150 +0,0 @@\\n-\"\"\"Module containing the implementation of the IRIReference class.\"\"\"\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Copyright (c) 2015 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-from collections import namedtuple\\n-\\n-from . import compat\\n-from . import exceptions\\n-from . import misc\\n-from . import normalizers\\n-from . import uri\\n-\\n-\\n-try:\\n-    import idna\\n-except ImportError:  # pragma: no cover\\n-    idna = None\\n-\\n-\\n-class IRIReference(namedtuple(\"IRIReference\", misc.URI_COMPONENTS), uri.URIMixin):\\n-    \"\"\"Immutable object representing a parsed IRI Reference.\\n-\\n-    Can be encoded into an URIReference object via the procedure\\n-    specified in RFC 3987 Section 3.1\\n-\\n-     .. note::\\n-        The IRI submodule is a new interface and may possibly change in\\n-        the future. Check for changes to the interface when upgrading.\\n-    \"\"\"\\n-\\n-    slots = ()\\n-\\n-    def __new__(cls, scheme, authority, path, query, fragment, encoding=\"utf-8\"):\\n-        \"\"\"Create a new IRIReference.\"\"\"\\n-        ref = super(IRIReference, cls).__new__(\\n-            cls, scheme or None, authority or None, path or None, query, fragment\\n-        )\\n-        ref.encoding = encoding\\n-        return ref\\n-\\n-    def __eq__(self, other):\\n-        \"\"\"Compare this reference to another.\"\"\"\\n-        other_ref = other\\n-        if isinstance(other, tuple):\\n-            other_ref = self.__class__(*other)\\n-        elif not isinstance(other, IRIReference):\\n-            try:\\n-                other_ref = self.__class__.from_string(other)\\n-            except TypeError:\\n-                raise TypeError(\\n-                    \"Unable to compare {0}() to {1}()\".format(\\n-                        type(self).__name__, type(other).__name__\\n-                    )\\n-                )\\n-\\n-        # See http://tools.ietf.org/html/rfc3986#section-6.2\\n-        return tuple(self) == tuple(other_ref)\\n-\\n-    def _match_subauthority(self):\\n-        return misc.ISUBAUTHORITY_MATCHER.match(self.authority)\\n-\\n-    @classmethod\\n-    def from_string(cls, iri_string, encoding=\"utf-8\"):\\n-        \"\"\"Parse a IRI reference from the given unicode IRI string.\\n-\\n-        :param str iri_string: Unicode IRI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :returns: :class:`IRIReference` or subclass thereof\\n-        \"\"\"\\n-        iri_string = compat.to_str(iri_string, encoding)\\n-\\n-        split_iri = misc.IRI_MATCHER.match(iri_string).groupdict()\\n-        return cls(\\n-            split_iri[\"scheme\"],\\n-            split_iri[\"authority\"],\\n-            normalizers.encode_component(split_iri[\"path\"], encoding),\\n-            normalizers.encode_component(split_iri[\"query\"], encoding),\\n-            normalizers.encode_component(split_iri[\"fragment\"], encoding),\\n-            encoding,\\n-        )\\n-\\n-    def encode(self, idna_encoder=None):  # noqa: C901\\n-        \"\"\"Encode an IRIReference into a URIReference instance.\\n-\\n-        If the ``idna`` module is installed or the ``rfc3986[idna]``\\n-        extra is used then unicode characters in the IRI host\\n-        component will be encoded with IDNA2008.\\n-\\n-        :param idna_encoder:\\n-            Function that encodes each part of the host component\\n-            If not given will raise an exception if the IRI\\n-            contains a host component.\\n-        :rtype: uri.URIReference\\n-        :returns: A URI reference\\n-        \"\"\"\\n-        authority = self.authority\\n-        if authority:\\n-            if idna_encoder is None:\\n-                if idna is None:  # pragma: no cover\\n-                    raise exceptions.MissingDependencyError(\\n-                        \"Could not import the \\'idna\\' module \"\\n-                        \"and the IRI hostname requires encoding\"\\n-                    )\\n-\\n-                def idna_encoder(name):\\n-                    if any(ord(c) > 128 for c in name):\\n-                        try:\\n-                            return idna.encode(\\n-                                name.lower(), strict=True, std3_rules=True\\n-                            )\\n-                        except idna.IDNAError:\\n-                            raise exceptions.InvalidAuthority(self.authority)\\n-                    return name\\n-\\n-            authority = \"\"\\n-            if self.host:\\n-                authority = \".\".join(\\n-                    [compat.to_str(idna_encoder(part)) for part in self.host.split(\".\")]\\n-                )\\n-\\n-            if self.userinfo is not None:\\n-                authority = (\\n-                    normalizers.encode_component(self.userinfo, self.encoding)\\n-                    + \"@\"\\n-                    + authority\\n-                )\\n-\\n-            if self.port is not None:\\n-                authority += \":\" + str(self.port)\\n-\\n-        return uri.URIReference(\\n-            self.scheme,\\n-            authority,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-            encoding=self.encoding,\\n-        )', '@@ -1,125 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"\\n-Module containing compiled regular expressions and constants.\\n-\\n-This module contains important constants, patterns, and compiled regular\\n-expressions for parsing and validating URIs and their components.\\n-\"\"\"\\n-\\n-import re\\n-\\n-from . import abnf_regexp\\n-\\n-# These are enumerated for the named tuple used as a superclass of\\n-# URIReference\\n-URI_COMPONENTS = [\"scheme\", \"authority\", \"path\", \"query\", \"fragment\"]\\n-\\n-important_characters = {\\n-    \"generic_delimiters\": abnf_regexp.GENERIC_DELIMITERS,\\n-    \"sub_delimiters\": abnf_regexp.SUB_DELIMITERS,\\n-    # We need to escape the \\'*\\' in this case\\n-    \"re_sub_delimiters\": abnf_regexp.SUB_DELIMITERS_RE,\\n-    \"unreserved_chars\": abnf_regexp.UNRESERVED_CHARS,\\n-    # We need to escape the \\'-\\' in this case:\\n-    \"re_unreserved\": abnf_regexp.UNRESERVED_RE,\\n-}\\n-\\n-# For details about delimiters and reserved characters, see:\\n-# http://tools.ietf.org/html/rfc3986#section-2.2\\n-GENERIC_DELIMITERS = abnf_regexp.GENERIC_DELIMITERS_SET\\n-SUB_DELIMITERS = abnf_regexp.SUB_DELIMITERS_SET\\n-RESERVED_CHARS = abnf_regexp.RESERVED_CHARS_SET\\n-# For details about unreserved characters, see:\\n-# http://tools.ietf.org/html/rfc3986#section-2.3\\n-UNRESERVED_CHARS = abnf_regexp.UNRESERVED_CHARS_SET\\n-NON_PCT_ENCODED = abnf_regexp.NON_PCT_ENCODED_SET\\n-\\n-URI_MATCHER = re.compile(abnf_regexp.URL_PARSING_RE)\\n-\\n-SUBAUTHORITY_MATCHER = re.compile(\\n-    (\\n-        \"^(?:(?P<userinfo>{0})@)?\"  # userinfo\\n-        \"(?P<host>{1})\"  # host\\n-        \":?(?P<port>{2})?$\"  # port\\n-    ).format(abnf_regexp.USERINFO_RE, abnf_regexp.HOST_PATTERN, abnf_regexp.PORT_RE)\\n-)\\n-\\n-\\n-HOST_MATCHER = re.compile(\"^\" + abnf_regexp.HOST_RE + \"$\")\\n-IPv4_MATCHER = re.compile(\"^\" + abnf_regexp.IPv4_RE + \"$\")\\n-IPv6_MATCHER = re.compile(r\"^\\\\[\" + abnf_regexp.IPv6_ADDRZ_RFC4007_RE + r\"\\\\]$\")\\n-\\n-# Used by host validator\\n-IPv6_NO_RFC4007_MATCHER = re.compile(r\"^\\\\[%s\\\\]$\" % (abnf_regexp.IPv6_ADDRZ_RE))\\n-\\n-# Matcher used to validate path components\\n-PATH_MATCHER = re.compile(abnf_regexp.PATH_RE)\\n-\\n-\\n-# ##################################\\n-# Query and Fragment Matcher Section\\n-# ##################################\\n-\\n-QUERY_MATCHER = re.compile(abnf_regexp.QUERY_RE)\\n-\\n-FRAGMENT_MATCHER = QUERY_MATCHER\\n-\\n-# Scheme validation, see: http://tools.ietf.org/html/rfc3986#section-3.1\\n-SCHEME_MATCHER = re.compile(\"^{0}$\".format(abnf_regexp.SCHEME_RE))\\n-\\n-RELATIVE_REF_MATCHER = re.compile(\\n-    r\"^%s(\\\\?%s)?(#%s)?$\"\\n-    % (abnf_regexp.RELATIVE_PART_RE, abnf_regexp.QUERY_RE, abnf_regexp.FRAGMENT_RE)\\n-)\\n-\\n-# See http://tools.ietf.org/html/rfc3986#section-4.3\\n-ABSOLUTE_URI_MATCHER = re.compile(\\n-    r\"^%s:%s(\\\\?%s)?$\"\\n-    % (\\n-        abnf_regexp.COMPONENT_PATTERN_DICT[\"scheme\"],\\n-        abnf_regexp.HIER_PART_RE,\\n-        abnf_regexp.QUERY_RE[1:-1],\\n-    )\\n-)\\n-\\n-# ###############\\n-# IRIs / RFC 3987\\n-# ###############\\n-\\n-IRI_MATCHER = re.compile(abnf_regexp.URL_PARSING_RE, re.UNICODE)\\n-\\n-ISUBAUTHORITY_MATCHER = re.compile(\\n-    (\\n-        u\"^(?:(?P<userinfo>{0})@)?\"  # iuserinfo\\n-        u\"(?P<host>{1})\"  # ihost\\n-        u\":?(?P<port>{2})?$\"  # port\\n-    ).format(abnf_regexp.IUSERINFO_RE, abnf_regexp.IHOST_RE, abnf_regexp.PORT_RE),\\n-    re.UNICODE,\\n-)\\n-\\n-\\n-# Path merger as defined in http://tools.ietf.org/html/rfc3986#section-5.2.3\\n-def merge_paths(base_uri, relative_path):\\n-    \"\"\"Merge a base URI\\'s path with a relative URI\\'s path.\"\"\"\\n-    if base_uri.path is None and base_uri.authority is not None:\\n-        return \"/\" + relative_path\\n-    else:\\n-        path = base_uri.path or \"\"\\n-        index = path.rfind(\"/\")\\n-        return path[:index] + \"/\" + relative_path\\n-\\n-\\n-UseExisting = object()', '@@ -1,6 +1,4 @@\\n-\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\\n-\\n-# Copyright (c) 2010-2015 Benjamin Peterson\\n+# Copyright (c) 2010-2019 Benjamin Peterson\\n #\\n # Permission is hereby granted, free of charge, to any person obtaining a copy\\n # of this software and associated documentation files (the \"Software\"), to deal\\n@@ -20,6 +18,8 @@\\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\n # SOFTWARE.\\n \\n+\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\\n+\\n from __future__ import absolute_import\\n \\n import functools\\n@@ -29,7 +29,7 @@\\n import types\\n \\n __author__ = \"Benjamin Peterson <benjamin@python.org>\"\\n-__version__ = \"1.10.0\"\\n+__version__ = \"1.12.0\"\\n \\n \\n # Useful for very coarse version differentiation.\\n@@ -242,6 +242,7 @@ class _MovedItems(_LazyModule):\\n     MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\\n     MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\\n     MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\\n+    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\\n     MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\\n     MovedAttribute(\\n         \"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"\\n@@ -267,12 +268,13 @@ class _MovedItems(_LazyModule):\\n     MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\\n     MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\\n     MovedModule(\"http_client\", \"httplib\", \"http.client\"),\\n+    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\\n+    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\\n     MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\\n     MovedModule(\\n         \"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"\\n     ),\\n     MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\\n-    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\\n     MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\\n     MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\\n     MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\\n@@ -339,10 +341,14 @@ class Module_six_moves_urllib_parse(_LazyModule):\\n     MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\\n+    MovedAttribute(\\n+        \"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"\\n+    ),\\n     MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\\n+    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\\n     MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\\n     MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\\n     MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\\n@@ -424,6 +430,8 @@ class Module_six_moves_urllib_request(_LazyModule):\\n     MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\\n     MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\\n     MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\\n+    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\\n+    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\\n ]\\n for attr in _urllib_request_moved_attributes:\\n     setattr(Module_six_moves_urllib_request, attr.name, attr)\\n@@ -665,6 +673,7 @@ def u(s):\\n \\n     StringIO = io.StringIO\\n     BytesIO = io.BytesIO\\n+    del io\\n     _assertCountEqual = \"assertCountEqual\"\\n     if sys.version_info[1] <= 1:\\n         _assertRaisesRegex = \"assertRaisesRegexp\"\\n@@ -718,11 +727,15 @@ def assertRegex(self, *args, **kwargs):\\n     exec_ = getattr(moves.builtins, \"exec\")\\n \\n     def reraise(tp, value, tb=None):\\n-        if value is None:\\n-            value = tp()\\n-        if value.__traceback__ is not tb:\\n-            raise value.with_traceback(tb)\\n-        raise value\\n+        try:\\n+            if value is None:\\n+                value = tp()\\n+            if value.__traceback__ is not tb:\\n+                raise value.with_traceback(tb)\\n+            raise value\\n+        finally:\\n+            value = None\\n+            tb = None\\n \\n \\n else:\\n@@ -741,23 +754,32 @@ def exec_(_code_, _globs_=None, _locs_=None):\\n \\n     exec_(\\n         \"\"\"def reraise(tp, value, tb=None):\\n-    raise tp, value, tb\\n+    try:\\n+        raise tp, value, tb\\n+    finally:\\n+        tb = None\\n \"\"\"\\n     )\\n \\n \\n if sys.version_info[:2] == (3, 2):\\n     exec_(\\n         \"\"\"def raise_from(value, from_value):\\n-    if from_value is None:\\n-        raise value\\n-    raise value from from_value\\n+    try:\\n+        if from_value is None:\\n+            raise value\\n+        raise value from from_value\\n+    finally:\\n+        value = None\\n \"\"\"\\n     )\\n elif sys.version_info[:2] > (3, 2):\\n     exec_(\\n         \"\"\"def raise_from(value, from_value):\\n-    raise value from from_value\\n+    try:\\n+        raise value from from_value\\n+    finally:\\n+        value = None\\n \"\"\"\\n     )\\n else:\\n@@ -864,10 +886,14 @@ def with_metaclass(meta, *bases):\\n     # This requires a bit of explanation: the basic idea is to make a dummy\\n     # metaclass for one level of class instantiation that replaces itself with\\n     # the actual metaclass.\\n-    class metaclass(meta):\\n+    class metaclass(type):\\n         def __new__(cls, name, this_bases, d):\\n             return meta(name, bases, d)\\n \\n+        @classmethod\\n+        def __prepare__(cls, name, this_bases):\\n+            return meta.__prepare__(name, bases)\\n+\\n     return type.__new__(metaclass, \"temporary_class\", (), {})\\n \\n \\n@@ -884,11 +910,71 @@ def wrapper(cls):\\n                 orig_vars.pop(slots_var)\\n         orig_vars.pop(\"__dict__\", None)\\n         orig_vars.pop(\"__weakref__\", None)\\n+        if hasattr(cls, \"__qualname__\"):\\n+            orig_vars[\"__qualname__\"] = cls.__qualname__\\n         return metaclass(cls.__name__, cls.__bases__, orig_vars)\\n \\n     return wrapper\\n \\n \\n+def ensure_binary(s, encoding=\"utf-8\", errors=\"strict\"):\\n+    \"\"\"Coerce **s** to six.binary_type.\\n+\\n+    For Python 2:\\n+      - `unicode` -> encoded to `str`\\n+      - `str` -> `str`\\n+\\n+    For Python 3:\\n+      - `str` -> encoded to `bytes`\\n+      - `bytes` -> `bytes`\\n+    \"\"\"\\n+    if isinstance(s, text_type):\\n+        return s.encode(encoding, errors)\\n+    elif isinstance(s, binary_type):\\n+        return s\\n+    else:\\n+        raise TypeError(\"not expecting type \\'%s\\'\" % type(s))\\n+\\n+\\n+def ensure_str(s, encoding=\"utf-8\", errors=\"strict\"):\\n+    \"\"\"Coerce *s* to `str`.\\n+\\n+    For Python 2:\\n+      - `unicode` -> encoded to `str`\\n+      - `str` -> `str`\\n+\\n+    For Python 3:\\n+      - `str` -> `str`\\n+      - `bytes` -> decoded to `str`\\n+    \"\"\"\\n+    if not isinstance(s, (text_type, binary_type)):\\n+        raise TypeError(\"not expecting type \\'%s\\'\" % type(s))\\n+    if PY2 and isinstance(s, text_type):\\n+        s = s.encode(encoding, errors)\\n+    elif PY3 and isinstance(s, binary_type):\\n+        s = s.decode(encoding, errors)\\n+    return s\\n+\\n+\\n+def ensure_text(s, encoding=\"utf-8\", errors=\"strict\"):\\n+    \"\"\"Coerce *s* to six.text_type.\\n+\\n+    For Python 2:\\n+      - `unicode` -> `unicode`\\n+      - `str` -> `unicode`\\n+\\n+    For Python 3:\\n+      - `str` -> `str`\\n+      - `bytes` -> decoded to `str`\\n+    \"\"\"\\n+    if isinstance(s, binary_type):\\n+        return s.decode(encoding, errors)\\n+    elif isinstance(s, text_type):\\n+        return s\\n+    else:\\n+        raise TypeError(\"not expecting type \\'%s\\'\" % type(s))\\n+\\n+\\n def python_2_unicode_compatible(klass):\\n     \"\"\"\\n     A decorator that defines __unicode__ and __str__ methods under Python 2.', '@@ -26,7 +26,6 @@\\n from .packages.ssl_match_hostname import CertificateError\\n from .packages import six\\n from .packages.six.moves import queue\\n-from .packages.rfc3986.normalizers import normalize_host\\n from .connection import (\\n     port_by_scheme,\\n     DummyConnection,\\n@@ -44,7 +43,7 @@\\n from .util.response import assert_header_parsing\\n from .util.retry import Retry\\n from .util.timeout import Timeout\\n-from .util.url import get_host, Url, NORMALIZABLE_SCHEMES\\n+from .util.url import get_host, Url, _normalize_host as normalize_host\\n from .util.queue import LifoQueue\\n \\n \\n@@ -1027,14 +1026,14 @@ def _normalize_host(host, scheme):\\n     Normalize hosts for comparisons and use with sockets.\\n     \"\"\"\\n \\n+    host = normalize_host(host, scheme)\\n+\\n     # httplib doesn\\'t like it when we include brackets in IPv6 addresses\\n     # Specifically, if we include brackets but also pass the port then\\n     # httplib crazily doubles up the square brackets on the Host header.\\n     # Instead, we need to make sure we never pass ``None`` as the port.\\n     # However, for backward compatibility reasons we can\\'t actually\\n     # *assert* that.  See http://bugs.python.org/issue28539\\n     if host.startswith(\"[\") and host.endswith(\"]\"):\\n-        host = host.strip(\"[]\")\\n-    if scheme in NORMALIZABLE_SCHEMES:\\n-        host = normalize_host(host)\\n+        host = host[1:-1]\\n     return host', '@@ -1,112 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-\"\"\"Exceptions module for rfc3986.\"\"\"\\n-\\n-from . import compat\\n-\\n-\\n-class RFC3986Exception(Exception):\\n-    \"\"\"Base class for all rfc3986 exception classes.\"\"\"\\n-\\n-    pass\\n-\\n-\\n-class InvalidAuthority(RFC3986Exception):\\n-    \"\"\"Exception when the authority string is invalid.\"\"\"\\n-\\n-    def __init__(self, authority):\\n-        \"\"\"Initialize the exception with the invalid authority.\"\"\"\\n-        super(InvalidAuthority, self).__init__(\\n-            u\"The authority ({0}) is not valid.\".format(compat.to_str(authority))\\n-        )\\n-\\n-\\n-class InvalidPort(RFC3986Exception):\\n-    \"\"\"Exception when the port is invalid.\"\"\"\\n-\\n-    def __init__(self, port):\\n-        \"\"\"Initialize the exception with the invalid port.\"\"\"\\n-        super(InvalidPort, self).__init__(\\'The port (\"{0}\") is not valid.\\'.format(port))\\n-\\n-\\n-class ResolutionError(RFC3986Exception):\\n-    \"\"\"Exception to indicate a failure to resolve a URI.\"\"\"\\n-\\n-    def __init__(self, uri):\\n-        \"\"\"Initialize the error with the failed URI.\"\"\"\\n-        super(ResolutionError, self).__init__(\\n-            \"{0} is not an absolute URI.\".format(uri.unsplit())\\n-        )\\n-\\n-\\n-class ValidationError(RFC3986Exception):\\n-    \"\"\"Exception raised during Validation of a URI.\"\"\"\\n-\\n-    pass\\n-\\n-\\n-class MissingComponentError(ValidationError):\\n-    \"\"\"Exception raised when a required component is missing.\"\"\"\\n-\\n-    def __init__(self, uri, *component_names):\\n-        \"\"\"Initialize the error with the missing component name.\"\"\"\\n-        verb = \"was\"\\n-        if len(component_names) > 1:\\n-            verb = \"were\"\\n-\\n-        self.uri = uri\\n-        self.components = sorted(component_names)\\n-        components = \", \".join(self.components)\\n-        super(MissingComponentError, self).__init__(\\n-            \"{} {} required but missing\".format(components, verb), uri, self.components\\n-        )\\n-\\n-\\n-class UnpermittedComponentError(ValidationError):\\n-    \"\"\"Exception raised when a component has an unpermitted value.\"\"\"\\n-\\n-    def __init__(self, component_name, component_value, allowed_values):\\n-        \"\"\"Initialize the error with the unpermitted component.\"\"\"\\n-        super(UnpermittedComponentError, self).__init__(\\n-            \"{} was required to be one of {!r} but was {!r}\".format(\\n-                component_name, list(sorted(allowed_values)), component_value\\n-            ),\\n-            component_name,\\n-            component_value,\\n-            allowed_values,\\n-        )\\n-        self.component_name = component_name\\n-        self.component_value = component_value\\n-        self.allowed_values = allowed_values\\n-\\n-\\n-class PasswordForbidden(ValidationError):\\n-    \"\"\"Exception raised when a URL has a password in the userinfo section.\"\"\"\\n-\\n-    def __init__(self, uri):\\n-        \"\"\"Initialize the error with the URI that failed validation.\"\"\"\\n-        unsplit = getattr(uri, \"unsplit\", lambda: uri)\\n-        super(PasswordForbidden, self).__init__(\\n-            \\'\"{}\" contained a password when validation forbade it\\'.format(unsplit())\\n-        )\\n-        self.uri = uri\\n-\\n-\\n-class InvalidComponentsError(ValidationError):\\n-    \"\"\"Exception raised when one or more components are invalid.\"\"\"\\n-\\n-    def __init__(self, uri, *component_names):\\n-        \"\"\"Initialize the error with the invalid component name(s).\"\"\"\\n-        verb = \"was\"\\n-        if len(component_names) > 1:\\n-            verb = \"were\"\\n-\\n-        self.uri = uri\\n-        self.components = sorted(component_names)\\n-        components = \", \".join(self.components)\\n-        super(InvalidComponentsError, self).__init__(\\n-            \"{} {} found to be invalid\".format(components, verb), uri, self.components\\n-        )\\n-\\n-\\n-class MissingDependencyError(RFC3986Exception):\\n-    \"\"\"Exception raised when an IRI is encoded without the \\'idna\\' module.\"\"\"', '@@ -1,371 +0,0 @@\\n-\"\"\"Module containing the implementation of the URIMixin class.\"\"\"\\n-import warnings\\n-\\n-from . import exceptions as exc\\n-from . import misc\\n-from . import normalizers\\n-from . import validators\\n-\\n-\\n-class URIMixin(object):\\n-    \"\"\"Mixin with all shared methods for URIs and IRIs.\"\"\"\\n-\\n-    __hash__ = tuple.__hash__\\n-\\n-    def authority_info(self):\\n-        \"\"\"Return a dictionary with the ``userinfo``, ``host``, and ``port``.\\n-\\n-        If the authority is not valid, it will raise a\\n-        :class:`~rfc3986.exceptions.InvalidAuthority` Exception.\\n-\\n-        :returns:\\n-            ``{\\'userinfo\\': \\'username:password\\', \\'host\\': \\'www.example.com\\',\\n-            \\'port\\': \\'80\\'}``\\n-        :rtype: dict\\n-        :raises rfc3986.exceptions.InvalidAuthority:\\n-            If the authority is not ``None`` and can not be parsed.\\n-        \"\"\"\\n-        if not self.authority:\\n-            return {\"userinfo\": None, \"host\": None, \"port\": None}\\n-\\n-        match = self._match_subauthority()\\n-\\n-        if match is None:\\n-            # In this case, we have an authority that was parsed from the URI\\n-            # Reference, but it cannot be further parsed by our\\n-            # misc.SUBAUTHORITY_MATCHER. In this case it must not be a valid\\n-            # authority.\\n-            raise exc.InvalidAuthority(self.authority.encode(self.encoding))\\n-\\n-        # We had a match, now let\\'s ensure that it is actually a valid host\\n-        # address if it is IPv4\\n-        matches = match.groupdict()\\n-        host = matches.get(\"host\")\\n-\\n-        if (\\n-            host\\n-            and misc.IPv4_MATCHER.match(host)\\n-            and not validators.valid_ipv4_host_address(host)\\n-        ):\\n-            # If we have a host, it appears to be IPv4 and it does not have\\n-            # valid bytes, it is an InvalidAuthority.\\n-            raise exc.InvalidAuthority(self.authority.encode(self.encoding))\\n-\\n-        return matches\\n-\\n-    def _match_subauthority(self):\\n-        return misc.SUBAUTHORITY_MATCHER.match(self.authority)\\n-\\n-    @property\\n-    def host(self):\\n-        \"\"\"If present, a string representing the host.\"\"\"\\n-        try:\\n-            authority = self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return None\\n-        return authority[\"host\"]\\n-\\n-    @property\\n-    def port(self):\\n-        \"\"\"If present, the port extracted from the authority.\"\"\"\\n-        try:\\n-            authority = self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return None\\n-        return authority[\"port\"]\\n-\\n-    @property\\n-    def userinfo(self):\\n-        \"\"\"If present, the userinfo extracted from the authority.\"\"\"\\n-        try:\\n-            authority = self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return None\\n-        return authority[\"userinfo\"]\\n-\\n-    def is_absolute(self):\\n-        \"\"\"Determine if this URI Reference is an absolute URI.\\n-\\n-        See http://tools.ietf.org/html/rfc3986#section-4.3 for explanation.\\n-\\n-        :returns: ``True`` if it is an absolute URI, ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        return bool(misc.ABSOLUTE_URI_MATCHER.match(self.unsplit()))\\n-\\n-    def is_valid(self, **kwargs):\\n-        \"\"\"Determine if the URI is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param bool require_scheme: Set to ``True`` if you wish to require the\\n-            presence of the scheme component.\\n-        :param bool require_authority: Set to ``True`` if you wish to require\\n-            the presence of the authority component.\\n-        :param bool require_path: Set to ``True`` if you wish to require the\\n-            presence of the path component.\\n-        :param bool require_query: Set to ``True`` if you wish to require the\\n-            presence of the query component.\\n-        :param bool require_fragment: Set to ``True`` if you wish to require\\n-            the presence of the fragment component.\\n-        :returns: ``True`` if the URI is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        validators = [\\n-            (self.scheme_is_valid, kwargs.get(\"require_scheme\", False)),\\n-            (self.authority_is_valid, kwargs.get(\"require_authority\", False)),\\n-            (self.path_is_valid, kwargs.get(\"require_path\", False)),\\n-            (self.query_is_valid, kwargs.get(\"require_query\", False)),\\n-            (self.fragment_is_valid, kwargs.get(\"require_fragment\", False)),\\n-        ]\\n-        return all(v(r) for v, r in validators)\\n-\\n-    def authority_is_valid(self, require=False):\\n-        \"\"\"Determine if the authority component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param bool require:\\n-            Set to ``True`` to require the presence of this component.\\n-        :returns:\\n-            ``True`` if the authority is valid. ``False`` otherwise.\\n-        :rtype:\\n-            bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        try:\\n-            self.authority_info()\\n-        except exc.InvalidAuthority:\\n-            return False\\n-\\n-        return validators.authority_is_valid(\\n-            self.authority, host=self.host, require=require\\n-        )\\n-\\n-    def scheme_is_valid(self, require=False):\\n-        \"\"\"Determine if the scheme component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the scheme is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.scheme_is_valid(self.scheme, require)\\n-\\n-    def path_is_valid(self, require=False):\\n-        \"\"\"Determine if the path component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the path is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.path_is_valid(self.path, require)\\n-\\n-    def query_is_valid(self, require=False):\\n-        \"\"\"Determine if the query component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the :class:`~rfc3986.validators.Validator` object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the query is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.query_is_valid(self.query, require)\\n-\\n-    def fragment_is_valid(self, require=False):\\n-        \"\"\"Determine if the fragment component is valid.\\n-\\n-        .. deprecated:: 1.1.0\\n-\\n-            Use the Validator object instead.\\n-\\n-        :param str require: Set to ``True`` to require the presence of this\\n-            component.\\n-        :returns: ``True`` if the fragment is valid. ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        warnings.warn(\\n-            \"Please use rfc3986.validators.Validator instead. \"\\n-            \"This method will be eventually removed.\",\\n-            DeprecationWarning,\\n-        )\\n-        return validators.fragment_is_valid(self.fragment, require)\\n-\\n-    def normalized_equality(self, other_ref):\\n-        \"\"\"Compare this URIReference to another URIReference.\\n-\\n-        :param URIReference other_ref: (required), The reference with which\\n-            we\\'re comparing.\\n-        :returns: ``True`` if the references are equal, ``False`` otherwise.\\n-        :rtype: bool\\n-        \"\"\"\\n-        return tuple(self.normalize()) == tuple(other_ref.normalize())\\n-\\n-    def resolve_with(self, base_uri, strict=False):\\n-        \"\"\"Use an absolute URI Reference to resolve this relative reference.\\n-\\n-        Assuming this is a relative reference that you would like to resolve,\\n-        use the provided base URI to resolve it.\\n-\\n-        See http://tools.ietf.org/html/rfc3986#section-5 for more information.\\n-\\n-        :param base_uri: Either a string or URIReference. It must be an\\n-            absolute URI or it will raise an exception.\\n-        :returns: A new URIReference which is the result of resolving this\\n-            reference using ``base_uri``.\\n-        :rtype: :class:`URIReference`\\n-        :raises rfc3986.exceptions.ResolutionError:\\n-            If the ``base_uri`` is not an absolute URI.\\n-        \"\"\"\\n-        if not isinstance(base_uri, URIMixin):\\n-            base_uri = type(self).from_string(base_uri)\\n-\\n-        if not base_uri.is_absolute():\\n-            raise exc.ResolutionError(base_uri)\\n-\\n-        # This is optional per\\n-        # http://tools.ietf.org/html/rfc3986#section-5.2.1\\n-        base_uri = base_uri.normalize()\\n-\\n-        # The reference we\\'re resolving\\n-        resolving = self\\n-\\n-        if not strict and resolving.scheme == base_uri.scheme:\\n-            resolving = resolving.copy_with(scheme=None)\\n-\\n-        # http://tools.ietf.org/html/rfc3986#page-32\\n-        if resolving.scheme is not None:\\n-            target = resolving.copy_with(\\n-                path=normalizers.normalize_path(resolving.path)\\n-            )\\n-        else:\\n-            if resolving.authority is not None:\\n-                target = resolving.copy_with(\\n-                    scheme=base_uri.scheme,\\n-                    path=normalizers.normalize_path(resolving.path),\\n-                )\\n-            else:\\n-                if resolving.path is None:\\n-                    if resolving.query is not None:\\n-                        query = resolving.query\\n-                    else:\\n-                        query = base_uri.query\\n-                    target = resolving.copy_with(\\n-                        scheme=base_uri.scheme,\\n-                        authority=base_uri.authority,\\n-                        path=base_uri.path,\\n-                        query=query,\\n-                    )\\n-                else:\\n-                    if resolving.path.startswith(\"/\"):\\n-                        path = normalizers.normalize_path(resolving.path)\\n-                    else:\\n-                        path = normalizers.normalize_path(\\n-                            misc.merge_paths(base_uri, resolving.path)\\n-                        )\\n-                    target = resolving.copy_with(\\n-                        scheme=base_uri.scheme,\\n-                        authority=base_uri.authority,\\n-                        path=path,\\n-                        query=resolving.query,\\n-                    )\\n-        return target\\n-\\n-    def unsplit(self):\\n-        \"\"\"Create a URI string from the components.\\n-\\n-        :returns: The URI Reference reconstituted as a string.\\n-        :rtype: str\\n-        \"\"\"\\n-        # See http://tools.ietf.org/html/rfc3986#section-5.3\\n-        result_list = []\\n-        if self.scheme:\\n-            result_list.extend([self.scheme, \":\"])\\n-        if self.authority:\\n-            result_list.extend([\"//\", self.authority])\\n-        if self.path:\\n-            result_list.append(self.path)\\n-        if self.query is not None:\\n-            result_list.extend([\"?\", self.query])\\n-        if self.fragment is not None:\\n-            result_list.extend([\"#\", self.fragment])\\n-        return \"\".join(result_list)\\n-\\n-    def copy_with(\\n-        self,\\n-        scheme=misc.UseExisting,\\n-        authority=misc.UseExisting,\\n-        path=misc.UseExisting,\\n-        query=misc.UseExisting,\\n-        fragment=misc.UseExisting,\\n-    ):\\n-        \"\"\"Create a copy of this reference with the new components.\\n-\\n-        :param str scheme:\\n-            (optional) The scheme to use for the new reference.\\n-        :param str authority:\\n-            (optional) The authority to use for the new reference.\\n-        :param str path:\\n-            (optional) The path to use for the new reference.\\n-        :param str query:\\n-            (optional) The query to use for the new reference.\\n-        :param str fragment:\\n-            (optional) The fragment to use for the new reference.\\n-        :returns:\\n-            New URIReference with provided components.\\n-        :rtype:\\n-            URIReference\\n-        \"\"\"\\n-        attributes = {\\n-            \"scheme\": scheme,\\n-            \"authority\": authority,\\n-            \"path\": path,\\n-            \"query\": query,\\n-            \"fragment\": fragment,\\n-        }\\n-        for key, value in list(attributes.items()):\\n-            if value is misc.UseExisting:\\n-                del attributes[key]\\n-        uri = self._replace(**attributes)\\n-        uri.encoding = self.encoding\\n-        return uri', '@@ -1,106 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"\\n-Module containing the simple and functional API for rfc3986.\\n-\\n-This module defines functions and provides access to the public attributes\\n-and classes of rfc3986.\\n-\"\"\"\\n-\\n-from .iri import IRIReference\\n-from .parseresult import ParseResult\\n-from .uri import URIReference\\n-\\n-\\n-def uri_reference(uri, encoding=\"utf-8\"):\\n-    \"\"\"Parse a URI string into a URIReference.\\n-\\n-    This is a convenience function. You could achieve the same end by using\\n-    ``URIReference.from_string(uri)``.\\n-\\n-    :param str uri: The URI which needs to be parsed into a reference.\\n-    :param str encoding: The encoding of the string provided\\n-    :returns: A parsed URI\\n-    :rtype: :class:`URIReference`\\n-    \"\"\"\\n-    return URIReference.from_string(uri, encoding)\\n-\\n-\\n-def iri_reference(iri, encoding=\"utf-8\"):\\n-    \"\"\"Parse a IRI string into an IRIReference.\\n-\\n-    This is a convenience function. You could achieve the same end by using\\n-    ``IRIReference.from_string(iri)``.\\n-\\n-    :param str iri: The IRI which needs to be parsed into a reference.\\n-    :param str encoding: The encoding of the string provided\\n-    :returns: A parsed IRI\\n-    :rtype: :class:`IRIReference`\\n-    \"\"\"\\n-    return IRIReference.from_string(iri, encoding)\\n-\\n-\\n-def is_valid_uri(uri, encoding=\"utf-8\", **kwargs):\\n-    \"\"\"Determine if the URI given is valid.\\n-\\n-    This is a convenience function. You could use either\\n-    ``uri_reference(uri).is_valid()`` or\\n-    ``URIReference.from_string(uri).is_valid()`` to achieve the same result.\\n-\\n-    :param str uri: The URI to be validated.\\n-    :param str encoding: The encoding of the string provided\\n-    :param bool require_scheme: Set to ``True`` if you wish to require the\\n-        presence of the scheme component.\\n-    :param bool require_authority: Set to ``True`` if you wish to require the\\n-        presence of the authority component.\\n-    :param bool require_path: Set to ``True`` if you wish to require the\\n-        presence of the path component.\\n-    :param bool require_query: Set to ``True`` if you wish to require the\\n-        presence of the query component.\\n-    :param bool require_fragment: Set to ``True`` if you wish to require the\\n-        presence of the fragment component.\\n-    :returns: ``True`` if the URI is valid, ``False`` otherwise.\\n-    :rtype: bool\\n-    \"\"\"\\n-    return URIReference.from_string(uri, encoding).is_valid(**kwargs)\\n-\\n-\\n-def normalize_uri(uri, encoding=\"utf-8\"):\\n-    \"\"\"Normalize the given URI.\\n-\\n-    This is a convenience function. You could use either\\n-    ``uri_reference(uri).normalize().unsplit()`` or\\n-    ``URIReference.from_string(uri).normalize().unsplit()`` instead.\\n-\\n-    :param str uri: The URI to be normalized.\\n-    :param str encoding: The encoding of the string provided\\n-    :returns: The normalized URI.\\n-    :rtype: str\\n-    \"\"\"\\n-    normalized_reference = URIReference.from_string(uri, encoding).normalize()\\n-    return normalized_reference.unsplit()\\n-\\n-\\n-def urlparse(uri, encoding=\"utf-8\"):\\n-    \"\"\"Parse a given URI and return a ParseResult.\\n-\\n-    This is a partial replacement of the standard library\\'s urlparse function.\\n-\\n-    :param str uri: The URI to be parsed.\\n-    :param str encoding: The encoding of the string provided.\\n-    :returns: A parsed URI\\n-    :rtype: :class:`~rfc3986.parseresult.ParseResult`\\n-    \"\"\"\\n-    return ParseResult.from_string(uri, encoding, strict=False)', '@@ -1,301 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2017 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module containing the logic for the URIBuilder object.\"\"\"\\n-from . import compat\\n-from . import normalizers\\n-from . import uri\\n-\\n-\\n-class URIBuilder(object):\\n-    \"\"\"Object to aid in building up a URI Reference from parts.\\n-\\n-    .. note::\\n-\\n-        This object should be instantiated by the user, but it\\'s recommended\\n-        that it is not provided with arguments. Instead, use the available\\n-        method to populate the fields.\\n-\\n-    \"\"\"\\n-\\n-    def __init__(\\n-        self,\\n-        scheme=None,\\n-        userinfo=None,\\n-        host=None,\\n-        port=None,\\n-        path=None,\\n-        query=None,\\n-        fragment=None,\\n-    ):\\n-        \"\"\"Initialize our URI builder.\\n-\\n-        :param str scheme:\\n-            (optional)\\n-        :param str userinfo:\\n-            (optional)\\n-        :param str host:\\n-            (optional)\\n-        :param int port:\\n-            (optional)\\n-        :param str path:\\n-            (optional)\\n-        :param str query:\\n-            (optional)\\n-        :param str fragment:\\n-            (optional)\\n-        \"\"\"\\n-        self.scheme = scheme\\n-        self.userinfo = userinfo\\n-        self.host = host\\n-        self.port = port\\n-        self.path = path\\n-        self.query = query\\n-        self.fragment = fragment\\n-\\n-    def __repr__(self):\\n-        \"\"\"Provide a convenient view of our builder object.\"\"\"\\n-        formatstr = (\\n-            \"URIBuilder(scheme={b.scheme}, userinfo={b.userinfo}, \"\\n-            \"host={b.host}, port={b.port}, path={b.path}, \"\\n-            \"query={b.query}, fragment={b.fragment})\"\\n-        )\\n-        return formatstr.format(b=self)\\n-\\n-    def add_scheme(self, scheme):\\n-        \"\"\"Add a scheme to our builder object.\\n-\\n-        After normalizing, this will generate a new URIBuilder instance with\\n-        the specified scheme and all other attributes the same.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_scheme(\\'HTTPS\\')\\n-            URIBuilder(scheme=\\'https\\', userinfo=None, host=None, port=None,\\n-                    path=None, query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        scheme = normalizers.normalize_scheme(scheme)\\n-        return URIBuilder(\\n-            scheme=scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_credentials(self, username, password):\\n-        \"\"\"Add credentials as the userinfo portion of the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_credentials(\\'root\\', \\'s3crete\\')\\n-            URIBuilder(scheme=None, userinfo=\\'root:s3crete\\', host=None,\\n-                    port=None, path=None, query=None, fragment=None)\\n-\\n-            >>> URIBuilder().add_credentials(\\'root\\', None)\\n-            URIBuilder(scheme=None, userinfo=\\'root\\', host=None,\\n-                    port=None, path=None, query=None, fragment=None)\\n-        \"\"\"\\n-        if username is None:\\n-            raise ValueError(\"Username cannot be None\")\\n-        userinfo = normalizers.normalize_username(username)\\n-\\n-        if password is not None:\\n-            userinfo = \"{}:{}\".format(\\n-                userinfo, normalizers.normalize_password(password)\\n-            )\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_host(self, host):\\n-        \"\"\"Add hostname to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_host(\\'google.com\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=\\'google.com\\',\\n-                    port=None, path=None, query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=normalizers.normalize_host(host),\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_port(self, port):\\n-        \"\"\"Add port to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_port(80)\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=\\'80\\',\\n-                    path=None, query=None, fragment=None)\\n-\\n-            >>> URIBuilder().add_port(443)\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=\\'443\\',\\n-                    path=None, query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        port_int = int(port)\\n-        if port_int < 0:\\n-            raise ValueError(\\n-                \"ports are not allowed to be negative. You provided {}\".format(port_int)\\n-            )\\n-        if port_int > 65535:\\n-            raise ValueError(\\n-                \"ports are not allowed to be larger than 65535. \"\\n-                \"You provided {}\".format(port_int)\\n-            )\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=\"{}\".format(port_int),\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_path(self, path):\\n-        \"\"\"Add a path to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_path(\\'sigmavirus24/rfc3985\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=\\'/sigmavirus24/rfc3986\\', query=None, fragment=None)\\n-\\n-            >>> URIBuilder().add_path(\\'/checkout.php\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=\\'/checkout.php\\', query=None, fragment=None)\\n-\\n-        \"\"\"\\n-        if not path.startswith(\"/\"):\\n-            path = \"/{}\".format(path)\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=normalizers.normalize_path(path),\\n-            query=self.query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_query_from(self, query_items):\\n-        \"\"\"Generate and add a query a dictionary or list of tuples.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_query_from({\\'a\\': \\'b c\\'})\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=\\'a=b+c\\', fragment=None)\\n-\\n-            >>> URIBuilder().add_query_from([(\\'a\\', \\'b c\\')])\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=\\'a=b+c\\', fragment=None)\\n-\\n-        \"\"\"\\n-        query = normalizers.normalize_query(compat.urlencode(query_items))\\n-\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=query,\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_query(self, query):\\n-        \"\"\"Add a pre-formated query string to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_query(\\'a=b&c=d\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=\\'a=b&c=d\\', fragment=None)\\n-\\n-        \"\"\"\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=normalizers.normalize_query(query),\\n-            fragment=self.fragment,\\n-        )\\n-\\n-    def add_fragment(self, fragment):\\n-        \"\"\"Add a fragment to the URI.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_fragment(\\'section-2.6.1\\')\\n-            URIBuilder(scheme=None, userinfo=None, host=None, port=None,\\n-                    path=None, query=None, fragment=\\'section-2.6.1\\')\\n-\\n-        \"\"\"\\n-        return URIBuilder(\\n-            scheme=self.scheme,\\n-            userinfo=self.userinfo,\\n-            host=self.host,\\n-            port=self.port,\\n-            path=self.path,\\n-            query=self.query,\\n-            fragment=normalizers.normalize_fragment(fragment),\\n-        )\\n-\\n-    def finalize(self):\\n-        \"\"\"Create a URIReference from our builder.\\n-\\n-        .. code-block:: python\\n-\\n-            >>> URIBuilder().add_scheme(\\'https\\').add_host(\\'github.com\\'\\n-            ...     ).add_path(\\'sigmavirus24/rfc3986\\').finalize().unsplit()\\n-            \\'https://github.com/sigmavirus24/rfc3986\\'\\n-\\n-            >>> URIBuilder().add_scheme(\\'https\\').add_host(\\'github.com\\'\\n-            ...     ).add_path(\\'sigmavirus24/rfc3986\\').add_credentials(\\n-            ...     \\'sigmavirus24\\', \\'not-re@l\\').finalize().unsplit()\\n-            \\'https://sigmavirus24:not-re%40l@github.com/sigmavirus24/rfc3986\\'\\n-\\n-        \"\"\"\\n-        return uri.URIReference(\\n-            self.scheme,\\n-            normalizers.normalize_authority((self.userinfo, self.host, self.port)),\\n-            self.path,\\n-            self.query,\\n-            self.fragment,\\n-        )', '@@ -1,152 +0,0 @@\\n-\"\"\"Module containing the implementation of the URIReference class.\"\"\"\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2014 Rackspace\\n-# Copyright (c) 2015 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-from collections import namedtuple\\n-\\n-from . import compat\\n-from . import misc\\n-from . import normalizers\\n-from ._mixin import URIMixin\\n-\\n-\\n-class URIReference(namedtuple(\"URIReference\", misc.URI_COMPONENTS), URIMixin):\\n-    \"\"\"Immutable object representing a parsed URI Reference.\\n-\\n-    .. note::\\n-\\n-        This class is not intended to be directly instantiated by the user.\\n-\\n-    This object exposes attributes for the following components of a\\n-    URI:\\n-\\n-    - scheme\\n-    - authority\\n-    - path\\n-    - query\\n-    - fragment\\n-\\n-    .. attribute:: scheme\\n-\\n-        The scheme that was parsed for the URI Reference. For example,\\n-        ``http``, ``https``, ``smtp``, ``imap``, etc.\\n-\\n-    .. attribute:: authority\\n-\\n-        Component of the URI that contains the user information, host,\\n-        and port sub-components. For example,\\n-        ``google.com``, ``127.0.0.1:5000``, ``username@[::1]``,\\n-        ``username:password@example.com:443``, etc.\\n-\\n-    .. attribute:: path\\n-\\n-        The path that was parsed for the given URI Reference. For example,\\n-        ``/``, ``/index.php``, etc.\\n-\\n-    .. attribute:: query\\n-\\n-        The query component for a given URI Reference. For example, ``a=b``,\\n-        ``a=b%20c``, ``a=b+c``, ``a=b,c=d,e=%20f``, etc.\\n-\\n-    .. attribute:: fragment\\n-\\n-        The fragment component of a URI. For example, ``section-3.1``.\\n-\\n-    This class also provides extra attributes for easier access to information\\n-    like the subcomponents of the authority component.\\n-\\n-    .. attribute:: userinfo\\n-\\n-        The user information parsed from the authority.\\n-\\n-    .. attribute:: host\\n-\\n-        The hostname, IPv4, or IPv6 adddres parsed from the authority.\\n-\\n-    .. attribute:: port\\n-\\n-        The port parsed from the authority.\\n-    \"\"\"\\n-\\n-    slots = ()\\n-\\n-    def __new__(cls, scheme, authority, path, query, fragment, encoding=\"utf-8\"):\\n-        \"\"\"Create a new URIReference.\"\"\"\\n-        ref = super(URIReference, cls).__new__(\\n-            cls, scheme or None, authority or None, path or None, query, fragment\\n-        )\\n-        ref.encoding = encoding\\n-        return ref\\n-\\n-    __hash__ = tuple.__hash__\\n-\\n-    def __eq__(self, other):\\n-        \"\"\"Compare this reference to another.\"\"\"\\n-        other_ref = other\\n-        if isinstance(other, tuple):\\n-            other_ref = URIReference(*other)\\n-        elif not isinstance(other, URIReference):\\n-            try:\\n-                other_ref = URIReference.from_string(other)\\n-            except TypeError:\\n-                raise TypeError(\\n-                    \"Unable to compare URIReference() to {0}()\".format(\\n-                        type(other).__name__\\n-                    )\\n-                )\\n-\\n-        # See http://tools.ietf.org/html/rfc3986#section-6.2\\n-        naive_equality = tuple(self) == tuple(other_ref)\\n-        return naive_equality or self.normalized_equality(other_ref)\\n-\\n-    def normalize(self):\\n-        \"\"\"Normalize this reference as described in Section 6.2.2.\\n-\\n-        This is not an in-place normalization. Instead this creates a new\\n-        URIReference.\\n-\\n-        :returns: A new reference object with normalized components.\\n-        :rtype: URIReference\\n-        \"\"\"\\n-        # See http://tools.ietf.org/html/rfc3986#section-6.2.2 for logic in\\n-        # this method.\\n-        return URIReference(\\n-            normalizers.normalize_scheme(self.scheme or \"\"),\\n-            normalizers.normalize_authority((self.userinfo, self.host, self.port)),\\n-            normalizers.normalize_path(self.path or \"\"),\\n-            normalizers.normalize_query(self.query),\\n-            normalizers.normalize_fragment(self.fragment),\\n-            self.encoding,\\n-        )\\n-\\n-    @classmethod\\n-    def from_string(cls, uri_string, encoding=\"utf-8\"):\\n-        \"\"\"Parse a URI reference from the given unicode URI string.\\n-\\n-        :param str uri_string: Unicode URI to be parsed into a reference.\\n-        :param str encoding: The encoding of the string provided\\n-        :returns: :class:`URIReference` or subclass thereof\\n-        \"\"\"\\n-        uri_string = compat.to_str(uri_string, encoding)\\n-\\n-        split_uri = misc.URI_MATCHER.match(uri_string).groupdict()\\n-        return cls(\\n-            split_uri[\"scheme\"],\\n-            split_uri[\"authority\"],\\n-            normalizers.encode_component(split_uri[\"path\"], encoding),\\n-            normalizers.encode_component(split_uri[\"query\"], encoding),\\n-            normalizers.encode_component(split_uri[\"fragment\"], encoding),\\n-            encoding,\\n-        )', '@@ -1,435 +0,0 @@\\n-# -*- coding: utf-8 -*-\\n-# Copyright (c) 2017 Ian Stapleton Cordasco\\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\\n-# you may not use this file except in compliance with the License.\\n-# You may obtain a copy of the License at\\n-#\\n-#    http://www.apache.org/licenses/LICENSE-2.0\\n-#\\n-# Unless required by applicable law or agreed to in writing, software\\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\\n-# implied.\\n-# See the License for the specific language governing permissions and\\n-# limitations under the License.\\n-\"\"\"Module containing the validation logic for rfc3986.\"\"\"\\n-from . import exceptions\\n-from . import misc\\n-from . import normalizers\\n-\\n-\\n-class Validator(object):\\n-    \"\"\"Object used to configure validation of all objects in rfc3986.\\n-\\n-    .. versionadded:: 1.0\\n-\\n-    Example usage::\\n-\\n-         >>> from rfc3986 import api, validators\\n-         >>> uri = api.uri_reference(\\'https://github.com/\\')\\n-         >>> validator = validators.Validator().require_presence_of(\\n-         ...    \\'scheme\\', \\'host\\', \\'path\\',\\n-         ... ).allow_schemes(\\n-         ...    \\'http\\', \\'https\\',\\n-         ... ).allow_hosts(\\n-         ...    \\'127.0.0.1\\', \\'github.com\\',\\n-         ... )\\n-         >>> validator.validate(uri)\\n-         >>> invalid_uri = rfc3986.uri_reference(\\'imap://mail.google.com\\')\\n-         >>> validator.validate(invalid_uri)\\n-         Traceback (most recent call last):\\n-         ...\\n-         rfc3986.exceptions.MissingComponentError: (\\'path was required but\\n-         missing\\', URIReference(scheme=u\\'imap\\', authority=u\\'mail.google.com\\',\\n-         path=None, query=None, fragment=None), [\\'path\\'])\\n-\\n-    \"\"\"\\n-\\n-    COMPONENT_NAMES = frozenset(\\n-        [\"scheme\", \"userinfo\", \"host\", \"port\", \"path\", \"query\", \"fragment\"]\\n-    )\\n-\\n-    def __init__(self):\\n-        \"\"\"Initialize our default validations.\"\"\"\\n-        self.allowed_schemes = set()\\n-        self.allowed_hosts = set()\\n-        self.allowed_ports = set()\\n-        self.allow_password = True\\n-        self.required_components = {\\n-            \"scheme\": False,\\n-            \"userinfo\": False,\\n-            \"host\": False,\\n-            \"port\": False,\\n-            \"path\": False,\\n-            \"query\": False,\\n-            \"fragment\": False,\\n-        }\\n-        self.validated_components = self.required_components.copy()\\n-\\n-    def allow_schemes(self, *schemes):\\n-        \"\"\"Require the scheme to be one of the provided schemes.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param schemes:\\n-            Schemes, without ``://`` that are allowed.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        for scheme in schemes:\\n-            self.allowed_schemes.add(normalizers.normalize_scheme(scheme))\\n-        return self\\n-\\n-    def allow_hosts(self, *hosts):\\n-        \"\"\"Require the host to be one of the provided hosts.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param hosts:\\n-            Hosts that are allowed.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        for host in hosts:\\n-            self.allowed_hosts.add(normalizers.normalize_host(host))\\n-        return self\\n-\\n-    def allow_ports(self, *ports):\\n-        \"\"\"Require the port to be one of the provided ports.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param ports:\\n-            Ports that are allowed.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        for port in ports:\\n-            port_int = int(port, base=10)\\n-            if 0 <= port_int <= 65535:\\n-                self.allowed_ports.add(port)\\n-        return self\\n-\\n-    def allow_use_of_password(self):\\n-        \"\"\"Allow passwords to be present in the URI.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        self.allow_password = True\\n-        return self\\n-\\n-    def forbid_use_of_password(self):\\n-        \"\"\"Prevent passwords from being included in the URI.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        self.allow_password = False\\n-        return self\\n-\\n-    def check_validity_of(self, *components):\\n-        \"\"\"Check the validity of the components provided.\\n-\\n-        This can be specified repeatedly.\\n-\\n-        .. versionadded:: 1.1\\n-\\n-        :param components:\\n-            Names of components from :attr:`Validator.COMPONENT_NAMES`.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        components = [c.lower() for c in components]\\n-        for component in components:\\n-            if component not in self.COMPONENT_NAMES:\\n-                raise ValueError(\\'\"{}\" is not a valid component\\'.format(component))\\n-        self.validated_components.update({component: True for component in components})\\n-        return self\\n-\\n-    def require_presence_of(self, *components):\\n-        \"\"\"Require the components provided.\\n-\\n-        This can be specified repeatedly.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param components:\\n-            Names of components from :attr:`Validator.COMPONENT_NAMES`.\\n-        :returns:\\n-            The validator instance.\\n-        :rtype:\\n-            Validator\\n-        \"\"\"\\n-        components = [c.lower() for c in components]\\n-        for component in components:\\n-            if component not in self.COMPONENT_NAMES:\\n-                raise ValueError(\\'\"{}\" is not a valid component\\'.format(component))\\n-        self.required_components.update({component: True for component in components})\\n-        return self\\n-\\n-    def validate(self, uri):\\n-        \"\"\"Check a URI for conditions specified on this validator.\\n-\\n-        .. versionadded:: 1.0\\n-\\n-        :param uri:\\n-            Parsed URI to validate.\\n-        :type uri:\\n-            rfc3986.uri.URIReference\\n-        :raises MissingComponentError:\\n-            When a required component is missing.\\n-        :raises UnpermittedComponentError:\\n-            When a component is not one of those allowed.\\n-        :raises PasswordForbidden:\\n-            When a password is present in the userinfo component but is\\n-            not permitted by configuration.\\n-        :raises InvalidComponentsError:\\n-            When a component was found to be invalid.\\n-        \"\"\"\\n-        if not self.allow_password:\\n-            check_password(uri)\\n-\\n-        required_components = [\\n-            component\\n-            for component, required in self.required_components.items()\\n-            if required\\n-        ]\\n-        validated_components = [\\n-            component\\n-            for component, required in self.validated_components.items()\\n-            if required\\n-        ]\\n-        if required_components:\\n-            ensure_required_components_exist(uri, required_components)\\n-        if validated_components:\\n-            ensure_components_are_valid(uri, validated_components)\\n-\\n-        ensure_one_of(self.allowed_schemes, uri, \"scheme\")\\n-        ensure_one_of(self.allowed_hosts, uri, \"host\")\\n-        ensure_one_of(self.allowed_ports, uri, \"port\")\\n-\\n-\\n-def check_password(uri):\\n-    \"\"\"Assert that there is no password present in the uri.\"\"\"\\n-    userinfo = uri.userinfo\\n-    if not userinfo:\\n-        return\\n-    credentials = userinfo.split(\":\", 1)\\n-    if len(credentials) <= 1:\\n-        return\\n-    raise exceptions.PasswordForbidden(uri)\\n-\\n-\\n-def ensure_one_of(allowed_values, uri, attribute):\\n-    \"\"\"Assert that the uri\\'s attribute is one of the allowed values.\"\"\"\\n-    value = getattr(uri, attribute)\\n-    if value is not None and allowed_values and value not in allowed_values:\\n-        raise exceptions.UnpermittedComponentError(attribute, value, allowed_values)\\n-\\n-\\n-def ensure_required_components_exist(uri, required_components):\\n-    \"\"\"Assert that all required components are present in the URI.\"\"\"\\n-    missing_components = sorted(\\n-        [\\n-            component\\n-            for component in required_components\\n-            if getattr(uri, component) is None\\n-        ]\\n-    )\\n-    if missing_components:\\n-        raise exceptions.MissingComponentError(uri, *missing_components)\\n-\\n-\\n-def is_valid(value, matcher, require):\\n-    \"\"\"Determine if a value is valid based on the provided matcher.\\n-\\n-    :param str value:\\n-        Value to validate.\\n-    :param matcher:\\n-        Compiled regular expression to use to validate the value.\\n-    :param require:\\n-        Whether or not the value is required.\\n-    \"\"\"\\n-    if require:\\n-        return value is not None and matcher.match(value)\\n-\\n-    # require is False and value is not None\\n-    return value is None or matcher.match(value)\\n-\\n-\\n-def authority_is_valid(authority, host=None, require=False):\\n-    \"\"\"Determine if the authority string is valid.\\n-\\n-    :param str authority:\\n-        The authority to validate.\\n-    :param str host:\\n-        (optional) The host portion of the authority to validate.\\n-    :param bool require:\\n-        (optional) Specify if authority must not be None.\\n-    :returns:\\n-        ``True`` if valid, ``False`` otherwise\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    validated = is_valid(authority, misc.SUBAUTHORITY_MATCHER, require)\\n-    if validated and host is not None:\\n-        return host_is_valid(host, require)\\n-    return validated\\n-\\n-\\n-def host_is_valid(host, require=False):\\n-    \"\"\"Determine if the host string is valid.\\n-\\n-    :param str host:\\n-        The host to validate.\\n-    :param bool require:\\n-        (optional) Specify if host must not be None.\\n-    :returns:\\n-        ``True`` if valid, ``False`` otherwise\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    validated = is_valid(host, misc.HOST_MATCHER, require)\\n-    if validated and host is not None and misc.IPv4_MATCHER.match(host):\\n-        return valid_ipv4_host_address(host)\\n-    elif validated and host is not None and misc.IPv6_MATCHER.match(host):\\n-        return misc.IPv6_NO_RFC4007_MATCHER.match(host) is not None\\n-    return validated\\n-\\n-\\n-def scheme_is_valid(scheme, require=False):\\n-    \"\"\"Determine if the scheme is valid.\\n-\\n-    :param str scheme:\\n-        The scheme string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a scheme.\\n-    :returns:\\n-        ``True`` if the scheme is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(scheme, misc.SCHEME_MATCHER, require)\\n-\\n-\\n-def path_is_valid(path, require=False):\\n-    \"\"\"Determine if the path component is valid.\\n-\\n-    :param str path:\\n-        The path string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a path.\\n-    :returns:\\n-        ``True`` if the path is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(path, misc.PATH_MATCHER, require)\\n-\\n-\\n-def query_is_valid(query, require=False):\\n-    \"\"\"Determine if the query component is valid.\\n-\\n-    :param str query:\\n-        The query string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a query.\\n-    :returns:\\n-        ``True`` if the query is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(query, misc.QUERY_MATCHER, require)\\n-\\n-\\n-def fragment_is_valid(fragment, require=False):\\n-    \"\"\"Determine if the fragment component is valid.\\n-\\n-    :param str fragment:\\n-        The fragment string to validate.\\n-    :param bool require:\\n-        (optional) Set to ``True`` to require the presence of a fragment.\\n-    :returns:\\n-        ``True`` if the fragment is valid. ``False`` otherwise.\\n-    :rtype:\\n-        bool\\n-    \"\"\"\\n-    return is_valid(fragment, misc.FRAGMENT_MATCHER, require)\\n-\\n-\\n-def valid_ipv4_host_address(host):\\n-    \"\"\"Determine if the given host is a valid IPv4 address.\"\"\"\\n-    # If the host exists, and it might be IPv4, check each byte in the\\n-    # address.\\n-    return all([0 <= int(byte, base=10) <= 255 for byte in host.split(\".\")])\\n-\\n-\\n-_COMPONENT_VALIDATORS = {\\n-    \"scheme\": scheme_is_valid,\\n-    \"path\": path_is_valid,\\n-    \"query\": query_is_valid,\\n-    \"fragment\": fragment_is_valid,\\n-}\\n-\\n-_SUBAUTHORITY_VALIDATORS = set([\"userinfo\", \"host\", \"port\"])\\n-\\n-\\n-def subauthority_component_is_valid(uri, component):\\n-    \"\"\"Determine if the userinfo, host, and port are valid.\"\"\"\\n-    try:\\n-        subauthority_dict = uri.authority_info()\\n-    except exceptions.InvalidAuthority:\\n-        return False\\n-\\n-    # If we can parse the authority into sub-components and we\\'re not\\n-    # validating the port, we can assume it\\'s valid.\\n-    if component == \"host\":\\n-        return host_is_valid(subauthority_dict[\"host\"])\\n-    elif component != \"port\":\\n-        return True\\n-\\n-    try:\\n-        port = int(subauthority_dict[\"port\"])\\n-    except TypeError:\\n-        # If the port wasn\\'t provided it\\'ll be None and int(None) raises a\\n-        # TypeError\\n-        return True\\n-\\n-    return 0 <= port <= 65535\\n-\\n-\\n-def ensure_components_are_valid(uri, validated_components):\\n-    \"\"\"Assert that all components are valid in the URI.\"\"\"\\n-    invalid_components = set([])\\n-    for component in validated_components:\\n-        if component in _SUBAUTHORITY_VALIDATORS:\\n-            if not subauthority_component_is_valid(uri, component):\\n-                invalid_components.add(component)\\n-            # Python\\'s peephole optimizer means that while this continue *is*\\n-            # actually executed, coverage.py cannot detect that. See also,\\n-            # https://bitbucket.org/ned/coveragepy/issues/198/continue-marked-as-not-covered\\n-            continue  # nocov: Python 2.7, 3.3, 3.4\\n-\\n-        validator = _COMPONENT_VALIDATORS[component]\\n-        if not validator(getattr(uri, component)):\\n-            invalid_components.add(component)\\n-\\n-    if invalid_components:\\n-        raise exceptions.InvalidComponentsError(uri, *invalid_components)'], 'file': ['src/urllib3/packages/rfc3986/abnf_regexp.py', 'src/urllib3/packages/rfc3986/normalizers.py', 'src/urllib3/packages/rfc3986/parseresult.py', 'noxfile.py', 'src/urllib3/util/url.py', 'src/urllib3/util/ssl_.py', '_travis/upload_coverage.sh', 'src/urllib3/packages/rfc3986/compat.py', 'src/urllib3/packages/rfc3986/iri.py', 'src/urllib3/packages/rfc3986/misc.py', 'src/urllib3/packages/six.py', 'src/urllib3/connectionpool.py', 'src/urllib3/packages/rfc3986/exceptions.py', 'src/urllib3/packages/rfc3986/_mixin.py', 'src/urllib3/packages/rfc3986/api.py', 'src/urllib3/packages/rfc3986/builder.py', 'src/urllib3/packages/rfc3986/uri.py', 'src/urllib3/packages/rfc3986/validators.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Shell', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('db4061bb-83a6-4331-8469-492ee8498d36'), UUID('4a696d85-be72-4abe-8e37-ddbc0a1fdbfc'), UUID('b7851a07-64e1-4e94-8001-7269619bec0d'), UUID('c3641038-97f3-4ecd-9e0a-ca2bb89432e3'), UUID('36013554-bfe0-42e5-b589-85d2d52067dc'), UUID('400bbab3-c7be-4d04-a16b-6a963afc059a'), UUID('bc5a78a3-0732-4dbb-929c-5e016fd432ff'), UUID('3e2c7474-c146-4ead-9209-e57dee7399cd'), UUID('c3ffe60f-6b6d-4e40-b09f-60c7d03bdd26'), UUID('4d90630d-232f-4c50-abf3-e66e3cfb66b0'), UUID('c62adbd8-c8c4-4b19-bd4c-7c1dd50dc59f'), UUID('27a13a68-ae1d-45bc-a1a4-ac9b375c72d9'), UUID('76ac5fb7-5a67-42da-a4f3-8620a0011b28'), UUID('2929b282-7973-4348-abc6-3b199fc614cb'), UUID('69ce87be-7481-4b52-94e4-480735f05efd'), UUID('612495e9-c4aa-49f4-bbcf-a3c36b8b46a6'), UUID('dd248d1d-e62d-4c4b-90b5-3b7c9c9abefc'), UUID('2b1c83a0-4317-4abd-8c12-4ef7cc325247')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 142:0:     del io\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 142:0:     del io\n",
      "  9%|â–Š         | 154/1800 [00:38<07:42,  3.56it/s]ERROR:src.process_code_changes:Error processing commit 3ba3955887fd6b7d4d646c8b260f21cebf5db852\n",
      "ERROR:src.process_code_changes:{'repo': 'peopledoc/vault-cli', 'vulnerability_id': '2021-43837', 'commit': '3ba3955887fd6b7d4d646c8b260f21cebf5db852', 'commit_source': 'github', 'cwe_id': ['CWE-74', 'CWE-74'], 'patch': ['@@ -5,7 +5,7 @@\\n import re\\n from typing import Dict, Mapping, NoReturn, Optional, Sequence\\n \\n-from vault_cli import client, exceptions\\n+from vault_cli import client, exceptions, types\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -25,7 +25,7 @@ def _normalize(name: str) -> str:\\n     return envvar_name\\n \\n \\n-def _make_env_value(value: client.JSONRecursive) -> str:\\n+def _make_env_value(value: types.JSONValue) -> str:\\n     if isinstance(value, str):\\n         return value\\n     return json.dumps(value)\\n@@ -45,7 +45,7 @@ def exec_command(\\n \\n \\n def get_envvars_for_secrets(\\n-    secrets: Dict[str, client.JSONDictRecursive],\\n+    secrets: Dict[str, types.JSONDict],\\n     path: str,\\n     prefix: str,\\n     omit_single_key: bool = False,', '@@ -2,7 +2,7 @@\\n import json\\n import logging\\n import pathlib\\n-from typing import Dict, Iterable, List, Optional, Set, Tuple, Type, Union, cast\\n+from typing import Dict, Iterable, List, Optional, Tuple, Type, cast\\n \\n import hvac  # type: ignore\\n import jinja2\\n@@ -12,9 +12,6 @@\\n \\n logger = logging.getLogger(__name__)\\n \\n-JSONRecursive = Union[types.JSONValue, utils.RecursiveValue]\\n-JSONDictRecursive = Dict[str, JSONRecursive]\\n-\\n \\n def get_client(**kwargs) -> \"VaultClientBase\":\\n     \"\"\"\\n@@ -49,8 +46,6 @@ def get_client(**kwargs) -> \"VaultClientBase\":\\n         Path to your config file, instead of the default ones\\n     safe_write : bool\\n         If set to True, will keep you from overwriting secrets without force=True\\n-    render : bool\\n-        If set to False, templated secrets will not be rendered\\n \\n     Returns\\n     -------\\n@@ -82,7 +77,6 @@ def __init__(\\n         username: Optional[str] = settings.DEFAULTS.username,\\n         password: Optional[str] = settings.DEFAULTS.password,\\n         safe_write: bool = settings.DEFAULTS.safe_write,\\n-        render: bool = settings.DEFAULTS.render,\\n     ):\\n         self.url = url\\n         self.verify: types.VerifyOrCABundle = verify\\n@@ -94,10 +88,8 @@ def __init__(\\n         self.username = username\\n         self.password = password\\n         self.safe_write = safe_write\\n-        self.render = render\\n         self.cache: Dict[str, types.JSONDict] = {}\\n         self.errors: List[str] = []\\n-        self._currently_fetching: Set[str] = set()\\n \\n     @property\\n     def base_path(self):\\n@@ -166,9 +158,7 @@ def _build_full_path(self, path: str) -> str:\\n             # path relative to base_path\\n             return self.base_path + path\\n \\n-    def _browse_recursive_secrets(\\n-        self, path: str, render: bool = True\\n-    ) -> Iterable[str]:\\n+    def _browse_recursive_secrets(self, path: str) -> Iterable[str]:\\n         \"\"\"\\n         Given a secret or folder path, return the path of all secrets\\n         under it (or the path itself)\\n@@ -194,12 +184,10 @@ def _browse_recursive_secrets(\\n                 yield key_url\\n                 continue\\n \\n-            for sub_path in self._browse_recursive_secrets(key_url, render=render):\\n+            for sub_path in self._browse_recursive_secrets(key_url):\\n                 yield sub_path\\n \\n-    def get_all_secrets(\\n-        self, *paths: str, render: bool = True, flat: bool = False\\n-    ) -> JSONDictRecursive:\\n+    def get_all_secrets(self, *paths: str, flat: bool = False) -> types.JSONDict:\\n         \"\"\"\\n         Takes several paths, return the nested dict of all secrets below\\n         those paths\\n@@ -208,8 +196,6 @@ def get_all_secrets(\\n         ----------\\n         *paths : str\\n             Paths to read recursively\\n-        render : bool, optional\\n-            Whether templated secrets should be rendered, by default True\\n         flat : bool, optional\\n             Whether to return flat structure with full path as keys or nested\\n             structure that looks like a tree\\n@@ -220,10 +206,10 @@ def get_all_secrets(\\n             {\"folder\": {\"subfolder\": {\"secret_key\": \"secret_value\"}}}\\n         \"\"\"\\n \\n-        result: JSONDictRecursive = {}\\n+        result: types.JSONDict = {}\\n \\n         for path in paths:\\n-            path_dict = self.get_secrets(path, render=render)\\n+            path_dict = self.get_secrets(path)\\n             if flat:\\n                 result.update(path_dict)\\n             else:\\n@@ -232,17 +218,15 @@ def get_all_secrets(\\n         return result\\n \\n     def get_secrets(\\n-        self, path: str, render: bool = True, relative: bool = False\\n-    ) -> Dict[str, JSONDictRecursive]:\\n+        self, path: str, relative: bool = False\\n+    ) -> Dict[str, types.JSONDict]:\\n         \"\"\"\\n         Takes a path, return all secrets below this path\\n \\n         Parameters\\n         ----------\\n         path : str\\n             Path to read recursively\\n-        render : bool, optional\\n-            Whether templated secrets should be rendered, by default True\\n         relative: bool, optional\\n             When false (default), the keys of the returned dict are the paths of the secrets\\n             When true, the keys are the relative paths of the secret to `path` (`\"\"` if the secret is directly at path `path`)\\n@@ -254,16 +238,14 @@ def get_secrets(\\n         \"\"\"\\n         path = path.rstrip(\"/\")\\n         try:\\n-            secrets_paths = list(\\n-                self._browse_recursive_secrets(path=path, render=render)\\n-            )\\n+            secrets_paths = list(self._browse_recursive_secrets(path=path))\\n         except exceptions.VaultAPIException:\\n             # If we cannot list secrets, we can\\'t browse them, but there\\'s still\\n             # a chance that the provided path is a single secret that we can\\n             # read\\n             secrets_paths = [path]\\n \\n-        result: Dict[str, JSONDictRecursive] = {}\\n+        result: Dict[str, types.JSONDict] = {}\\n         path_obj = pathlib.Path(path)\\n         for subpath in secrets_paths:\\n             if relative:\\n@@ -275,13 +257,10 @@ def get_secrets(\\n                 key = subpath\\n \\n             try:\\n-                secret = self.get_secret(path=subpath, render=render)\\n-                secret = cast(JSONDictRecursive, secret)\\n+                secret = self.get_secret(path=subpath)\\n+                secret = cast(types.JSONDict, secret)\\n                 result[key] = secret\\n-            except (\\n-                exceptions.VaultAPIException,\\n-                exceptions.VaultRenderTemplateError,\\n-            ) as exc:\\n+            except exceptions.VaultAPIException as exc:\\n                 for message in utils.extract_error_messages(exc):\\n                     logger.error(message)\\n                     self.errors.append(message)\\n@@ -305,9 +284,7 @@ def list_secrets(self, path: str) -> Iterable[str]:\\n         \"\"\"\\n         return self._list_secrets(path=self._build_full_path(path))\\n \\n-    def get_secret(\\n-        self, path: str, key: Optional[str] = None, render: bool = True\\n-    ) -> Union[types.JSONValue, utils.RecursiveValue]:\\n+    def get_secret(self, path: str, key: Optional[str] = None) -> types.JSONValue:\\n         \"\"\"\\n         Retrieve the value of a single secret\\n \\n@@ -319,35 +296,18 @@ def get_secret(\\n         key : str, optional\\n             If set, return only this key\\n \\n-        render : bool, optional\\n-            Whether to render templated secret or not, by default True\\n-\\n         Returns\\n         -------\\n         types.JSONValue\\n             Secret value\\n         \"\"\"\\n         full_path = self._build_full_path(path)\\n-        if full_path in self._currently_fetching:\\n-            return utils.RecursiveValue(path)\\n \\n-        self._currently_fetching.add(full_path)\\n+        assert self.cache is not None\\n         try:\\n-            assert self.cache is not None\\n-            try:\\n-                mapping = self.cache[full_path]\\n-            except KeyError:\\n-                mapping = self.cache[full_path] = self._get_secret(path=full_path)\\n-\\n-            if mapping and render and self.render:\\n-                try:\\n-                    mapping = self._render_template_dict(mapping)\\n-                except exceptions.VaultRenderTemplateError as exc:\\n-                    message = f\\'Error while rendering secret at path \"{path}\"\\'\\n-                    raise exceptions.VaultRenderTemplateError(message) from exc\\n-\\n-        finally:\\n-            self._currently_fetching.remove(full_path)\\n+            mapping = self.cache[full_path]\\n+        except KeyError:\\n+            mapping = self.cache[full_path] = self._get_secret(path=full_path)\\n \\n         if key is not None:\\n             try:\\n@@ -379,7 +339,7 @@ def delete_secret(self, path: str, key: Optional[str] = None) -> None:\\n         else:\\n             # Delete only one attribute\\n             try:\\n-                secret = self.get_secret(path, render=False)\\n+                secret = self.get_secret(path)\\n             except exceptions.VaultSecretNotFound:\\n                 # secret does not exist\\n                 return\\n@@ -401,7 +361,7 @@ def delete_secret(self, path: str, key: Optional[str] = None) -> None:\\n     def delete_all_secrets_iter(self, *paths: str) -> Iterable[str]:\\n         for path in paths:\\n             path = path.rstrip(\"/\")\\n-            secrets_paths = self._browse_recursive_secrets(path=path, render=False)\\n+            secrets_paths = self._browse_recursive_secrets(path=path)\\n             for secret_path in secrets_paths:\\n                 yield secret_path\\n                 self.delete_secret(secret_path)\\n@@ -435,7 +395,7 @@ def copy_secrets_iter(\\n         delete_source: Optional[bool] = False,\\n     ) -> Iterable[Tuple[str, str]]:\\n \\n-        source_secrets = self.get_secrets(path=source, render=False)\\n+        source_secrets = self.get_secrets(path=source)\\n \\n         for old_path, secret in source_secrets.items():\\n             new_path = dest + old_path[len(source) :]\\n@@ -517,41 +477,9 @@ def copy_secrets(\\n             return iterator\\n         return list(iterator)\\n \\n-    template_prefix = \"!template!\"\\n-\\n-    def _render_template_value(self, secret: types.JSONValue) -> types.JSONValue:\\n-\\n-        if isinstance(secret, dict):\\n-            return {k: self._render_template_value(v) for k, v in secret.items()}\\n-        if not isinstance(secret, str):\\n-            return secret\\n-\\n-        if not secret.startswith(self.template_prefix):\\n-            return secret\\n-\\n-        logger.warn(\\n-            \"Templated values are deprecated and will be removed in the \"\\n-            \"following major versions.\",\\n-        )\\n-        return self.render_template(secret[len(self.template_prefix) :])\\n-\\n-    def _render_template_dict(\\n-        self, secrets: Dict[str, types.JSONValue]\\n-    ) -> Dict[str, types.JSONValue]:\\n-        result = {}\\n-        for key, value in secrets.items():\\n-            try:\\n-                result[key] = self._render_template_value(value)\\n-            except exceptions.VaultRenderTemplateError as exc:\\n-                message = f\\'Error while rendering secret value for key \"{key}\"\\'\\n-                raise exceptions.VaultRenderTemplateError(message) from exc\\n-\\n-        return result\\n-\\n     def render_template(\\n         self,\\n         template: str,\\n-        render: bool = True,\\n         search_path: pathlib.Path = pathlib.Path(\".\"),\\n     ) -> str:\\n         \"\"\"\\n@@ -581,7 +509,7 @@ def render_template(\\n \\n         def vault(path):\\n             try:\\n-                return self.get_secret(path, render=render)\\n+                return self.get_secret(path)\\n             except exceptions.VaultException as exc:\\n                 raise exceptions.VaultRenderTemplateError(\\n                     \"Error while rendering template\"\\n@@ -631,7 +559,7 @@ def set_secret(\\n         force = self.get_force(force)\\n \\n         try:\\n-            existing_value = self.get_secret(path=path, render=False)\\n+            existing_value = self.get_secret(path=path)\\n             assert isinstance(existing_value, dict)\\n         except exceptions.VaultSecretNotFound:\\n             pass\\n@@ -675,7 +603,7 @@ def set_secret(\\n         path = path.rstrip(\"/\")\\n         for parent in list(pathlib.PurePath(path).parents)[:-1]:\\n             try:\\n-                self.get_secret(str(parent), render=False)\\n+                self.get_secret(str(parent))\\n             except exceptions.VaultSecretNotFound:\\n                 pass\\n             except exceptions.VaultForbidden:\\n@@ -691,7 +619,7 @@ def set_secret(\\n \\n     def set_secrets(\\n         self,\\n-        secrets: Dict[str, JSONDictRecursive],\\n+        secrets: Dict[str, types.JSONDict],\\n         force: Optional[bool] = None,\\n         update: Optional[bool] = None,\\n     ) -> None:', '@@ -133,8 +133,8 @@ def repr_octal(value: Optional[int]) -> Optional[str]:\\n )\\n @click.option(\\n     \"--render/--no-render\",\\n-    default=settings.DEFAULTS.render,\\n-    help=\"Render templated values\",\\n+    default=False,\\n+    help=\"Deprecated / unused\",\\n )\\n @click.option(\\n     \"--umask\",\\n@@ -174,6 +174,7 @@ def cli(ctx: click.Context, verbose: int, umask: int, **kwargs) -> None:\\n     (including VAULT_CLI_PASSWORD and VAULT_CLI_TOKEN).\\n \\n     \"\"\"\\n+    kwargs.pop(\"render\")\\n     kwargs.pop(\"config_file\")\\n     set_verbosity(verbose)\\n     set_umask(umask)', '@@ -32,17 +32,6 @@ def path_to_nested(dict_obj: Dict) -> Dict:\\n     return dict_obj\\n \\n \\n-class RecursiveValue:\\n-    def __init__(self, name: str):\\n-        self.name = name\\n-\\n-    def __str__(self) -> str:\\n-        return f\\'<recursive value \"{self.name}\">\\'\\n-\\n-    def __getitem__(self, key: str) -> str:\\n-        return str(self)\\n-\\n-\\n def extract_error_messages(exc: BaseException) -> Iterable[str]:\\n     while True:\\n         exc_str = str(exc).strip()', '@@ -27,7 +27,6 @@ class DEFAULTS:\\n     verify = True\\n     ca_bundle = None\\n     safe_write = False\\n-    render = True\\n \\n     @staticmethod\\n     def _as_dict():'], 'file': ['vault_cli/environment.py', 'vault_cli/client.py', 'vault_cli/cli.py', 'vault_cli/utils.py', 'vault_cli/settings.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('7b912c07-46a3-42de-bb7d-6724eee13d37'), UUID('17793125-c5b7-4efd-a618-1711459b8f88'), UUID('8f0fc417-a79c-4298-8fac-5cd8fd24adf2'), UUID('d25dd271-3602-493f-8970-c4fbaeb3b611'), UUID('a0843bfa-a6d5-4ea5-8613-88f97733c7bf')]}\n",
      "ERROR:root:Error in {'repo': 'peopledoc/vault-cli', 'vulnerability_id': '2021-43837', 'commit': '3ba3955887fd6b7d4d646c8b260f21cebf5db852', 'commit_source': 'github', 'cwe_id': ['CWE-74', 'CWE-74'], 'patch': ['@@ -5,7 +5,7 @@\\n import re\\n from typing import Dict, Mapping, NoReturn, Optional, Sequence\\n \\n-from vault_cli import client, exceptions\\n+from vault_cli import client, exceptions, types\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -25,7 +25,7 @@ def _normalize(name: str) -> str:\\n     return envvar_name\\n \\n \\n-def _make_env_value(value: client.JSONRecursive) -> str:\\n+def _make_env_value(value: types.JSONValue) -> str:\\n     if isinstance(value, str):\\n         return value\\n     return json.dumps(value)\\n@@ -45,7 +45,7 @@ def exec_command(\\n \\n \\n def get_envvars_for_secrets(\\n-    secrets: Dict[str, client.JSONDictRecursive],\\n+    secrets: Dict[str, types.JSONDict],\\n     path: str,\\n     prefix: str,\\n     omit_single_key: bool = False,', '@@ -2,7 +2,7 @@\\n import json\\n import logging\\n import pathlib\\n-from typing import Dict, Iterable, List, Optional, Set, Tuple, Type, Union, cast\\n+from typing import Dict, Iterable, List, Optional, Tuple, Type, cast\\n \\n import hvac  # type: ignore\\n import jinja2\\n@@ -12,9 +12,6 @@\\n \\n logger = logging.getLogger(__name__)\\n \\n-JSONRecursive = Union[types.JSONValue, utils.RecursiveValue]\\n-JSONDictRecursive = Dict[str, JSONRecursive]\\n-\\n \\n def get_client(**kwargs) -> \"VaultClientBase\":\\n     \"\"\"\\n@@ -49,8 +46,6 @@ def get_client(**kwargs) -> \"VaultClientBase\":\\n         Path to your config file, instead of the default ones\\n     safe_write : bool\\n         If set to True, will keep you from overwriting secrets without force=True\\n-    render : bool\\n-        If set to False, templated secrets will not be rendered\\n \\n     Returns\\n     -------\\n@@ -82,7 +77,6 @@ def __init__(\\n         username: Optional[str] = settings.DEFAULTS.username,\\n         password: Optional[str] = settings.DEFAULTS.password,\\n         safe_write: bool = settings.DEFAULTS.safe_write,\\n-        render: bool = settings.DEFAULTS.render,\\n     ):\\n         self.url = url\\n         self.verify: types.VerifyOrCABundle = verify\\n@@ -94,10 +88,8 @@ def __init__(\\n         self.username = username\\n         self.password = password\\n         self.safe_write = safe_write\\n-        self.render = render\\n         self.cache: Dict[str, types.JSONDict] = {}\\n         self.errors: List[str] = []\\n-        self._currently_fetching: Set[str] = set()\\n \\n     @property\\n     def base_path(self):\\n@@ -166,9 +158,7 @@ def _build_full_path(self, path: str) -> str:\\n             # path relative to base_path\\n             return self.base_path + path\\n \\n-    def _browse_recursive_secrets(\\n-        self, path: str, render: bool = True\\n-    ) -> Iterable[str]:\\n+    def _browse_recursive_secrets(self, path: str) -> Iterable[str]:\\n         \"\"\"\\n         Given a secret or folder path, return the path of all secrets\\n         under it (or the path itself)\\n@@ -194,12 +184,10 @@ def _browse_recursive_secrets(\\n                 yield key_url\\n                 continue\\n \\n-            for sub_path in self._browse_recursive_secrets(key_url, render=render):\\n+            for sub_path in self._browse_recursive_secrets(key_url):\\n                 yield sub_path\\n \\n-    def get_all_secrets(\\n-        self, *paths: str, render: bool = True, flat: bool = False\\n-    ) -> JSONDictRecursive:\\n+    def get_all_secrets(self, *paths: str, flat: bool = False) -> types.JSONDict:\\n         \"\"\"\\n         Takes several paths, return the nested dict of all secrets below\\n         those paths\\n@@ -208,8 +196,6 @@ def get_all_secrets(\\n         ----------\\n         *paths : str\\n             Paths to read recursively\\n-        render : bool, optional\\n-            Whether templated secrets should be rendered, by default True\\n         flat : bool, optional\\n             Whether to return flat structure with full path as keys or nested\\n             structure that looks like a tree\\n@@ -220,10 +206,10 @@ def get_all_secrets(\\n             {\"folder\": {\"subfolder\": {\"secret_key\": \"secret_value\"}}}\\n         \"\"\"\\n \\n-        result: JSONDictRecursive = {}\\n+        result: types.JSONDict = {}\\n \\n         for path in paths:\\n-            path_dict = self.get_secrets(path, render=render)\\n+            path_dict = self.get_secrets(path)\\n             if flat:\\n                 result.update(path_dict)\\n             else:\\n@@ -232,17 +218,15 @@ def get_all_secrets(\\n         return result\\n \\n     def get_secrets(\\n-        self, path: str, render: bool = True, relative: bool = False\\n-    ) -> Dict[str, JSONDictRecursive]:\\n+        self, path: str, relative: bool = False\\n+    ) -> Dict[str, types.JSONDict]:\\n         \"\"\"\\n         Takes a path, return all secrets below this path\\n \\n         Parameters\\n         ----------\\n         path : str\\n             Path to read recursively\\n-        render : bool, optional\\n-            Whether templated secrets should be rendered, by default True\\n         relative: bool, optional\\n             When false (default), the keys of the returned dict are the paths of the secrets\\n             When true, the keys are the relative paths of the secret to `path` (`\"\"` if the secret is directly at path `path`)\\n@@ -254,16 +238,14 @@ def get_secrets(\\n         \"\"\"\\n         path = path.rstrip(\"/\")\\n         try:\\n-            secrets_paths = list(\\n-                self._browse_recursive_secrets(path=path, render=render)\\n-            )\\n+            secrets_paths = list(self._browse_recursive_secrets(path=path))\\n         except exceptions.VaultAPIException:\\n             # If we cannot list secrets, we can\\'t browse them, but there\\'s still\\n             # a chance that the provided path is a single secret that we can\\n             # read\\n             secrets_paths = [path]\\n \\n-        result: Dict[str, JSONDictRecursive] = {}\\n+        result: Dict[str, types.JSONDict] = {}\\n         path_obj = pathlib.Path(path)\\n         for subpath in secrets_paths:\\n             if relative:\\n@@ -275,13 +257,10 @@ def get_secrets(\\n                 key = subpath\\n \\n             try:\\n-                secret = self.get_secret(path=subpath, render=render)\\n-                secret = cast(JSONDictRecursive, secret)\\n+                secret = self.get_secret(path=subpath)\\n+                secret = cast(types.JSONDict, secret)\\n                 result[key] = secret\\n-            except (\\n-                exceptions.VaultAPIException,\\n-                exceptions.VaultRenderTemplateError,\\n-            ) as exc:\\n+            except exceptions.VaultAPIException as exc:\\n                 for message in utils.extract_error_messages(exc):\\n                     logger.error(message)\\n                     self.errors.append(message)\\n@@ -305,9 +284,7 @@ def list_secrets(self, path: str) -> Iterable[str]:\\n         \"\"\"\\n         return self._list_secrets(path=self._build_full_path(path))\\n \\n-    def get_secret(\\n-        self, path: str, key: Optional[str] = None, render: bool = True\\n-    ) -> Union[types.JSONValue, utils.RecursiveValue]:\\n+    def get_secret(self, path: str, key: Optional[str] = None) -> types.JSONValue:\\n         \"\"\"\\n         Retrieve the value of a single secret\\n \\n@@ -319,35 +296,18 @@ def get_secret(\\n         key : str, optional\\n             If set, return only this key\\n \\n-        render : bool, optional\\n-            Whether to render templated secret or not, by default True\\n-\\n         Returns\\n         -------\\n         types.JSONValue\\n             Secret value\\n         \"\"\"\\n         full_path = self._build_full_path(path)\\n-        if full_path in self._currently_fetching:\\n-            return utils.RecursiveValue(path)\\n \\n-        self._currently_fetching.add(full_path)\\n+        assert self.cache is not None\\n         try:\\n-            assert self.cache is not None\\n-            try:\\n-                mapping = self.cache[full_path]\\n-            except KeyError:\\n-                mapping = self.cache[full_path] = self._get_secret(path=full_path)\\n-\\n-            if mapping and render and self.render:\\n-                try:\\n-                    mapping = self._render_template_dict(mapping)\\n-                except exceptions.VaultRenderTemplateError as exc:\\n-                    message = f\\'Error while rendering secret at path \"{path}\"\\'\\n-                    raise exceptions.VaultRenderTemplateError(message) from exc\\n-\\n-        finally:\\n-            self._currently_fetching.remove(full_path)\\n+            mapping = self.cache[full_path]\\n+        except KeyError:\\n+            mapping = self.cache[full_path] = self._get_secret(path=full_path)\\n \\n         if key is not None:\\n             try:\\n@@ -379,7 +339,7 @@ def delete_secret(self, path: str, key: Optional[str] = None) -> None:\\n         else:\\n             # Delete only one attribute\\n             try:\\n-                secret = self.get_secret(path, render=False)\\n+                secret = self.get_secret(path)\\n             except exceptions.VaultSecretNotFound:\\n                 # secret does not exist\\n                 return\\n@@ -401,7 +361,7 @@ def delete_secret(self, path: str, key: Optional[str] = None) -> None:\\n     def delete_all_secrets_iter(self, *paths: str) -> Iterable[str]:\\n         for path in paths:\\n             path = path.rstrip(\"/\")\\n-            secrets_paths = self._browse_recursive_secrets(path=path, render=False)\\n+            secrets_paths = self._browse_recursive_secrets(path=path)\\n             for secret_path in secrets_paths:\\n                 yield secret_path\\n                 self.delete_secret(secret_path)\\n@@ -435,7 +395,7 @@ def copy_secrets_iter(\\n         delete_source: Optional[bool] = False,\\n     ) -> Iterable[Tuple[str, str]]:\\n \\n-        source_secrets = self.get_secrets(path=source, render=False)\\n+        source_secrets = self.get_secrets(path=source)\\n \\n         for old_path, secret in source_secrets.items():\\n             new_path = dest + old_path[len(source) :]\\n@@ -517,41 +477,9 @@ def copy_secrets(\\n             return iterator\\n         return list(iterator)\\n \\n-    template_prefix = \"!template!\"\\n-\\n-    def _render_template_value(self, secret: types.JSONValue) -> types.JSONValue:\\n-\\n-        if isinstance(secret, dict):\\n-            return {k: self._render_template_value(v) for k, v in secret.items()}\\n-        if not isinstance(secret, str):\\n-            return secret\\n-\\n-        if not secret.startswith(self.template_prefix):\\n-            return secret\\n-\\n-        logger.warn(\\n-            \"Templated values are deprecated and will be removed in the \"\\n-            \"following major versions.\",\\n-        )\\n-        return self.render_template(secret[len(self.template_prefix) :])\\n-\\n-    def _render_template_dict(\\n-        self, secrets: Dict[str, types.JSONValue]\\n-    ) -> Dict[str, types.JSONValue]:\\n-        result = {}\\n-        for key, value in secrets.items():\\n-            try:\\n-                result[key] = self._render_template_value(value)\\n-            except exceptions.VaultRenderTemplateError as exc:\\n-                message = f\\'Error while rendering secret value for key \"{key}\"\\'\\n-                raise exceptions.VaultRenderTemplateError(message) from exc\\n-\\n-        return result\\n-\\n     def render_template(\\n         self,\\n         template: str,\\n-        render: bool = True,\\n         search_path: pathlib.Path = pathlib.Path(\".\"),\\n     ) -> str:\\n         \"\"\"\\n@@ -581,7 +509,7 @@ def render_template(\\n \\n         def vault(path):\\n             try:\\n-                return self.get_secret(path, render=render)\\n+                return self.get_secret(path)\\n             except exceptions.VaultException as exc:\\n                 raise exceptions.VaultRenderTemplateError(\\n                     \"Error while rendering template\"\\n@@ -631,7 +559,7 @@ def set_secret(\\n         force = self.get_force(force)\\n \\n         try:\\n-            existing_value = self.get_secret(path=path, render=False)\\n+            existing_value = self.get_secret(path=path)\\n             assert isinstance(existing_value, dict)\\n         except exceptions.VaultSecretNotFound:\\n             pass\\n@@ -675,7 +603,7 @@ def set_secret(\\n         path = path.rstrip(\"/\")\\n         for parent in list(pathlib.PurePath(path).parents)[:-1]:\\n             try:\\n-                self.get_secret(str(parent), render=False)\\n+                self.get_secret(str(parent))\\n             except exceptions.VaultSecretNotFound:\\n                 pass\\n             except exceptions.VaultForbidden:\\n@@ -691,7 +619,7 @@ def set_secret(\\n \\n     def set_secrets(\\n         self,\\n-        secrets: Dict[str, JSONDictRecursive],\\n+        secrets: Dict[str, types.JSONDict],\\n         force: Optional[bool] = None,\\n         update: Optional[bool] = None,\\n     ) -> None:', '@@ -133,8 +133,8 @@ def repr_octal(value: Optional[int]) -> Optional[str]:\\n )\\n @click.option(\\n     \"--render/--no-render\",\\n-    default=settings.DEFAULTS.render,\\n-    help=\"Render templated values\",\\n+    default=False,\\n+    help=\"Deprecated / unused\",\\n )\\n @click.option(\\n     \"--umask\",\\n@@ -174,6 +174,7 @@ def cli(ctx: click.Context, verbose: int, umask: int, **kwargs) -> None:\\n     (including VAULT_CLI_PASSWORD and VAULT_CLI_TOKEN).\\n \\n     \"\"\"\\n+    kwargs.pop(\"render\")\\n     kwargs.pop(\"config_file\")\\n     set_verbosity(verbose)\\n     set_umask(umask)', '@@ -32,17 +32,6 @@ def path_to_nested(dict_obj: Dict) -> Dict:\\n     return dict_obj\\n \\n \\n-class RecursiveValue:\\n-    def __init__(self, name: str):\\n-        self.name = name\\n-\\n-    def __str__(self) -> str:\\n-        return f\\'<recursive value \"{self.name}\">\\'\\n-\\n-    def __getitem__(self, key: str) -> str:\\n-        return str(self)\\n-\\n-\\n def extract_error_messages(exc: BaseException) -> Iterable[str]:\\n     while True:\\n         exc_str = str(exc).strip()', '@@ -27,7 +27,6 @@ class DEFAULTS:\\n     verify = True\\n     ca_bundle = None\\n     safe_write = False\\n-    render = True\\n \\n     @staticmethod\\n     def _as_dict():'], 'file': ['vault_cli/environment.py', 'vault_cli/client.py', 'vault_cli/cli.py', 'vault_cli/utils.py', 'vault_cli/settings.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('7b912c07-46a3-42de-bb7d-6724eee13d37'), UUID('17793125-c5b7-4efd-a618-1711459b8f88'), UUID('8f0fc417-a79c-4298-8fac-5cd8fd24adf2'), UUID('d25dd271-3602-493f-8970-c4fbaeb3b611'), UUID('a0843bfa-a6d5-4ea5-8613-88f97733c7bf')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 85:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 85:0: <line number missing in source>\n",
      "  9%|â–‰         | 159/1800 [00:40<07:57,  3.44it/s]ERROR:src.process_code_changes:Error processing commit d9805606f88f00c0be56438247605cefde73e14e\n",
      "ERROR:src.process_code_changes:{'repo': 'web2py/web2py', 'vulnerability_id': '2022-33146', 'commit': 'd9805606f88f00c0be56438247605cefde73e14e', 'commit_source': 'github', 'cwe_id': ['CWE-601'], 'patch': ['@@ -0,0 +1,228 @@\\n+from gluon.dal import DAL\\n+from gluon.storage import Storage\\n+from gluon.utils import web2py_uuid\\n+try:\\n+    # web3py\\n+    from gluon.current import current\\n+    from gluon.url import URL\\n+    from gluon.helpers import *\\n+except:\\n+    # web2py\\n+    from gluon import current\\n+    from gluon.html import *\\n+\\n+\\n+\\n+def FormStyleDefault(table, vars, errors, readonly, deletable):\\n+\\n+    form = FORM(TABLE(),_method=\\'POST\\',_action=\\'#\\',_enctype=\\'multipart/form-data\\')\\n+    for field in table:\\n+\\n+        input_id = \\'%s_%s\\' % (field.tablename, field.name)\\n+        value = field.formatter(vars.get(field.name))\\n+        error = errors.get(field.name)\\n+        field_class = field.type.split()[0].replace(\\':\\',\\'-\\')\\n+\\n+        if field.type == \\'blob\\': # never display blobs (mistake?)\\n+            continue\\n+        elif readonly or field.type==\\'id\\':\\n+            if not field.readable:\\n+                continue\\n+            else:\\n+                control = field.represent and field.represent(value) or value or \\'\\'\\n+        elif not field.writable:\\n+            continue\\n+        elif field.widget:\\n+            control = field.widget(table, value)\\n+        elif field.type == \\'text\\':\\n+            control = TEXTAREA(value or \\'\\', _id=input_id,_name=field.name)\\n+        elif field.type == \\'boolean\\':\\n+            control = INPUT(_type=\\'checkbox\\', _id=input_id, _name=field.name,\\n+                            _value=\\'ON\\', _checked = value)\\n+        elif field.type == \\'upload\\':\\n+            control = DIV(INPUT(_type=\\'file\\', _id=input_id, _name=field.name))\\n+            if value:\\n+                control.append(A(\\'download\\',\\n+                                 _href=URL(\\'default\\',\\'download\\',args=value)))\\n+                control.append(INPUT(_type=\\'checkbox\\',_value=\\'ON\\',\\n+                                     _name=\\'_delete_\\'+field.name))\\n+                control.append(\\'(check to remove)\\')\\n+        elif hasattr(field.requires, \\'options\\'):\\n+            multiple = field.type.startswith(\\'list:\\')\\n+            value = value if isinstance(value, list) else [value]\\n+            options = [OPTION(v,_value=k,_selected=(k in value))\\n+                       for k,v in field.requires.options()]\\n+            control = SELECT(*options, _id=input_id, _name=field.name,\\n+                              _multiple=multiple)\\n+        else:\\n+            field_type = \\'password\\' if field.type == \\'password\\' else \\'text\\'\\n+            control = INPUT(_type=field_type, _id=input_id, _name=field.name,\\n+                            _value=value, _class=field_class)\\n+\\n+        form[0].append(TR(TD(LABEL(field.label,_for=input_id)),\\n+                          TD(control,DIV(error,_class=\\'error\\') if error else \\'\\'),\\n+                          TD(field.comment or \\'\\')))\\n+\\n+    td = TD(INPUT(_type=\\'submit\\',_value=\\'Submit\\'))\\n+    if deletable:\\n+        td.append(INPUT(_type=\\'checkbox\\',_value=\\'ON\\',_name=\\'_delete\\'))\\n+        td.append(\\'(check to delete)\\')\\n+    form[0].append(TR(TD(),td,TD()))\\n+    return form\\n+\\n+# ################################################################\\n+# Form object (replaced SQLFORM)\\n+# ################################################################\\n+\\n+class Form(object):\\n+    \"\"\"\\n+    Usage in web2py controller:\\n+\\n+       def index():\\n+           form = Form(db.thing, record=1)\\n+           if form.accepted: ...\\n+           elif form.errors: ...\\n+           else: ...\\n+           return dict(form=form)\\n+\\n+    Arguments:\\n+    - table: a DAL table or a list of fields (equivalent to old SQLFORM.factory)\\n+    - record: a DAL record or record id\\n+    - readonly: set to True to make a readonly form\\n+    - deletable: set to False to disallow deletion of record\\n+    - formstyle: a function that renders the form using helpers (FormStyleDefault)\\n+    - dbio: set to False to prevent any DB write\\n+    - keepvalues: (NOT IMPLEMENTED)\\n+    - formname: the optional name of this form\\n+    - csrf: set to False to disable CRSF protection\\n+    \"\"\"\\n+\\n+    def __init__(self,\\n+                 table,\\n+                 record=None,\\n+                 readonly=False,\\n+                 deletable=True,\\n+                 formstyle=FormStyleDefault,\\n+                 dbio=True,\\n+                 keepvalues=False,\\n+                 formname=False,\\n+                 hidden=None,\\n+                 csrf=True):\\n+\\n+        if isinstance(table, list):\\n+            dbio = False\\n+            # mimic a table from a list of fields without calling define_table\\n+            formname = formname or \\'none\\'\\n+            for field in table: field.tablename = getattr(field,\\'tablename\\',formname)\\n+\\n+        if isinstance(record, (int, long, basestring)):\\n+            record_id = int(str(record))\\n+            self.record = table[record_id]\\n+        else:\\n+            self.record = record\\n+\\n+        self.table = table\\n+        self.readonly = readonly\\n+        self.deletable = deletable and not readonly and self.record\\n+        self.formstyle = formstyle\\n+        self.dbio = dbio\\n+        self.keepvalues = True if keepvalues or self.record else False\\n+        self.csrf = csrf\\n+        self.vars = Storage()\\n+        self.errors = Storage()\\n+        self.submitted = False\\n+        self.deleted = False\\n+        self.accepted = False\\n+        self.cached_helper = False\\n+        self.formname = formname or table._tablename\\n+        self.hidden = hidden\\n+        self.formkey = None\\n+\\n+        request = current.request\\n+\\n+        if readonly or request.method==\\'GET\\':\\n+            if self.record:\\n+                self.vars = self.record\\n+        else:\\n+            post_vars = request.post_vars\\n+            print post_vars\\n+            self.submitted = True\\n+            # check for CSRF\\n+            if csrf and self.formname in (current.session._formkeys or {}):\\n+                self.formkey = current.session._formkeys[self.formname]\\n+            # validate fields\\n+            if not csrf or post_vars._formkey == self.formkey:\\n+                if not post_vars._delete:\\n+                    for field in self.table:\\n+                        if field.writable:\\n+                            value = post_vars.get(field.name)\\n+                            # FIX THIS deal with set_self_id before validate\\n+                            (value, error) = field.validate(value)\\n+                            if field.type == \\'upload\\':\\n+                                delete = post_vars.get(\\'_delete_\\'+field.name)\\n+                                if value is not None and hasattr(value,\\'file\\'):\\n+                                    value = field.store(value.file,\\n+                                                        value.filename,\\n+                                                        field.uploadfolder)\\n+                                elif self.record and not delete:\\n+                                    value = self.record.get(field.name)\\n+                                else:\\n+                                    value = None\\n+                            self.vars[field.name] = value\\n+                            if error:\\n+                                self.errors[field.name] = error\\n+                    if self.record:\\n+                        self.vars.id = self.record.id\\n+                    if not self.errors:\\n+                        self.accepted = True\\n+                        if dbio:\\n+                            self.update_or_insert()\\n+                elif dbio:\\n+                    self.deleted = True\\n+                    self.record.delete_record()\\n+        # store key for future CSRF\\n+        if csrf:\\n+            session = current.session\\n+            if not session._formkeys:\\n+                session._formkeys = {}\\n+            if self.formname not in current.session._formkeys:\\n+                session._formkeys[self.formname] = web2py_uuid()\\n+            self.formkey = session._formkeys[self.formname]\\n+\\n+    def update_or_insert(self):\\n+        if self.record:\\n+            self.record.update_record(**self.vars)\\n+        else:\\n+            # warning, should we really insert if record\\n+            self.vars.id = self.table.insert(**self.vars)\\n+\\n+    def clear():\\n+        self.vars.clear()\\n+        self.errors.clear()\\n+        for field in self.table:\\n+            self.vars[field.name] = field.default\\n+\\n+    def helper(self):\\n+        if not self.cached_helper:\\n+            cached_helper = self.formstyle(self.table,\\n+                                           self.vars,\\n+                                           self.errors,\\n+                                           self.readonly,\\n+                                           self.deletable)\\n+            if self.csrf:\\n+                cached_helper.append(INPUT(_type=\\'hidden\\',_name=\\'_formkey\\',\\n+                                           _value=self.formkey))\\n+            for key in self.hidden or {}:\\n+                cached_helper.append(INPUT(_type=\\'hidden\\',_name=key,\\n+                                           _value=self.hidden[key]))\\n+            self.cached_helper = cached_helper\\n+        return cached_helper\\n+\\n+    def xml(self):\\n+        return self.helper().xml()\\n+\\n+    def __unicode__(self):\\n+        return self.xml()\\n+\\n+    def __str__(self):\\n+        return self.xml().encode(\\'utf8\\')', '@@ -105,6 +105,22 @@ def replace_id(url, form):\\n             return url\\n     return URL(url)\\n \\n+REGEX_OPEN_REDIRECT = re.compile(r\"^(\\\\w+)?[:]?(/$|//.*|/\\\\\\\\.*|[~]/.*)\")\\n+\\n+def prevent_open_redirect(url):\\n+    # Prevent an attacker from adding an arbitrary url after the\\n+    # _next variable in the request.\\n+    host = current.request.env.http_host\\n+    print(host)\\n+    if not url:\\n+        return None\\n+    if REGEX_OPEN_REDIRECT.match(url):\\n+        parts = url.split(\\'/\\')\\n+        if len(parts) > 2 and parts[2] == host:\\n+            return url\\n+        return None\\n+    return url\\n+\\n \\n class Mail(object):\\n     \"\"\"\\n@@ -1752,25 +1768,12 @@ def __init__(self, environment=None, db=None, mailer=True,\\n \\n     def get_vars_next(self):\\n         next = current.request.vars._next\\n-        host = current.request.env.http_host\\n         if isinstance(next, (list, tuple)):\\n             next = next[0]\\n         if next and self.settings.prevent_open_redirect_attacks:\\n-            return self.prevent_open_redirect(next, host)\\n+            return prevent_open_redirect(next)\\n         return next or None\\n \\n-    @staticmethod\\n-    def prevent_open_redirect(next, host):\\n-        # Prevent an attacker from adding an arbitrary url after the\\n-        # _next variable in the request.\\n-        if next:\\n-            parts = next.split(\\'/\\')\\n-            if \\':\\' not in parts[0] and parts[:2] != [\\'\\', \\'\\']:\\n-                return next\\n-            elif len(parts) > 2 and parts[0].endswith(\\':\\') and parts[1:3] == [\\'\\', host]:\\n-                return next\\n-        return None\\n-\\n     def table_cas(self):\\n         return self.db[self.settings.table_cas_name]\\n \\n@@ -4276,8 +4279,8 @@ def update(self,\\n         if request.extension == \\'json\\' and request.vars.json:\\n             request.vars.update(json.loads(request.vars.json))\\n         if next is DEFAULT:\\n-            next = request.get_vars._next \\\\\\n-                or request.post_vars._next \\\\\\n+            next = prevent_open_redirect(request.get_vars._next) \\\\\\n+                or prevent_open_redirect(request.post_vars._next) \\\\\\n                 or self.settings.update_next\\n         if onvalidation is DEFAULT:\\n             onvalidation = self.settings.update_onvalidation\\n@@ -4422,8 +4425,8 @@ def delete(self,\\n         request = current.request\\n         session = current.session\\n         if next is DEFAULT:\\n-            next = request.get_vars._next \\\\\\n-                or request.post_vars._next \\\\\\n+            next = prevent_open_redirect(request.get_vars._next) \\\\\\n+                or prevent_open_redirect(request.post_vars._next) \\\\\\n                 or self.settings.delete_next\\n         if message is DEFAULT:\\n             message = self.messages.record_deleted', '@@ -0,0 +1,62 @@\\n+#!/usr/bin/env python\\n+# -*- coding: utf-8 -*-\\n+\\n+\"\"\"\\n+| This file is part of the web2py Web Framework\\n+| Copyrighted by Massimo Di Pierro <mdipierro@cs.depaul.edu>\\n+| License: LGPLv3 (http://www.gnu.org/licenses/lgpl.html)\\n+\\n+This file specifically includes utilities for security.\\n+--------------------------------------------------------\\n+\"\"\"\\n+\\n+import hashlib\\n+import hmac\\n+from gluon._compat import basestring, pickle, PY2, xrange, to_bytes, to_native\\n+\\n+def pbkdf2_hex(data, salt, iterations=1000, keylen=24, hashfunc=None):\\n+    hashfunc = hashfunc or sha1\\n+    hmac = hashlib.pbkdf2_hmac(hashfunc().name, to_bytes(data),\\n+                               to_bytes(salt), iterations, keylen)\\n+    return binascii.hexlify(hmac)\\n+\\n+\\n+def simple_hash(text, key=\\'\\', salt=\\'\\', digest_alg=\\'md5\\'):\\n+    \"\"\"Generate hash with the given text using the specified digest algorithm.\"\"\"\\n+    text = to_bytes(text)\\n+    key = to_bytes(key)\\n+    salt = to_bytes(salt)\\n+    if not digest_alg:\\n+        raise RuntimeError(\"simple_hash with digest_alg=None\")\\n+    elif not isinstance(digest_alg, str):  # manual approach\\n+        h = digest_alg(text + key + salt)\\n+    elif digest_alg.startswith(\\'pbkdf2\\'):  # latest and coolest!\\n+        iterations, keylen, alg = digest_alg[7:-1].split(\\',\\')\\n+        return to_native(pbkdf2_hex(text, salt, int(iterations),\\n+                                    int(keylen), get_digest(alg)))\\n+    elif key:  # use hmac\\n+        digest_alg = get_digest(digest_alg)\\n+        h = hmac.new(key + salt, text, digest_alg)\\n+    else:  # compatible with third party systems\\n+        h = get_digest(digest_alg)()\\n+        h.update(text + salt)\\n+    return h.hexdigest()\\n+\\n+\\n+def get_digest(value):\\n+    \"\"\"Return a hashlib digest algorithm from a string.\"\"\"\\n+    if isinstance(value, str):\\n+        value = value.lower()\\n+        if value not in (\\'md5\\', \\'sha1\\', \\'sha224\\', \\'sha256\\', \\'sha384\\', \\'sha512\\'):\\n+            raise ValueError(\"Invalid digest algorithm: %s\" % value)\\n+        value = getattr(hashlib, value)\\n+    return value\\n+\\n+DIGEST_ALG_BY_SIZE = {\\n+    128 // 4: \\'md5\\',\\n+    160 // 4: \\'sha1\\',\\n+    224 // 4: \\'sha224\\',\\n+    256 // 4: \\'sha256\\',\\n+    384 // 4: \\'sha384\\',\\n+    512 // 4: \\'sha512\\',\\n+}'], 'file': ['gluon/form.py', 'gluon/tools.py', 'gluon/digest.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('f8b2b68a-3ae4-4089-9526-7b2207a4a709'), UUID('42718097-1c44-49e9-94ac-cf29fd160766'), UUID('925ac960-1bd1-4d8f-8116-0e043b547c40')]}\n",
      "ERROR:root:Error in {'repo': 'web2py/web2py', 'vulnerability_id': '2022-33146', 'commit': 'd9805606f88f00c0be56438247605cefde73e14e', 'commit_source': 'github', 'cwe_id': ['CWE-601'], 'patch': ['@@ -0,0 +1,228 @@\\n+from gluon.dal import DAL\\n+from gluon.storage import Storage\\n+from gluon.utils import web2py_uuid\\n+try:\\n+    # web3py\\n+    from gluon.current import current\\n+    from gluon.url import URL\\n+    from gluon.helpers import *\\n+except:\\n+    # web2py\\n+    from gluon import current\\n+    from gluon.html import *\\n+\\n+\\n+\\n+def FormStyleDefault(table, vars, errors, readonly, deletable):\\n+\\n+    form = FORM(TABLE(),_method=\\'POST\\',_action=\\'#\\',_enctype=\\'multipart/form-data\\')\\n+    for field in table:\\n+\\n+        input_id = \\'%s_%s\\' % (field.tablename, field.name)\\n+        value = field.formatter(vars.get(field.name))\\n+        error = errors.get(field.name)\\n+        field_class = field.type.split()[0].replace(\\':\\',\\'-\\')\\n+\\n+        if field.type == \\'blob\\': # never display blobs (mistake?)\\n+            continue\\n+        elif readonly or field.type==\\'id\\':\\n+            if not field.readable:\\n+                continue\\n+            else:\\n+                control = field.represent and field.represent(value) or value or \\'\\'\\n+        elif not field.writable:\\n+            continue\\n+        elif field.widget:\\n+            control = field.widget(table, value)\\n+        elif field.type == \\'text\\':\\n+            control = TEXTAREA(value or \\'\\', _id=input_id,_name=field.name)\\n+        elif field.type == \\'boolean\\':\\n+            control = INPUT(_type=\\'checkbox\\', _id=input_id, _name=field.name,\\n+                            _value=\\'ON\\', _checked = value)\\n+        elif field.type == \\'upload\\':\\n+            control = DIV(INPUT(_type=\\'file\\', _id=input_id, _name=field.name))\\n+            if value:\\n+                control.append(A(\\'download\\',\\n+                                 _href=URL(\\'default\\',\\'download\\',args=value)))\\n+                control.append(INPUT(_type=\\'checkbox\\',_value=\\'ON\\',\\n+                                     _name=\\'_delete_\\'+field.name))\\n+                control.append(\\'(check to remove)\\')\\n+        elif hasattr(field.requires, \\'options\\'):\\n+            multiple = field.type.startswith(\\'list:\\')\\n+            value = value if isinstance(value, list) else [value]\\n+            options = [OPTION(v,_value=k,_selected=(k in value))\\n+                       for k,v in field.requires.options()]\\n+            control = SELECT(*options, _id=input_id, _name=field.name,\\n+                              _multiple=multiple)\\n+        else:\\n+            field_type = \\'password\\' if field.type == \\'password\\' else \\'text\\'\\n+            control = INPUT(_type=field_type, _id=input_id, _name=field.name,\\n+                            _value=value, _class=field_class)\\n+\\n+        form[0].append(TR(TD(LABEL(field.label,_for=input_id)),\\n+                          TD(control,DIV(error,_class=\\'error\\') if error else \\'\\'),\\n+                          TD(field.comment or \\'\\')))\\n+\\n+    td = TD(INPUT(_type=\\'submit\\',_value=\\'Submit\\'))\\n+    if deletable:\\n+        td.append(INPUT(_type=\\'checkbox\\',_value=\\'ON\\',_name=\\'_delete\\'))\\n+        td.append(\\'(check to delete)\\')\\n+    form[0].append(TR(TD(),td,TD()))\\n+    return form\\n+\\n+# ################################################################\\n+# Form object (replaced SQLFORM)\\n+# ################################################################\\n+\\n+class Form(object):\\n+    \"\"\"\\n+    Usage in web2py controller:\\n+\\n+       def index():\\n+           form = Form(db.thing, record=1)\\n+           if form.accepted: ...\\n+           elif form.errors: ...\\n+           else: ...\\n+           return dict(form=form)\\n+\\n+    Arguments:\\n+    - table: a DAL table or a list of fields (equivalent to old SQLFORM.factory)\\n+    - record: a DAL record or record id\\n+    - readonly: set to True to make a readonly form\\n+    - deletable: set to False to disallow deletion of record\\n+    - formstyle: a function that renders the form using helpers (FormStyleDefault)\\n+    - dbio: set to False to prevent any DB write\\n+    - keepvalues: (NOT IMPLEMENTED)\\n+    - formname: the optional name of this form\\n+    - csrf: set to False to disable CRSF protection\\n+    \"\"\"\\n+\\n+    def __init__(self,\\n+                 table,\\n+                 record=None,\\n+                 readonly=False,\\n+                 deletable=True,\\n+                 formstyle=FormStyleDefault,\\n+                 dbio=True,\\n+                 keepvalues=False,\\n+                 formname=False,\\n+                 hidden=None,\\n+                 csrf=True):\\n+\\n+        if isinstance(table, list):\\n+            dbio = False\\n+            # mimic a table from a list of fields without calling define_table\\n+            formname = formname or \\'none\\'\\n+            for field in table: field.tablename = getattr(field,\\'tablename\\',formname)\\n+\\n+        if isinstance(record, (int, long, basestring)):\\n+            record_id = int(str(record))\\n+            self.record = table[record_id]\\n+        else:\\n+            self.record = record\\n+\\n+        self.table = table\\n+        self.readonly = readonly\\n+        self.deletable = deletable and not readonly and self.record\\n+        self.formstyle = formstyle\\n+        self.dbio = dbio\\n+        self.keepvalues = True if keepvalues or self.record else False\\n+        self.csrf = csrf\\n+        self.vars = Storage()\\n+        self.errors = Storage()\\n+        self.submitted = False\\n+        self.deleted = False\\n+        self.accepted = False\\n+        self.cached_helper = False\\n+        self.formname = formname or table._tablename\\n+        self.hidden = hidden\\n+        self.formkey = None\\n+\\n+        request = current.request\\n+\\n+        if readonly or request.method==\\'GET\\':\\n+            if self.record:\\n+                self.vars = self.record\\n+        else:\\n+            post_vars = request.post_vars\\n+            print post_vars\\n+            self.submitted = True\\n+            # check for CSRF\\n+            if csrf and self.formname in (current.session._formkeys or {}):\\n+                self.formkey = current.session._formkeys[self.formname]\\n+            # validate fields\\n+            if not csrf or post_vars._formkey == self.formkey:\\n+                if not post_vars._delete:\\n+                    for field in self.table:\\n+                        if field.writable:\\n+                            value = post_vars.get(field.name)\\n+                            # FIX THIS deal with set_self_id before validate\\n+                            (value, error) = field.validate(value)\\n+                            if field.type == \\'upload\\':\\n+                                delete = post_vars.get(\\'_delete_\\'+field.name)\\n+                                if value is not None and hasattr(value,\\'file\\'):\\n+                                    value = field.store(value.file,\\n+                                                        value.filename,\\n+                                                        field.uploadfolder)\\n+                                elif self.record and not delete:\\n+                                    value = self.record.get(field.name)\\n+                                else:\\n+                                    value = None\\n+                            self.vars[field.name] = value\\n+                            if error:\\n+                                self.errors[field.name] = error\\n+                    if self.record:\\n+                        self.vars.id = self.record.id\\n+                    if not self.errors:\\n+                        self.accepted = True\\n+                        if dbio:\\n+                            self.update_or_insert()\\n+                elif dbio:\\n+                    self.deleted = True\\n+                    self.record.delete_record()\\n+        # store key for future CSRF\\n+        if csrf:\\n+            session = current.session\\n+            if not session._formkeys:\\n+                session._formkeys = {}\\n+            if self.formname not in current.session._formkeys:\\n+                session._formkeys[self.formname] = web2py_uuid()\\n+            self.formkey = session._formkeys[self.formname]\\n+\\n+    def update_or_insert(self):\\n+        if self.record:\\n+            self.record.update_record(**self.vars)\\n+        else:\\n+            # warning, should we really insert if record\\n+            self.vars.id = self.table.insert(**self.vars)\\n+\\n+    def clear():\\n+        self.vars.clear()\\n+        self.errors.clear()\\n+        for field in self.table:\\n+            self.vars[field.name] = field.default\\n+\\n+    def helper(self):\\n+        if not self.cached_helper:\\n+            cached_helper = self.formstyle(self.table,\\n+                                           self.vars,\\n+                                           self.errors,\\n+                                           self.readonly,\\n+                                           self.deletable)\\n+            if self.csrf:\\n+                cached_helper.append(INPUT(_type=\\'hidden\\',_name=\\'_formkey\\',\\n+                                           _value=self.formkey))\\n+            for key in self.hidden or {}:\\n+                cached_helper.append(INPUT(_type=\\'hidden\\',_name=key,\\n+                                           _value=self.hidden[key]))\\n+            self.cached_helper = cached_helper\\n+        return cached_helper\\n+\\n+    def xml(self):\\n+        return self.helper().xml()\\n+\\n+    def __unicode__(self):\\n+        return self.xml()\\n+\\n+    def __str__(self):\\n+        return self.xml().encode(\\'utf8\\')', '@@ -105,6 +105,22 @@ def replace_id(url, form):\\n             return url\\n     return URL(url)\\n \\n+REGEX_OPEN_REDIRECT = re.compile(r\"^(\\\\w+)?[:]?(/$|//.*|/\\\\\\\\.*|[~]/.*)\")\\n+\\n+def prevent_open_redirect(url):\\n+    # Prevent an attacker from adding an arbitrary url after the\\n+    # _next variable in the request.\\n+    host = current.request.env.http_host\\n+    print(host)\\n+    if not url:\\n+        return None\\n+    if REGEX_OPEN_REDIRECT.match(url):\\n+        parts = url.split(\\'/\\')\\n+        if len(parts) > 2 and parts[2] == host:\\n+            return url\\n+        return None\\n+    return url\\n+\\n \\n class Mail(object):\\n     \"\"\"\\n@@ -1752,25 +1768,12 @@ def __init__(self, environment=None, db=None, mailer=True,\\n \\n     def get_vars_next(self):\\n         next = current.request.vars._next\\n-        host = current.request.env.http_host\\n         if isinstance(next, (list, tuple)):\\n             next = next[0]\\n         if next and self.settings.prevent_open_redirect_attacks:\\n-            return self.prevent_open_redirect(next, host)\\n+            return prevent_open_redirect(next)\\n         return next or None\\n \\n-    @staticmethod\\n-    def prevent_open_redirect(next, host):\\n-        # Prevent an attacker from adding an arbitrary url after the\\n-        # _next variable in the request.\\n-        if next:\\n-            parts = next.split(\\'/\\')\\n-            if \\':\\' not in parts[0] and parts[:2] != [\\'\\', \\'\\']:\\n-                return next\\n-            elif len(parts) > 2 and parts[0].endswith(\\':\\') and parts[1:3] == [\\'\\', host]:\\n-                return next\\n-        return None\\n-\\n     def table_cas(self):\\n         return self.db[self.settings.table_cas_name]\\n \\n@@ -4276,8 +4279,8 @@ def update(self,\\n         if request.extension == \\'json\\' and request.vars.json:\\n             request.vars.update(json.loads(request.vars.json))\\n         if next is DEFAULT:\\n-            next = request.get_vars._next \\\\\\n-                or request.post_vars._next \\\\\\n+            next = prevent_open_redirect(request.get_vars._next) \\\\\\n+                or prevent_open_redirect(request.post_vars._next) \\\\\\n                 or self.settings.update_next\\n         if onvalidation is DEFAULT:\\n             onvalidation = self.settings.update_onvalidation\\n@@ -4422,8 +4425,8 @@ def delete(self,\\n         request = current.request\\n         session = current.session\\n         if next is DEFAULT:\\n-            next = request.get_vars._next \\\\\\n-                or request.post_vars._next \\\\\\n+            next = prevent_open_redirect(request.get_vars._next) \\\\\\n+                or prevent_open_redirect(request.post_vars._next) \\\\\\n                 or self.settings.delete_next\\n         if message is DEFAULT:\\n             message = self.messages.record_deleted', '@@ -0,0 +1,62 @@\\n+#!/usr/bin/env python\\n+# -*- coding: utf-8 -*-\\n+\\n+\"\"\"\\n+| This file is part of the web2py Web Framework\\n+| Copyrighted by Massimo Di Pierro <mdipierro@cs.depaul.edu>\\n+| License: LGPLv3 (http://www.gnu.org/licenses/lgpl.html)\\n+\\n+This file specifically includes utilities for security.\\n+--------------------------------------------------------\\n+\"\"\"\\n+\\n+import hashlib\\n+import hmac\\n+from gluon._compat import basestring, pickle, PY2, xrange, to_bytes, to_native\\n+\\n+def pbkdf2_hex(data, salt, iterations=1000, keylen=24, hashfunc=None):\\n+    hashfunc = hashfunc or sha1\\n+    hmac = hashlib.pbkdf2_hmac(hashfunc().name, to_bytes(data),\\n+                               to_bytes(salt), iterations, keylen)\\n+    return binascii.hexlify(hmac)\\n+\\n+\\n+def simple_hash(text, key=\\'\\', salt=\\'\\', digest_alg=\\'md5\\'):\\n+    \"\"\"Generate hash with the given text using the specified digest algorithm.\"\"\"\\n+    text = to_bytes(text)\\n+    key = to_bytes(key)\\n+    salt = to_bytes(salt)\\n+    if not digest_alg:\\n+        raise RuntimeError(\"simple_hash with digest_alg=None\")\\n+    elif not isinstance(digest_alg, str):  # manual approach\\n+        h = digest_alg(text + key + salt)\\n+    elif digest_alg.startswith(\\'pbkdf2\\'):  # latest and coolest!\\n+        iterations, keylen, alg = digest_alg[7:-1].split(\\',\\')\\n+        return to_native(pbkdf2_hex(text, salt, int(iterations),\\n+                                    int(keylen), get_digest(alg)))\\n+    elif key:  # use hmac\\n+        digest_alg = get_digest(digest_alg)\\n+        h = hmac.new(key + salt, text, digest_alg)\\n+    else:  # compatible with third party systems\\n+        h = get_digest(digest_alg)()\\n+        h.update(text + salt)\\n+    return h.hexdigest()\\n+\\n+\\n+def get_digest(value):\\n+    \"\"\"Return a hashlib digest algorithm from a string.\"\"\"\\n+    if isinstance(value, str):\\n+        value = value.lower()\\n+        if value not in (\\'md5\\', \\'sha1\\', \\'sha224\\', \\'sha256\\', \\'sha384\\', \\'sha512\\'):\\n+            raise ValueError(\"Invalid digest algorithm: %s\" % value)\\n+        value = getattr(hashlib, value)\\n+    return value\\n+\\n+DIGEST_ALG_BY_SIZE = {\\n+    128 // 4: \\'md5\\',\\n+    160 // 4: \\'sha1\\',\\n+    224 // 4: \\'sha224\\',\\n+    256 // 4: \\'sha256\\',\\n+    384 // 4: \\'sha384\\',\\n+    512 // 4: \\'sha512\\',\\n+}'], 'file': ['gluon/form.py', 'gluon/tools.py', 'gluon/digest.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('f8b2b68a-3ae4-4089-9526-7b2207a4a709'), UUID('42718097-1c44-49e9-94ac-cf29fd160766'), UUID('925ac960-1bd1-4d8f-8116-0e043b547c40')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 123:18:             print post_vars\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 143:18:             print post_vars\n",
      "  9%|â–‰         | 161/1800 [00:42<11:37,  2.35it/s]ERROR:src.process_code_changes:Error processing commit 3c82a0296d227cb1be295356df314c11716f4ff6\n",
      "ERROR:src.process_code_changes:{'repo': 'dbt-labs/dbt-core', 'vulnerability_id': '2024-40637', 'commit': '3c82a0296d227cb1be295356df314c11716f4ff6', 'commit_source': 'github', 'cwe_id': ['CWE-74', 'CWE-74', 'CWE-74'], 'patch': ['@@ -96,6 +96,25 @@ class CollectFreshnessReturnSignature(DBTDeprecation):\\n     _event = \"CollectFreshnessReturnSignature\"\\n \\n \\n+class ProjectFlagsMovedDeprecation(DBTDeprecation):\\n+    _name = \"project-flags-moved\"\\n+    _event = \"ProjectFlagsMovedDeprecation\"\\n+\\n+    def show(self, *args, **kwargs) -> None:\\n+        if self.name not in active_deprecations:\\n+            event = self.event(**kwargs)\\n+            # We can\\'t do warn_or_error because the ProjectFlags\\n+            # is where that is set up and we\\'re just reading it.\\n+            dbt.events.functions.fire_event(event)\\n+            self.track_deprecation_warn()\\n+            active_deprecations.add(self.name)\\n+\\n+\\n+class PackageMaterializationOverrideDeprecation(DBTDeprecation):\\n+    _name = \"package-materialization-override\"\\n+    _event = \"PackageMaterializationOverrideDeprecation\"\\n+\\n+\\n def renamed_env_var(old_name: str, new_name: str):\\n     class EnvironmentVariableRenamed(DBTDeprecation):\\n         _name = f\"environment-variable-renamed:{old_name}\"\\n@@ -134,6 +153,8 @@ def warn(name, *args, **kwargs):\\n     ConfigLogPathDeprecation(),\\n     ConfigTargetPathDeprecation(),\\n     CollectFreshnessReturnSignature(),\\n+    ProjectFlagsMovedDeprecation(),\\n+    PackageMaterializationOverrideDeprecation(),\\n ]\\n \\n deprecations: Dict[str, DBTDeprecation] = {d.name: d for d in deprecations_list}', '@@ -16,7 +16,7 @@\\n \\n from dbt.flags import get_flags\\n from dbt import deprecations\\n-from dbt.constants import DEPENDENCIES_FILE_NAME, PACKAGES_FILE_NAME\\n+from dbt.constants import DEPENDENCIES_FILE_NAME, PACKAGES_FILE_NAME, DBT_PROJECT_FILE_NAME\\n from dbt.clients.system import path_exists, resolve_path_from_base, load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n from dbt.contracts.connection import QueryComment\\n@@ -31,12 +31,13 @@\\n from dbt.helper_types import NoValue\\n from dbt.semver import VersionSpecifier, versions_compatible\\n from dbt.version import get_installed_version\\n-from dbt.utils import MultiDict, md5\\n+from dbt.utils import MultiDict, md5, coerce_dict_str\\n from dbt.node_types import NodeType\\n from dbt.config.selectors import SelectorDict\\n from dbt.contracts.project import (\\n     Project as ProjectContract,\\n     SemverString,\\n+    ProjectFlags,\\n )\\n from dbt.contracts.project import PackageConfig, ProjectPackageMetadata\\n from dbt.dataclass_schema import ValidationError\\n@@ -77,8 +78,8 @@\\n \"\"\"\\n \\n MISSING_DBT_PROJECT_ERROR = \"\"\"\\\\\\n-No dbt_project.yml found at expected path {path}\\n-Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml\\n+No {DBT_PROJECT_FILE_NAME} found at expected path {path}\\n+Verify that each entry within packages.yml (and their transitive dependencies) contains a file named {DBT_PROJECT_FILE_NAME}\\n \"\"\"\\n \\n \\n@@ -183,16 +184,20 @@ def value_or(value: Optional[T], default: T) -> T:\\n def load_raw_project(project_root: str) -> Dict[str, Any]:\\n \\n     project_root = os.path.normpath(project_root)\\n-    project_yaml_filepath = os.path.join(project_root, \"dbt_project.yml\")\\n+    project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n \\n     # get the project.yml contents\\n     if not path_exists(project_yaml_filepath):\\n-        raise DbtProjectError(MISSING_DBT_PROJECT_ERROR.format(path=project_yaml_filepath))\\n+        raise DbtProjectError(\\n+            MISSING_DBT_PROJECT_ERROR.format(\\n+                path=project_yaml_filepath, DBT_PROJECT_FILE_NAME=DBT_PROJECT_FILE_NAME\\n+            )\\n+        )\\n \\n     project_dict = _load_yaml(project_yaml_filepath)\\n \\n     if not isinstance(project_dict, dict):\\n-        raise DbtProjectError(\"dbt_project.yml does not parse to a dictionary\")\\n+        raise DbtProjectError(f\"{DBT_PROJECT_FILE_NAME} does not parse to a dictionary\")\\n \\n     return project_dict\\n \\n@@ -307,21 +312,21 @@ def get_rendered(\\n             selectors_dict=rendered_selectors,\\n         )\\n \\n-    # Called by Project.from_project_root (not PartialProject.from_project_root!)\\n+    # Called by Project.from_project_root which first calls PartialProject.from_project_root\\n     def render(self, renderer: DbtProjectYamlRenderer) -> \"Project\":\\n         try:\\n             rendered = self.get_rendered(renderer)\\n             return self.create_project(rendered)\\n         except DbtProjectError as exc:\\n             if exc.path is None:\\n-                exc.path = os.path.join(self.project_root, \"dbt_project.yml\")\\n+                exc.path = os.path.join(self.project_root, DBT_PROJECT_FILE_NAME)\\n             raise\\n \\n     def render_package_metadata(self, renderer: PackageRenderer) -> ProjectPackageMetadata:\\n         packages_data = renderer.render_data(self.packages_dict)\\n         packages_config = package_config_from_data(packages_data)\\n         if not self.project_name:\\n-            raise DbtProjectError(\"Package dbt_project.yml must have a name!\")\\n+            raise DbtProjectError(f\"Package defined in {DBT_PROJECT_FILE_NAME} must have a name!\")\\n         return ProjectPackageMetadata(self.project_name, packages_config.packages)\\n \\n     def check_config_path(\\n@@ -332,7 +337,7 @@ def check_config_path(\\n                 msg = (\\n                     \"{deprecated_path} and {expected_path} cannot both be defined. The \"\\n                     \"`{deprecated_path}` config has been deprecated in favor of `{expected_path}`. \"\\n-                    \"Please update your `dbt_project.yml` configuration to reflect this \"\\n+                    f\"Please update your `{DBT_PROJECT_FILE_NAME}` configuration to reflect this \"\\n                     \"change.\"\\n                 )\\n                 raise DbtProjectError(\\n@@ -404,11 +409,11 @@ def create_project(self, rendered: RenderComponents) -> \"Project\":\\n \\n         docs_paths: List[str] = value_or(cfg.docs_paths, all_source_paths)\\n         asset_paths: List[str] = value_or(cfg.asset_paths, [])\\n-        flags = get_flags()\\n+        global_flags = get_flags()\\n \\n-        flag_target_path = str(flags.TARGET_PATH) if flags.TARGET_PATH else None\\n+        flag_target_path = str(global_flags.TARGET_PATH) if global_flags.TARGET_PATH else None\\n         target_path: str = flag_or(flag_target_path, cfg.target_path, \"target\")\\n-        log_path: str = str(flags.LOG_PATH)\\n+        log_path: str = str(global_flags.LOG_PATH)\\n \\n         clean_targets: List[str] = value_or(cfg.clean_targets, [target_path])\\n         packages_install_path: str = value_or(cfg.packages_install_path, \"dbt_packages\")\\n@@ -545,6 +550,12 @@ def from_project_root(\\n             packages_specified_path,\\n         ) = package_and_project_data_from_root(project_root)\\n         selectors_dict = selector_data_from_root(project_root)\\n+\\n+        if \"flags\" in project_dict:\\n+            # We don\\'t want to include \"flags\" in the Project,\\n+            # it goes in ProjectFlags\\n+            project_dict.pop(\"flags\")\\n+\\n         return cls.from_dicts(\\n             project_root=project_root,\\n             project_dict=project_dict,\\n@@ -681,7 +692,6 @@ def to_project_config(self, with_packages=False):\\n                 \"exposures\": self.exposures,\\n                 \"vars\": self.vars.to_dict(),\\n                 \"require-dbt-version\": [v.to_version_string() for v in self.dbt_version],\\n-                \"config-version\": self.config_version,\\n                 \"restrict-access\": self.restrict_access,\\n                 \"dbt-cloud\": self.dbt_cloud,\\n             }\\n@@ -745,3 +755,52 @@ def get_macro_search_order(self, macro_namespace: str):\\n     def project_target_path(self):\\n         # If target_path is absolute, project_root will not be included\\n         return os.path.join(self.project_root, self.target_path)\\n+\\n+\\n+def read_project_flags(project_dir: str, profiles_dir: str) -> ProjectFlags:\\n+    try:\\n+        project_flags: Dict[str, Any] = {}\\n+        # Read project_flags from dbt_project.yml first\\n+        # Flags are instantiated before the project, so we don\\'t\\n+        # want to throw an error for non-existence of dbt_project.yml here\\n+        # because it breaks things.\\n+        project_root = os.path.normpath(project_dir)\\n+        project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n+        if path_exists(project_yaml_filepath):\\n+            try:\\n+                project_dict = load_raw_project(project_root)\\n+                if \"flags\" in project_dict:\\n+                    project_flags = project_dict.pop(\"flags\")\\n+            except Exception:\\n+                # This is probably a yaml load error.The error will be reported\\n+                # later, when the project loads.\\n+                pass\\n+\\n+        from dbt.config.profile import read_profile\\n+\\n+        profile = read_profile(profiles_dir)\\n+        profile_project_flags: Optional[Dict[str, Any]] = {}\\n+        if profile:\\n+            profile_project_flags = coerce_dict_str(profile.get(\"config\", {}))\\n+\\n+        if project_flags and profile_project_flags:\\n+            raise DbtProjectError(\\n+                f\"Do not specify both \\'config\\' in profiles.yml and \\'flags\\' in {DBT_PROJECT_FILE_NAME}. \"\\n+                \"Using \\'config\\' in profiles.yml is deprecated.\"\\n+            )\\n+\\n+        if profile_project_flags:\\n+            # This can\\'t use WARN_ERROR or WARN_ERROR_OPTIONS because they\\'re in\\n+            # the config that we\\'re loading. Uses special \"warn\" method.\\n+            deprecations.warn(\"project-flags-moved\")\\n+            project_flags = profile_project_flags\\n+\\n+        if project_flags is not None:\\n+            ProjectFlags.validate(project_flags)\\n+            return ProjectFlags.from_dict(project_flags)\\n+    except (DbtProjectError) as exc:\\n+        # We don\\'t want to eat the DbtProjectError for UserConfig to ProjectFlags\\n+        raise exc\\n+    except (DbtRuntimeError, ValidationError):\\n+        pass\\n+    return ProjectFlags()', '@@ -3,6 +3,7 @@\\n from dataclasses import dataclass\\n from importlib import import_module\\n from multiprocessing import get_context\\n+from pathlib import Path\\n from pprint import pformat as pf\\n from typing import Any, Callable, Dict, List, Optional, Set, Union\\n \\n@@ -11,8 +12,8 @@\\n from dbt.cli.exceptions import DbtUsageException\\n from dbt.cli.resolvers import default_log_path, default_project_dir\\n from dbt.cli.types import Command as CliCommand\\n-from dbt.config.profile import read_user_config\\n-from dbt.contracts.project import UserConfig\\n+from dbt.config.project import read_project_flags\\n+from dbt.contracts.project import ProjectFlags\\n from dbt.exceptions import DbtInternalError\\n from dbt.deprecations import renamed_env_var\\n from dbt.helper_types import WarnErrorOptions\\n@@ -24,7 +25,8 @@\\n FLAGS_DEFAULTS = {\\n     \"INDIRECT_SELECTION\": \"eager\",\\n     \"TARGET_PATH\": None,\\n-    # Cli args without user_config or env var option.\\n+    \"WARN_ERROR\": None,\\n+    # Cli args without project_flags or env var option.\\n     \"FULL_REFRESH\": False,\\n     \"STRICT_MODE\": False,\\n     \"STORE_FAILURES\": False,\\n@@ -76,7 +78,7 @@ class Flags:\\n     \"\"\"Primary configuration artifact for running dbt\"\"\"\\n \\n     def __init__(\\n-        self, ctx: Optional[Context] = None, user_config: Optional[UserConfig] = None\\n+        self, ctx: Optional[Context] = None, project_flags: Optional[ProjectFlags] = None\\n     ) -> None:\\n \\n         # Set the default flags.\\n@@ -201,27 +203,40 @@ def _assign_params(\\n                 invoked_subcommand_ctx, params_assigned_from_default, deprecated_env_vars\\n             )\\n \\n-        if not user_config:\\n+        if not project_flags:\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             profiles_dir = getattr(self, \"PROFILES_DIR\", None)\\n-            user_config = read_user_config(profiles_dir) if profiles_dir else None\\n+            if profiles_dir and project_dir:\\n+                project_flags = read_project_flags(project_dir, profiles_dir)\\n+            else:\\n+                project_flags = None\\n \\n         # Add entire invocation command to flags\\n         object.__setattr__(self, \"INVOCATION_COMMAND\", \"dbt \" + \" \".join(sys.argv[1:]))\\n \\n-        # Overwrite default assignments with user config if available.\\n-        if user_config:\\n+        if project_flags:\\n+            # Overwrite default assignments with project flags if available.\\n             param_assigned_from_default_copy = params_assigned_from_default.copy()\\n             for param_assigned_from_default in params_assigned_from_default:\\n-                user_config_param_value = getattr(user_config, param_assigned_from_default, None)\\n-                if user_config_param_value is not None:\\n+                project_flags_param_value = getattr(\\n+                    project_flags, param_assigned_from_default, None\\n+                )\\n+                if project_flags_param_value is not None:\\n                     object.__setattr__(\\n                         self,\\n                         param_assigned_from_default.upper(),\\n-                        convert_config(param_assigned_from_default, user_config_param_value),\\n+                        convert_config(param_assigned_from_default, project_flags_param_value),\\n                     )\\n                     param_assigned_from_default_copy.remove(param_assigned_from_default)\\n             params_assigned_from_default = param_assigned_from_default_copy\\n \\n+            # Add project-level flags that are not available as CLI options / env vars\\n+            for (\\n+                project_level_flag_name,\\n+                project_level_flag_value,\\n+            ) in project_flags.project_only_flags.items():\\n+                object.__setattr__(self, project_level_flag_name.upper(), project_level_flag_value)\\n+\\n         # Set hard coded flags.\\n         object.__setattr__(self, \"WHICH\", invoked_subcommand_name or ctx.info_name)\\n         object.__setattr__(self, \"MP_CONTEXT\", get_context(\"spawn\"))\\n@@ -235,9 +250,11 @@ def _assign_params(\\n         # Starting in v1.5, if `log-path` is set in `dbt_project.yml`, it will raise a deprecation warning,\\n         # with the possibility of removing it in a future release.\\n         if getattr(self, \"LOG_PATH\", None) is None:\\n-            project_dir = getattr(self, \"PROJECT_DIR\", default_project_dir())\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             version_check = getattr(self, \"VERSION_CHECK\", True)\\n-            object.__setattr__(self, \"LOG_PATH\", default_log_path(project_dir, version_check))\\n+            object.__setattr__(\\n+                self, \"LOG_PATH\", default_log_path(Path(project_dir), version_check)\\n+            )\\n \\n         # Support console DO NOT TRACK initiative.\\n         if os.getenv(\"DO_NOT_TRACK\", \"\").lower() in (\"1\", \"t\", \"true\", \"y\", \"yes\"):', '@@ -1,5 +1,5 @@\\n from dbt.contracts.util import Replaceable, Mergeable, list_str, Identifier\\n-from dbt.contracts.connection import QueryComment, UserConfigContract\\n+from dbt.contracts.connection import QueryComment\\n from dbt.helper_types import NoValue\\n from dbt.dataclass_schema import (\\n     dbtClassMixin,\\n@@ -248,7 +248,7 @@ def validate(cls, data):\\n \\n \\n @dataclass\\n-class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n+class ProjectFlags(ExtensibleDbtClassMixin, Replaceable):\\n     cache_selected_only: Optional[bool] = None\\n     debug: Optional[bool] = None\\n     fail_fast: Optional[bool] = None\\n@@ -260,6 +260,7 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     partial_parse: Optional[bool] = None\\n     populate_cache: Optional[bool] = None\\n     printer_width: Optional[int] = None\\n+    require_explicit_package_overrides_for_builtin_materializations: bool = False\\n     send_anonymous_usage_stats: bool = DEFAULT_SEND_ANONYMOUS_USAGE_STATS\\n     static_parser: Optional[bool] = None\\n     use_colors: Optional[bool] = None\\n@@ -270,12 +271,17 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     warn_error_options: Optional[Dict[str, Union[str, List[str]]]] = None\\n     write_json: Optional[bool] = None\\n \\n+    @property\\n+    def project_only_flags(self) -> Dict[str, Any]:\\n+        return {\\n+            \"require_explicit_package_overrides_for_builtin_materializations\": self.require_explicit_package_overrides_for_builtin_materializations,\\n+        }\\n+\\n \\n @dataclass\\n class ProfileConfig(HyphenatedDbtClassMixin, Replaceable):\\n     profile_name: str = field(metadata={\"preserve_underscore\": True})\\n     target_name: str = field(metadata={\"preserve_underscore\": True})\\n-    user_config: UserConfig = field(metadata={\"preserve_underscore\": True})\\n     threads: int\\n     # TODO: make this a dynamic union of some kind?\\n     credentials: Optional[Dict[str, Any]]', '@@ -797,6 +797,29 @@ def message(self) -> str:\\n         return line_wrap_message(warning_tag(msg))\\n \\n \\n+class ProjectFlagsMovedDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D013\"\\n+\\n+    def message(self) -> str:\\n+        description = (\\n+            \"User config should be moved from the \\'config\\' key in profiles.yml to the \\'flags\\' \"\\n+            \"key in dbt_project.yml.\"\\n+        )\\n+        # Can\\'t use line_wrap_message here because flags.printer_width isn\\'t available yet\\n+        return warning_tag(f\"Deprecated functionality\\\\n\\\\n{description}\")\\n+\\n+\\n+class PackageMaterializationOverrideDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D016\"\\n+\\n+    def message(self) -> str:\\n+        description = f\"Installed package \\'{self.package_name}\\' is overriding the built-in materialization \\'{self.materialization_name}\\'. Overrides of built-in materializations from installed packages will be deprecated in future versions of dbt. Please refer to https://docs.getdbt.com/reference/global-configs/legacy-behaviors#require_explicit_package_overrides_for_builtin_materializations for detailed documentation and suggested workarounds.\"\\n+\\n+        return line_wrap_message(warning_tag(description))\\n+\\n+\\n # =======================================================\\n # I - Project parsing\\n # =======================================================', '@@ -623,7 +623,7 @@ def _connection_exception_retry(fn, max_attempts: int, attempt: int = 0):\\n def args_to_dict(args):\\n     var_args = vars(args).copy()\\n     # update the args with the flags, which could also come from environment\\n-    # variables or user_config\\n+    # variables or project_flags\\n     flag_dict = flags.get_flag_dict()\\n     var_args.update(flag_dict)\\n     dict_args = {}', '@@ -23,7 +23,6 @@\\n )\\n from typing_extensions import Protocol\\n from uuid import UUID\\n-\\n from dbt.contracts.graph.nodes import (\\n     BaseNode,\\n     Documentation,\\n@@ -59,7 +58,7 @@\\n from dbt.events.contextvars import get_node_info\\n from dbt.node_types import NodeType, AccessType\\n from dbt.flags import get_flags, MP_CONTEXT\\n-from dbt import tracking\\n+from dbt import tracking, deprecations\\n import dbt.utils\\n \\n \\n@@ -562,11 +561,29 @@ def __lt__(self, other: object) -> bool:\\n \\n \\n class CandidateList(List[M]):\\n-    def last(self) -> Optional[Macro]:\\n+    def last_candidate(\\n+        self, valid_localities: Optional[List[Locality]] = None\\n+    ) -> Optional[MacroCandidate]:\\n+        \"\"\"\\n+        Obtain the last (highest precedence) MacroCandidate from the CandidateList of any locality in valid_localities.\\n+        If valid_localities is not specified, return the last MacroCandidate of any locality.\\n+        \"\"\"\\n         if not self:\\n             return None\\n         self.sort()\\n-        return self[-1].macro\\n+\\n+        if valid_localities is None:\\n+            return self[-1]\\n+\\n+        for candidate in reversed(self):\\n+            if candidate.locality in valid_localities:\\n+                return candidate\\n+\\n+        return None\\n+\\n+    def last(self) -> Optional[Macro]:\\n+        last_candidate = self.last_candidate()\\n+        return last_candidate.macro if last_candidate is not None else None\\n \\n \\n def _get_locality(macro: Macro, root_project_name: str, internal_packages: Set[str]) -> Locality:\\n@@ -850,7 +867,33 @@ def find_materialization_macro_by_name(\\n                 for specificity, atype in enumerate(self._get_parent_adapter_types(adapter_type))\\n             )\\n         )\\n-        return candidates.last()\\n+        core_candidates = [\\n+            candidate for candidate in candidates if candidate.locality == Locality.Core\\n+        ]\\n+\\n+        materialization_candidate = candidates.last_candidate()\\n+        # If an imported materialization macro was found that also had a core candidate, fire a deprecation\\n+        if (\\n+            materialization_candidate is not None\\n+            and materialization_candidate.locality == Locality.Imported\\n+            and core_candidates\\n+        ):\\n+            # preserve legacy behaviour - allow materialization override\\n+            if (\\n+                get_flags().require_explicit_package_overrides_for_builtin_materializations\\n+                is False\\n+            ):\\n+                deprecations.warn(\\n+                    \"package-materialization-override\",\\n+                    package_name=materialization_candidate.macro.package_name,\\n+                    materialization_name=materialization_name,\\n+                )\\n+            else:\\n+                materialization_candidate = candidates.last_candidate(\\n+                    valid_localities=[Locality.Core, Locality.Root]\\n+                )\\n+\\n+        return materialization_candidate.macro if materialization_candidate else None\\n \\n     def get_resource_fqns(self) -> Mapping[str, PathSet]:\\n         resource_fqns: Dict[str, Set[Tuple[str, ...]]] = {}', '@@ -181,17 +181,9 @@ def __post_serialize__(self, dct):\\n         return dct\\n \\n \\n-class UserConfigContract(Protocol):\\n-    send_anonymous_usage_stats: bool\\n-    use_colors: Optional[bool] = None\\n-    partial_parse: Optional[bool] = None\\n-    printer_width: Optional[int] = None\\n-\\n-\\n class HasCredentials(Protocol):\\n     credentials: Credentials\\n     profile_name: str\\n-    user_config: UserConfigContract\\n     target_name: str\\n     threads: int\\n ', '@@ -20,7 +20,7 @@\\n from dbt.config.project import load_raw_project\\n from dbt.contracts.connection import AdapterRequiredConfig, Credentials, HasCredentials\\n from dbt.contracts.graph.manifest import ManifestMetadata\\n-from dbt.contracts.project import Configuration, UserConfig\\n+from dbt.contracts.project import Configuration\\n from dbt.contracts.relation import ComponentName\\n from dbt.dataclass_schema import ValidationError\\n from dbt.events.functions import warn_or_error\\n@@ -176,7 +176,6 @@ def from_parts(\\n             profile_env_vars=profile.profile_env_vars,\\n             profile_name=profile.profile_name,\\n             target_name=profile.target_name,\\n-            user_config=profile.user_config,\\n             threads=profile.threads,\\n             credentials=profile.credentials,\\n             args=args,\\n@@ -428,7 +427,6 @@ def _connection_keys(self):\\n class UnsetProfile(Profile):\\n     def __init__(self):\\n         self.credentials = UnsetCredentials()\\n-        self.user_config = UserConfig()  # This will be read in _get_rendered_profile\\n         self.profile_name = \"\"\\n         self.target_name = \"\"\\n         self.threads = -1', '@@ -9,6 +9,7 @@\\n     \"https://docs.getdbt.com/docs/package-management#section-specifying-package-versions\"\\n )\\n \\n+DBT_PROJECT_FILE_NAME = \"dbt_project.yml\"\\n PACKAGES_FILE_NAME = \"packages.yml\"\\n DEPENDENCIES_FILE_NAME = \"dependencies.yml\"\\n MANIFEST_FILE_NAME = \"manifest.json\"', '@@ -8,7 +8,7 @@\\n from dbt.clients.system import load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n from dbt.contracts.connection import Credentials, HasCredentials\\n-from dbt.contracts.project import ProfileConfig, UserConfig\\n+from dbt.contracts.project import ProfileConfig\\n from dbt.exceptions import (\\n     CompilationError,\\n     DbtProfileError,\\n@@ -19,7 +19,6 @@\\n )\\n from dbt.events.types import MissingProfileTarget\\n from dbt.events.functions import fire_event\\n-from dbt.utils import coerce_dict_str\\n \\n from .renderer import ProfileRenderer\\n \\n@@ -51,27 +50,13 @@ def read_profile(profiles_dir: str) -> Dict[str, Any]:\\n     return {}\\n \\n \\n-def read_user_config(directory: str) -> UserConfig:\\n-    try:\\n-        profile = read_profile(directory)\\n-        if profile:\\n-            user_config = coerce_dict_str(profile.get(\"config\", {}))\\n-            if user_config is not None:\\n-                UserConfig.validate(user_config)\\n-                return UserConfig.from_dict(user_config)\\n-    except (DbtRuntimeError, ValidationError):\\n-        pass\\n-    return UserConfig()\\n-\\n-\\n # The Profile class is included in RuntimeConfig, so any attribute\\n # additions must also be set where the RuntimeConfig class is created\\n # `init=False` is a workaround for https://bugs.python.org/issue45081\\n @dataclass(init=False)\\n class Profile(HasCredentials):\\n     profile_name: str\\n     target_name: str\\n-    user_config: UserConfig\\n     threads: int\\n     credentials: Credentials\\n     profile_env_vars: Dict[str, Any]\\n@@ -80,7 +65,6 @@ def __init__(\\n         self,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: UserConfig,\\n         threads: int,\\n         credentials: Credentials,\\n     ):\\n@@ -89,7 +73,6 @@ def __init__(\\n         \"\"\"\\n         self.profile_name = profile_name\\n         self.target_name = target_name\\n-        self.user_config = user_config\\n         self.threads = threads\\n         self.credentials = credentials\\n         self.profile_env_vars = {}  # never available on init\\n@@ -106,12 +89,10 @@ def to_profile_info(self, serialize_credentials: bool = False) -> Dict[str, Any]\\n         result = {\\n             \"profile_name\": self.profile_name,\\n             \"target_name\": self.target_name,\\n-            \"user_config\": self.user_config,\\n             \"threads\": self.threads,\\n             \"credentials\": self.credentials,\\n         }\\n         if serialize_credentials:\\n-            result[\"user_config\"] = self.user_config.to_dict(omit_none=True)\\n             result[\"credentials\"] = self.credentials.to_dict(omit_none=True)\\n         return result\\n \\n@@ -124,7 +105,6 @@ def to_target_dict(self) -> Dict[str, Any]:\\n                 \"name\": self.target_name,\\n                 \"target_name\": self.target_name,\\n                 \"profile_name\": self.profile_name,\\n-                \"config\": self.user_config.to_dict(omit_none=True),\\n             }\\n         )\\n         return target\\n@@ -246,7 +226,6 @@ def from_credentials(\\n         threads: int,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n     ) -> \"Profile\":\\n         \"\"\"Create a profile from an existing set of Credentials and the\\n         remaining information.\\n@@ -255,20 +234,13 @@ def from_credentials(\\n         :param threads: The number of threads to use for connections.\\n         :param profile_name: The profile name used for this profile.\\n         :param target_name: The target name used for this profile.\\n-        :param user_config: The user-level config block from the\\n-            raw profiles, if specified.\\n         :raises DbtProfileError: If the profile is invalid.\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        if user_config is None:\\n-            user_config = {}\\n-        UserConfig.validate(user_config)\\n-        user_config_obj: UserConfig = UserConfig.from_dict(user_config)\\n \\n         profile = cls(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n-            user_config=user_config_obj,\\n             threads=threads,\\n             credentials=credentials,\\n         )\\n@@ -316,7 +288,6 @@ def from_raw_profile_info(\\n         raw_profile: Dict[str, Any],\\n         profile_name: str,\\n         renderer: ProfileRenderer,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n         target_override: Optional[str] = None,\\n         threads_override: Optional[int] = None,\\n     ) -> \"Profile\":\\n@@ -328,8 +299,6 @@ def from_raw_profile_info(\\n             disk as yaml and its values rendered with jinja.\\n         :param profile_name: The profile name used.\\n         :param renderer: The config renderer.\\n-        :param user_config: The global config for the user, if it\\n-            was present.\\n         :param target_override: The target to use, if provided on\\n             the command line.\\n         :param threads_override: The thread count to use, if\\n@@ -338,9 +307,6 @@ def from_raw_profile_info(\\n             target could not be found\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        # user_config is not rendered.\\n-        if user_config is None:\\n-            user_config = raw_profile.get(\"config\")\\n         # TODO: should it be, and the values coerced to bool?\\n         target_name, profile_data = cls.render_profile(\\n             raw_profile, profile_name, target_override, renderer\\n@@ -361,7 +327,6 @@ def from_raw_profile_info(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n             threads=threads,\\n-            user_config=user_config,\\n         )\\n \\n     @classmethod\\n@@ -396,13 +361,11 @@ def from_raw_profiles(\\n         if not raw_profile:\\n             msg = f\"Profile {profile_name} in profiles.yml is empty\"\\n             raise DbtProfileError(INVALID_PROFILE_MESSAGE.format(error_string=msg))\\n-        user_config = raw_profiles.get(\"config\")\\n \\n         return cls.from_raw_profile_info(\\n             raw_profile=raw_profile,\\n             profile_name=profile_name,\\n             renderer=renderer,\\n-            user_config=user_config,\\n             target_override=target_override,\\n             threads_override=threads_override,\\n         )'], 'file': ['core/dbt/deprecations.py', 'core/dbt/config/project.py', 'core/dbt/cli/flags.py', 'core/dbt/contracts/project.py', 'core/dbt/events/types.py', 'core/dbt/utils.py', 'core/dbt/contracts/graph/manifest.py', 'core/dbt/contracts/connection.py', 'core/dbt/config/runtime.py', 'core/dbt/constants.py', 'core/dbt/config/profile.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('7db1e3bb-8ede-4f4c-b464-ffbcb3307af9'), UUID('0e54ffde-b189-40dc-b4ac-bd69d889de68'), UUID('d24b4a8b-2c5b-4bc2-bfba-6867c2b254d1'), UUID('2454bc88-7bc3-467d-af0d-af2ce2aeb1aa'), UUID('448f26b2-1499-4287-8db4-93e48abdce66'), UUID('c7911f36-2e87-4dce-b8bf-25d1c606cb25'), UUID('96170f49-1fff-43ed-a9d1-496812ea167c'), UUID('48fe51aa-f7ff-4f39-a724-b809ee3edb91'), UUID('a22b78a5-1696-4014-bd3c-08c3cc7a16df'), UUID('d303a02b-9a04-4e93-bd76-4c997f8257bf'), UUID('59c12282-11b8-49c6-9c82-d67dc6ef0a27')]}\n",
      "ERROR:root:Error in {'repo': 'dbt-labs/dbt-core', 'vulnerability_id': '2024-40637', 'commit': '3c82a0296d227cb1be295356df314c11716f4ff6', 'commit_source': 'github', 'cwe_id': ['CWE-74', 'CWE-74', 'CWE-74'], 'patch': ['@@ -96,6 +96,25 @@ class CollectFreshnessReturnSignature(DBTDeprecation):\\n     _event = \"CollectFreshnessReturnSignature\"\\n \\n \\n+class ProjectFlagsMovedDeprecation(DBTDeprecation):\\n+    _name = \"project-flags-moved\"\\n+    _event = \"ProjectFlagsMovedDeprecation\"\\n+\\n+    def show(self, *args, **kwargs) -> None:\\n+        if self.name not in active_deprecations:\\n+            event = self.event(**kwargs)\\n+            # We can\\'t do warn_or_error because the ProjectFlags\\n+            # is where that is set up and we\\'re just reading it.\\n+            dbt.events.functions.fire_event(event)\\n+            self.track_deprecation_warn()\\n+            active_deprecations.add(self.name)\\n+\\n+\\n+class PackageMaterializationOverrideDeprecation(DBTDeprecation):\\n+    _name = \"package-materialization-override\"\\n+    _event = \"PackageMaterializationOverrideDeprecation\"\\n+\\n+\\n def renamed_env_var(old_name: str, new_name: str):\\n     class EnvironmentVariableRenamed(DBTDeprecation):\\n         _name = f\"environment-variable-renamed:{old_name}\"\\n@@ -134,6 +153,8 @@ def warn(name, *args, **kwargs):\\n     ConfigLogPathDeprecation(),\\n     ConfigTargetPathDeprecation(),\\n     CollectFreshnessReturnSignature(),\\n+    ProjectFlagsMovedDeprecation(),\\n+    PackageMaterializationOverrideDeprecation(),\\n ]\\n \\n deprecations: Dict[str, DBTDeprecation] = {d.name: d for d in deprecations_list}', '@@ -16,7 +16,7 @@\\n \\n from dbt.flags import get_flags\\n from dbt import deprecations\\n-from dbt.constants import DEPENDENCIES_FILE_NAME, PACKAGES_FILE_NAME\\n+from dbt.constants import DEPENDENCIES_FILE_NAME, PACKAGES_FILE_NAME, DBT_PROJECT_FILE_NAME\\n from dbt.clients.system import path_exists, resolve_path_from_base, load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n from dbt.contracts.connection import QueryComment\\n@@ -31,12 +31,13 @@\\n from dbt.helper_types import NoValue\\n from dbt.semver import VersionSpecifier, versions_compatible\\n from dbt.version import get_installed_version\\n-from dbt.utils import MultiDict, md5\\n+from dbt.utils import MultiDict, md5, coerce_dict_str\\n from dbt.node_types import NodeType\\n from dbt.config.selectors import SelectorDict\\n from dbt.contracts.project import (\\n     Project as ProjectContract,\\n     SemverString,\\n+    ProjectFlags,\\n )\\n from dbt.contracts.project import PackageConfig, ProjectPackageMetadata\\n from dbt.dataclass_schema import ValidationError\\n@@ -77,8 +78,8 @@\\n \"\"\"\\n \\n MISSING_DBT_PROJECT_ERROR = \"\"\"\\\\\\n-No dbt_project.yml found at expected path {path}\\n-Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml\\n+No {DBT_PROJECT_FILE_NAME} found at expected path {path}\\n+Verify that each entry within packages.yml (and their transitive dependencies) contains a file named {DBT_PROJECT_FILE_NAME}\\n \"\"\"\\n \\n \\n@@ -183,16 +184,20 @@ def value_or(value: Optional[T], default: T) -> T:\\n def load_raw_project(project_root: str) -> Dict[str, Any]:\\n \\n     project_root = os.path.normpath(project_root)\\n-    project_yaml_filepath = os.path.join(project_root, \"dbt_project.yml\")\\n+    project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n \\n     # get the project.yml contents\\n     if not path_exists(project_yaml_filepath):\\n-        raise DbtProjectError(MISSING_DBT_PROJECT_ERROR.format(path=project_yaml_filepath))\\n+        raise DbtProjectError(\\n+            MISSING_DBT_PROJECT_ERROR.format(\\n+                path=project_yaml_filepath, DBT_PROJECT_FILE_NAME=DBT_PROJECT_FILE_NAME\\n+            )\\n+        )\\n \\n     project_dict = _load_yaml(project_yaml_filepath)\\n \\n     if not isinstance(project_dict, dict):\\n-        raise DbtProjectError(\"dbt_project.yml does not parse to a dictionary\")\\n+        raise DbtProjectError(f\"{DBT_PROJECT_FILE_NAME} does not parse to a dictionary\")\\n \\n     return project_dict\\n \\n@@ -307,21 +312,21 @@ def get_rendered(\\n             selectors_dict=rendered_selectors,\\n         )\\n \\n-    # Called by Project.from_project_root (not PartialProject.from_project_root!)\\n+    # Called by Project.from_project_root which first calls PartialProject.from_project_root\\n     def render(self, renderer: DbtProjectYamlRenderer) -> \"Project\":\\n         try:\\n             rendered = self.get_rendered(renderer)\\n             return self.create_project(rendered)\\n         except DbtProjectError as exc:\\n             if exc.path is None:\\n-                exc.path = os.path.join(self.project_root, \"dbt_project.yml\")\\n+                exc.path = os.path.join(self.project_root, DBT_PROJECT_FILE_NAME)\\n             raise\\n \\n     def render_package_metadata(self, renderer: PackageRenderer) -> ProjectPackageMetadata:\\n         packages_data = renderer.render_data(self.packages_dict)\\n         packages_config = package_config_from_data(packages_data)\\n         if not self.project_name:\\n-            raise DbtProjectError(\"Package dbt_project.yml must have a name!\")\\n+            raise DbtProjectError(f\"Package defined in {DBT_PROJECT_FILE_NAME} must have a name!\")\\n         return ProjectPackageMetadata(self.project_name, packages_config.packages)\\n \\n     def check_config_path(\\n@@ -332,7 +337,7 @@ def check_config_path(\\n                 msg = (\\n                     \"{deprecated_path} and {expected_path} cannot both be defined. The \"\\n                     \"`{deprecated_path}` config has been deprecated in favor of `{expected_path}`. \"\\n-                    \"Please update your `dbt_project.yml` configuration to reflect this \"\\n+                    f\"Please update your `{DBT_PROJECT_FILE_NAME}` configuration to reflect this \"\\n                     \"change.\"\\n                 )\\n                 raise DbtProjectError(\\n@@ -404,11 +409,11 @@ def create_project(self, rendered: RenderComponents) -> \"Project\":\\n \\n         docs_paths: List[str] = value_or(cfg.docs_paths, all_source_paths)\\n         asset_paths: List[str] = value_or(cfg.asset_paths, [])\\n-        flags = get_flags()\\n+        global_flags = get_flags()\\n \\n-        flag_target_path = str(flags.TARGET_PATH) if flags.TARGET_PATH else None\\n+        flag_target_path = str(global_flags.TARGET_PATH) if global_flags.TARGET_PATH else None\\n         target_path: str = flag_or(flag_target_path, cfg.target_path, \"target\")\\n-        log_path: str = str(flags.LOG_PATH)\\n+        log_path: str = str(global_flags.LOG_PATH)\\n \\n         clean_targets: List[str] = value_or(cfg.clean_targets, [target_path])\\n         packages_install_path: str = value_or(cfg.packages_install_path, \"dbt_packages\")\\n@@ -545,6 +550,12 @@ def from_project_root(\\n             packages_specified_path,\\n         ) = package_and_project_data_from_root(project_root)\\n         selectors_dict = selector_data_from_root(project_root)\\n+\\n+        if \"flags\" in project_dict:\\n+            # We don\\'t want to include \"flags\" in the Project,\\n+            # it goes in ProjectFlags\\n+            project_dict.pop(\"flags\")\\n+\\n         return cls.from_dicts(\\n             project_root=project_root,\\n             project_dict=project_dict,\\n@@ -681,7 +692,6 @@ def to_project_config(self, with_packages=False):\\n                 \"exposures\": self.exposures,\\n                 \"vars\": self.vars.to_dict(),\\n                 \"require-dbt-version\": [v.to_version_string() for v in self.dbt_version],\\n-                \"config-version\": self.config_version,\\n                 \"restrict-access\": self.restrict_access,\\n                 \"dbt-cloud\": self.dbt_cloud,\\n             }\\n@@ -745,3 +755,52 @@ def get_macro_search_order(self, macro_namespace: str):\\n     def project_target_path(self):\\n         # If target_path is absolute, project_root will not be included\\n         return os.path.join(self.project_root, self.target_path)\\n+\\n+\\n+def read_project_flags(project_dir: str, profiles_dir: str) -> ProjectFlags:\\n+    try:\\n+        project_flags: Dict[str, Any] = {}\\n+        # Read project_flags from dbt_project.yml first\\n+        # Flags are instantiated before the project, so we don\\'t\\n+        # want to throw an error for non-existence of dbt_project.yml here\\n+        # because it breaks things.\\n+        project_root = os.path.normpath(project_dir)\\n+        project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n+        if path_exists(project_yaml_filepath):\\n+            try:\\n+                project_dict = load_raw_project(project_root)\\n+                if \"flags\" in project_dict:\\n+                    project_flags = project_dict.pop(\"flags\")\\n+            except Exception:\\n+                # This is probably a yaml load error.The error will be reported\\n+                # later, when the project loads.\\n+                pass\\n+\\n+        from dbt.config.profile import read_profile\\n+\\n+        profile = read_profile(profiles_dir)\\n+        profile_project_flags: Optional[Dict[str, Any]] = {}\\n+        if profile:\\n+            profile_project_flags = coerce_dict_str(profile.get(\"config\", {}))\\n+\\n+        if project_flags and profile_project_flags:\\n+            raise DbtProjectError(\\n+                f\"Do not specify both \\'config\\' in profiles.yml and \\'flags\\' in {DBT_PROJECT_FILE_NAME}. \"\\n+                \"Using \\'config\\' in profiles.yml is deprecated.\"\\n+            )\\n+\\n+        if profile_project_flags:\\n+            # This can\\'t use WARN_ERROR or WARN_ERROR_OPTIONS because they\\'re in\\n+            # the config that we\\'re loading. Uses special \"warn\" method.\\n+            deprecations.warn(\"project-flags-moved\")\\n+            project_flags = profile_project_flags\\n+\\n+        if project_flags is not None:\\n+            ProjectFlags.validate(project_flags)\\n+            return ProjectFlags.from_dict(project_flags)\\n+    except (DbtProjectError) as exc:\\n+        # We don\\'t want to eat the DbtProjectError for UserConfig to ProjectFlags\\n+        raise exc\\n+    except (DbtRuntimeError, ValidationError):\\n+        pass\\n+    return ProjectFlags()', '@@ -3,6 +3,7 @@\\n from dataclasses import dataclass\\n from importlib import import_module\\n from multiprocessing import get_context\\n+from pathlib import Path\\n from pprint import pformat as pf\\n from typing import Any, Callable, Dict, List, Optional, Set, Union\\n \\n@@ -11,8 +12,8 @@\\n from dbt.cli.exceptions import DbtUsageException\\n from dbt.cli.resolvers import default_log_path, default_project_dir\\n from dbt.cli.types import Command as CliCommand\\n-from dbt.config.profile import read_user_config\\n-from dbt.contracts.project import UserConfig\\n+from dbt.config.project import read_project_flags\\n+from dbt.contracts.project import ProjectFlags\\n from dbt.exceptions import DbtInternalError\\n from dbt.deprecations import renamed_env_var\\n from dbt.helper_types import WarnErrorOptions\\n@@ -24,7 +25,8 @@\\n FLAGS_DEFAULTS = {\\n     \"INDIRECT_SELECTION\": \"eager\",\\n     \"TARGET_PATH\": None,\\n-    # Cli args without user_config or env var option.\\n+    \"WARN_ERROR\": None,\\n+    # Cli args without project_flags or env var option.\\n     \"FULL_REFRESH\": False,\\n     \"STRICT_MODE\": False,\\n     \"STORE_FAILURES\": False,\\n@@ -76,7 +78,7 @@ class Flags:\\n     \"\"\"Primary configuration artifact for running dbt\"\"\"\\n \\n     def __init__(\\n-        self, ctx: Optional[Context] = None, user_config: Optional[UserConfig] = None\\n+        self, ctx: Optional[Context] = None, project_flags: Optional[ProjectFlags] = None\\n     ) -> None:\\n \\n         # Set the default flags.\\n@@ -201,27 +203,40 @@ def _assign_params(\\n                 invoked_subcommand_ctx, params_assigned_from_default, deprecated_env_vars\\n             )\\n \\n-        if not user_config:\\n+        if not project_flags:\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             profiles_dir = getattr(self, \"PROFILES_DIR\", None)\\n-            user_config = read_user_config(profiles_dir) if profiles_dir else None\\n+            if profiles_dir and project_dir:\\n+                project_flags = read_project_flags(project_dir, profiles_dir)\\n+            else:\\n+                project_flags = None\\n \\n         # Add entire invocation command to flags\\n         object.__setattr__(self, \"INVOCATION_COMMAND\", \"dbt \" + \" \".join(sys.argv[1:]))\\n \\n-        # Overwrite default assignments with user config if available.\\n-        if user_config:\\n+        if project_flags:\\n+            # Overwrite default assignments with project flags if available.\\n             param_assigned_from_default_copy = params_assigned_from_default.copy()\\n             for param_assigned_from_default in params_assigned_from_default:\\n-                user_config_param_value = getattr(user_config, param_assigned_from_default, None)\\n-                if user_config_param_value is not None:\\n+                project_flags_param_value = getattr(\\n+                    project_flags, param_assigned_from_default, None\\n+                )\\n+                if project_flags_param_value is not None:\\n                     object.__setattr__(\\n                         self,\\n                         param_assigned_from_default.upper(),\\n-                        convert_config(param_assigned_from_default, user_config_param_value),\\n+                        convert_config(param_assigned_from_default, project_flags_param_value),\\n                     )\\n                     param_assigned_from_default_copy.remove(param_assigned_from_default)\\n             params_assigned_from_default = param_assigned_from_default_copy\\n \\n+            # Add project-level flags that are not available as CLI options / env vars\\n+            for (\\n+                project_level_flag_name,\\n+                project_level_flag_value,\\n+            ) in project_flags.project_only_flags.items():\\n+                object.__setattr__(self, project_level_flag_name.upper(), project_level_flag_value)\\n+\\n         # Set hard coded flags.\\n         object.__setattr__(self, \"WHICH\", invoked_subcommand_name or ctx.info_name)\\n         object.__setattr__(self, \"MP_CONTEXT\", get_context(\"spawn\"))\\n@@ -235,9 +250,11 @@ def _assign_params(\\n         # Starting in v1.5, if `log-path` is set in `dbt_project.yml`, it will raise a deprecation warning,\\n         # with the possibility of removing it in a future release.\\n         if getattr(self, \"LOG_PATH\", None) is None:\\n-            project_dir = getattr(self, \"PROJECT_DIR\", default_project_dir())\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             version_check = getattr(self, \"VERSION_CHECK\", True)\\n-            object.__setattr__(self, \"LOG_PATH\", default_log_path(project_dir, version_check))\\n+            object.__setattr__(\\n+                self, \"LOG_PATH\", default_log_path(Path(project_dir), version_check)\\n+            )\\n \\n         # Support console DO NOT TRACK initiative.\\n         if os.getenv(\"DO_NOT_TRACK\", \"\").lower() in (\"1\", \"t\", \"true\", \"y\", \"yes\"):', '@@ -1,5 +1,5 @@\\n from dbt.contracts.util import Replaceable, Mergeable, list_str, Identifier\\n-from dbt.contracts.connection import QueryComment, UserConfigContract\\n+from dbt.contracts.connection import QueryComment\\n from dbt.helper_types import NoValue\\n from dbt.dataclass_schema import (\\n     dbtClassMixin,\\n@@ -248,7 +248,7 @@ def validate(cls, data):\\n \\n \\n @dataclass\\n-class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n+class ProjectFlags(ExtensibleDbtClassMixin, Replaceable):\\n     cache_selected_only: Optional[bool] = None\\n     debug: Optional[bool] = None\\n     fail_fast: Optional[bool] = None\\n@@ -260,6 +260,7 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     partial_parse: Optional[bool] = None\\n     populate_cache: Optional[bool] = None\\n     printer_width: Optional[int] = None\\n+    require_explicit_package_overrides_for_builtin_materializations: bool = False\\n     send_anonymous_usage_stats: bool = DEFAULT_SEND_ANONYMOUS_USAGE_STATS\\n     static_parser: Optional[bool] = None\\n     use_colors: Optional[bool] = None\\n@@ -270,12 +271,17 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     warn_error_options: Optional[Dict[str, Union[str, List[str]]]] = None\\n     write_json: Optional[bool] = None\\n \\n+    @property\\n+    def project_only_flags(self) -> Dict[str, Any]:\\n+        return {\\n+            \"require_explicit_package_overrides_for_builtin_materializations\": self.require_explicit_package_overrides_for_builtin_materializations,\\n+        }\\n+\\n \\n @dataclass\\n class ProfileConfig(HyphenatedDbtClassMixin, Replaceable):\\n     profile_name: str = field(metadata={\"preserve_underscore\": True})\\n     target_name: str = field(metadata={\"preserve_underscore\": True})\\n-    user_config: UserConfig = field(metadata={\"preserve_underscore\": True})\\n     threads: int\\n     # TODO: make this a dynamic union of some kind?\\n     credentials: Optional[Dict[str, Any]]', '@@ -797,6 +797,29 @@ def message(self) -> str:\\n         return line_wrap_message(warning_tag(msg))\\n \\n \\n+class ProjectFlagsMovedDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D013\"\\n+\\n+    def message(self) -> str:\\n+        description = (\\n+            \"User config should be moved from the \\'config\\' key in profiles.yml to the \\'flags\\' \"\\n+            \"key in dbt_project.yml.\"\\n+        )\\n+        # Can\\'t use line_wrap_message here because flags.printer_width isn\\'t available yet\\n+        return warning_tag(f\"Deprecated functionality\\\\n\\\\n{description}\")\\n+\\n+\\n+class PackageMaterializationOverrideDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D016\"\\n+\\n+    def message(self) -> str:\\n+        description = f\"Installed package \\'{self.package_name}\\' is overriding the built-in materialization \\'{self.materialization_name}\\'. Overrides of built-in materializations from installed packages will be deprecated in future versions of dbt. Please refer to https://docs.getdbt.com/reference/global-configs/legacy-behaviors#require_explicit_package_overrides_for_builtin_materializations for detailed documentation and suggested workarounds.\"\\n+\\n+        return line_wrap_message(warning_tag(description))\\n+\\n+\\n # =======================================================\\n # I - Project parsing\\n # =======================================================', '@@ -623,7 +623,7 @@ def _connection_exception_retry(fn, max_attempts: int, attempt: int = 0):\\n def args_to_dict(args):\\n     var_args = vars(args).copy()\\n     # update the args with the flags, which could also come from environment\\n-    # variables or user_config\\n+    # variables or project_flags\\n     flag_dict = flags.get_flag_dict()\\n     var_args.update(flag_dict)\\n     dict_args = {}', '@@ -23,7 +23,6 @@\\n )\\n from typing_extensions import Protocol\\n from uuid import UUID\\n-\\n from dbt.contracts.graph.nodes import (\\n     BaseNode,\\n     Documentation,\\n@@ -59,7 +58,7 @@\\n from dbt.events.contextvars import get_node_info\\n from dbt.node_types import NodeType, AccessType\\n from dbt.flags import get_flags, MP_CONTEXT\\n-from dbt import tracking\\n+from dbt import tracking, deprecations\\n import dbt.utils\\n \\n \\n@@ -562,11 +561,29 @@ def __lt__(self, other: object) -> bool:\\n \\n \\n class CandidateList(List[M]):\\n-    def last(self) -> Optional[Macro]:\\n+    def last_candidate(\\n+        self, valid_localities: Optional[List[Locality]] = None\\n+    ) -> Optional[MacroCandidate]:\\n+        \"\"\"\\n+        Obtain the last (highest precedence) MacroCandidate from the CandidateList of any locality in valid_localities.\\n+        If valid_localities is not specified, return the last MacroCandidate of any locality.\\n+        \"\"\"\\n         if not self:\\n             return None\\n         self.sort()\\n-        return self[-1].macro\\n+\\n+        if valid_localities is None:\\n+            return self[-1]\\n+\\n+        for candidate in reversed(self):\\n+            if candidate.locality in valid_localities:\\n+                return candidate\\n+\\n+        return None\\n+\\n+    def last(self) -> Optional[Macro]:\\n+        last_candidate = self.last_candidate()\\n+        return last_candidate.macro if last_candidate is not None else None\\n \\n \\n def _get_locality(macro: Macro, root_project_name: str, internal_packages: Set[str]) -> Locality:\\n@@ -850,7 +867,33 @@ def find_materialization_macro_by_name(\\n                 for specificity, atype in enumerate(self._get_parent_adapter_types(adapter_type))\\n             )\\n         )\\n-        return candidates.last()\\n+        core_candidates = [\\n+            candidate for candidate in candidates if candidate.locality == Locality.Core\\n+        ]\\n+\\n+        materialization_candidate = candidates.last_candidate()\\n+        # If an imported materialization macro was found that also had a core candidate, fire a deprecation\\n+        if (\\n+            materialization_candidate is not None\\n+            and materialization_candidate.locality == Locality.Imported\\n+            and core_candidates\\n+        ):\\n+            # preserve legacy behaviour - allow materialization override\\n+            if (\\n+                get_flags().require_explicit_package_overrides_for_builtin_materializations\\n+                is False\\n+            ):\\n+                deprecations.warn(\\n+                    \"package-materialization-override\",\\n+                    package_name=materialization_candidate.macro.package_name,\\n+                    materialization_name=materialization_name,\\n+                )\\n+            else:\\n+                materialization_candidate = candidates.last_candidate(\\n+                    valid_localities=[Locality.Core, Locality.Root]\\n+                )\\n+\\n+        return materialization_candidate.macro if materialization_candidate else None\\n \\n     def get_resource_fqns(self) -> Mapping[str, PathSet]:\\n         resource_fqns: Dict[str, Set[Tuple[str, ...]]] = {}', '@@ -181,17 +181,9 @@ def __post_serialize__(self, dct):\\n         return dct\\n \\n \\n-class UserConfigContract(Protocol):\\n-    send_anonymous_usage_stats: bool\\n-    use_colors: Optional[bool] = None\\n-    partial_parse: Optional[bool] = None\\n-    printer_width: Optional[int] = None\\n-\\n-\\n class HasCredentials(Protocol):\\n     credentials: Credentials\\n     profile_name: str\\n-    user_config: UserConfigContract\\n     target_name: str\\n     threads: int\\n ', '@@ -20,7 +20,7 @@\\n from dbt.config.project import load_raw_project\\n from dbt.contracts.connection import AdapterRequiredConfig, Credentials, HasCredentials\\n from dbt.contracts.graph.manifest import ManifestMetadata\\n-from dbt.contracts.project import Configuration, UserConfig\\n+from dbt.contracts.project import Configuration\\n from dbt.contracts.relation import ComponentName\\n from dbt.dataclass_schema import ValidationError\\n from dbt.events.functions import warn_or_error\\n@@ -176,7 +176,6 @@ def from_parts(\\n             profile_env_vars=profile.profile_env_vars,\\n             profile_name=profile.profile_name,\\n             target_name=profile.target_name,\\n-            user_config=profile.user_config,\\n             threads=profile.threads,\\n             credentials=profile.credentials,\\n             args=args,\\n@@ -428,7 +427,6 @@ def _connection_keys(self):\\n class UnsetProfile(Profile):\\n     def __init__(self):\\n         self.credentials = UnsetCredentials()\\n-        self.user_config = UserConfig()  # This will be read in _get_rendered_profile\\n         self.profile_name = \"\"\\n         self.target_name = \"\"\\n         self.threads = -1', '@@ -9,6 +9,7 @@\\n     \"https://docs.getdbt.com/docs/package-management#section-specifying-package-versions\"\\n )\\n \\n+DBT_PROJECT_FILE_NAME = \"dbt_project.yml\"\\n PACKAGES_FILE_NAME = \"packages.yml\"\\n DEPENDENCIES_FILE_NAME = \"dependencies.yml\"\\n MANIFEST_FILE_NAME = \"manifest.json\"', '@@ -8,7 +8,7 @@\\n from dbt.clients.system import load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n from dbt.contracts.connection import Credentials, HasCredentials\\n-from dbt.contracts.project import ProfileConfig, UserConfig\\n+from dbt.contracts.project import ProfileConfig\\n from dbt.exceptions import (\\n     CompilationError,\\n     DbtProfileError,\\n@@ -19,7 +19,6 @@\\n )\\n from dbt.events.types import MissingProfileTarget\\n from dbt.events.functions import fire_event\\n-from dbt.utils import coerce_dict_str\\n \\n from .renderer import ProfileRenderer\\n \\n@@ -51,27 +50,13 @@ def read_profile(profiles_dir: str) -> Dict[str, Any]:\\n     return {}\\n \\n \\n-def read_user_config(directory: str) -> UserConfig:\\n-    try:\\n-        profile = read_profile(directory)\\n-        if profile:\\n-            user_config = coerce_dict_str(profile.get(\"config\", {}))\\n-            if user_config is not None:\\n-                UserConfig.validate(user_config)\\n-                return UserConfig.from_dict(user_config)\\n-    except (DbtRuntimeError, ValidationError):\\n-        pass\\n-    return UserConfig()\\n-\\n-\\n # The Profile class is included in RuntimeConfig, so any attribute\\n # additions must also be set where the RuntimeConfig class is created\\n # `init=False` is a workaround for https://bugs.python.org/issue45081\\n @dataclass(init=False)\\n class Profile(HasCredentials):\\n     profile_name: str\\n     target_name: str\\n-    user_config: UserConfig\\n     threads: int\\n     credentials: Credentials\\n     profile_env_vars: Dict[str, Any]\\n@@ -80,7 +65,6 @@ def __init__(\\n         self,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: UserConfig,\\n         threads: int,\\n         credentials: Credentials,\\n     ):\\n@@ -89,7 +73,6 @@ def __init__(\\n         \"\"\"\\n         self.profile_name = profile_name\\n         self.target_name = target_name\\n-        self.user_config = user_config\\n         self.threads = threads\\n         self.credentials = credentials\\n         self.profile_env_vars = {}  # never available on init\\n@@ -106,12 +89,10 @@ def to_profile_info(self, serialize_credentials: bool = False) -> Dict[str, Any]\\n         result = {\\n             \"profile_name\": self.profile_name,\\n             \"target_name\": self.target_name,\\n-            \"user_config\": self.user_config,\\n             \"threads\": self.threads,\\n             \"credentials\": self.credentials,\\n         }\\n         if serialize_credentials:\\n-            result[\"user_config\"] = self.user_config.to_dict(omit_none=True)\\n             result[\"credentials\"] = self.credentials.to_dict(omit_none=True)\\n         return result\\n \\n@@ -124,7 +105,6 @@ def to_target_dict(self) -> Dict[str, Any]:\\n                 \"name\": self.target_name,\\n                 \"target_name\": self.target_name,\\n                 \"profile_name\": self.profile_name,\\n-                \"config\": self.user_config.to_dict(omit_none=True),\\n             }\\n         )\\n         return target\\n@@ -246,7 +226,6 @@ def from_credentials(\\n         threads: int,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n     ) -> \"Profile\":\\n         \"\"\"Create a profile from an existing set of Credentials and the\\n         remaining information.\\n@@ -255,20 +234,13 @@ def from_credentials(\\n         :param threads: The number of threads to use for connections.\\n         :param profile_name: The profile name used for this profile.\\n         :param target_name: The target name used for this profile.\\n-        :param user_config: The user-level config block from the\\n-            raw profiles, if specified.\\n         :raises DbtProfileError: If the profile is invalid.\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        if user_config is None:\\n-            user_config = {}\\n-        UserConfig.validate(user_config)\\n-        user_config_obj: UserConfig = UserConfig.from_dict(user_config)\\n \\n         profile = cls(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n-            user_config=user_config_obj,\\n             threads=threads,\\n             credentials=credentials,\\n         )\\n@@ -316,7 +288,6 @@ def from_raw_profile_info(\\n         raw_profile: Dict[str, Any],\\n         profile_name: str,\\n         renderer: ProfileRenderer,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n         target_override: Optional[str] = None,\\n         threads_override: Optional[int] = None,\\n     ) -> \"Profile\":\\n@@ -328,8 +299,6 @@ def from_raw_profile_info(\\n             disk as yaml and its values rendered with jinja.\\n         :param profile_name: The profile name used.\\n         :param renderer: The config renderer.\\n-        :param user_config: The global config for the user, if it\\n-            was present.\\n         :param target_override: The target to use, if provided on\\n             the command line.\\n         :param threads_override: The thread count to use, if\\n@@ -338,9 +307,6 @@ def from_raw_profile_info(\\n             target could not be found\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        # user_config is not rendered.\\n-        if user_config is None:\\n-            user_config = raw_profile.get(\"config\")\\n         # TODO: should it be, and the values coerced to bool?\\n         target_name, profile_data = cls.render_profile(\\n             raw_profile, profile_name, target_override, renderer\\n@@ -361,7 +327,6 @@ def from_raw_profile_info(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n             threads=threads,\\n-            user_config=user_config,\\n         )\\n \\n     @classmethod\\n@@ -396,13 +361,11 @@ def from_raw_profiles(\\n         if not raw_profile:\\n             msg = f\"Profile {profile_name} in profiles.yml is empty\"\\n             raise DbtProfileError(INVALID_PROFILE_MESSAGE.format(error_string=msg))\\n-        user_config = raw_profiles.get(\"config\")\\n \\n         return cls.from_raw_profile_info(\\n             raw_profile=raw_profile,\\n             profile_name=profile_name,\\n             renderer=renderer,\\n-            user_config=user_config,\\n             target_override=target_override,\\n             threads_override=threads_override,\\n         )'], 'file': ['core/dbt/deprecations.py', 'core/dbt/config/project.py', 'core/dbt/cli/flags.py', 'core/dbt/contracts/project.py', 'core/dbt/events/types.py', 'core/dbt/utils.py', 'core/dbt/contracts/graph/manifest.py', 'core/dbt/contracts/connection.py', 'core/dbt/config/runtime.py', 'core/dbt/constants.py', 'core/dbt/config/profile.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('7db1e3bb-8ede-4f4c-b464-ffbcb3307af9'), UUID('0e54ffde-b189-40dc-b4ac-bd69d889de68'), UUID('d24b4a8b-2c5b-4bc2-bfba-6867c2b254d1'), UUID('2454bc88-7bc3-467d-af0d-af2ce2aeb1aa'), UUID('448f26b2-1499-4287-8db4-93e48abdce66'), UUID('c7911f36-2e87-4dce-b8bf-25d1c606cb25'), UUID('96170f49-1fff-43ed-a9d1-496812ea167c'), UUID('48fe51aa-f7ff-4f39-a724-b809ee3edb91'), UUID('a22b78a5-1696-4014-bd3c-08c3cc7a16df'), UUID('d303a02b-9a04-4e93-bd76-4c997f8257bf'), UUID('59c12282-11b8-49c6-9c82-d67dc6ef0a27')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: class ProfileConfig(HyphenatedDbtClassMixin, Replaceable):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: class ProfileConfig(HyphenatedDbtClassMixin, Replaceable):\n",
      " 10%|â–‰         | 172/1800 [00:47<10:48,  2.51it/s]ERROR:src.process_code_changes:Error processing commit 9bc80d782d72881b16e20873dcd0b8314324c70c\n",
      "ERROR:src.process_code_changes:{'repo': 'OctoPrint/OctoPrint', 'vulnerability_id': '2024-51493', 'commit': '9bc80d782d72881b16e20873dcd0b8314324c70c', 'commit_source': 'github', 'cwe_id': ['CWE-620', 'CWE-620'], 'patch': ['@@ -30,6 +30,9 @@ $(function () {\\n         self.access_apikey = ko.observable(undefined);\\n         self.interface_language = ko.observable(undefined);\\n \\n+        self.apiKeyVisible = ko.observable(false);\\n+        self.revealingApiKey = ko.observable(false);\\n+\\n         self.currentUser = ko.observable(undefined);\\n         self.currentUser.subscribe(function (newUser) {\\n             self.access_password(undefined);\\n@@ -57,32 +60,44 @@ $(function () {\\n             return self.access_password() !== self.access_repeatedPassword();\\n         });\\n \\n-        self.show = function (user) {\\n+        self.show = (user) => {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n             if (user === undefined) {\\n                 user = self.loginState.currentUser();\\n             }\\n \\n-            var process = function (user) {\\n-                self.currentUser(user);\\n-                self.userSettingsDialog.modal(\"show\");\\n-            };\\n-\\n             // make sure we have the current user data, see #2534\\n-            OctoPrint.access.users\\n-                .get(user.name)\\n-                .done(function (data) {\\n-                    process(data);\\n+            self.requestData(user.name)\\n+                .done(() => {\\n+                    self.userSettingsDialog.modal(\"show\");\\n                 })\\n-                .fail(function () {\\n+                .fail(() => {\\n                     log.warn(\\n                         \"Could not fetch current user data, proceeding with client side data copy\"\\n                     );\\n-                    process(user);\\n+                    self.fromResponse(user);\\n                 });\\n         };\\n \\n+        self.requestData = (name) => {\\n+            if (name === undefined && self.currentUser() === undefined) return;\\n+            if (name === undefined) {\\n+                name = self.currentUser().name;\\n+            }\\n+\\n+            return OctoPrint.access.users.get(name).done((data) => {\\n+                self.fromResponse(data);\\n+            });\\n+        };\\n+\\n+        self.fromResponse = (data) => {\\n+            self.currentUser(data);\\n+\\n+            // this should only ever return true if we triggered the request through the \"reveal api key\" button\\n+            self.apiKeyVisible(self.revealingApiKey());\\n+        };\\n+\\n         self.save = function () {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n@@ -129,12 +144,14 @@ $(function () {\\n         self.generateApikey = function () {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-            var generate = function () {\\n-                self.users\\n-                    .generateApikey(self.currentUser().name)\\n-                    .done(function (response) {\\n-                        self.access_apikey(response.apikey);\\n-                    });\\n+            const generate = () => {\\n+                self.loginState.reauthenticateIfNecessary(() => {\\n+                    self.users\\n+                        .generateApikey(self.currentUser().name)\\n+                        .done((response) => {\\n+                            self.access_apikey(response.apikey);\\n+                        });\\n+                });\\n             };\\n \\n             if (self.access_apikey()) {\\n@@ -157,14 +174,25 @@ $(function () {\\n                 gettext(\\n                     \"This will delete the API Key. It will cease to to function immediately.\"\\n                 ),\\n-                function () {\\n-                    self.users.deleteApikey(self.currentUser().name).done(function () {\\n-                        self.access_apikey(undefined);\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        self.users.deleteApikey(self.currentUser().name).done(() => {\\n+                            self.access_apikey(undefined);\\n+                        });\\n                     });\\n                 }\\n             );\\n         };\\n \\n+        self.revealApiKey = () => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                self.revealingApiKey(true);\\n+                self.requestData(self.currentUser().name).always(() => {\\n+                    self.revealingApiKey(false);\\n+                });\\n+            });\\n+        };\\n+\\n         self.updateSettings = function (username, settings) {\\n             return OctoPrint.access.users.saveSettings(username, settings);\\n         };\\n@@ -188,6 +216,11 @@ $(function () {\\n                 callViewModels(allViewModels, \"onUserSettingsBeforeSave\");\\n             });\\n         };\\n+\\n+        self.onUserCredentialsOutdated = () => {\\n+            self.apiKeyVisible(false);\\n+            self.requestData();\\n+        };\\n     }\\n \\n     OCTOPRINT_VIEWMODELS.push({', '@@ -11,6 +11,7 @@\\n from octoprint.server import SUCCESS, groupManager, userManager\\n from octoprint.server.api import api, valid_boolean_trues\\n from octoprint.server.util.flask import (\\n+    credentials_checked_recently,\\n     ensure_credentials_checked_recently,\\n     no_firstrun_access,\\n     require_credentials_checked_recently,\\n@@ -132,7 +133,11 @@ def remove_group(key):\\n @no_firstrun_access\\n @Permissions.ADMIN.require(403)\\n def get_users():\\n-    return jsonify(users=list(map(lambda u: u.as_dict(), userManager.get_all_users())))\\n+    users = [u.as_dict() for u in userManager.get_all_users()]\\n+    if not credentials_checked_recently():\\n+        for u in users:\\n+            u[\"apikey\"] = None\\n+    return jsonify(users=users)\\n \\n \\n @api.route(\"/access/users\", methods=[\"POST\"])\\n@@ -178,7 +183,10 @@ def get_user(username):\\n     ):\\n         user = userManager.find_user(username)\\n         if user is not None:\\n-            return jsonify(user)\\n+            user_dict = user.as_dict()\\n+            if not credentials_checked_recently():\\n+                user_dict[\"apikey\"] = None\\n+            return jsonify(user_dict)\\n         else:\\n             abort(404)\\n     else:\\n@@ -323,6 +331,7 @@ def change_settings_for_user(username):\\n \\n @api.route(\"/access/users/<username>/apikey\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n def delete_apikey_for_user(username):\\n     if (\\n         current_user is not None\\n@@ -347,6 +356,7 @@ def delete_apikey_for_user(username):\\n \\n @api.route(\"/access/users/<username>/apikey\", methods=[\"POST\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n def generate_apikey_for_user(username):\\n     if not userManager.enabled:\\n         return jsonify(SUCCESS)', '@@ -32,7 +32,7 @@\\n         <legend>{{ _(\\'API Key\\') }}</legend>\\n         <div class=\"control-group\">\\n             <label class=\"control-label\" for=\"userSettings-access_apikey\">{{ _(\\'Current API Key\\') }}</label>\\n-            <div class=\"controls\">\\n+            <div class=\"controls\" data-bind=\"visible: apiKeyVisible\">\\n                 <div class=\"input-append input-block-level\">\\n                     <input type=\"text\" readonly=\"readonly\" id=\"userSettings-access_apikey\" data-bind=\"value: access_apikey, attr: {placeholder: \\'{{ _(\\'N/A\\')|esq }}\\'}\">\\n                     <a class=\"btn add-on\" title=\"Copy API Key to clipboard\" data-bind=\"click: copyApikey, css: {\\'disabled\\': !access_apikey()}\"><i class=\"fas fa-copy\"></i></a>\\n@@ -41,8 +41,11 @@\\n                 </div>\\n                 <span class=\"help-block\">{{ _(\\'Please note that changes to the API key are applied immediately, without having to \"Confirm\" first.\\') }}</span>\\n             </div>\\n+            <div class=\"controls\" data-bind=\"visible: !apiKeyVisible()\">\\n+                <button class=\"btn\" data-bind=\"click: revealApiKey, enabled: !revealingApiKey(), css: {disabled: revealingApiKey}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: revealingApiKey\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n+            </div>\\n         </div>\\n-        <div class=\"control-group\" data-bind=\"visible: access_apikey\">\\n+        <div class=\"control-group\" data-bind=\"visible: access_apikey() && apiKeyVisible()\">\\n             <label class=\"control-label\">{{ _(\\'QR Code\\') }}</label>\\n             <div class=\"controls\">\\n                 <div data-bind=\"qrcode: {text: access_apikey, size: 150}\"></div>', '@@ -16,6 +16,7 @@\\n from octoprint.server.util.flask import (\\n     credentials_checked_recently,\\n     no_firstrun_access,\\n+    require_credentials_checked_recently,\\n     with_revalidation_checking,\\n )\\n from octoprint.settings import settings, valid_boolean_trues\\n@@ -478,6 +479,7 @@ def setSettings():\\n @api.route(\"/settings/apikey\", methods=[\"POST\"])\\n @no_firstrun_access\\n @Permissions.ADMIN.require(403)\\n+@require_credentials_checked_recently\\n def generateApiKey():\\n     apikey = settings().generateApiKey()\\n     return jsonify(apikey=apikey)\\n@@ -486,6 +488,7 @@ def generateApiKey():\\n @api.route(\"/settings/apikey\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n @Permissions.ADMIN.require(403)\\n+@require_credentials_checked_recently\\n def deleteApiKey():\\n     settings().deleteApiKey()\\n     return NO_CONTENT\\n@@ -581,7 +584,8 @@ def _saveSettings(data):\\n             s.set([\"appearance\", \"color\"], data[\"appearance\"][\"color\"])\\n         if \"colorTransparent\" in data[\"appearance\"]:\\n             s.setBoolean(\\n-                [\"appearance\", \"colorTransparent\"], data[\"appearance\"][\"colorTransparent\"]\\n+                [\"appearance\", \"colorTransparent\"],\\n+                data[\"appearance\"][\"colorTransparent\"],\\n             )\\n         if \"colorIcon\" in data[\"appearance\"]:\\n             s.setBoolean([\"appearance\", \"colorIcon\"], data[\"appearance\"][\"colorIcon\"])\\n@@ -639,7 +643,10 @@ def _saveSettings(data):\\n         if \"ffmpegCommandline\" in data[\"webcam\"]:\\n             commandline = data[\"webcam\"][\"ffmpegCommandline\"]\\n             if not all(\\n-                map(lambda x: \"{\" + x + \"}\" in commandline, (\"ffmpeg\", \"input\", \"output\"))\\n+                map(\\n+                    lambda x: \"{\" + x + \"}\" in commandline,\\n+                    (\"ffmpeg\", \"input\", \"output\"),\\n+                )\\n             ):\\n                 abort(\\n                     400,\\n@@ -855,7 +862,8 @@ def _saveSettings(data):\\n             data[\"serial\"][\"blacklistedBaudrates\"], (list, tuple)\\n         ):\\n             s.set(\\n-                [\"serial\", \"blacklistedBaudrates\"], data[\"serial\"][\"blacklistedBaudrates\"]\\n+                [\"serial\", \"blacklistedBaudrates\"],\\n+                data[\"serial\"][\"blacklistedBaudrates\"],\\n             )\\n         if \"longRunningCommands\" in data[\"serial\"] and isinstance(\\n             data[\"serial\"][\"longRunningCommands\"], (list, tuple)\\n@@ -942,7 +950,8 @@ def _saveSettings(data):\\n             s.setBoolean([\"serial\", \"sdLowerCase\"], data[\"serial\"][\"sdLowerCase\"])\\n         if \"swallowOkAfterResend\" in data[\"serial\"]:\\n             s.setBoolean(\\n-                [\"serial\", \"swallowOkAfterResend\"], data[\"serial\"][\"swallowOkAfterResend\"]\\n+                [\"serial\", \"swallowOkAfterResend\"],\\n+                data[\"serial\"][\"swallowOkAfterResend\"],\\n             )\\n         if \"repetierTargetTemp\" in data[\"serial\"]:\\n             s.setBoolean(\\n@@ -1040,18 +1049,21 @@ def _saveSettings(data):\\n                 data[\"serial\"][\"capEmergencyParser\"],\\n             )\\n         if \"capExtendedM20\" in data[\"serial\"]:\\n-            s.setBoolean(\\n-                [\"serial\", \"capabilities\", \"extended_m20\"],\\n-                data[\"serial\"][\"capExtendedM20\"],\\n-            ),\\n+            (\\n+                s.setBoolean(\\n+                    [\"serial\", \"capabilities\", \"extended_m20\"],\\n+                    data[\"serial\"][\"capExtendedM20\"],\\n+                ),\\n+            )\\n         if \"capLfnWrite\" in data[\"serial\"]:\\n             s.setBoolean(\\n                 [\"serial\", \"capabilities\", \"lfn_write\"],\\n                 data[\"serial\"][\"capLfnWrite\"],\\n             )\\n         if \"resendRatioThreshold\" in data[\"serial\"]:\\n             s.setInt(\\n-                [\"serial\", \"resendRatioThreshold\"], data[\"serial\"][\"resendRatioThreshold\"]\\n+                [\"serial\", \"resendRatioThreshold\"],\\n+                data[\"serial\"][\"resendRatioThreshold\"],\\n             )\\n         if \"resendRatioStart\" in data[\"serial\"]:\\n             s.setInt([\"serial\", \"resendRatioStart\"], data[\"serial\"][\"resendRatioStart\"])', '@@ -43,6 +43,9 @@ $(function () {\\n                 return !self.isCurrentUser(user);\\n             };\\n \\n+            self.apikeysVisible = ko.observable(false);\\n+            self.revealingApikeys = ko.observable(false);\\n+\\n             self.editor = {\\n                 name: ko.observable(undefined),\\n                 groups: ko.observableArray([]),\\n@@ -163,6 +166,9 @@ $(function () {\\n \\n             self.fromResponse = function (response) {\\n                 self.listHelper.updateItems(response.users);\\n+\\n+                // This should only be true if the request was triggered by a click on \"reveal api key\"\\n+                self.apikeysVisible(self.revealingApikeys());\\n             };\\n \\n             self.showAddUserDialog = function () {\\n@@ -355,6 +361,22 @@ $(function () {\\n                 });\\n             };\\n \\n+            self.revealApikeys = () => {\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.revealingApikeys(true);\\n+                    self.requestData().always(() => {\\n+                        self.revealingApikeys(false);\\n+                        if (self.currentUser()) {\\n+                            OctoPrint.access.users\\n+                                .get(self.currentUser().name)\\n+                                .done((data) => {\\n+                                    self.currentUser(data);\\n+                                });\\n+                        }\\n+                    });\\n+                });\\n+            };\\n+\\n             self.copyApikey = function () {\\n                 copyToClipboard(self.editor.apikey());\\n             };\\n@@ -381,6 +403,16 @@ $(function () {\\n                 self.changePasswordDialog = $(\"#settings-usersDialogChangePassword\");\\n             };\\n \\n+            self.onUserCredentialsOutdated = () => {\\n+                self.apikeysVisible(false);\\n+                self.requestData();\\n+                if (self.currentUser()) {\\n+                    OctoPrint.access.users.get(self.currentUser().name).done((data) => {\\n+                        self.currentUser(data);\\n+                    });\\n+                }\\n+            };\\n+\\n             //~~ API calls\\n \\n             self.addUser = function (user) {\\n@@ -980,6 +1012,10 @@ $(function () {\\n             access.permissions.initialize();\\n         };\\n \\n+        access.onUserCredentialsOutdated = () => {\\n+            access.users.onUserCredentialsOutdated();\\n+        };\\n+\\n         access.onUserPermissionsChanged =\\n             access.onUserLoggedIn =\\n             access.onUserLoggedOut =', '@@ -38,7 +38,7 @@\\n                 <button class=\"btn\" data-bind=\"click: revealApiKey, enabled: !revealingApiKey(), css: {disabled: revealingApiKey}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: revealingApiKey\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n             </div>\\n         </div>\\n-        <div class=\"control-group\" data-bind=\"visible: api_key\">\\n+        <div class=\"control-group\" data-bind=\"visible: api_key() && apiKeyVisible()\">\\n             <label class=\"control-label\">{{ _(\\'QR Code\\') }}</label>\\n             <div class=\"controls\">\\n                 <div data-bind=\"qrcode: {text: api_key, size: 180}\"></div>', '@@ -157,14 +157,16 @@\\n                 <legend>API Key</legend>\\n                 <div class=\"control-group\">\\n                     <label class=\"control-label\">{{ _(\\'Current API Key\\') }}</label>\\n-                    <div class=\"controls\">\\n+                    <div class=\"controls\" data-bind=\"visible: $root.access.users.apikeysVisible\">\\n                         <div class=\"input-append input-block-level \">\\n                             <input type=\"text\" readonly=\"readonly\" data-bind=\"value: $root.access.users.editor.apikey, valueUpdate: \\'input\\', attr: {placeholder: \\'{{ _(\\'N/A\\')|esq }}\\'}\">\\n                             <a class=\"btn add-on\" title=\"Copy API Key to clipboard\" data-bind=\"click: $root.access.users.copyApikey, css: {\\'disabled\\': !$root.access.users.editor.apikey()}\"><i class=\"fas fa-copy\"></i></a>\\n                             <a class=\"btn add-on\" title=\"Generate new API Key\" data-bind=\"click: function() { $root.access.users.confirmGenerateApikey(); }\"><i class=\"fas fa-sync\"></i></a>\\n                             <a class=\"btn add-on btn-danger\" title=\"Delete API Key\" data-bind=\"click: function() { $root.access.users.confirmDeleteApikey(); }, css: {\\'disabled\\': !$root.access.users.editor.apikey()}\"><i class=\"far fa-trash-alt\"></i></a>\\n                         </div>\\n-\\n+                    </div>\\n+                    <div class=\"controls\" data-bind=\"visible: !$root.access.users.apikeysVisible()\">\\n+                        <button class=\"btn\" data-bind=\"click: $root.access.users.revealApikeys, enabled: !$root.access.users.revealingApikeys(), css: {disabled: $root.access.users.revealingApikeys}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: $root.access.users.revealingApikeys\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n                     </div>\\n                 </div>\\n             </fieldset>', '@@ -21,7 +21,35 @@ $(function () {\\n \\n         self.currentUser = ko.observable(undefined);\\n         self.currentLoginMechanism = ko.observable(undefined);\\n+\\n         self.credentialsSeen = ko.observable(undefined);\\n+        self.credentialsSeenTimeout = undefined;\\n+        self.credentialsSeen.subscribe(() => {\\n+            const credentialsSeen = self.credentialsSeen();\\n+            if (credentialsSeen === undefined) {\\n+                return;\\n+            }\\n+\\n+            if (CONFIG_REAUTHENTICATION_TIMEOUT <= 0) return;\\n+\\n+            if (self.credentialsSeenTimeout)\\n+                window.clearTimeout(self.credentialsSeenTimeout);\\n+\\n+            const now = new Date();\\n+            const seen = new Date(credentialsSeen);\\n+            const timeout =\\n+                seen.getTime() +\\n+                (CONFIG_REAUTHENTICATION_TIMEOUT * 60 + 10) * 1000 -\\n+                now.getTime();\\n+\\n+            if (timeout > 0) {\\n+                callViewModels(self.allViewModels, \"onUserCredentialsRefreshed\");\\n+                window.setTimeout(() => {\\n+                    callViewModels(self.allViewModels, \"onUserCredentialsOutdated\");\\n+                    self.credentialsSeenTimeout = undefined;\\n+                }, timeout);\\n+            }\\n+        });\\n \\n         self.elementUsernameInput = undefined;\\n         self.elementPasswordInput = undefined;', '@@ -128,6 +128,7 @@ $(function () {\\n         self.api_allowCrossOrigin = ko.observable(undefined);\\n \\n         self.apiKeyVisible = ko.observable(false);\\n+        self.revealingApiKey = ko.observable(false);\\n \\n         self.appearance_name = ko.observable(undefined);\\n         self.appearance_color = ko.observable(undefined);\\n@@ -689,17 +690,19 @@ $(function () {\\n             self.settingsDialog.modal(\"hide\");\\n         };\\n \\n-        self.generateApiKey = function () {\\n+        self.generateApiKey = () => {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n             showConfirmationDialog(\\n                 gettext(\\n                     \"This will generate a new API Key. The old API Key will cease to function immediately.\"\\n                 ),\\n-                function () {\\n-                    OctoPrint.settings.generateApiKey().done(function (response) {\\n-                        self.api_key(response.apikey);\\n-                        self.requestData();\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        OctoPrint.settings.generateApiKey().done((response) => {\\n+                            self.api_key(response.apikey);\\n+                            self.requestData();\\n+                        });\\n                     });\\n                 }\\n             );\\n@@ -713,9 +716,11 @@ $(function () {\\n                 gettext(\\n                     \"This will delete the API Key. It will cease to to function immediately.\"\\n                 ),\\n-                function () {\\n-                    OctoPrint.settings.deleteApiKey().done(() => {\\n-                        self.api_key(undefined);\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        OctoPrint.settings.deleteApiKey().done(() => {\\n+                            self.api_key(undefined);\\n+                        });\\n                     });\\n                 }\\n             );\\n@@ -725,7 +730,6 @@ $(function () {\\n             copyToClipboard(self.api_key());\\n         };\\n \\n-        self.revealingApiKey = ko.observable(false);\\n         self.revealApiKey = () => {\\n             self.loginState.reauthenticateIfNecessary(() => {\\n                 self.revealingApiKey(true);\\n@@ -1095,7 +1099,6 @@ $(function () {\\n             return data;\\n         };\\n \\n-        self.reauthenticationTimeout = undefined;\\n         self.fromResponse = function (response, local) {\\n             // server side changes to set\\n             var serverChangedData;\\n@@ -1284,14 +1287,8 @@ $(function () {\\n \\n             firstRequest.resolve();\\n \\n-            // special delivery for the API key flag\\n-            self.apiKeyVisible(self.loginState.checkCredentialsSeen());\\n-            if (self.apiKeyVisible()) {\\n-                self.reauthenticationTimeout =\\n-                    self.loginState.afterReauthenticationTimeout(() => {\\n-                        self.requestData();\\n-                    }, self.reauthenticationTimeout);\\n-            }\\n+            // this should only ever return true if we triggered the request through the \"reveal api key\" button\\n+            self.apiKeyVisible(self.revealingApiKey());\\n \\n             // if autologinLocal is enabled and the heads-up not yet acknowledged, show it now\\n             if (\\n@@ -1494,6 +1491,11 @@ $(function () {\\n                     self.requestData();\\n                 };\\n \\n+        self.onUserCredentialsOutdated = () => {\\n+            self.apiKeyVisible(false);\\n+            self.requestData();\\n+        };\\n+\\n         self.validURL = function (str) {\\n             var pattern = new RegExp(\\n                 \"^(https?:\\\\\\\\/\\\\\\\\/)?\" + // protocol'], 'file': ['src/octoprint/static/js/app/viewmodels/usersettings.js', 'src/octoprint/server/api/access.py', 'src/octoprint/templates/dialogs/usersettings/access.jinja2', 'src/octoprint/server/api/settings.py', 'src/octoprint/static/js/app/viewmodels/access.js', 'src/octoprint/templates/dialogs/settings/api.jinja2', 'src/octoprint/templates/snippets/settings/accesscontrol/users.jinja2', 'src/octoprint/static/js/app/viewmodels/loginstate.js', 'src/octoprint/static/js/app/viewmodels/settings.js'], 'language': ['JavaScript/TypeScript', 'Python', 'Jinja2', 'Python', 'JavaScript/TypeScript', 'Jinja2', 'Jinja2', 'JavaScript/TypeScript', 'JavaScript/TypeScript'], 'temp_id': [UUID('123f206b-e519-4a6c-a578-d44147075fe5'), UUID('ac0c47fc-6fd4-4c1a-8927-aab64d286897'), UUID('42cbcf30-625b-413f-b1e9-6178baf0fc4d'), UUID('ec68d0f4-b6a7-4d95-a1d9-22b5deafd966'), UUID('2689f768-fbb4-4dbb-8386-f57b3fcf5926'), UUID('33a53f5a-2602-4154-a354-78b6b5dafce4'), UUID('a77d3f8f-07f5-4ab3-a71b-5c72778a136d'), UUID('b8792f66-f44f-49ae-ac55-1b5169f54c62'), UUID('7bc8fe7b-826d-4606-9e2c-8ecff5773570')]}\n",
      "ERROR:root:Error in {'repo': 'OctoPrint/OctoPrint', 'vulnerability_id': '2024-51493', 'commit': '9bc80d782d72881b16e20873dcd0b8314324c70c', 'commit_source': 'github', 'cwe_id': ['CWE-620', 'CWE-620'], 'patch': ['@@ -30,6 +30,9 @@ $(function () {\\n         self.access_apikey = ko.observable(undefined);\\n         self.interface_language = ko.observable(undefined);\\n \\n+        self.apiKeyVisible = ko.observable(false);\\n+        self.revealingApiKey = ko.observable(false);\\n+\\n         self.currentUser = ko.observable(undefined);\\n         self.currentUser.subscribe(function (newUser) {\\n             self.access_password(undefined);\\n@@ -57,32 +60,44 @@ $(function () {\\n             return self.access_password() !== self.access_repeatedPassword();\\n         });\\n \\n-        self.show = function (user) {\\n+        self.show = (user) => {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n             if (user === undefined) {\\n                 user = self.loginState.currentUser();\\n             }\\n \\n-            var process = function (user) {\\n-                self.currentUser(user);\\n-                self.userSettingsDialog.modal(\"show\");\\n-            };\\n-\\n             // make sure we have the current user data, see #2534\\n-            OctoPrint.access.users\\n-                .get(user.name)\\n-                .done(function (data) {\\n-                    process(data);\\n+            self.requestData(user.name)\\n+                .done(() => {\\n+                    self.userSettingsDialog.modal(\"show\");\\n                 })\\n-                .fail(function () {\\n+                .fail(() => {\\n                     log.warn(\\n                         \"Could not fetch current user data, proceeding with client side data copy\"\\n                     );\\n-                    process(user);\\n+                    self.fromResponse(user);\\n                 });\\n         };\\n \\n+        self.requestData = (name) => {\\n+            if (name === undefined && self.currentUser() === undefined) return;\\n+            if (name === undefined) {\\n+                name = self.currentUser().name;\\n+            }\\n+\\n+            return OctoPrint.access.users.get(name).done((data) => {\\n+                self.fromResponse(data);\\n+            });\\n+        };\\n+\\n+        self.fromResponse = (data) => {\\n+            self.currentUser(data);\\n+\\n+            // this should only ever return true if we triggered the request through the \"reveal api key\" button\\n+            self.apiKeyVisible(self.revealingApiKey());\\n+        };\\n+\\n         self.save = function () {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n@@ -129,12 +144,14 @@ $(function () {\\n         self.generateApikey = function () {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n-            var generate = function () {\\n-                self.users\\n-                    .generateApikey(self.currentUser().name)\\n-                    .done(function (response) {\\n-                        self.access_apikey(response.apikey);\\n-                    });\\n+            const generate = () => {\\n+                self.loginState.reauthenticateIfNecessary(() => {\\n+                    self.users\\n+                        .generateApikey(self.currentUser().name)\\n+                        .done((response) => {\\n+                            self.access_apikey(response.apikey);\\n+                        });\\n+                });\\n             };\\n \\n             if (self.access_apikey()) {\\n@@ -157,14 +174,25 @@ $(function () {\\n                 gettext(\\n                     \"This will delete the API Key. It will cease to to function immediately.\"\\n                 ),\\n-                function () {\\n-                    self.users.deleteApikey(self.currentUser().name).done(function () {\\n-                        self.access_apikey(undefined);\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        self.users.deleteApikey(self.currentUser().name).done(() => {\\n+                            self.access_apikey(undefined);\\n+                        });\\n                     });\\n                 }\\n             );\\n         };\\n \\n+        self.revealApiKey = () => {\\n+            self.loginState.reauthenticateIfNecessary(() => {\\n+                self.revealingApiKey(true);\\n+                self.requestData(self.currentUser().name).always(() => {\\n+                    self.revealingApiKey(false);\\n+                });\\n+            });\\n+        };\\n+\\n         self.updateSettings = function (username, settings) {\\n             return OctoPrint.access.users.saveSettings(username, settings);\\n         };\\n@@ -188,6 +216,11 @@ $(function () {\\n                 callViewModels(allViewModels, \"onUserSettingsBeforeSave\");\\n             });\\n         };\\n+\\n+        self.onUserCredentialsOutdated = () => {\\n+            self.apiKeyVisible(false);\\n+            self.requestData();\\n+        };\\n     }\\n \\n     OCTOPRINT_VIEWMODELS.push({', '@@ -11,6 +11,7 @@\\n from octoprint.server import SUCCESS, groupManager, userManager\\n from octoprint.server.api import api, valid_boolean_trues\\n from octoprint.server.util.flask import (\\n+    credentials_checked_recently,\\n     ensure_credentials_checked_recently,\\n     no_firstrun_access,\\n     require_credentials_checked_recently,\\n@@ -132,7 +133,11 @@ def remove_group(key):\\n @no_firstrun_access\\n @Permissions.ADMIN.require(403)\\n def get_users():\\n-    return jsonify(users=list(map(lambda u: u.as_dict(), userManager.get_all_users())))\\n+    users = [u.as_dict() for u in userManager.get_all_users()]\\n+    if not credentials_checked_recently():\\n+        for u in users:\\n+            u[\"apikey\"] = None\\n+    return jsonify(users=users)\\n \\n \\n @api.route(\"/access/users\", methods=[\"POST\"])\\n@@ -178,7 +183,10 @@ def get_user(username):\\n     ):\\n         user = userManager.find_user(username)\\n         if user is not None:\\n-            return jsonify(user)\\n+            user_dict = user.as_dict()\\n+            if not credentials_checked_recently():\\n+                user_dict[\"apikey\"] = None\\n+            return jsonify(user_dict)\\n         else:\\n             abort(404)\\n     else:\\n@@ -323,6 +331,7 @@ def change_settings_for_user(username):\\n \\n @api.route(\"/access/users/<username>/apikey\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n def delete_apikey_for_user(username):\\n     if (\\n         current_user is not None\\n@@ -347,6 +356,7 @@ def delete_apikey_for_user(username):\\n \\n @api.route(\"/access/users/<username>/apikey\", methods=[\"POST\"])\\n @no_firstrun_access\\n+@require_credentials_checked_recently\\n def generate_apikey_for_user(username):\\n     if not userManager.enabled:\\n         return jsonify(SUCCESS)', '@@ -32,7 +32,7 @@\\n         <legend>{{ _(\\'API Key\\') }}</legend>\\n         <div class=\"control-group\">\\n             <label class=\"control-label\" for=\"userSettings-access_apikey\">{{ _(\\'Current API Key\\') }}</label>\\n-            <div class=\"controls\">\\n+            <div class=\"controls\" data-bind=\"visible: apiKeyVisible\">\\n                 <div class=\"input-append input-block-level\">\\n                     <input type=\"text\" readonly=\"readonly\" id=\"userSettings-access_apikey\" data-bind=\"value: access_apikey, attr: {placeholder: \\'{{ _(\\'N/A\\')|esq }}\\'}\">\\n                     <a class=\"btn add-on\" title=\"Copy API Key to clipboard\" data-bind=\"click: copyApikey, css: {\\'disabled\\': !access_apikey()}\"><i class=\"fas fa-copy\"></i></a>\\n@@ -41,8 +41,11 @@\\n                 </div>\\n                 <span class=\"help-block\">{{ _(\\'Please note that changes to the API key are applied immediately, without having to \"Confirm\" first.\\') }}</span>\\n             </div>\\n+            <div class=\"controls\" data-bind=\"visible: !apiKeyVisible()\">\\n+                <button class=\"btn\" data-bind=\"click: revealApiKey, enabled: !revealingApiKey(), css: {disabled: revealingApiKey}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: revealingApiKey\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n+            </div>\\n         </div>\\n-        <div class=\"control-group\" data-bind=\"visible: access_apikey\">\\n+        <div class=\"control-group\" data-bind=\"visible: access_apikey() && apiKeyVisible()\">\\n             <label class=\"control-label\">{{ _(\\'QR Code\\') }}</label>\\n             <div class=\"controls\">\\n                 <div data-bind=\"qrcode: {text: access_apikey, size: 150}\"></div>', '@@ -16,6 +16,7 @@\\n from octoprint.server.util.flask import (\\n     credentials_checked_recently,\\n     no_firstrun_access,\\n+    require_credentials_checked_recently,\\n     with_revalidation_checking,\\n )\\n from octoprint.settings import settings, valid_boolean_trues\\n@@ -478,6 +479,7 @@ def setSettings():\\n @api.route(\"/settings/apikey\", methods=[\"POST\"])\\n @no_firstrun_access\\n @Permissions.ADMIN.require(403)\\n+@require_credentials_checked_recently\\n def generateApiKey():\\n     apikey = settings().generateApiKey()\\n     return jsonify(apikey=apikey)\\n@@ -486,6 +488,7 @@ def generateApiKey():\\n @api.route(\"/settings/apikey\", methods=[\"DELETE\"])\\n @no_firstrun_access\\n @Permissions.ADMIN.require(403)\\n+@require_credentials_checked_recently\\n def deleteApiKey():\\n     settings().deleteApiKey()\\n     return NO_CONTENT\\n@@ -581,7 +584,8 @@ def _saveSettings(data):\\n             s.set([\"appearance\", \"color\"], data[\"appearance\"][\"color\"])\\n         if \"colorTransparent\" in data[\"appearance\"]:\\n             s.setBoolean(\\n-                [\"appearance\", \"colorTransparent\"], data[\"appearance\"][\"colorTransparent\"]\\n+                [\"appearance\", \"colorTransparent\"],\\n+                data[\"appearance\"][\"colorTransparent\"],\\n             )\\n         if \"colorIcon\" in data[\"appearance\"]:\\n             s.setBoolean([\"appearance\", \"colorIcon\"], data[\"appearance\"][\"colorIcon\"])\\n@@ -639,7 +643,10 @@ def _saveSettings(data):\\n         if \"ffmpegCommandline\" in data[\"webcam\"]:\\n             commandline = data[\"webcam\"][\"ffmpegCommandline\"]\\n             if not all(\\n-                map(lambda x: \"{\" + x + \"}\" in commandline, (\"ffmpeg\", \"input\", \"output\"))\\n+                map(\\n+                    lambda x: \"{\" + x + \"}\" in commandline,\\n+                    (\"ffmpeg\", \"input\", \"output\"),\\n+                )\\n             ):\\n                 abort(\\n                     400,\\n@@ -855,7 +862,8 @@ def _saveSettings(data):\\n             data[\"serial\"][\"blacklistedBaudrates\"], (list, tuple)\\n         ):\\n             s.set(\\n-                [\"serial\", \"blacklistedBaudrates\"], data[\"serial\"][\"blacklistedBaudrates\"]\\n+                [\"serial\", \"blacklistedBaudrates\"],\\n+                data[\"serial\"][\"blacklistedBaudrates\"],\\n             )\\n         if \"longRunningCommands\" in data[\"serial\"] and isinstance(\\n             data[\"serial\"][\"longRunningCommands\"], (list, tuple)\\n@@ -942,7 +950,8 @@ def _saveSettings(data):\\n             s.setBoolean([\"serial\", \"sdLowerCase\"], data[\"serial\"][\"sdLowerCase\"])\\n         if \"swallowOkAfterResend\" in data[\"serial\"]:\\n             s.setBoolean(\\n-                [\"serial\", \"swallowOkAfterResend\"], data[\"serial\"][\"swallowOkAfterResend\"]\\n+                [\"serial\", \"swallowOkAfterResend\"],\\n+                data[\"serial\"][\"swallowOkAfterResend\"],\\n             )\\n         if \"repetierTargetTemp\" in data[\"serial\"]:\\n             s.setBoolean(\\n@@ -1040,18 +1049,21 @@ def _saveSettings(data):\\n                 data[\"serial\"][\"capEmergencyParser\"],\\n             )\\n         if \"capExtendedM20\" in data[\"serial\"]:\\n-            s.setBoolean(\\n-                [\"serial\", \"capabilities\", \"extended_m20\"],\\n-                data[\"serial\"][\"capExtendedM20\"],\\n-            ),\\n+            (\\n+                s.setBoolean(\\n+                    [\"serial\", \"capabilities\", \"extended_m20\"],\\n+                    data[\"serial\"][\"capExtendedM20\"],\\n+                ),\\n+            )\\n         if \"capLfnWrite\" in data[\"serial\"]:\\n             s.setBoolean(\\n                 [\"serial\", \"capabilities\", \"lfn_write\"],\\n                 data[\"serial\"][\"capLfnWrite\"],\\n             )\\n         if \"resendRatioThreshold\" in data[\"serial\"]:\\n             s.setInt(\\n-                [\"serial\", \"resendRatioThreshold\"], data[\"serial\"][\"resendRatioThreshold\"]\\n+                [\"serial\", \"resendRatioThreshold\"],\\n+                data[\"serial\"][\"resendRatioThreshold\"],\\n             )\\n         if \"resendRatioStart\" in data[\"serial\"]:\\n             s.setInt([\"serial\", \"resendRatioStart\"], data[\"serial\"][\"resendRatioStart\"])', '@@ -43,6 +43,9 @@ $(function () {\\n                 return !self.isCurrentUser(user);\\n             };\\n \\n+            self.apikeysVisible = ko.observable(false);\\n+            self.revealingApikeys = ko.observable(false);\\n+\\n             self.editor = {\\n                 name: ko.observable(undefined),\\n                 groups: ko.observableArray([]),\\n@@ -163,6 +166,9 @@ $(function () {\\n \\n             self.fromResponse = function (response) {\\n                 self.listHelper.updateItems(response.users);\\n+\\n+                // This should only be true if the request was triggered by a click on \"reveal api key\"\\n+                self.apikeysVisible(self.revealingApikeys());\\n             };\\n \\n             self.showAddUserDialog = function () {\\n@@ -355,6 +361,22 @@ $(function () {\\n                 });\\n             };\\n \\n+            self.revealApikeys = () => {\\n+                access.loginState.reauthenticateIfNecessary(() => {\\n+                    self.revealingApikeys(true);\\n+                    self.requestData().always(() => {\\n+                        self.revealingApikeys(false);\\n+                        if (self.currentUser()) {\\n+                            OctoPrint.access.users\\n+                                .get(self.currentUser().name)\\n+                                .done((data) => {\\n+                                    self.currentUser(data);\\n+                                });\\n+                        }\\n+                    });\\n+                });\\n+            };\\n+\\n             self.copyApikey = function () {\\n                 copyToClipboard(self.editor.apikey());\\n             };\\n@@ -381,6 +403,16 @@ $(function () {\\n                 self.changePasswordDialog = $(\"#settings-usersDialogChangePassword\");\\n             };\\n \\n+            self.onUserCredentialsOutdated = () => {\\n+                self.apikeysVisible(false);\\n+                self.requestData();\\n+                if (self.currentUser()) {\\n+                    OctoPrint.access.users.get(self.currentUser().name).done((data) => {\\n+                        self.currentUser(data);\\n+                    });\\n+                }\\n+            };\\n+\\n             //~~ API calls\\n \\n             self.addUser = function (user) {\\n@@ -980,6 +1012,10 @@ $(function () {\\n             access.permissions.initialize();\\n         };\\n \\n+        access.onUserCredentialsOutdated = () => {\\n+            access.users.onUserCredentialsOutdated();\\n+        };\\n+\\n         access.onUserPermissionsChanged =\\n             access.onUserLoggedIn =\\n             access.onUserLoggedOut =', '@@ -38,7 +38,7 @@\\n                 <button class=\"btn\" data-bind=\"click: revealApiKey, enabled: !revealingApiKey(), css: {disabled: revealingApiKey}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: revealingApiKey\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n             </div>\\n         </div>\\n-        <div class=\"control-group\" data-bind=\"visible: api_key\">\\n+        <div class=\"control-group\" data-bind=\"visible: api_key() && apiKeyVisible()\">\\n             <label class=\"control-label\">{{ _(\\'QR Code\\') }}</label>\\n             <div class=\"controls\">\\n                 <div data-bind=\"qrcode: {text: api_key, size: 180}\"></div>', '@@ -157,14 +157,16 @@\\n                 <legend>API Key</legend>\\n                 <div class=\"control-group\">\\n                     <label class=\"control-label\">{{ _(\\'Current API Key\\') }}</label>\\n-                    <div class=\"controls\">\\n+                    <div class=\"controls\" data-bind=\"visible: $root.access.users.apikeysVisible\">\\n                         <div class=\"input-append input-block-level \">\\n                             <input type=\"text\" readonly=\"readonly\" data-bind=\"value: $root.access.users.editor.apikey, valueUpdate: \\'input\\', attr: {placeholder: \\'{{ _(\\'N/A\\')|esq }}\\'}\">\\n                             <a class=\"btn add-on\" title=\"Copy API Key to clipboard\" data-bind=\"click: $root.access.users.copyApikey, css: {\\'disabled\\': !$root.access.users.editor.apikey()}\"><i class=\"fas fa-copy\"></i></a>\\n                             <a class=\"btn add-on\" title=\"Generate new API Key\" data-bind=\"click: function() { $root.access.users.confirmGenerateApikey(); }\"><i class=\"fas fa-sync\"></i></a>\\n                             <a class=\"btn add-on btn-danger\" title=\"Delete API Key\" data-bind=\"click: function() { $root.access.users.confirmDeleteApikey(); }, css: {\\'disabled\\': !$root.access.users.editor.apikey()}\"><i class=\"far fa-trash-alt\"></i></a>\\n                         </div>\\n-\\n+                    </div>\\n+                    <div class=\"controls\" data-bind=\"visible: !$root.access.users.apikeysVisible()\">\\n+                        <button class=\"btn\" data-bind=\"click: $root.access.users.revealApikeys, enabled: !$root.access.users.revealingApikeys(), css: {disabled: $root.access.users.revealingApikeys}\"><i class=\"fas fa-spinner fa-spin\" data-bind=\"visible: $root.access.users.revealingApikeys\"></i> {{ _(\\'Reveal API Key\\') }}</button>\\n                     </div>\\n                 </div>\\n             </fieldset>', '@@ -21,7 +21,35 @@ $(function () {\\n \\n         self.currentUser = ko.observable(undefined);\\n         self.currentLoginMechanism = ko.observable(undefined);\\n+\\n         self.credentialsSeen = ko.observable(undefined);\\n+        self.credentialsSeenTimeout = undefined;\\n+        self.credentialsSeen.subscribe(() => {\\n+            const credentialsSeen = self.credentialsSeen();\\n+            if (credentialsSeen === undefined) {\\n+                return;\\n+            }\\n+\\n+            if (CONFIG_REAUTHENTICATION_TIMEOUT <= 0) return;\\n+\\n+            if (self.credentialsSeenTimeout)\\n+                window.clearTimeout(self.credentialsSeenTimeout);\\n+\\n+            const now = new Date();\\n+            const seen = new Date(credentialsSeen);\\n+            const timeout =\\n+                seen.getTime() +\\n+                (CONFIG_REAUTHENTICATION_TIMEOUT * 60 + 10) * 1000 -\\n+                now.getTime();\\n+\\n+            if (timeout > 0) {\\n+                callViewModels(self.allViewModels, \"onUserCredentialsRefreshed\");\\n+                window.setTimeout(() => {\\n+                    callViewModels(self.allViewModels, \"onUserCredentialsOutdated\");\\n+                    self.credentialsSeenTimeout = undefined;\\n+                }, timeout);\\n+            }\\n+        });\\n \\n         self.elementUsernameInput = undefined;\\n         self.elementPasswordInput = undefined;', '@@ -128,6 +128,7 @@ $(function () {\\n         self.api_allowCrossOrigin = ko.observable(undefined);\\n \\n         self.apiKeyVisible = ko.observable(false);\\n+        self.revealingApiKey = ko.observable(false);\\n \\n         self.appearance_name = ko.observable(undefined);\\n         self.appearance_color = ko.observable(undefined);\\n@@ -689,17 +690,19 @@ $(function () {\\n             self.settingsDialog.modal(\"hide\");\\n         };\\n \\n-        self.generateApiKey = function () {\\n+        self.generateApiKey = () => {\\n             if (!CONFIG_ACCESS_CONTROL) return;\\n \\n             showConfirmationDialog(\\n                 gettext(\\n                     \"This will generate a new API Key. The old API Key will cease to function immediately.\"\\n                 ),\\n-                function () {\\n-                    OctoPrint.settings.generateApiKey().done(function (response) {\\n-                        self.api_key(response.apikey);\\n-                        self.requestData();\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        OctoPrint.settings.generateApiKey().done((response) => {\\n+                            self.api_key(response.apikey);\\n+                            self.requestData();\\n+                        });\\n                     });\\n                 }\\n             );\\n@@ -713,9 +716,11 @@ $(function () {\\n                 gettext(\\n                     \"This will delete the API Key. It will cease to to function immediately.\"\\n                 ),\\n-                function () {\\n-                    OctoPrint.settings.deleteApiKey().done(() => {\\n-                        self.api_key(undefined);\\n+                () => {\\n+                    self.loginState.reauthenticateIfNecessary(() => {\\n+                        OctoPrint.settings.deleteApiKey().done(() => {\\n+                            self.api_key(undefined);\\n+                        });\\n                     });\\n                 }\\n             );\\n@@ -725,7 +730,6 @@ $(function () {\\n             copyToClipboard(self.api_key());\\n         };\\n \\n-        self.revealingApiKey = ko.observable(false);\\n         self.revealApiKey = () => {\\n             self.loginState.reauthenticateIfNecessary(() => {\\n                 self.revealingApiKey(true);\\n@@ -1095,7 +1099,6 @@ $(function () {\\n             return data;\\n         };\\n \\n-        self.reauthenticationTimeout = undefined;\\n         self.fromResponse = function (response, local) {\\n             // server side changes to set\\n             var serverChangedData;\\n@@ -1284,14 +1287,8 @@ $(function () {\\n \\n             firstRequest.resolve();\\n \\n-            // special delivery for the API key flag\\n-            self.apiKeyVisible(self.loginState.checkCredentialsSeen());\\n-            if (self.apiKeyVisible()) {\\n-                self.reauthenticationTimeout =\\n-                    self.loginState.afterReauthenticationTimeout(() => {\\n-                        self.requestData();\\n-                    }, self.reauthenticationTimeout);\\n-            }\\n+            // this should only ever return true if we triggered the request through the \"reveal api key\" button\\n+            self.apiKeyVisible(self.revealingApiKey());\\n \\n             // if autologinLocal is enabled and the heads-up not yet acknowledged, show it now\\n             if (\\n@@ -1494,6 +1491,11 @@ $(function () {\\n                     self.requestData();\\n                 };\\n \\n+        self.onUserCredentialsOutdated = () => {\\n+            self.apiKeyVisible(false);\\n+            self.requestData();\\n+        };\\n+\\n         self.validURL = function (str) {\\n             var pattern = new RegExp(\\n                 \"^(https?:\\\\\\\\/\\\\\\\\/)?\" + // protocol'], 'file': ['src/octoprint/static/js/app/viewmodels/usersettings.js', 'src/octoprint/server/api/access.py', 'src/octoprint/templates/dialogs/usersettings/access.jinja2', 'src/octoprint/server/api/settings.py', 'src/octoprint/static/js/app/viewmodels/access.js', 'src/octoprint/templates/dialogs/settings/api.jinja2', 'src/octoprint/templates/snippets/settings/accesscontrol/users.jinja2', 'src/octoprint/static/js/app/viewmodels/loginstate.js', 'src/octoprint/static/js/app/viewmodels/settings.js'], 'language': ['JavaScript/TypeScript', 'Python', 'Jinja2', 'Python', 'JavaScript/TypeScript', 'Jinja2', 'Jinja2', 'JavaScript/TypeScript', 'JavaScript/TypeScript'], 'temp_id': [UUID('123f206b-e519-4a6c-a578-d44147075fe5'), UUID('ac0c47fc-6fd4-4c1a-8927-aab64d286897'), UUID('42cbcf30-625b-413f-b1e9-6178baf0fc4d'), UUID('ec68d0f4-b6a7-4d95-a1d9-22b5deafd966'), UUID('2689f768-fbb4-4dbb-8386-f57b3fcf5926'), UUID('33a53f5a-2602-4154-a354-78b6b5dafce4'), UUID('a77d3f8f-07f5-4ab3-a71b-5c72778a136d'), UUID('b8792f66-f44f-49ae-ac55-1b5169f54c62'), UUID('7bc8fe7b-826d-4606-9e2c-8ecff5773570')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 34:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 34:0: <line number missing in source>\n",
      " 11%|â–ˆ         | 201/1800 [00:52<05:19,  5.01it/s]ERROR:src.process_code_changes:Error processing commit 30307c4616ad67c01ddae2e1e8e34fabf6028414\n",
      "ERROR:src.process_code_changes:{'repo': 'sqlalchemy/sqlalchemy', 'vulnerability_id': '2019-7164', 'commit': '30307c4616ad67c01ddae2e1e8e34fabf6028414', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -37,6 +37,20 @@ def _clone(element, **kw):\\n     return element._clone()\\n \\n \\n+def _document_text_coercion(paramname, meth_rst, param_rst):\\n+    return util.add_parameter_text(\\n+        paramname,\\n+        (\\n+            \".. warning:: \"\\n+            \"The %s argument to %s can be passed as a Python string argument, \"\\n+            \"which will be treated \"\\n+            \"as **trusted SQL text** and rendered as given.  **DO NOT PASS \"\\n+            \"UNTRUSTED INPUT TO THIS PARAMETER**.\"\\n+        )\\n+        % (param_rst, meth_rst),\\n+    )\\n+\\n+\\n def collate(expression, collation):\\n     \"\"\"Return the clause ``expression COLLATE collation``.\\n \\n@@ -1343,6 +1357,7 @@ def repl(m):\\n             \"refer to the :meth:`.TextClause.columns` method.\",\\n         ),\\n     )\\n+    @_document_text_coercion(\"text\", \":func:`.text`\", \":paramref:`.text.text`\")\\n     def _create_text(\\n         self, text, bind=None, bindparams=None, typemap=None, autocommit=None\\n     ):\\n@@ -4430,32 +4445,64 @@ def _literal_and_labels_as_label_reference(element):\\n \\n \\n def _expression_literal_as_text(element):\\n-    return _literal_as_text(element, warn=True)\\n+    return _literal_as_text(element)\\n \\n \\n-def _literal_as_text(element, warn=False):\\n+def _literal_as(element, text_fallback):\\n     if isinstance(element, Visitable):\\n         return element\\n     elif hasattr(element, \"__clause_element__\"):\\n         return element.__clause_element__()\\n     elif isinstance(element, util.string_types):\\n-        if warn:\\n-            util.warn_limited(\\n-                \"Textual SQL expression %(expr)r should be \"\\n-                \"explicitly declared as text(%(expr)r)\",\\n-                {\"expr\": util.ellipses_string(element)},\\n-            )\\n-\\n-        return TextClause(util.text_type(element))\\n+        return text_fallback(element)\\n     elif isinstance(element, (util.NoneType, bool)):\\n         return _const_expr(element)\\n     else:\\n         raise exc.ArgumentError(\\n-            \"SQL expression object or string expected, got object of type %r \"\\n+            \"SQL expression object expected, got object of type %r \"\\n             \"instead\" % type(element)\\n         )\\n \\n \\n+def _literal_as_text(element, allow_coercion_to_text=False):\\n+    if allow_coercion_to_text:\\n+        return _literal_as(element, TextClause)\\n+    else:\\n+        return _literal_as(element, _no_text_coercion)\\n+\\n+\\n+def _literal_as_column(element):\\n+    return _literal_as(element, ColumnClause)\\n+\\n+\\n+def _no_column_coercion(element):\\n+    element = str(element)\\n+    guess_is_literal = not _guess_straight_column.match(element)\\n+    raise exc.ArgumentError(\\n+        \"Textual column expression %(column)r should be \"\\n+        \"explicitly declared with text(%(column)r), \"\\n+        \"or use %(literal_column)s(%(column)r) \"\\n+        \"for more specificity\"\\n+        % {\\n+            \"column\": util.ellipses_string(element),\\n+            \"literal_column\": \"literal_column\"\\n+            if guess_is_literal\\n+            else \"column\",\\n+        }\\n+    )\\n+\\n+\\n+def _no_text_coercion(element, exc_cls=exc.ArgumentError, extra=None):\\n+    raise exc_cls(\\n+        \"%(extra)sTextual SQL expression %(expr)r should be \"\\n+        \"explicitly declared as text(%(expr)r)\"\\n+        % {\\n+            \"expr\": util.ellipses_string(element),\\n+            \"extra\": \"%s \" % extra if extra else \"\",\\n+        }\\n+    )\\n+\\n+\\n def _no_literals(element):\\n     if hasattr(element, \"__clause_element__\"):\\n         return element.__clause_element__()\\n@@ -4529,23 +4576,7 @@ def _interpret_as_column_or_from(element):\\n     elif isinstance(element, (numbers.Number)):\\n         return ColumnClause(str(element), is_literal=True)\\n     else:\\n-        element = str(element)\\n-        # give into temptation, as this fact we are guessing about\\n-        # is not one we\\'ve previously ever needed our users tell us;\\n-        # but let them know we are not happy about it\\n-        guess_is_literal = not _guess_straight_column.match(element)\\n-        util.warn_limited(\\n-            \"Textual column expression %(column)r should be \"\\n-            \"explicitly declared with text(%(column)r), \"\\n-            \"or use %(literal_column)s(%(column)r) \"\\n-            \"for more specificity\",\\n-            {\\n-                \"column\": util.ellipses_string(element),\\n-                \"literal_column\": \"literal_column\"\\n-                if guess_is_literal\\n-                else \"column\",\\n-            },\\n-        )\\n+        _no_column_coercion(element)\\n     return ColumnClause(element, is_literal=guess_is_literal)\\n \\n ', '@@ -139,8 +139,16 @@\\n )\\n \\n LEGAL_CHARACTERS = re.compile(r\"^[A-Z0-9_$]+$\", re.I)\\n+LEGAL_CHARACTERS_PLUS_SPACE = re.compile(r\"^[A-Z0-9_ $]+$\", re.I)\\n ILLEGAL_INITIAL_CHARACTERS = {str(x) for x in range(0, 10)}.union([\"$\"])\\n \\n+FK_ON_DELETE = re.compile(\\n+    r\"^(?:RESTRICT|CASCADE|SET NULL|NO ACTION|SET DEFAULT)$\", re.I\\n+)\\n+FK_ON_UPDATE = re.compile(\\n+    r\"^(?:RESTRICT|CASCADE|SET NULL|NO ACTION|SET DEFAULT)$\", re.I\\n+)\\n+FK_INITIALLY = re.compile(r\"^(?:DEFERRED|IMMEDIATE)$\", re.I)\\n BIND_PARAMS = re.compile(r\"(?<![:\\\\w\\\\$\\\\x5c]):([\\\\w\\\\$]+)(?![:\\\\w\\\\$])\", re.UNICODE)\\n BIND_PARAMS_ESC = re.compile(r\"\\\\x5c(:[\\\\w\\\\$]*)(?![:\\\\w\\\\$])\", re.UNICODE)\\n \\n@@ -758,12 +766,11 @@ def visit_textual_label_reference(\\n             else:\\n                 col = with_cols[element.element]\\n         except KeyError:\\n-            # treat it like text()\\n-            util.warn_limited(\\n-                \"Can\\'t resolve label reference %r; converting to text()\",\\n-                util.ellipses_string(element.element),\\n+            elements._no_text_coercion(\\n+                element.element,\\n+                exc.CompileError,\\n+                \"Can\\'t resolve label reference for ORDER BY / GROUP BY.\",\\n             )\\n-            return self.process(element._text_clause)\\n         else:\\n             kwargs[\"render_label_as_label\"] = col\\n             return self.process(\\n@@ -1076,10 +1083,24 @@ def visit_function(self, func, add_to_result_map=None, **kwargs):\\n                 if func._has_args:\\n                     name += \"%(expr)s\"\\n             else:\\n-                name = func.name + \"%(expr)s\"\\n-            return \".\".join(list(func.packagenames) + [name]) % {\\n-                \"expr\": self.function_argspec(func, **kwargs)\\n-            }\\n+                name = func.name\\n+                name = (\\n+                    self.preparer.quote(name)\\n+                    if self.preparer._requires_quotes_illegal_chars(name)\\n+                    else name\\n+                )\\n+                name = name + \"%(expr)s\"\\n+            return \".\".join(\\n+                [\\n+                    (\\n+                        self.preparer.quote(tok)\\n+                        if self.preparer._requires_quotes_illegal_chars(tok)\\n+                        else tok\\n+                    )\\n+                    for tok in func.packagenames\\n+                ]\\n+                + [name]\\n+            ) % {\"expr\": self.function_argspec(func, **kwargs)}\\n \\n     def visit_next_value_func(self, next_value, **kw):\\n         return self.visit_sequence(next_value.sequence)\\n@@ -3153,9 +3174,13 @@ def visit_unique_constraint(self, constraint):\\n     def define_constraint_cascades(self, constraint):\\n         text = \"\"\\n         if constraint.ondelete is not None:\\n-            text += \" ON DELETE %s\" % constraint.ondelete\\n+            text += \" ON DELETE %s\" % self.preparer.validate_sql_phrase(\\n+                constraint.ondelete, FK_ON_DELETE\\n+            )\\n         if constraint.onupdate is not None:\\n-            text += \" ON UPDATE %s\" % constraint.onupdate\\n+            text += \" ON UPDATE %s\" % self.preparer.validate_sql_phrase(\\n+                constraint.onupdate, FK_ON_UPDATE\\n+            )\\n         return text\\n \\n     def define_constraint_deferrability(self, constraint):\\n@@ -3166,7 +3191,9 @@ def define_constraint_deferrability(self, constraint):\\n             else:\\n                 text += \" NOT DEFERRABLE\"\\n         if constraint.initially is not None:\\n-            text += \" INITIALLY %s\" % constraint.initially\\n+            text += \" INITIALLY %s\" % self.preparer.validate_sql_phrase(\\n+                constraint.initially, FK_INITIALLY\\n+            )\\n         return text\\n \\n     def define_constraint_match(self, constraint):\\n@@ -3416,6 +3443,24 @@ def _unescape_identifier(self, value):\\n \\n         return value.replace(self.escape_to_quote, self.escape_quote)\\n \\n+    def validate_sql_phrase(self, element, reg):\\n+        \"\"\"keyword sequence filter.\\n+\\n+        a filter for elements that are intended to represent keyword sequences,\\n+        such as \"INITIALLY\", \"INTIALLY DEFERRED\", etc.   no special characters\\n+        should be present.\\n+\\n+        .. versionadded:: 1.3\\n+\\n+        \"\"\"\\n+\\n+        if element is not None and not reg.match(element):\\n+            raise exc.CompileError(\\n+                \"Unexpected SQL phrase: %r (matching against %r)\"\\n+                % (element, reg.pattern)\\n+            )\\n+        return element\\n+\\n     def quote_identifier(self, value):\\n         \"\"\"Quote an identifier.\\n \\n@@ -3439,6 +3484,11 @@ def _requires_quotes(self, value):\\n             or (lc_value != value)\\n         )\\n \\n+    def _requires_quotes_illegal_chars(self, value):\\n+        \"\"\"Return True if the given identifier requires quoting, but\\n+        not taking case convention into account.\"\"\"\\n+        return not self.legal_characters.match(util.text_type(value))\\n+\\n     def quote_schema(self, schema, force=None):\\n         \"\"\"Conditionally quote a schema name.\\n ', '@@ -101,6 +101,7 @@\\n from .elements import _is_column  # noqa\\n from .elements import _labeled  # noqa\\n from .elements import _literal_as_binds  # noqa\\n+from .elements import _literal_as_column  # noqa\\n from .elements import _literal_as_label_reference  # noqa\\n from .elements import _literal_as_text  # noqa\\n from .elements import _only_column_elements  # noqa', '@@ -31,19 +31,20 @@\\n from .elements import _clone\\n from .elements import _cloned_difference\\n from .elements import _cloned_intersection\\n+from .elements import _document_text_coercion\\n from .elements import _expand_cloned\\n from .elements import _interpret_as_column_or_from\\n from .elements import _literal_and_labels_as_label_reference\\n from .elements import _literal_as_label_reference\\n from .elements import _literal_as_text\\n+from .elements import _no_text_coercion\\n from .elements import _select_iterables\\n from .elements import and_\\n from .elements import BindParameter\\n from .elements import ClauseElement\\n from .elements import ClauseList\\n from .elements import Grouping\\n from .elements import literal_column\\n-from .elements import TextClause\\n from .elements import True_\\n from .elements import UnaryExpression\\n from .. import exc\\n@@ -55,14 +56,7 @@ def _interpret_as_from(element):\\n     insp = inspection.inspect(element, raiseerr=False)\\n     if insp is None:\\n         if isinstance(element, util.string_types):\\n-            util.warn_limited(\\n-                \"Textual SQL FROM expression %(expr)r should be \"\\n-                \"explicitly declared as text(%(expr)r), \"\\n-                \"or use table(%(expr)r) for more specificity\",\\n-                {\"expr\": util.ellipses_string(element)},\\n-            )\\n-\\n-            return TextClause(util.text_type(element))\\n+            _no_text_coercion(element)\\n     try:\\n         return insp.selectable\\n     except AttributeError:\\n@@ -266,6 +260,11 @@ class HasPrefixes(object):\\n     _prefixes = ()\\n \\n     @_generative\\n+    @_document_text_coercion(\\n+        \"expr\",\\n+        \":meth:`.HasPrefixes.prefix_with`\",\\n+        \":paramref:`.HasPrefixes.prefix_with.*expr`\",\\n+    )\\n     def prefix_with(self, *expr, **kw):\\n         r\"\"\"Add one or more expressions following the statement keyword, i.e.\\n         SELECT, INSERT, UPDATE, or DELETE. Generative.\\n@@ -297,14 +296,22 @@ def prefix_with(self, *expr, **kw):\\n \\n     def _setup_prefixes(self, prefixes, dialect=None):\\n         self._prefixes = self._prefixes + tuple(\\n-            [(_literal_as_text(p, warn=False), dialect) for p in prefixes]\\n+            [\\n+                (_literal_as_text(p, allow_coercion_to_text=True), dialect)\\n+                for p in prefixes\\n+            ]\\n         )\\n \\n \\n class HasSuffixes(object):\\n     _suffixes = ()\\n \\n     @_generative\\n+    @_document_text_coercion(\\n+        \"expr\",\\n+        \":meth:`.HasSuffixes.suffix_with`\",\\n+        \":paramref:`.HasSuffixes.suffix_with.*expr`\",\\n+    )\\n     def suffix_with(self, *expr, **kw):\\n         r\"\"\"Add one or more expressions following the statement as a whole.\\n \\n@@ -335,7 +342,10 @@ def suffix_with(self, *expr, **kw):\\n \\n     def _setup_suffixes(self, suffixes, dialect=None):\\n         self._suffixes = self._suffixes + tuple(\\n-            [(_literal_as_text(p, warn=False), dialect) for p in suffixes]\\n+            [\\n+                (_literal_as_text(p, allow_coercion_to_text=True), dialect)\\n+                for p in suffixes\\n+            ]\\n         )\\n \\n ', '@@ -42,6 +42,7 @@\\n from .base import DialectKWArgs\\n from .base import SchemaEventTarget\\n from .elements import _as_truncated\\n+from .elements import _document_text_coercion\\n from .elements import _literal_as_text\\n from .elements import ClauseElement\\n from .elements import ColumnClause\\n@@ -2884,6 +2885,11 @@ class CheckConstraint(ColumnCollectionConstraint):\\n \\n     _allow_multiple_tables = True\\n \\n+    @_document_text_coercion(\\n+        \"sqltext\",\\n+        \":class:`.CheckConstraint`\",\\n+        \":paramref:`.CheckConstraint.sqltext`\",\\n+    )\\n     def __init__(\\n         self,\\n         sqltext,\\n@@ -2925,7 +2931,7 @@ def __init__(\\n \\n         \"\"\"\\n \\n-        self.sqltext = _literal_as_text(sqltext, warn=False)\\n+        self.sqltext = _literal_as_text(sqltext, allow_coercion_to_text=True)\\n \\n         columns = []\\n         visitors.traverse(self.sqltext, {}, {\"column\": columns.append})', '@@ -948,6 +948,8 @@ def bind_expression(self, bindvalue):\\n     _python_UUID = None\\n \\n \\n+IDX_USING = re.compile(r\"^(?:btree|hash|gist|gin|[\\\\w_]+)$\", re.I)\\n+\\n AUTOCOMMIT_REGEXP = re.compile(\\n     r\"\\\\s*(?:UPDATE|INSERT|CREATE|DELETE|DROP|ALTER|GRANT|REVOKE|\"\\n     \"IMPORT FOREIGN SCHEMA|REFRESH MATERIALIZED VIEW|TRUNCATE)\",\\n@@ -1908,7 +1910,10 @@ def visit_create_index(self, create):\\n \\n         using = index.dialect_options[\"postgresql\"][\"using\"]\\n         if using:\\n-            text += \"USING %s \" % preparer.quote(using)\\n+            text += (\\n+                \"USING %s \"\\n+                % self.preparer.validate_sql_phrase(using, IDX_USING).lower()\\n+            )\\n \\n         ops = index.dialect_options[\"postgresql\"][\"ops\"]\\n         text += \"(%s)\" % (\\n@@ -1983,7 +1988,9 @@ def visit_exclude_constraint(self, constraint, **kw):\\n                 \"%s WITH %s\" % (self.sql_compiler.process(expr, **kw), op)\\n             )\\n         text += \"EXCLUDE USING %s (%s)\" % (\\n-            constraint.using,\\n+            self.preparer.validate_sql_phrase(\\n+                constraint.using, IDX_USING\\n+            ).lower(),\\n             \", \".join(elements),\\n         )\\n         if constraint.where is not None:', '@@ -91,6 +91,11 @@ class ExcludeConstraint(ColumnCollectionConstraint):\\n \\n     where = None\\n \\n+    @elements._document_text_coercion(\\n+        \"where\",\\n+        \":class:`.ExcludeConstraint`\",\\n+        \":paramref:`.ExcludeConstraint.where`\",\\n+    )\\n     def __init__(self, *elements, **kw):\\n         r\"\"\"\\n         Create an :class:`.ExcludeConstraint` object.\\n@@ -123,21 +128,15 @@ def __init__(self, *elements, **kw):\\n             )\\n \\n         :param \\\\*elements:\\n+\\n           A sequence of two tuples of the form ``(column, operator)`` where\\n           \"column\" is a SQL expression element or a raw SQL string, most\\n-          typically a :class:`.Column` object,\\n-          and \"operator\" is a string containing the operator to use.\\n-\\n-          .. note::\\n-\\n-                A plain string passed for the value of \"column\" is interpreted\\n-                as an arbitrary SQL  expression; when passing a plain string,\\n-                any necessary quoting and escaping syntaxes must be applied\\n-                manually. In order to specify a column name when a\\n-                :class:`.Column` object is not available, while ensuring that\\n-                any necessary quoting rules take effect, an ad-hoc\\n-                :class:`.Column` or :func:`.sql.expression.column` object may\\n-                be used.\\n+          typically a :class:`.Column` object, and \"operator\" is a string\\n+          containing the operator to use.   In order to specify a column name\\n+          when a  :class:`.Column` object is not available, while ensuring\\n+          that any necessary quoting rules take effect, an ad-hoc\\n+          :class:`.Column` or :func:`.sql.expression.column` object should be\\n+          used.\\n \\n         :param name:\\n           Optional, the in-database name of this constraint.\\n@@ -159,12 +158,6 @@ def __init__(self, *elements, **kw):\\n           If set, emit WHERE <predicate> when issuing DDL\\n           for this constraint.\\n \\n-          .. note::\\n-\\n-                A plain string passed here is interpreted as an arbitrary SQL\\n-                expression; when passing a plain string, any necessary quoting\\n-                and escaping syntaxes must be applied manually.\\n-\\n         \"\"\"\\n         columns = []\\n         render_exprs = []\\n@@ -184,11 +177,12 @@ def __init__(self, *elements, **kw):\\n                 # backwards compat\\n                 self.operators[name] = operator\\n \\n-            expr = expression._literal_as_text(expr)\\n+            expr = expression._literal_as_column(expr)\\n \\n             render_exprs.append((expr, name, operator))\\n \\n         self._render_exprs = render_exprs\\n+\\n         ColumnCollectionConstraint.__init__(\\n             self,\\n             *columns,\\n@@ -199,7 +193,9 @@ def __init__(self, *elements, **kw):\\n         self.using = kw.get(\"using\", \"gist\")\\n         where = kw.get(\"where\")\\n         if where is not None:\\n-            self.where = expression._literal_as_text(where)\\n+            self.where = expression._literal_as_text(\\n+                where, allow_coercion_to_text=True\\n+            )\\n \\n     def copy(self, **kw):\\n         elements = [(col, self.operators[col]) for col in self.columns.keys()]', '@@ -1257,7 +1257,9 @@ def execute(self, clause, params=None, mapper=None, bind=None, **kw):\\n             in order to execute the statement.\\n \\n         \"\"\"\\n-        clause = expression._literal_as_text(clause)\\n+        clause = expression._literal_as_text(\\n+            clause, allow_coercion_to_text=True\\n+        )\\n \\n         if bind is None:\\n             bind = self.get_bind(mapper, clause=clause, **kw)', '@@ -9,11 +9,12 @@\\n functionality.\"\"\"\\n \\n import re\\n-import textwrap\\n import warnings\\n \\n from . import compat\\n from .langhelpers import decorator\\n+from .langhelpers import inject_docstring_text\\n+from .langhelpers import inject_param_text\\n from .. import exc\\n \\n \\n@@ -247,64 +248,3 @@ def warned(fn, *args, **kwargs):\\n     decorated.__doc__ = doc\\n     decorated._sa_warn = lambda: warnings.warn(message, wtype, stacklevel=3)\\n     return decorated\\n-\\n-\\n-def _dedent_docstring(text):\\n-    split_text = text.split(\"\\\\n\", 1)\\n-    if len(split_text) == 1:\\n-        return text\\n-    else:\\n-        firstline, remaining = split_text\\n-    if not firstline.startswith(\" \"):\\n-        return firstline + \"\\\\n\" + textwrap.dedent(remaining)\\n-    else:\\n-        return textwrap.dedent(text)\\n-\\n-\\n-def inject_docstring_text(doctext, injecttext, pos):\\n-    doctext = _dedent_docstring(doctext or \"\")\\n-    lines = doctext.split(\"\\\\n\")\\n-    injectlines = textwrap.dedent(injecttext).split(\"\\\\n\")\\n-    if injectlines[0]:\\n-        injectlines.insert(0, \"\")\\n-\\n-    blanks = [num for num, line in enumerate(lines) if not line.strip()]\\n-    blanks.insert(0, 0)\\n-\\n-    inject_pos = blanks[min(pos, len(blanks) - 1)]\\n-\\n-    lines = lines[0:inject_pos] + injectlines + lines[inject_pos:]\\n-    return \"\\\\n\".join(lines)\\n-\\n-\\n-def inject_param_text(doctext, inject_params):\\n-    doclines = doctext.splitlines()\\n-    lines = []\\n-\\n-    to_inject = None\\n-    while doclines:\\n-        line = doclines.pop(0)\\n-        if to_inject is None:\\n-            m = re.match(r\"(\\\\s+):param (.+?):\", line)\\n-            if m:\\n-                param = m.group(2)\\n-                if param in inject_params:\\n-                    # default indent to that of :param: plus one\\n-                    indent = \" \" * len(m.group(1)) + \" \"\\n-\\n-                    # but if the next line has text, use that line\\'s\\n-                    # indentntation\\n-                    if doclines:\\n-                        m2 = re.match(r\"(\\\\s+)\\\\S\", doclines[0])\\n-                        if m2:\\n-                            indent = \" \" * len(m2.group(1))\\n-\\n-                    to_inject = indent + inject_params[param]\\n-        elif not line.rstrip():\\n-            lines.append(line)\\n-            lines.append(to_inject)\\n-            lines.append(\"\\\\n\")\\n-            to_inject = None\\n-        lines.append(line)\\n-\\n-    return \"\\\\n\".join(lines)', '@@ -16,6 +16,7 @@\\n import operator\\n import re\\n import sys\\n+import textwrap\\n import types\\n import warnings\\n \\n@@ -1572,3 +1573,82 @@ def quoted_token_parser(value):\\n         idx += 1\\n \\n     return [\"\".join(token) for token in result]\\n+\\n+\\n+def add_parameter_text(params, text):\\n+    params = _collections.to_list(params)\\n+\\n+    def decorate(fn):\\n+        doc = fn.__doc__ is not None and fn.__doc__ or \"\"\\n+        if doc:\\n+            doc = inject_param_text(doc, {param: text for param in params})\\n+        fn.__doc__ = doc\\n+        return fn\\n+\\n+    return decorate\\n+\\n+\\n+def _dedent_docstring(text):\\n+    split_text = text.split(\"\\\\n\", 1)\\n+    if len(split_text) == 1:\\n+        return text\\n+    else:\\n+        firstline, remaining = split_text\\n+    if not firstline.startswith(\" \"):\\n+        return firstline + \"\\\\n\" + textwrap.dedent(remaining)\\n+    else:\\n+        return textwrap.dedent(text)\\n+\\n+\\n+def inject_docstring_text(doctext, injecttext, pos):\\n+    doctext = _dedent_docstring(doctext or \"\")\\n+    lines = doctext.split(\"\\\\n\")\\n+    injectlines = textwrap.dedent(injecttext).split(\"\\\\n\")\\n+    if injectlines[0]:\\n+        injectlines.insert(0, \"\")\\n+\\n+    blanks = [num for num, line in enumerate(lines) if not line.strip()]\\n+    blanks.insert(0, 0)\\n+\\n+    inject_pos = blanks[min(pos, len(blanks) - 1)]\\n+\\n+    lines = lines[0:inject_pos] + injectlines + lines[inject_pos:]\\n+    return \"\\\\n\".join(lines)\\n+\\n+\\n+def inject_param_text(doctext, inject_params):\\n+    doclines = doctext.splitlines()\\n+    lines = []\\n+\\n+    to_inject = None\\n+    while doclines:\\n+        line = doclines.pop(0)\\n+        if to_inject is None:\\n+            m = re.match(r\"(\\\\s+):param (?:\\\\\\\\\\\\*\\\\*?)?(.+?):\", line)\\n+            if m:\\n+                param = m.group(2)\\n+                if param in inject_params:\\n+                    # default indent to that of :param: plus one\\n+                    indent = \" \" * len(m.group(1)) + \" \"\\n+\\n+                    # but if the next line has text, use that line\\'s\\n+                    # indentntation\\n+                    if doclines:\\n+                        m2 = re.match(r\"(\\\\s+)\\\\S\", doclines[0])\\n+                        if m2:\\n+                            indent = \" \" * len(m2.group(1))\\n+\\n+                    to_inject = indent + inject_params[param]\\n+        elif line.lstrip().startswith(\":param \"):\\n+            lines.append(\"\\\\n\")\\n+            lines.append(to_inject)\\n+            lines.append(\"\\\\n\")\\n+            to_inject = None\\n+        elif not line.rstrip():\\n+            lines.append(line)\\n+            lines.append(to_inject)\\n+            lines.append(\"\\\\n\")\\n+            to_inject = None\\n+        lines.append(line)\\n+\\n+    return \"\\\\n\".join(lines)'], 'file': ['lib/sqlalchemy/sql/elements.py', 'lib/sqlalchemy/sql/compiler.py', 'lib/sqlalchemy/sql/expression.py', 'lib/sqlalchemy/sql/selectable.py', 'lib/sqlalchemy/sql/schema.py', 'lib/sqlalchemy/dialects/postgresql/base.py', 'lib/sqlalchemy/dialects/postgresql/ext.py', 'lib/sqlalchemy/orm/session.py', 'lib/sqlalchemy/util/deprecations.py', 'lib/sqlalchemy/util/langhelpers.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('a12c40ec-6fb2-4d76-a91a-b1e5da8d18ab'), UUID('5883278e-8000-4db9-b46d-5ee1d94a4499'), UUID('71f2b08a-97a3-4ed5-b430-7fca7f9b0d8d'), UUID('2b6c19c7-2d00-40e5-b401-ae3dd056e825'), UUID('7254767f-8cdd-4da5-9240-2a875ff05bb2'), UUID('78135a00-6466-4a9b-90cd-988c64ef2b58'), UUID('5ad89b8e-944c-4bbd-ad6d-82ba3352218d'), UUID('db77391b-62ee-410c-b3ec-1ab2205398e8'), UUID('7d285e4c-b7f3-474e-9305-c2f5aa4fdc7a'), UUID('5300f7ee-f752-4b12-a720-6a4601796caa')]}\n",
      "ERROR:root:Error in {'repo': 'sqlalchemy/sqlalchemy', 'vulnerability_id': '2019-7164', 'commit': '30307c4616ad67c01ddae2e1e8e34fabf6028414', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -37,6 +37,20 @@ def _clone(element, **kw):\\n     return element._clone()\\n \\n \\n+def _document_text_coercion(paramname, meth_rst, param_rst):\\n+    return util.add_parameter_text(\\n+        paramname,\\n+        (\\n+            \".. warning:: \"\\n+            \"The %s argument to %s can be passed as a Python string argument, \"\\n+            \"which will be treated \"\\n+            \"as **trusted SQL text** and rendered as given.  **DO NOT PASS \"\\n+            \"UNTRUSTED INPUT TO THIS PARAMETER**.\"\\n+        )\\n+        % (param_rst, meth_rst),\\n+    )\\n+\\n+\\n def collate(expression, collation):\\n     \"\"\"Return the clause ``expression COLLATE collation``.\\n \\n@@ -1343,6 +1357,7 @@ def repl(m):\\n             \"refer to the :meth:`.TextClause.columns` method.\",\\n         ),\\n     )\\n+    @_document_text_coercion(\"text\", \":func:`.text`\", \":paramref:`.text.text`\")\\n     def _create_text(\\n         self, text, bind=None, bindparams=None, typemap=None, autocommit=None\\n     ):\\n@@ -4430,32 +4445,64 @@ def _literal_and_labels_as_label_reference(element):\\n \\n \\n def _expression_literal_as_text(element):\\n-    return _literal_as_text(element, warn=True)\\n+    return _literal_as_text(element)\\n \\n \\n-def _literal_as_text(element, warn=False):\\n+def _literal_as(element, text_fallback):\\n     if isinstance(element, Visitable):\\n         return element\\n     elif hasattr(element, \"__clause_element__\"):\\n         return element.__clause_element__()\\n     elif isinstance(element, util.string_types):\\n-        if warn:\\n-            util.warn_limited(\\n-                \"Textual SQL expression %(expr)r should be \"\\n-                \"explicitly declared as text(%(expr)r)\",\\n-                {\"expr\": util.ellipses_string(element)},\\n-            )\\n-\\n-        return TextClause(util.text_type(element))\\n+        return text_fallback(element)\\n     elif isinstance(element, (util.NoneType, bool)):\\n         return _const_expr(element)\\n     else:\\n         raise exc.ArgumentError(\\n-            \"SQL expression object or string expected, got object of type %r \"\\n+            \"SQL expression object expected, got object of type %r \"\\n             \"instead\" % type(element)\\n         )\\n \\n \\n+def _literal_as_text(element, allow_coercion_to_text=False):\\n+    if allow_coercion_to_text:\\n+        return _literal_as(element, TextClause)\\n+    else:\\n+        return _literal_as(element, _no_text_coercion)\\n+\\n+\\n+def _literal_as_column(element):\\n+    return _literal_as(element, ColumnClause)\\n+\\n+\\n+def _no_column_coercion(element):\\n+    element = str(element)\\n+    guess_is_literal = not _guess_straight_column.match(element)\\n+    raise exc.ArgumentError(\\n+        \"Textual column expression %(column)r should be \"\\n+        \"explicitly declared with text(%(column)r), \"\\n+        \"or use %(literal_column)s(%(column)r) \"\\n+        \"for more specificity\"\\n+        % {\\n+            \"column\": util.ellipses_string(element),\\n+            \"literal_column\": \"literal_column\"\\n+            if guess_is_literal\\n+            else \"column\",\\n+        }\\n+    )\\n+\\n+\\n+def _no_text_coercion(element, exc_cls=exc.ArgumentError, extra=None):\\n+    raise exc_cls(\\n+        \"%(extra)sTextual SQL expression %(expr)r should be \"\\n+        \"explicitly declared as text(%(expr)r)\"\\n+        % {\\n+            \"expr\": util.ellipses_string(element),\\n+            \"extra\": \"%s \" % extra if extra else \"\",\\n+        }\\n+    )\\n+\\n+\\n def _no_literals(element):\\n     if hasattr(element, \"__clause_element__\"):\\n         return element.__clause_element__()\\n@@ -4529,23 +4576,7 @@ def _interpret_as_column_or_from(element):\\n     elif isinstance(element, (numbers.Number)):\\n         return ColumnClause(str(element), is_literal=True)\\n     else:\\n-        element = str(element)\\n-        # give into temptation, as this fact we are guessing about\\n-        # is not one we\\'ve previously ever needed our users tell us;\\n-        # but let them know we are not happy about it\\n-        guess_is_literal = not _guess_straight_column.match(element)\\n-        util.warn_limited(\\n-            \"Textual column expression %(column)r should be \"\\n-            \"explicitly declared with text(%(column)r), \"\\n-            \"or use %(literal_column)s(%(column)r) \"\\n-            \"for more specificity\",\\n-            {\\n-                \"column\": util.ellipses_string(element),\\n-                \"literal_column\": \"literal_column\"\\n-                if guess_is_literal\\n-                else \"column\",\\n-            },\\n-        )\\n+        _no_column_coercion(element)\\n     return ColumnClause(element, is_literal=guess_is_literal)\\n \\n ', '@@ -139,8 +139,16 @@\\n )\\n \\n LEGAL_CHARACTERS = re.compile(r\"^[A-Z0-9_$]+$\", re.I)\\n+LEGAL_CHARACTERS_PLUS_SPACE = re.compile(r\"^[A-Z0-9_ $]+$\", re.I)\\n ILLEGAL_INITIAL_CHARACTERS = {str(x) for x in range(0, 10)}.union([\"$\"])\\n \\n+FK_ON_DELETE = re.compile(\\n+    r\"^(?:RESTRICT|CASCADE|SET NULL|NO ACTION|SET DEFAULT)$\", re.I\\n+)\\n+FK_ON_UPDATE = re.compile(\\n+    r\"^(?:RESTRICT|CASCADE|SET NULL|NO ACTION|SET DEFAULT)$\", re.I\\n+)\\n+FK_INITIALLY = re.compile(r\"^(?:DEFERRED|IMMEDIATE)$\", re.I)\\n BIND_PARAMS = re.compile(r\"(?<![:\\\\w\\\\$\\\\x5c]):([\\\\w\\\\$]+)(?![:\\\\w\\\\$])\", re.UNICODE)\\n BIND_PARAMS_ESC = re.compile(r\"\\\\x5c(:[\\\\w\\\\$]*)(?![:\\\\w\\\\$])\", re.UNICODE)\\n \\n@@ -758,12 +766,11 @@ def visit_textual_label_reference(\\n             else:\\n                 col = with_cols[element.element]\\n         except KeyError:\\n-            # treat it like text()\\n-            util.warn_limited(\\n-                \"Can\\'t resolve label reference %r; converting to text()\",\\n-                util.ellipses_string(element.element),\\n+            elements._no_text_coercion(\\n+                element.element,\\n+                exc.CompileError,\\n+                \"Can\\'t resolve label reference for ORDER BY / GROUP BY.\",\\n             )\\n-            return self.process(element._text_clause)\\n         else:\\n             kwargs[\"render_label_as_label\"] = col\\n             return self.process(\\n@@ -1076,10 +1083,24 @@ def visit_function(self, func, add_to_result_map=None, **kwargs):\\n                 if func._has_args:\\n                     name += \"%(expr)s\"\\n             else:\\n-                name = func.name + \"%(expr)s\"\\n-            return \".\".join(list(func.packagenames) + [name]) % {\\n-                \"expr\": self.function_argspec(func, **kwargs)\\n-            }\\n+                name = func.name\\n+                name = (\\n+                    self.preparer.quote(name)\\n+                    if self.preparer._requires_quotes_illegal_chars(name)\\n+                    else name\\n+                )\\n+                name = name + \"%(expr)s\"\\n+            return \".\".join(\\n+                [\\n+                    (\\n+                        self.preparer.quote(tok)\\n+                        if self.preparer._requires_quotes_illegal_chars(tok)\\n+                        else tok\\n+                    )\\n+                    for tok in func.packagenames\\n+                ]\\n+                + [name]\\n+            ) % {\"expr\": self.function_argspec(func, **kwargs)}\\n \\n     def visit_next_value_func(self, next_value, **kw):\\n         return self.visit_sequence(next_value.sequence)\\n@@ -3153,9 +3174,13 @@ def visit_unique_constraint(self, constraint):\\n     def define_constraint_cascades(self, constraint):\\n         text = \"\"\\n         if constraint.ondelete is not None:\\n-            text += \" ON DELETE %s\" % constraint.ondelete\\n+            text += \" ON DELETE %s\" % self.preparer.validate_sql_phrase(\\n+                constraint.ondelete, FK_ON_DELETE\\n+            )\\n         if constraint.onupdate is not None:\\n-            text += \" ON UPDATE %s\" % constraint.onupdate\\n+            text += \" ON UPDATE %s\" % self.preparer.validate_sql_phrase(\\n+                constraint.onupdate, FK_ON_UPDATE\\n+            )\\n         return text\\n \\n     def define_constraint_deferrability(self, constraint):\\n@@ -3166,7 +3191,9 @@ def define_constraint_deferrability(self, constraint):\\n             else:\\n                 text += \" NOT DEFERRABLE\"\\n         if constraint.initially is not None:\\n-            text += \" INITIALLY %s\" % constraint.initially\\n+            text += \" INITIALLY %s\" % self.preparer.validate_sql_phrase(\\n+                constraint.initially, FK_INITIALLY\\n+            )\\n         return text\\n \\n     def define_constraint_match(self, constraint):\\n@@ -3416,6 +3443,24 @@ def _unescape_identifier(self, value):\\n \\n         return value.replace(self.escape_to_quote, self.escape_quote)\\n \\n+    def validate_sql_phrase(self, element, reg):\\n+        \"\"\"keyword sequence filter.\\n+\\n+        a filter for elements that are intended to represent keyword sequences,\\n+        such as \"INITIALLY\", \"INTIALLY DEFERRED\", etc.   no special characters\\n+        should be present.\\n+\\n+        .. versionadded:: 1.3\\n+\\n+        \"\"\"\\n+\\n+        if element is not None and not reg.match(element):\\n+            raise exc.CompileError(\\n+                \"Unexpected SQL phrase: %r (matching against %r)\"\\n+                % (element, reg.pattern)\\n+            )\\n+        return element\\n+\\n     def quote_identifier(self, value):\\n         \"\"\"Quote an identifier.\\n \\n@@ -3439,6 +3484,11 @@ def _requires_quotes(self, value):\\n             or (lc_value != value)\\n         )\\n \\n+    def _requires_quotes_illegal_chars(self, value):\\n+        \"\"\"Return True if the given identifier requires quoting, but\\n+        not taking case convention into account.\"\"\"\\n+        return not self.legal_characters.match(util.text_type(value))\\n+\\n     def quote_schema(self, schema, force=None):\\n         \"\"\"Conditionally quote a schema name.\\n ', '@@ -101,6 +101,7 @@\\n from .elements import _is_column  # noqa\\n from .elements import _labeled  # noqa\\n from .elements import _literal_as_binds  # noqa\\n+from .elements import _literal_as_column  # noqa\\n from .elements import _literal_as_label_reference  # noqa\\n from .elements import _literal_as_text  # noqa\\n from .elements import _only_column_elements  # noqa', '@@ -31,19 +31,20 @@\\n from .elements import _clone\\n from .elements import _cloned_difference\\n from .elements import _cloned_intersection\\n+from .elements import _document_text_coercion\\n from .elements import _expand_cloned\\n from .elements import _interpret_as_column_or_from\\n from .elements import _literal_and_labels_as_label_reference\\n from .elements import _literal_as_label_reference\\n from .elements import _literal_as_text\\n+from .elements import _no_text_coercion\\n from .elements import _select_iterables\\n from .elements import and_\\n from .elements import BindParameter\\n from .elements import ClauseElement\\n from .elements import ClauseList\\n from .elements import Grouping\\n from .elements import literal_column\\n-from .elements import TextClause\\n from .elements import True_\\n from .elements import UnaryExpression\\n from .. import exc\\n@@ -55,14 +56,7 @@ def _interpret_as_from(element):\\n     insp = inspection.inspect(element, raiseerr=False)\\n     if insp is None:\\n         if isinstance(element, util.string_types):\\n-            util.warn_limited(\\n-                \"Textual SQL FROM expression %(expr)r should be \"\\n-                \"explicitly declared as text(%(expr)r), \"\\n-                \"or use table(%(expr)r) for more specificity\",\\n-                {\"expr\": util.ellipses_string(element)},\\n-            )\\n-\\n-            return TextClause(util.text_type(element))\\n+            _no_text_coercion(element)\\n     try:\\n         return insp.selectable\\n     except AttributeError:\\n@@ -266,6 +260,11 @@ class HasPrefixes(object):\\n     _prefixes = ()\\n \\n     @_generative\\n+    @_document_text_coercion(\\n+        \"expr\",\\n+        \":meth:`.HasPrefixes.prefix_with`\",\\n+        \":paramref:`.HasPrefixes.prefix_with.*expr`\",\\n+    )\\n     def prefix_with(self, *expr, **kw):\\n         r\"\"\"Add one or more expressions following the statement keyword, i.e.\\n         SELECT, INSERT, UPDATE, or DELETE. Generative.\\n@@ -297,14 +296,22 @@ def prefix_with(self, *expr, **kw):\\n \\n     def _setup_prefixes(self, prefixes, dialect=None):\\n         self._prefixes = self._prefixes + tuple(\\n-            [(_literal_as_text(p, warn=False), dialect) for p in prefixes]\\n+            [\\n+                (_literal_as_text(p, allow_coercion_to_text=True), dialect)\\n+                for p in prefixes\\n+            ]\\n         )\\n \\n \\n class HasSuffixes(object):\\n     _suffixes = ()\\n \\n     @_generative\\n+    @_document_text_coercion(\\n+        \"expr\",\\n+        \":meth:`.HasSuffixes.suffix_with`\",\\n+        \":paramref:`.HasSuffixes.suffix_with.*expr`\",\\n+    )\\n     def suffix_with(self, *expr, **kw):\\n         r\"\"\"Add one or more expressions following the statement as a whole.\\n \\n@@ -335,7 +342,10 @@ def suffix_with(self, *expr, **kw):\\n \\n     def _setup_suffixes(self, suffixes, dialect=None):\\n         self._suffixes = self._suffixes + tuple(\\n-            [(_literal_as_text(p, warn=False), dialect) for p in suffixes]\\n+            [\\n+                (_literal_as_text(p, allow_coercion_to_text=True), dialect)\\n+                for p in suffixes\\n+            ]\\n         )\\n \\n ', '@@ -42,6 +42,7 @@\\n from .base import DialectKWArgs\\n from .base import SchemaEventTarget\\n from .elements import _as_truncated\\n+from .elements import _document_text_coercion\\n from .elements import _literal_as_text\\n from .elements import ClauseElement\\n from .elements import ColumnClause\\n@@ -2884,6 +2885,11 @@ class CheckConstraint(ColumnCollectionConstraint):\\n \\n     _allow_multiple_tables = True\\n \\n+    @_document_text_coercion(\\n+        \"sqltext\",\\n+        \":class:`.CheckConstraint`\",\\n+        \":paramref:`.CheckConstraint.sqltext`\",\\n+    )\\n     def __init__(\\n         self,\\n         sqltext,\\n@@ -2925,7 +2931,7 @@ def __init__(\\n \\n         \"\"\"\\n \\n-        self.sqltext = _literal_as_text(sqltext, warn=False)\\n+        self.sqltext = _literal_as_text(sqltext, allow_coercion_to_text=True)\\n \\n         columns = []\\n         visitors.traverse(self.sqltext, {}, {\"column\": columns.append})', '@@ -948,6 +948,8 @@ def bind_expression(self, bindvalue):\\n     _python_UUID = None\\n \\n \\n+IDX_USING = re.compile(r\"^(?:btree|hash|gist|gin|[\\\\w_]+)$\", re.I)\\n+\\n AUTOCOMMIT_REGEXP = re.compile(\\n     r\"\\\\s*(?:UPDATE|INSERT|CREATE|DELETE|DROP|ALTER|GRANT|REVOKE|\"\\n     \"IMPORT FOREIGN SCHEMA|REFRESH MATERIALIZED VIEW|TRUNCATE)\",\\n@@ -1908,7 +1910,10 @@ def visit_create_index(self, create):\\n \\n         using = index.dialect_options[\"postgresql\"][\"using\"]\\n         if using:\\n-            text += \"USING %s \" % preparer.quote(using)\\n+            text += (\\n+                \"USING %s \"\\n+                % self.preparer.validate_sql_phrase(using, IDX_USING).lower()\\n+            )\\n \\n         ops = index.dialect_options[\"postgresql\"][\"ops\"]\\n         text += \"(%s)\" % (\\n@@ -1983,7 +1988,9 @@ def visit_exclude_constraint(self, constraint, **kw):\\n                 \"%s WITH %s\" % (self.sql_compiler.process(expr, **kw), op)\\n             )\\n         text += \"EXCLUDE USING %s (%s)\" % (\\n-            constraint.using,\\n+            self.preparer.validate_sql_phrase(\\n+                constraint.using, IDX_USING\\n+            ).lower(),\\n             \", \".join(elements),\\n         )\\n         if constraint.where is not None:', '@@ -91,6 +91,11 @@ class ExcludeConstraint(ColumnCollectionConstraint):\\n \\n     where = None\\n \\n+    @elements._document_text_coercion(\\n+        \"where\",\\n+        \":class:`.ExcludeConstraint`\",\\n+        \":paramref:`.ExcludeConstraint.where`\",\\n+    )\\n     def __init__(self, *elements, **kw):\\n         r\"\"\"\\n         Create an :class:`.ExcludeConstraint` object.\\n@@ -123,21 +128,15 @@ def __init__(self, *elements, **kw):\\n             )\\n \\n         :param \\\\*elements:\\n+\\n           A sequence of two tuples of the form ``(column, operator)`` where\\n           \"column\" is a SQL expression element or a raw SQL string, most\\n-          typically a :class:`.Column` object,\\n-          and \"operator\" is a string containing the operator to use.\\n-\\n-          .. note::\\n-\\n-                A plain string passed for the value of \"column\" is interpreted\\n-                as an arbitrary SQL  expression; when passing a plain string,\\n-                any necessary quoting and escaping syntaxes must be applied\\n-                manually. In order to specify a column name when a\\n-                :class:`.Column` object is not available, while ensuring that\\n-                any necessary quoting rules take effect, an ad-hoc\\n-                :class:`.Column` or :func:`.sql.expression.column` object may\\n-                be used.\\n+          typically a :class:`.Column` object, and \"operator\" is a string\\n+          containing the operator to use.   In order to specify a column name\\n+          when a  :class:`.Column` object is not available, while ensuring\\n+          that any necessary quoting rules take effect, an ad-hoc\\n+          :class:`.Column` or :func:`.sql.expression.column` object should be\\n+          used.\\n \\n         :param name:\\n           Optional, the in-database name of this constraint.\\n@@ -159,12 +158,6 @@ def __init__(self, *elements, **kw):\\n           If set, emit WHERE <predicate> when issuing DDL\\n           for this constraint.\\n \\n-          .. note::\\n-\\n-                A plain string passed here is interpreted as an arbitrary SQL\\n-                expression; when passing a plain string, any necessary quoting\\n-                and escaping syntaxes must be applied manually.\\n-\\n         \"\"\"\\n         columns = []\\n         render_exprs = []\\n@@ -184,11 +177,12 @@ def __init__(self, *elements, **kw):\\n                 # backwards compat\\n                 self.operators[name] = operator\\n \\n-            expr = expression._literal_as_text(expr)\\n+            expr = expression._literal_as_column(expr)\\n \\n             render_exprs.append((expr, name, operator))\\n \\n         self._render_exprs = render_exprs\\n+\\n         ColumnCollectionConstraint.__init__(\\n             self,\\n             *columns,\\n@@ -199,7 +193,9 @@ def __init__(self, *elements, **kw):\\n         self.using = kw.get(\"using\", \"gist\")\\n         where = kw.get(\"where\")\\n         if where is not None:\\n-            self.where = expression._literal_as_text(where)\\n+            self.where = expression._literal_as_text(\\n+                where, allow_coercion_to_text=True\\n+            )\\n \\n     def copy(self, **kw):\\n         elements = [(col, self.operators[col]) for col in self.columns.keys()]', '@@ -1257,7 +1257,9 @@ def execute(self, clause, params=None, mapper=None, bind=None, **kw):\\n             in order to execute the statement.\\n \\n         \"\"\"\\n-        clause = expression._literal_as_text(clause)\\n+        clause = expression._literal_as_text(\\n+            clause, allow_coercion_to_text=True\\n+        )\\n \\n         if bind is None:\\n             bind = self.get_bind(mapper, clause=clause, **kw)', '@@ -9,11 +9,12 @@\\n functionality.\"\"\"\\n \\n import re\\n-import textwrap\\n import warnings\\n \\n from . import compat\\n from .langhelpers import decorator\\n+from .langhelpers import inject_docstring_text\\n+from .langhelpers import inject_param_text\\n from .. import exc\\n \\n \\n@@ -247,64 +248,3 @@ def warned(fn, *args, **kwargs):\\n     decorated.__doc__ = doc\\n     decorated._sa_warn = lambda: warnings.warn(message, wtype, stacklevel=3)\\n     return decorated\\n-\\n-\\n-def _dedent_docstring(text):\\n-    split_text = text.split(\"\\\\n\", 1)\\n-    if len(split_text) == 1:\\n-        return text\\n-    else:\\n-        firstline, remaining = split_text\\n-    if not firstline.startswith(\" \"):\\n-        return firstline + \"\\\\n\" + textwrap.dedent(remaining)\\n-    else:\\n-        return textwrap.dedent(text)\\n-\\n-\\n-def inject_docstring_text(doctext, injecttext, pos):\\n-    doctext = _dedent_docstring(doctext or \"\")\\n-    lines = doctext.split(\"\\\\n\")\\n-    injectlines = textwrap.dedent(injecttext).split(\"\\\\n\")\\n-    if injectlines[0]:\\n-        injectlines.insert(0, \"\")\\n-\\n-    blanks = [num for num, line in enumerate(lines) if not line.strip()]\\n-    blanks.insert(0, 0)\\n-\\n-    inject_pos = blanks[min(pos, len(blanks) - 1)]\\n-\\n-    lines = lines[0:inject_pos] + injectlines + lines[inject_pos:]\\n-    return \"\\\\n\".join(lines)\\n-\\n-\\n-def inject_param_text(doctext, inject_params):\\n-    doclines = doctext.splitlines()\\n-    lines = []\\n-\\n-    to_inject = None\\n-    while doclines:\\n-        line = doclines.pop(0)\\n-        if to_inject is None:\\n-            m = re.match(r\"(\\\\s+):param (.+?):\", line)\\n-            if m:\\n-                param = m.group(2)\\n-                if param in inject_params:\\n-                    # default indent to that of :param: plus one\\n-                    indent = \" \" * len(m.group(1)) + \" \"\\n-\\n-                    # but if the next line has text, use that line\\'s\\n-                    # indentntation\\n-                    if doclines:\\n-                        m2 = re.match(r\"(\\\\s+)\\\\S\", doclines[0])\\n-                        if m2:\\n-                            indent = \" \" * len(m2.group(1))\\n-\\n-                    to_inject = indent + inject_params[param]\\n-        elif not line.rstrip():\\n-            lines.append(line)\\n-            lines.append(to_inject)\\n-            lines.append(\"\\\\n\")\\n-            to_inject = None\\n-        lines.append(line)\\n-\\n-    return \"\\\\n\".join(lines)', '@@ -16,6 +16,7 @@\\n import operator\\n import re\\n import sys\\n+import textwrap\\n import types\\n import warnings\\n \\n@@ -1572,3 +1573,82 @@ def quoted_token_parser(value):\\n         idx += 1\\n \\n     return [\"\".join(token) for token in result]\\n+\\n+\\n+def add_parameter_text(params, text):\\n+    params = _collections.to_list(params)\\n+\\n+    def decorate(fn):\\n+        doc = fn.__doc__ is not None and fn.__doc__ or \"\"\\n+        if doc:\\n+            doc = inject_param_text(doc, {param: text for param in params})\\n+        fn.__doc__ = doc\\n+        return fn\\n+\\n+    return decorate\\n+\\n+\\n+def _dedent_docstring(text):\\n+    split_text = text.split(\"\\\\n\", 1)\\n+    if len(split_text) == 1:\\n+        return text\\n+    else:\\n+        firstline, remaining = split_text\\n+    if not firstline.startswith(\" \"):\\n+        return firstline + \"\\\\n\" + textwrap.dedent(remaining)\\n+    else:\\n+        return textwrap.dedent(text)\\n+\\n+\\n+def inject_docstring_text(doctext, injecttext, pos):\\n+    doctext = _dedent_docstring(doctext or \"\")\\n+    lines = doctext.split(\"\\\\n\")\\n+    injectlines = textwrap.dedent(injecttext).split(\"\\\\n\")\\n+    if injectlines[0]:\\n+        injectlines.insert(0, \"\")\\n+\\n+    blanks = [num for num, line in enumerate(lines) if not line.strip()]\\n+    blanks.insert(0, 0)\\n+\\n+    inject_pos = blanks[min(pos, len(blanks) - 1)]\\n+\\n+    lines = lines[0:inject_pos] + injectlines + lines[inject_pos:]\\n+    return \"\\\\n\".join(lines)\\n+\\n+\\n+def inject_param_text(doctext, inject_params):\\n+    doclines = doctext.splitlines()\\n+    lines = []\\n+\\n+    to_inject = None\\n+    while doclines:\\n+        line = doclines.pop(0)\\n+        if to_inject is None:\\n+            m = re.match(r\"(\\\\s+):param (?:\\\\\\\\\\\\*\\\\*?)?(.+?):\", line)\\n+            if m:\\n+                param = m.group(2)\\n+                if param in inject_params:\\n+                    # default indent to that of :param: plus one\\n+                    indent = \" \" * len(m.group(1)) + \" \"\\n+\\n+                    # but if the next line has text, use that line\\'s\\n+                    # indentntation\\n+                    if doclines:\\n+                        m2 = re.match(r\"(\\\\s+)\\\\S\", doclines[0])\\n+                        if m2:\\n+                            indent = \" \" * len(m2.group(1))\\n+\\n+                    to_inject = indent + inject_params[param]\\n+        elif line.lstrip().startswith(\":param \"):\\n+            lines.append(\"\\\\n\")\\n+            lines.append(to_inject)\\n+            lines.append(\"\\\\n\")\\n+            to_inject = None\\n+        elif not line.rstrip():\\n+            lines.append(line)\\n+            lines.append(to_inject)\\n+            lines.append(\"\\\\n\")\\n+            to_inject = None\\n+        lines.append(line)\\n+\\n+    return \"\\\\n\".join(lines)'], 'file': ['lib/sqlalchemy/sql/elements.py', 'lib/sqlalchemy/sql/compiler.py', 'lib/sqlalchemy/sql/expression.py', 'lib/sqlalchemy/sql/selectable.py', 'lib/sqlalchemy/sql/schema.py', 'lib/sqlalchemy/dialects/postgresql/base.py', 'lib/sqlalchemy/dialects/postgresql/ext.py', 'lib/sqlalchemy/orm/session.py', 'lib/sqlalchemy/util/deprecations.py', 'lib/sqlalchemy/util/langhelpers.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('a12c40ec-6fb2-4d76-a91a-b1e5da8d18ab'), UUID('5883278e-8000-4db9-b46d-5ee1d94a4499'), UUID('71f2b08a-97a3-4ed5-b430-7fca7f9b0d8d'), UUID('2b6c19c7-2d00-40e5-b401-ae3dd056e825'), UUID('7254767f-8cdd-4da5-9240-2a875ff05bb2'), UUID('78135a00-6466-4a9b-90cd-988c64ef2b58'), UUID('5ad89b8e-944c-4bbd-ad6d-82ba3352218d'), UUID('db77391b-62ee-410c-b3ec-1ab2205398e8'), UUID('7d285e4c-b7f3-474e-9305-c2f5aa4fdc7a'), UUID('5300f7ee-f752-4b12-a720-6a4601796caa')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 14:0: def _expression_literal_as_text(element):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 14:0: def _expression_literal_as_text(element):\n",
      " 12%|â–ˆâ–        | 211/1800 [02:23<1:15:31,  2.85s/it]ERROR:src.process_code_changes:Error processing commit fa0c0bd59b8588814756942fe4cb5452e76c1dcd\n",
      "ERROR:src.process_code_changes:{'repo': 'ReFirmLabs/binwalk', 'vulnerability_id': '2021-4287', 'commit': 'fa0c0bd59b8588814756942fe4cb5452e76c1dcd', 'commit_source': 'github', 'cwe_id': ['CWE-61', 'CWE-61'], 'patch': ['@@ -4,12 +4,14 @@\\n \\n import os\\n import re\\n+import pwd\\n import stat\\n import shlex\\n import tempfile\\n import subprocess\\n import binwalk.core.common\\n from binwalk.core.compat import *\\n+from binwalk.core.exceptions import ModuleException\\n from binwalk.core.module import Module, Option, Kwarg\\n from binwalk.core.common import file_size, file_md5, unique_file_name, BlockFile\\n \\n@@ -87,11 +89,20 @@ class Extractor(Module):\\n                type=int,\\n                kwargs={\\'max_count\\': 0},\\n                description=\\'Limit the number of extracted files\\'),\\n+        Option(short=\\'0\\',\\n+               long=\\'run-as\\',\\n+               type=str,\\n+               kwargs={\\'runas_user\\': 0},\\n+               description=\"Execute external extraction utilities with the specified user\\'s privileges\"),\\n         #Option(short=\\'u\\',\\n         #       long=\\'limit\\',\\n         #       type=int,\\n         #       kwargs={\\'recursive_max_size\\': 0},\\n         #       description=\"Limit the total size of all extracted files\"),\\n+        Option(short=\\'1\\',\\n+               long=\\'preserve-symlinks\\',\\n+               kwargs={\\'do_not_sanitize_symlinks\\': True},\\n+               description=\"Do not sanitize extracted symlinks that point outside the extraction directory (dangerous)\"),\\n         Option(short=\\'r\\',\\n                long=\\'rm\\',\\n                kwargs={\\'remove_after_execute\\': True},\\n@@ -111,16 +122,43 @@ class Extractor(Module):\\n         Kwarg(name=\\'recursive_max_size\\', default=None),\\n         Kwarg(name=\\'max_count\\', default=None),\\n         Kwarg(name=\\'base_directory\\', default=None),\\n+        Kwarg(name=\\'do_not_sanitize_symlinks\\', default=False),\\n         Kwarg(name=\\'remove_after_execute\\', default=False),\\n         Kwarg(name=\\'load_default_rules\\', default=False),\\n         Kwarg(name=\\'run_extractors\\', default=True),\\n         Kwarg(name=\\'extract_into_subdirs\\', default=False),\\n         Kwarg(name=\\'manual_rules\\', default=[]),\\n         Kwarg(name=\\'matryoshka\\', default=0),\\n         Kwarg(name=\\'enabled\\', default=False),\\n+        Kwarg(name=\\'runas_user\\', default=None),\\n     ]\\n \\n     def load(self):\\n+        self.runas_uid = None\\n+        self.runas_gid = None\\n+\\n+        if self.enabled is True:\\n+            if self.runas_user is None:\\n+                # Get some info about the current user we\\'re running under\\n+                user_info = pwd.getpwuid(os.getuid())\\n+\\n+                # Don\\'t run as root, unless explicitly instructed to\\n+                if user_info.pw_uid == 0:\\n+                    raise ModuleException(\"Binwalk extraction uses many third party utilities, which may not be secure. If you wish to have extraction utilities executed as the current user, use \\'--run-as=%s\\' (binwalk itself must be run as root).\" % user_info.pw_name)\\n+\\n+                # Run external applications as the current user\\n+                self.runas_uid = user_info.pw_uid\\n+                self.runas_gid = user_info.pw_gid\\n+            else:\\n+                # Run external applications as the specified user\\n+                user_info = pwd.getpwnam(self.runas_user)\\n+                self.runas_uid = user_info.pw_uid\\n+                self.runas_gid = user_info.pw_gid\\n+\\n+                # Make sure we\\'ll have permissions to switch to the different user\\n+                if self.runas_uid != os.getuid() and os.getuid() != 0:\\n+                    raise ModuleException(\"In order to execute third party applications as %s, binwalk must be run with root privileges.\" % self.runas_user)\\n+\\n         # Holds a list of extraction rules loaded either from a file or when\\n         # manually specified.\\n         self.extract_rules = []\\n@@ -148,8 +186,8 @@ def load(self):\\n             self.config.verbose = True\\n \\n     def add_pending(self, f):\\n-        # Ignore symlinks\\n-        if os.path.islink(f):\\n+        # Ignore symlinks, don\\'t add new files unless recursion was requested\\n+        if os.path.islink(f) or not self.matryoshka:\\n             return\\n \\n         # Get the file mode to check and see if it\\'s a block/char device\\n@@ -260,30 +298,34 @@ def callback(self, r):\\n \\n                     # If recursion was specified, and the file is not the same\\n                     # one we just dd\\'d\\n-                    if (self.matryoshka and\\n-                        file_path != dd_file_path and\\n-                        scan_extracted_files and\\n-                            self.directory in real_file_path):\\n-                        # If the recursion level of this file is less than or\\n-                        # equal to our desired recursion level\\n-                        if len(real_file_path.split(self.directory)[1].split(os.path.sep)) <= self.matryoshka:\\n-                            # If this is a directory and we are supposed to process directories for this extractor,\\n-                            # then add all files under that directory to the\\n-                            # list of pending files.\\n-                            if os.path.isdir(file_path):\\n-                                for root, dirs, files in os.walk(file_path):\\n-                                    for f in files:\\n-                                        full_path = os.path.join(root, f)\\n-                                        self.add_pending(full_path)\\n-                            # If it\\'s just a file, it to the list of pending\\n-                            # files\\n-                            else:\\n-                                self.add_pending(file_path)\\n+                    if file_path != dd_file_path:\\n+                        # Symlinks can cause security issues if they point outside the extraction directory.\\n+                        self.symlink_sanitizer(file_path, extraction_directory)\\n+\\n+                        # If this is a directory and we are supposed to process directories for this extractor,\\n+                        # then add all files under that directory to the\\n+                        # list of pending files.\\n+                        if os.path.isdir(file_path):\\n+                            for root, dirs, files in os.walk(file_path):\\n+                                # Symlinks can cause security issues if they point outside the extraction directory.\\n+                                self.symlink_sanitizer([os.path.join(root, x) for x in dirs+files], extraction_directory)\\n+\\n+                                for f in files:\\n+                                    full_path = os.path.join(root, f)\\n+\\n+                                    # If the recursion level of this file is less than or equal to our desired recursion level\\n+                                    if len(real_file_path.split(self.directory)[1].split(os.path.sep)) <= self.matryoshka:\\n+                                        if scan_extracted_files and self.directory in real_file_path:\\n+                                                self.add_pending(full_path)\\n+\\n+                        # If it\\'s just a file, it to the list of pending\\n+                        # files\\n+                        elif scan_extracted_files and self.directory in real_file_path:\\n+                            self.add_pending(file_path)\\n \\n                 # Update the last directory listing for the next time we\\n                 # extract a file to this same output directory\\n-                self.last_directory_listing[\\n-                    extraction_directory] = directory_listing\\n+                self.last_directory_listing[extraction_directory] = directory_listing\\n \\n     def append_rule(self, r):\\n         self.extract_rules.append(r.copy())\\n@@ -534,6 +576,9 @@ def build_output_directory(self, path):\\n         else:\\n             output_directory = self.extraction_directories[path]\\n \\n+        # Make sure run-as user can access this directory\\n+        os.chown(output_directory, self.runas_uid, self.runas_gid)\\n+\\n         return output_directory\\n \\n     def cleanup_extracted_files(self, tf=None):\\n@@ -826,6 +871,9 @@ def _dd(self, file_name, offset, size, extension, output_file_name=None):\\n             # Cleanup\\n             fdout.close()\\n             fdin.close()\\n+\\n+            # Make sure run-as user can access this file\\n+            os.chown(fname, self.runas_uid, self.runas_gid)\\n         except KeyboardInterrupt as e:\\n             raise e\\n         except Exception as e:\\n@@ -846,7 +894,6 @@ def execute(self, cmd, fname, codes=[0, None]):\\n \\n         Returns True on success, False on failure, or None if the external extraction utility could not be found.\\n         \\'\\'\\'\\n-        tmp = None\\n         rval = 0\\n         retval = True\\n         command_list = []\\n@@ -865,16 +912,10 @@ def execute(self, cmd, fname, codes=[0, None]):\\n                     retval = False\\n                     binwalk.core.common.warning(\"Internal extractor \\'%s\\' failed with exception: \\'%s\\'\" % (str(cmd), str(e)))\\n             elif cmd:\\n-                # If not in debug mode, create a temporary file to redirect\\n-                # stdout and stderr to\\n-                if not binwalk.core.common.DEBUG:\\n-                    tmp = tempfile.TemporaryFile()\\n-\\n                 # Generate unique file paths for all paths in the current\\n                 # command that are surrounded by UNIQUE_PATH_DELIMITER\\n                 while self.UNIQUE_PATH_DELIMITER in cmd:\\n-                    need_unique_path = cmd.split(self.UNIQUE_PATH_DELIMITER)[\\n-                        1].split(self.UNIQUE_PATH_DELIMITER)[0]\\n+                    need_unique_path = cmd.split(self.UNIQUE_PATH_DELIMITER)[1].split(self.UNIQUE_PATH_DELIMITER)[0]\\n                     unique_path = binwalk.core.common.unique_file_name(need_unique_path)\\n                     cmd = cmd.replace(self.UNIQUE_PATH_DELIMITER + need_unique_path + self.UNIQUE_PATH_DELIMITER, unique_path)\\n \\n@@ -885,9 +926,10 @@ def execute(self, cmd, fname, codes=[0, None]):\\n                     # command with fname\\n                     command = command.strip().replace(self.FILE_NAME_PLACEHOLDER, fname)\\n \\n-                    binwalk.core.common.debug(\"subprocess.call(%s, stdout=%s, stderr=%s)\" % (command, str(tmp), str(tmp)))\\n-                    rval = subprocess.call(shlex.split(command), stdout=tmp, stderr=tmp)\\n+                    # Execute external extractor\\n+                    rval = self.shell_call(command)\\n \\n+                    # Check the return value to see if extraction was successful or not\\n                     if rval in codes:\\n                         retval = True\\n                     else:\\n@@ -909,7 +951,61 @@ def execute(self, cmd, fname, codes=[0, None]):\\n             binwalk.core.common.warning(\"Extractor.execute failed to run external extractor \\'%s\\': %s, \\'%s\\' might not be installed correctly\" % (str(cmd), str(e), str(cmd)))\\n             retval = None\\n \\n-        if tmp is not None:\\n-            tmp.close()\\n-\\n         return (retval, \\'&&\\'.join(command_list))\\n+\\n+    def shell_call(self, command):\\n+        # If not in debug mode, redirect output to /dev/null\\n+        if not binwalk.core.common.DEBUG:\\n+            tmp = subprocess.DEVNULL\\n+        else:\\n+            tmp = None\\n+\\n+        # If a run-as user is not the current user, we\\'ll need to switch privileges to that user account\\n+        if self.runas_uid != os.getuid():\\n+            binwalk.core.common.debug(\"Switching privileges to %s (%d:%d)\" % (self.runas_user, self.runas_uid, self.runas_gid))\\n+            \\n+            # Fork a child process\\n+            child_pid = os.fork()\\n+            if child_pid is 0:\\n+                # Switch to the run-as user privileges, if one has been set\\n+                if self.runas_uid is not None and self.runas_gid is not None:\\n+                    os.setgid(self.runas_uid)\\n+                    os.setuid(self.runas_gid)\\n+        else:\\n+            # child_pid of None indicates that no os.fork() occured\\n+            child_pid = None\\n+            \\n+        # If we\\'re the child, or there was no os.fork(), execute the command\\n+        if child_pid in [0, None]:\\n+            binwalk.core.common.debug(\"subprocess.call(%s, stdout=%s, stderr=%s)\" % (command, str(tmp), str(tmp)))\\n+            rval = subprocess.call(shlex.split(command), stdout=tmp, stderr=tmp)\\n+\\n+        # A true child process should exit with the subprocess exit value\\n+        if child_pid is 0:\\n+            sys.exit(rval)\\n+        # If no os.fork() happened, just return the subprocess exit value\\n+        elif child_pid is None:\\n+            return rval\\n+        # Else, os.fork() happened and we\\'re the parent. Wait and return the child\\'s exit value.\\n+        else:\\n+            return os.wait()[1]\\n+\\n+    def symlink_sanitizer(self, file_list, extraction_directory):\\n+        # User can disable this if desired\\n+        if self.do_not_sanitize_symlinks is True:\\n+            return \\n+\\n+        # Allows either a single file path, or a list of file paths to be passed in for sanitization.\\n+        if type(file_list) is not list:\\n+            file_list = [file_list]\\n+\\n+        # Sanitize any files in the list that are symlinks outside of the specified extraction directory.\\n+        for file_name in file_list:\\n+            if os.path.islink(file_name):\\n+                linktarget = os.path.realpath(file_name)\\n+                binwalk.core.common.debug(\"Analysing symlink: %s -> %s\" % (file_name, linktarget))\\n+\\n+                if not linktarget.startswith(extraction_directory) and linktarget != os.devnull:\\n+                    binwalk.core.common.warning(\"Symlink points outside of the extraction directory: %s -> %s; changing link target to %s for security purposes.\" % (file_name, linktarget, os.devnull))\\n+                    os.remove(file_name)\\n+                    os.symlink(os.devnull, file_name)'], 'file': ['src/binwalk/modules/extractor.py'], 'language': ['Python'], 'temp_id': [UUID('f6d5c717-df17-4422-81ee-3a40a0106bc2')]}\n",
      "ERROR:root:Error in {'repo': 'ReFirmLabs/binwalk', 'vulnerability_id': '2021-4287', 'commit': 'fa0c0bd59b8588814756942fe4cb5452e76c1dcd', 'commit_source': 'github', 'cwe_id': ['CWE-61', 'CWE-61'], 'patch': ['@@ -4,12 +4,14 @@\\n \\n import os\\n import re\\n+import pwd\\n import stat\\n import shlex\\n import tempfile\\n import subprocess\\n import binwalk.core.common\\n from binwalk.core.compat import *\\n+from binwalk.core.exceptions import ModuleException\\n from binwalk.core.module import Module, Option, Kwarg\\n from binwalk.core.common import file_size, file_md5, unique_file_name, BlockFile\\n \\n@@ -87,11 +89,20 @@ class Extractor(Module):\\n                type=int,\\n                kwargs={\\'max_count\\': 0},\\n                description=\\'Limit the number of extracted files\\'),\\n+        Option(short=\\'0\\',\\n+               long=\\'run-as\\',\\n+               type=str,\\n+               kwargs={\\'runas_user\\': 0},\\n+               description=\"Execute external extraction utilities with the specified user\\'s privileges\"),\\n         #Option(short=\\'u\\',\\n         #       long=\\'limit\\',\\n         #       type=int,\\n         #       kwargs={\\'recursive_max_size\\': 0},\\n         #       description=\"Limit the total size of all extracted files\"),\\n+        Option(short=\\'1\\',\\n+               long=\\'preserve-symlinks\\',\\n+               kwargs={\\'do_not_sanitize_symlinks\\': True},\\n+               description=\"Do not sanitize extracted symlinks that point outside the extraction directory (dangerous)\"),\\n         Option(short=\\'r\\',\\n                long=\\'rm\\',\\n                kwargs={\\'remove_after_execute\\': True},\\n@@ -111,16 +122,43 @@ class Extractor(Module):\\n         Kwarg(name=\\'recursive_max_size\\', default=None),\\n         Kwarg(name=\\'max_count\\', default=None),\\n         Kwarg(name=\\'base_directory\\', default=None),\\n+        Kwarg(name=\\'do_not_sanitize_symlinks\\', default=False),\\n         Kwarg(name=\\'remove_after_execute\\', default=False),\\n         Kwarg(name=\\'load_default_rules\\', default=False),\\n         Kwarg(name=\\'run_extractors\\', default=True),\\n         Kwarg(name=\\'extract_into_subdirs\\', default=False),\\n         Kwarg(name=\\'manual_rules\\', default=[]),\\n         Kwarg(name=\\'matryoshka\\', default=0),\\n         Kwarg(name=\\'enabled\\', default=False),\\n+        Kwarg(name=\\'runas_user\\', default=None),\\n     ]\\n \\n     def load(self):\\n+        self.runas_uid = None\\n+        self.runas_gid = None\\n+\\n+        if self.enabled is True:\\n+            if self.runas_user is None:\\n+                # Get some info about the current user we\\'re running under\\n+                user_info = pwd.getpwuid(os.getuid())\\n+\\n+                # Don\\'t run as root, unless explicitly instructed to\\n+                if user_info.pw_uid == 0:\\n+                    raise ModuleException(\"Binwalk extraction uses many third party utilities, which may not be secure. If you wish to have extraction utilities executed as the current user, use \\'--run-as=%s\\' (binwalk itself must be run as root).\" % user_info.pw_name)\\n+\\n+                # Run external applications as the current user\\n+                self.runas_uid = user_info.pw_uid\\n+                self.runas_gid = user_info.pw_gid\\n+            else:\\n+                # Run external applications as the specified user\\n+                user_info = pwd.getpwnam(self.runas_user)\\n+                self.runas_uid = user_info.pw_uid\\n+                self.runas_gid = user_info.pw_gid\\n+\\n+                # Make sure we\\'ll have permissions to switch to the different user\\n+                if self.runas_uid != os.getuid() and os.getuid() != 0:\\n+                    raise ModuleException(\"In order to execute third party applications as %s, binwalk must be run with root privileges.\" % self.runas_user)\\n+\\n         # Holds a list of extraction rules loaded either from a file or when\\n         # manually specified.\\n         self.extract_rules = []\\n@@ -148,8 +186,8 @@ def load(self):\\n             self.config.verbose = True\\n \\n     def add_pending(self, f):\\n-        # Ignore symlinks\\n-        if os.path.islink(f):\\n+        # Ignore symlinks, don\\'t add new files unless recursion was requested\\n+        if os.path.islink(f) or not self.matryoshka:\\n             return\\n \\n         # Get the file mode to check and see if it\\'s a block/char device\\n@@ -260,30 +298,34 @@ def callback(self, r):\\n \\n                     # If recursion was specified, and the file is not the same\\n                     # one we just dd\\'d\\n-                    if (self.matryoshka and\\n-                        file_path != dd_file_path and\\n-                        scan_extracted_files and\\n-                            self.directory in real_file_path):\\n-                        # If the recursion level of this file is less than or\\n-                        # equal to our desired recursion level\\n-                        if len(real_file_path.split(self.directory)[1].split(os.path.sep)) <= self.matryoshka:\\n-                            # If this is a directory and we are supposed to process directories for this extractor,\\n-                            # then add all files under that directory to the\\n-                            # list of pending files.\\n-                            if os.path.isdir(file_path):\\n-                                for root, dirs, files in os.walk(file_path):\\n-                                    for f in files:\\n-                                        full_path = os.path.join(root, f)\\n-                                        self.add_pending(full_path)\\n-                            # If it\\'s just a file, it to the list of pending\\n-                            # files\\n-                            else:\\n-                                self.add_pending(file_path)\\n+                    if file_path != dd_file_path:\\n+                        # Symlinks can cause security issues if they point outside the extraction directory.\\n+                        self.symlink_sanitizer(file_path, extraction_directory)\\n+\\n+                        # If this is a directory and we are supposed to process directories for this extractor,\\n+                        # then add all files under that directory to the\\n+                        # list of pending files.\\n+                        if os.path.isdir(file_path):\\n+                            for root, dirs, files in os.walk(file_path):\\n+                                # Symlinks can cause security issues if they point outside the extraction directory.\\n+                                self.symlink_sanitizer([os.path.join(root, x) for x in dirs+files], extraction_directory)\\n+\\n+                                for f in files:\\n+                                    full_path = os.path.join(root, f)\\n+\\n+                                    # If the recursion level of this file is less than or equal to our desired recursion level\\n+                                    if len(real_file_path.split(self.directory)[1].split(os.path.sep)) <= self.matryoshka:\\n+                                        if scan_extracted_files and self.directory in real_file_path:\\n+                                                self.add_pending(full_path)\\n+\\n+                        # If it\\'s just a file, it to the list of pending\\n+                        # files\\n+                        elif scan_extracted_files and self.directory in real_file_path:\\n+                            self.add_pending(file_path)\\n \\n                 # Update the last directory listing for the next time we\\n                 # extract a file to this same output directory\\n-                self.last_directory_listing[\\n-                    extraction_directory] = directory_listing\\n+                self.last_directory_listing[extraction_directory] = directory_listing\\n \\n     def append_rule(self, r):\\n         self.extract_rules.append(r.copy())\\n@@ -534,6 +576,9 @@ def build_output_directory(self, path):\\n         else:\\n             output_directory = self.extraction_directories[path]\\n \\n+        # Make sure run-as user can access this directory\\n+        os.chown(output_directory, self.runas_uid, self.runas_gid)\\n+\\n         return output_directory\\n \\n     def cleanup_extracted_files(self, tf=None):\\n@@ -826,6 +871,9 @@ def _dd(self, file_name, offset, size, extension, output_file_name=None):\\n             # Cleanup\\n             fdout.close()\\n             fdin.close()\\n+\\n+            # Make sure run-as user can access this file\\n+            os.chown(fname, self.runas_uid, self.runas_gid)\\n         except KeyboardInterrupt as e:\\n             raise e\\n         except Exception as e:\\n@@ -846,7 +894,6 @@ def execute(self, cmd, fname, codes=[0, None]):\\n \\n         Returns True on success, False on failure, or None if the external extraction utility could not be found.\\n         \\'\\'\\'\\n-        tmp = None\\n         rval = 0\\n         retval = True\\n         command_list = []\\n@@ -865,16 +912,10 @@ def execute(self, cmd, fname, codes=[0, None]):\\n                     retval = False\\n                     binwalk.core.common.warning(\"Internal extractor \\'%s\\' failed with exception: \\'%s\\'\" % (str(cmd), str(e)))\\n             elif cmd:\\n-                # If not in debug mode, create a temporary file to redirect\\n-                # stdout and stderr to\\n-                if not binwalk.core.common.DEBUG:\\n-                    tmp = tempfile.TemporaryFile()\\n-\\n                 # Generate unique file paths for all paths in the current\\n                 # command that are surrounded by UNIQUE_PATH_DELIMITER\\n                 while self.UNIQUE_PATH_DELIMITER in cmd:\\n-                    need_unique_path = cmd.split(self.UNIQUE_PATH_DELIMITER)[\\n-                        1].split(self.UNIQUE_PATH_DELIMITER)[0]\\n+                    need_unique_path = cmd.split(self.UNIQUE_PATH_DELIMITER)[1].split(self.UNIQUE_PATH_DELIMITER)[0]\\n                     unique_path = binwalk.core.common.unique_file_name(need_unique_path)\\n                     cmd = cmd.replace(self.UNIQUE_PATH_DELIMITER + need_unique_path + self.UNIQUE_PATH_DELIMITER, unique_path)\\n \\n@@ -885,9 +926,10 @@ def execute(self, cmd, fname, codes=[0, None]):\\n                     # command with fname\\n                     command = command.strip().replace(self.FILE_NAME_PLACEHOLDER, fname)\\n \\n-                    binwalk.core.common.debug(\"subprocess.call(%s, stdout=%s, stderr=%s)\" % (command, str(tmp), str(tmp)))\\n-                    rval = subprocess.call(shlex.split(command), stdout=tmp, stderr=tmp)\\n+                    # Execute external extractor\\n+                    rval = self.shell_call(command)\\n \\n+                    # Check the return value to see if extraction was successful or not\\n                     if rval in codes:\\n                         retval = True\\n                     else:\\n@@ -909,7 +951,61 @@ def execute(self, cmd, fname, codes=[0, None]):\\n             binwalk.core.common.warning(\"Extractor.execute failed to run external extractor \\'%s\\': %s, \\'%s\\' might not be installed correctly\" % (str(cmd), str(e), str(cmd)))\\n             retval = None\\n \\n-        if tmp is not None:\\n-            tmp.close()\\n-\\n         return (retval, \\'&&\\'.join(command_list))\\n+\\n+    def shell_call(self, command):\\n+        # If not in debug mode, redirect output to /dev/null\\n+        if not binwalk.core.common.DEBUG:\\n+            tmp = subprocess.DEVNULL\\n+        else:\\n+            tmp = None\\n+\\n+        # If a run-as user is not the current user, we\\'ll need to switch privileges to that user account\\n+        if self.runas_uid != os.getuid():\\n+            binwalk.core.common.debug(\"Switching privileges to %s (%d:%d)\" % (self.runas_user, self.runas_uid, self.runas_gid))\\n+            \\n+            # Fork a child process\\n+            child_pid = os.fork()\\n+            if child_pid is 0:\\n+                # Switch to the run-as user privileges, if one has been set\\n+                if self.runas_uid is not None and self.runas_gid is not None:\\n+                    os.setgid(self.runas_uid)\\n+                    os.setuid(self.runas_gid)\\n+        else:\\n+            # child_pid of None indicates that no os.fork() occured\\n+            child_pid = None\\n+            \\n+        # If we\\'re the child, or there was no os.fork(), execute the command\\n+        if child_pid in [0, None]:\\n+            binwalk.core.common.debug(\"subprocess.call(%s, stdout=%s, stderr=%s)\" % (command, str(tmp), str(tmp)))\\n+            rval = subprocess.call(shlex.split(command), stdout=tmp, stderr=tmp)\\n+\\n+        # A true child process should exit with the subprocess exit value\\n+        if child_pid is 0:\\n+            sys.exit(rval)\\n+        # If no os.fork() happened, just return the subprocess exit value\\n+        elif child_pid is None:\\n+            return rval\\n+        # Else, os.fork() happened and we\\'re the parent. Wait and return the child\\'s exit value.\\n+        else:\\n+            return os.wait()[1]\\n+\\n+    def symlink_sanitizer(self, file_list, extraction_directory):\\n+        # User can disable this if desired\\n+        if self.do_not_sanitize_symlinks is True:\\n+            return \\n+\\n+        # Allows either a single file path, or a list of file paths to be passed in for sanitization.\\n+        if type(file_list) is not list:\\n+            file_list = [file_list]\\n+\\n+        # Sanitize any files in the list that are symlinks outside of the specified extraction directory.\\n+        for file_name in file_list:\\n+            if os.path.islink(file_name):\\n+                linktarget = os.path.realpath(file_name)\\n+                binwalk.core.common.debug(\"Analysing symlink: %s -> %s\" % (file_name, linktarget))\\n+\\n+                if not linktarget.startswith(extraction_directory) and linktarget != os.devnull:\\n+                    binwalk.core.common.warning(\"Symlink points outside of the extraction directory: %s -> %s; changing link target to %s for security purposes.\" % (file_name, linktarget, os.devnull))\\n+                    os.remove(file_name)\\n+                    os.symlink(os.devnull, file_name)'], 'file': ['src/binwalk/modules/extractor.py'], 'language': ['Python'], 'temp_id': [UUID('f6d5c717-df17-4422-81ee-3a40a0106bc2')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 143, in get_changes\n",
      "    local_repo = Repo.clone_from(repo.clone_url, f\"{REPOS_PATH}/{repo_name}\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1541, in clone_from\n",
      "    return cls._clone(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1412, in _clone\n",
      "    finalize_process(proc, stderr=stderr)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/util.py\", line 504, in finalize_process\n",
      "    proc.wait(**kwargs)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/cmd.py\", line 834, in wait\n",
      "    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\n",
      "git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git clone -v -- https://github.com/ReFirmLabs/binwalk.git /Users/somen/repos/ReFirmLabs/binwalk\n",
      "  stderr: 'Cloning into '/Users/somen/repos/ReFirmLabs/binwalk'...\n",
      "POST git-upload-pack (175 bytes)\n",
      "POST git-upload-pack (817 bytes)\n",
      "error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly: CANCEL (err 8)\n",
      "error: 3226 bytes of body are still expected\n",
      "fetch-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: early EOF\n",
      "fatal: fetch-pack: invalid index-pack output\n",
      "'\n",
      " 14%|â–ˆâ–        | 252/1800 [03:36<34:35,  1.34s/it]  ERROR:src.process_code_changes:Error processing commit 764e499f61a87641916a7a427d4c4b1ac3f321a9\n",
      "ERROR:src.process_code_changes:{'repo': 'jupyterhub/jupyter-server-proxy', 'vulnerability_id': '2024-28179', 'commit': '764e499f61a87641916a7a427d4c4b1ac3f321a9', 'commit_source': 'github', 'cwe_id': ['CWE-306'], 'patch': ['@@ -130,6 +130,39 @@ def check_origin(self, origin=None):\\n     async def open(self, port, proxied_path):\\n         raise NotImplementedError(\"Subclasses of ProxyHandler should implement open\")\\n \\n+    async def prepare(self, *args, **kwargs):\\n+        \"\"\"\\n+        Enforce authentication on *all* requests.\\n+\\n+        This method is called *before* any other method for all requests.\\n+        See https://www.tornadoweb.org/en/stable/web.html#tornado.web.RequestHandler.prepare.\\n+        \"\"\"\\n+        # Due to https://github.com/jupyter-server/jupyter_server/issues/1012,\\n+        # we can not decorate `prepare` with `@web.authenticated`.\\n+        # `super().prepare`, which calls `JupyterHandler.prepare`, *must* be called\\n+        # before `@web.authenticated` can work. Since `@web.authenticated` is a decorator\\n+        # that relies on the decorated method to get access to request information, we can\\n+        # not call it directly. Instead, we create an empty lambda that takes a request_handler,\\n+        # decorate that with web.authenticated, and call the decorated function.\\n+        # super().prepare became async with jupyter_server v2\\n+        _prepared = super().prepare(*args, **kwargs)\\n+        if _prepared is not None:\\n+            await _prepared\\n+\\n+        # If this is a GET request that wants to be upgraded to a websocket, users not\\n+        # already authenticated gets a straightforward 403. Everything else is dealt\\n+        # with by `web.authenticated`, which does a 302 to the appropriate login url.\\n+        # Websockets are purely API calls made by JS rather than a direct user facing page,\\n+        # so redirects do not make sense for them.\\n+        if (\\n+            self.request.method == \"GET\"\\n+            and self.request.headers.get(\"Upgrade\", \"\").lower() == \"websocket\"\\n+        ):\\n+            if not self.current_user:\\n+                raise web.HTTPError(403)\\n+        else:\\n+            web.authenticated(lambda request_handler: None)(self)\\n+\\n     async def http_get(self, host, port, proxy_path=\"\"):\\n         \"\"\"Our non-websocket GET.\"\"\"\\n         raise NotImplementedError(\\n@@ -280,7 +313,6 @@ def _check_host_allowlist(self, host):\\n         else:\\n             return host in self.host_allowlist\\n \\n-    @web.authenticated\\n     async def proxy(self, host, port, proxied_path):\\n         \"\"\"\\n         This serverextension handles:\\n@@ -682,7 +714,6 @@ def _realize_rendered_template(self, attribute):\\n             attribute = call_with_asked_args(attribute, self.process_args)\\n         return self._render_template(attribute)\\n \\n-    @web.authenticated\\n     async def proxy(self, port, path):\\n         if not path.startswith(\"/\"):\\n             path = \"/\" + path\\n@@ -866,7 +897,6 @@ async def ensure_process(self):\\n                     del self.state[\"proc\"]\\n                     raise\\n \\n-    @web.authenticated\\n     async def proxy(self, port, path):\\n         await self.ensure_process()\\n         return await ensure_async(super().proxy(port, path))'], 'file': ['jupyter_server_proxy/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('510f8f7f-a13c-4472-af89-292b0edb8aca')]}\n",
      "ERROR:root:Error in {'repo': 'jupyterhub/jupyter-server-proxy', 'vulnerability_id': '2024-28179', 'commit': '764e499f61a87641916a7a427d4c4b1ac3f321a9', 'commit_source': 'github', 'cwe_id': ['CWE-306'], 'patch': ['@@ -130,6 +130,39 @@ def check_origin(self, origin=None):\\n     async def open(self, port, proxied_path):\\n         raise NotImplementedError(\"Subclasses of ProxyHandler should implement open\")\\n \\n+    async def prepare(self, *args, **kwargs):\\n+        \"\"\"\\n+        Enforce authentication on *all* requests.\\n+\\n+        This method is called *before* any other method for all requests.\\n+        See https://www.tornadoweb.org/en/stable/web.html#tornado.web.RequestHandler.prepare.\\n+        \"\"\"\\n+        # Due to https://github.com/jupyter-server/jupyter_server/issues/1012,\\n+        # we can not decorate `prepare` with `@web.authenticated`.\\n+        # `super().prepare`, which calls `JupyterHandler.prepare`, *must* be called\\n+        # before `@web.authenticated` can work. Since `@web.authenticated` is a decorator\\n+        # that relies on the decorated method to get access to request information, we can\\n+        # not call it directly. Instead, we create an empty lambda that takes a request_handler,\\n+        # decorate that with web.authenticated, and call the decorated function.\\n+        # super().prepare became async with jupyter_server v2\\n+        _prepared = super().prepare(*args, **kwargs)\\n+        if _prepared is not None:\\n+            await _prepared\\n+\\n+        # If this is a GET request that wants to be upgraded to a websocket, users not\\n+        # already authenticated gets a straightforward 403. Everything else is dealt\\n+        # with by `web.authenticated`, which does a 302 to the appropriate login url.\\n+        # Websockets are purely API calls made by JS rather than a direct user facing page,\\n+        # so redirects do not make sense for them.\\n+        if (\\n+            self.request.method == \"GET\"\\n+            and self.request.headers.get(\"Upgrade\", \"\").lower() == \"websocket\"\\n+        ):\\n+            if not self.current_user:\\n+                raise web.HTTPError(403)\\n+        else:\\n+            web.authenticated(lambda request_handler: None)(self)\\n+\\n     async def http_get(self, host, port, proxy_path=\"\"):\\n         \"\"\"Our non-websocket GET.\"\"\"\\n         raise NotImplementedError(\\n@@ -280,7 +313,6 @@ def _check_host_allowlist(self, host):\\n         else:\\n             return host in self.host_allowlist\\n \\n-    @web.authenticated\\n     async def proxy(self, host, port, proxied_path):\\n         \"\"\"\\n         This serverextension handles:\\n@@ -682,7 +714,6 @@ def _realize_rendered_template(self, attribute):\\n             attribute = call_with_asked_args(attribute, self.process_args)\\n         return self._render_template(attribute)\\n \\n-    @web.authenticated\\n     async def proxy(self, port, path):\\n         if not path.startswith(\"/\"):\\n             path = \"/\" + path\\n@@ -866,7 +897,6 @@ async def ensure_process(self):\\n                     del self.state[\"proc\"]\\n                     raise\\n \\n-    @web.authenticated\\n     async def proxy(self, port, path):\\n         await self.ensure_process()\\n         return await ensure_async(super().proxy(port, path))'], 'file': ['jupyter_server_proxy/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('510f8f7f-a13c-4472-af89-292b0edb8aca')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @web.authenticated\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @web.authenticated\n",
      " 15%|â–ˆâ–        | 262/1800 [03:37<21:39,  1.18it/s]ERROR:src.process_code_changes:Error processing commit bead903b7c0354b6efd8b4cde94b89afab653e03\n",
      "ERROR:src.process_code_changes:{'repo': 'jupyterhub/jupyter-server-proxy', 'vulnerability_id': '2024-28179', 'commit': 'bead903b7c0354b6efd8b4cde94b89afab653e03', 'commit_source': 'github', 'cwe_id': ['CWE-306'], 'patch': ['@@ -124,6 +124,39 @@ def check_origin(self, origin=None):\\n     async def open(self, port, proxied_path):\\n         raise NotImplementedError(\\'Subclasses of ProxyHandler should implement open\\')\\n \\n+    async def prepare(self, *args, **kwargs):\\n+        \"\"\"\\n+        Enforce authentication on *all* requests.\\n+\\n+        This method is called *before* any other method for all requests.\\n+        See https://www.tornadoweb.org/en/stable/web.html#tornado.web.RequestHandler.prepare.\\n+        \"\"\"\\n+        # Due to https://github.com/jupyter-server/jupyter_server/issues/1012,\\n+        # we can not decorate `prepare` with `@web.authenticated`.\\n+        # `super().prepare`, which calls `JupyterHandler.prepare`, *must* be called\\n+        # before `@web.authenticated` can work. Since `@web.authenticated` is a decorator\\n+        # that relies on the decorated method to get access to request information, we can\\n+        # not call it directly. Instead, we create an empty lambda that takes a request_handler,\\n+        # decorate that with web.authenticated, and call the decorated function.\\n+        # super().prepare became async with jupyter_server v2\\n+        _prepared = super().prepare(*args, **kwargs)\\n+        if _prepared is not None:\\n+            await _prepared\\n+\\n+        # If this is a GET request that wants to be upgraded to a websocket, users not\\n+        # already authenticated gets a straightforward 403. Everything else is dealt\\n+        # with by `web.authenticated`, which does a 302 to the appropriate login url.\\n+        # Websockets are purely API calls made by JS rather than a direct user facing page,\\n+        # so redirects do not make sense for them.\\n+        if (\\n+            self.request.method == \"GET\"\\n+            and self.request.headers.get(\"Upgrade\", \"\").lower() == \"websocket\"\\n+        ):\\n+            if not self.current_user:\\n+                raise web.HTTPError(403)\\n+        else:\\n+            web.authenticated(lambda request_handler: None)(self)\\n+\\n     async def http_get(self, host, port, proxy_path=\\'\\'):\\n         \\'\\'\\'Our non-websocket GET.\\'\\'\\'\\n         raise NotImplementedError(\\'Subclasses of ProxyHandler should implement http_get\\')\\n@@ -265,7 +298,6 @@ def _check_host_allowlist(self, host):\\n         else:\\n             return host in self.host_allowlist\\n \\n-    @web.authenticated\\n     async def proxy(self, host, port, proxied_path):\\n         \\'\\'\\'\\n         This serverextension handles:\\n@@ -664,7 +696,6 @@ async def ensure_process(self):\\n                     raise\\n \\n \\n-    @web.authenticated\\n     async def proxy(self, port, path):\\n         if not path.startswith(\\'/\\'):\\n             path = \\'/\\' + path'], 'file': ['jupyter_server_proxy/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('814892c7-07e7-43c0-869b-b1197225b2a6')]}\n",
      "ERROR:root:Error in {'repo': 'jupyterhub/jupyter-server-proxy', 'vulnerability_id': '2024-28179', 'commit': 'bead903b7c0354b6efd8b4cde94b89afab653e03', 'commit_source': 'github', 'cwe_id': ['CWE-306'], 'patch': ['@@ -124,6 +124,39 @@ def check_origin(self, origin=None):\\n     async def open(self, port, proxied_path):\\n         raise NotImplementedError(\\'Subclasses of ProxyHandler should implement open\\')\\n \\n+    async def prepare(self, *args, **kwargs):\\n+        \"\"\"\\n+        Enforce authentication on *all* requests.\\n+\\n+        This method is called *before* any other method for all requests.\\n+        See https://www.tornadoweb.org/en/stable/web.html#tornado.web.RequestHandler.prepare.\\n+        \"\"\"\\n+        # Due to https://github.com/jupyter-server/jupyter_server/issues/1012,\\n+        # we can not decorate `prepare` with `@web.authenticated`.\\n+        # `super().prepare`, which calls `JupyterHandler.prepare`, *must* be called\\n+        # before `@web.authenticated` can work. Since `@web.authenticated` is a decorator\\n+        # that relies on the decorated method to get access to request information, we can\\n+        # not call it directly. Instead, we create an empty lambda that takes a request_handler,\\n+        # decorate that with web.authenticated, and call the decorated function.\\n+        # super().prepare became async with jupyter_server v2\\n+        _prepared = super().prepare(*args, **kwargs)\\n+        if _prepared is not None:\\n+            await _prepared\\n+\\n+        # If this is a GET request that wants to be upgraded to a websocket, users not\\n+        # already authenticated gets a straightforward 403. Everything else is dealt\\n+        # with by `web.authenticated`, which does a 302 to the appropriate login url.\\n+        # Websockets are purely API calls made by JS rather than a direct user facing page,\\n+        # so redirects do not make sense for them.\\n+        if (\\n+            self.request.method == \"GET\"\\n+            and self.request.headers.get(\"Upgrade\", \"\").lower() == \"websocket\"\\n+        ):\\n+            if not self.current_user:\\n+                raise web.HTTPError(403)\\n+        else:\\n+            web.authenticated(lambda request_handler: None)(self)\\n+\\n     async def http_get(self, host, port, proxy_path=\\'\\'):\\n         \\'\\'\\'Our non-websocket GET.\\'\\'\\'\\n         raise NotImplementedError(\\'Subclasses of ProxyHandler should implement http_get\\')\\n@@ -265,7 +298,6 @@ def _check_host_allowlist(self, host):\\n         else:\\n             return host in self.host_allowlist\\n \\n-    @web.authenticated\\n     async def proxy(self, host, port, proxied_path):\\n         \\'\\'\\'\\n         This serverextension handles:\\n@@ -664,7 +696,6 @@ async def ensure_process(self):\\n                     raise\\n \\n \\n-    @web.authenticated\\n     async def proxy(self, port, path):\\n         if not path.startswith(\\'/\\'):\\n             path = \\'/\\' + path'], 'file': ['jupyter_server_proxy/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('814892c7-07e7-43c0-869b-b1197225b2a6')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @web.authenticated\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @web.authenticated\n",
      " 15%|â–ˆâ–        | 269/1800 [03:38<16:36,  1.54it/s]ERROR:src.process_code_changes:Error processing commit 628149159ba25adbfc29a3ae1d4b10c7eb936dd3\n",
      "ERROR:src.process_code_changes:{'repo': 'boxug/trape', 'vulnerability_id': '2017-17714', 'commit': '628149159ba25adbfc29a3ae1d4b10c7eb936dd3', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -28,74 +28,91 @@ def loadDatabase(self):\\n         return True\\n \\n     def sql_execute(self, sentence):\\n-        self.cursor.execute(sentence)\\n+    \\tif type(sentence) is str:\\n+        \\tself.cursor.execute(sentence)\\n+    \\telse:\\n+        \\tself.cursor.execute(sentence[0], sentence[1])\\n         return self.cursor.fetchall()\\n \\n     def sql_one_row(self, sentence, column):\\n-        self.cursor.execute(sentence)\\n+        if type(sentence) is str:\\n+        \\tself.cursor.execute(sentence)\\n+    \\telse:\\n+        \\tself.cursor.execute(sentence[0], sentence[1])\\t\\n         return self.cursor.fetchone()[column]\\n \\n     def sql_insert(self, sentence):\\n-        self.cursor.execute(sentence)\\n+        if type(sentence) is str:\\n+        \\tself.cursor.execute(sentence)\\n+    \\telse:\\n+        \\tself.cursor.execute(sentence[0], sentence[1])\\n         self.conn.commit()\\n         return True\\n \\n     def prop_sentences_stats(self, type, vId = None):\\n         return {\\n-            \\'get_data\\' : \"SELECT victims.*, geo.*, victims.ip AS ip_local, COUNT(clicks.id) FROM victims INNER JOIN geo ON victims.id = geo.id LEFT JOIN clicks ON clicks.id = victims.id GROUP BY victims.id ORDER BY victims.time DESC\",\\n-            \\'all_networks\\' : \"SELECT networks.* FROM networks ORDER BY id\",\\n-            \\'get_preview\\' : \"SELECT victims.*, geo.*, victims.ip AS ip_local FROM victims INNER JOIN geo ON victims.id = geo.id WHERE victims.id = \\'%s\\'\" % (vId),\\n-            \\'id_networks\\' : \"SELECT networks.* FROM networks WHERE id = \\'%s\\'\" % (vId),\\n-            \\'get_requests\\' : \"SELECT requests.*, geo.ip FROM requests INNER JOIN geo on geo.id = requests.user_id ORDER BY requests.date DESC, requests.id \",\\n-            \\'get_sessions\\' : \"SELECT COUNT(*) AS Total FROM networks\",\\n-            \\'get_clicks\\' : \"SELECT COUNT(*) AS Total FROM clicks\",\\n-            \\'get_online\\' : \"SELECT COUNT(*) AS Total FROM victims WHERE status = \\'%s\\'\" % (\\'online\\')\\n+        \\t\\'get_data\\' : \"SELECT victims.*, geo.*, victims.ip AS ip_local, COUNT(clicks.id) FROM victims INNER JOIN geo ON victims.id = geo.id LEFT JOIN clicks ON clicks.id = victims.id GROUP BY victims.id ORDER BY victims.time DESC\",\\n+        \\t\\'all_networks\\' : \"SELECT networks.* FROM networks ORDER BY id\",\\n+        \\t\\'get_preview\\' : (\"SELECT victims.*, geo.*, victims.ip AS ip_local FROM victims INNER JOIN geo ON victims.id = geo.id WHERE victims.id = ?\" , vId),\\n+        \\t\\'id_networks\\' : (\"SELECT networks.* FROM networks WHERE id = ?\", vId),\\n+        \\t\\'get_requests\\' : \"SELECT requests.*, geo.ip FROM requests INNER JOIN geo on geo.id = requests.user_id ORDER BY requests.date DESC, requests.id \",\\n+        \\t\\'get_sessions\\' : \"SELECT COUNT(*) AS Total FROM networks\",\\n+        \\t\\'get_clicks\\' : \"SELECT COUNT(*) AS Total FROM clicks\",\\n+        \\t\\'get_online\\' : (\"SELECT COUNT(*) AS Total FROM victims WHERE status = ?\", vId)\\n         }.get(type, False)\\n \\n     def sentences_stats(self, type, vId = None):\\n-        return self.sql_execute(self.prop_sentences_stats(type, vId))\\n+    \\treturn self.sql_execute(self.prop_sentences_stats(type, vId))\\n \\n     def prop_sentences_victim(self, type, data = None):\\n         if type == \\'count_victim\\':\\n-            return \"SELECT COUNT(*) AS C FROM victims WHERE id = \\'%s\\'\" % (data)\\n+        \\tt = (data,)\\n+        \\treturn (\"SELECT COUNT(*) AS C FROM victims WHERE id = ?\" , t)\\n         elif type == \\'count_times\\':\\n-            return \"SELECT COUNT(*) AS C FROM clicks WHERE id = \\'%s\\'\" % (data)\\n+        \\tt = (data,)\\n+        \\treturn (\"SELECT COUNT(*) AS C FROM clicks WHERE id = ?\" , t)\\n         elif type == \\'update_victim\\':\\n-            return \"UPDATE victims SET ip = \\'%s\\', date = \\'%s\\', bVersion = \\'%s\\', browser = \\'%s\\', device = \\'%s\\', ports = \\'%s\\', time = \\'%s\\', cpu = \\'%s\\', status = \\'%s\\' WHERE id = \\'%s\\'\" % (data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\', data[1])\\n+        \\tt = (data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\', data[1],)\\n+        \\treturn (\"UPDATE victims SET ip = ?, date = ?, bVersion = ?, browser = ?, device = ?, ports = ?, time = ?, cpu = ?, status = ? WHERE id = ?\", t)\\n         elif type == \\'update_victim_geo\\':\\n-            return \"UPDATE geo SET city = \\'%s\\', country_code = \\'%s\\', country_name = \\'%s\\', ip = \\'%s\\', latitude = \\'%s\\', longitude = \\'%s\\', metro_code = \\'%s\\', region_code = \\'%s\\', region_name = \\'%s\\', time_zone = \\'%s\\', zip_code = \\'%s\\', isp = \\'%s\\', ua=\\'%s\\' WHERE id = \\'%s\\'\" % (data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua, data[1])\\n+        \\tt = (data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua, data[1],)\\n+        \\treturn (\"UPDATE geo SET city = ?, country_code = ?, country_name = ?, ip = ?, latitude = ?, longitude = ?, metro_code = ?, region_code = ?, region_name = ?, time_zone = ?, zip_code = ?, isp = ?, ua=? WHERE id = ?\", t)\\n         elif type == \\'insert_victim\\':\\n-            return \"INSERT INTO victims(id, ip, date, bVersion, browser, device, ports, time, cpu, status) VALUES(\\'%s\\',\\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\')\" % (data[1], data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\')\\n+        \\tt = (data[1], data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\',)\\n+        \\treturn (\"INSERT INTO victims(id, ip, date, bVersion, browser, device, ports, time, cpu, status) VALUES(?,?, ?,?, ?,?, ?, ?, ?, ?)\", t)\\n         elif type == \\'insert_victim_geo\\':\\n-            return \"INSERT INTO geo(id, city, country_code, country_name, ip, latitude, longitude, metro_code, region_code, region_name, time_zone, zip_code, isp, ua) VALUES(\\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\')\"  % (data[1], data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua)\\n+        \\tt = (data[1], data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua,)\\n+        \\treturn (\"INSERT INTO geo(id, city, country_code, country_name, ip, latitude, longitude, metro_code, region_code, region_name, time_zone, zip_code, isp, ua) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\" , t)\\n         elif type == \\'count_victim_network\\':\\n-            return \"SELECT COUNT(*) AS C FROM networks WHERE id = \\'%s\\' AND network = \\'%s\\'\" % (data[0], data[1])\\n+        \\treturn (\"SELECT COUNT(*) AS C FROM networks WHERE id = ? AND network = ?\", (data[0], data[1],))\\n         elif type == \\'delete_networks\\':\\n-            return \"DELETE FROM networks WHERE id = \\'%s\\'\" % (data[0])\\n+        \\treturn (\"DELETE FROM networks WHERE id = ?\", (data[0],))\\n         elif type == \\'update_network\\':\\n-            return \"UPDATE networks SET date = \\'%s\\' WHERE id = \\'%s\\' AND network = \\'%s\\'\" % (data[2], data[0], data[1])\\n+        \\treturn (\"UPDATE networks SET date = ? WHERE id = ? AND network = ?\" , (data[2], data[0], data[1],))\\n         elif type == \\'insert_networks\\':\\n-            return \"INSERT INTO networks(id, public_ip, ip, network, date) VALUES(\\'%s\\',\\'%s\\', \\'%s\\', \\'%s\\',\\'%s\\')\" % (data[0], data[1], data[2], data[3], data[4])\\n+        \\tt = (data[0], data[1], data[2], data[3], data[4],)\\n+        \\treturn (\"INSERT INTO networks(id, public_ip, ip, network, date) VALUES(?,?, ?, ?,?)\" , t)\\n         elif type == \\'insert_requests\\':\\n-            return \"INSERT INTO requests(id, user_id, site, fid, name, value, date) VALUES(\\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\')\" % (data[0].sId, data[0].id, data[0].site, data[0].fid, data[0].name, data[0].value, data[1])\\n+        \\tt = (data[0].sId, data[0].id, data[0].site, data[0].fid, data[0].name, data[0].value, data[1],)\\n+        \\treturn (\"INSERT INTO requests(id, user_id, site, fid, name, value, date) VALUES(?, ?,?, ?, ?,?, ?)\" , t)\\n         elif type == \\'insert_click\\':\\n-            return \"INSERT INTO clicks(id, site, date) VALUES(\\'%s\\', \\'%s\\',\\'%s\\')\" % (data[0], data[1], data[2])\\n+        \\treturn (\"INSERT INTO clicks(id, site, date) VALUES(?, ?,?)\", (data[0], data[1], data[2],))\\n         elif type == \\'report_online\\':\\n-            return \"UPDATE victims SET status = \\'%s\\' WHERE id = \\'%s\\'\" % (\\'online\\', data[0])\\n+        \\treturn (\"UPDATE victims SET status = ? WHERE id = ?\" , (\\'online\\', data[0],))\\n         elif type == \\'clean_online\\':\\n-            return \"UPDATE victims SET status = \\'%s\\' \" % (\\'offline\\')\\n+        \\treturn (\"UPDATE victims SET status = ? \", (\\'offline\\',))\\n         elif type == \\'disconnect_victim\\':\\n-            return \"UPDATE victims SET status = \\'%s\\' WHERE id = \\'%s\\'\" % (\\'offline\\', data)\\n+        \\treturn (\"UPDATE victims SET status = ? WHERE id = ?\" , (\\'offline\\', data,))\\n         else:\\n-            return False\\n+        \\treturn False\\n \\n     def sentences_victim(self, type, data = None, sRun = 1, column = 0):\\n         if sRun == 2:\\n-            return self.sql_insert(self.prop_sentences_victim(type, data))\\n+        \\treturn self.sql_insert(self.prop_sentences_victim(type, data))\\n         elif sRun == 3:\\n-            return self.sql_one_row(self.prop_sentences_victim(type, data), column)\\n+        \\treturn self.sql_one_row(self.prop_sentences_victim(type, data), column)\\n         else:\\n-            return self.sql_execute(self.prop_sentences_victim(type, data))\\n+        \\treturn self.sql_execute(self.prop_sentences_victim(type, data))\\n \\n     def __del__(self):\\n         self.conn.close()\\n\\\\ No newline at end of file', '@@ -97,5 +97,5 @@ def registerRequest():\\n     @app.route(\"/tping\", methods=[\"POST\"])\\n     def receivePing():\\n         vrequest = request.form[\\'id\\']\\n-        db.sentences_victim(\\'report_online\\', [vrequest])\\n+        db.sentences_victim(\\'report_online\\', [vrequest], 2)\\n         return json.dumps({\\'status\\' : \\'OK\\', \\'vId\\' : vrequest});', '@@ -47,21 +47,22 @@ def home_get_dat():\\n     d = db.sentences_stats(\\'get_data\\')\\n     n = db.sentences_stats(\\'all_networks\\')\\n \\n-    (\\'clean_online\\')\\n     rows = db.sentences_stats(\\'get_clicks\\')\\n     c = rows[0][0]\\n     rows = db.sentences_stats(\\'get_sessions\\')\\n     s = rows[0][0]\\n-    rows = db.sentences_stats(\\'get_online\\')\\n+    vId = (\\'online\\', )\\n+    rows = db.sentences_stats(\\'get_online\\', vId)\\n     o = rows[0][0]\\n \\n     return json.dumps({\\'status\\' : \\'OK\\', \\'d\\' : d, \\'n\\' : n, \\'c\\' : c, \\'s\\' : s, \\'o\\' : o});\\n \\n @app.route(\"/get_preview\", methods=[\"POST\"])\\n def home_get_preview():\\n     vId = request.form[\\'vId\\']\\n-    d = db.sentences_stats(\\'get_preview\\', vId)\\n-    n = db.sentences_stats(\\'id_networks\\', vId)\\n+    t = (vId,)\\n+    d = db.sentences_stats(\\'get_preview\\', t)\\n+    n = db.sentences_stats(\\'id_networks\\', t)\\n     return json.dumps({\\'status\\' : \\'OK\\', \\'vId\\' : vId, \\'d\\' : d, \\'n\\' : n});\\n \\n @app.route(\"/get_title\", methods=[\"POST\"])'], 'file': ['core/db.py', 'core/victim.py', 'core/stats.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('77f27b8d-d25a-4321-bf35-7e874a736363'), UUID('b4fe97f9-bd83-4555-9122-4116e334e0d5'), UUID('d77bdaa4-cba5-4d3b-b8f3-5101e8ca6b01')]}\n",
      "ERROR:root:Error in {'repo': 'boxug/trape', 'vulnerability_id': '2017-17714', 'commit': '628149159ba25adbfc29a3ae1d4b10c7eb936dd3', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -28,74 +28,91 @@ def loadDatabase(self):\\n         return True\\n \\n     def sql_execute(self, sentence):\\n-        self.cursor.execute(sentence)\\n+    \\tif type(sentence) is str:\\n+        \\tself.cursor.execute(sentence)\\n+    \\telse:\\n+        \\tself.cursor.execute(sentence[0], sentence[1])\\n         return self.cursor.fetchall()\\n \\n     def sql_one_row(self, sentence, column):\\n-        self.cursor.execute(sentence)\\n+        if type(sentence) is str:\\n+        \\tself.cursor.execute(sentence)\\n+    \\telse:\\n+        \\tself.cursor.execute(sentence[0], sentence[1])\\t\\n         return self.cursor.fetchone()[column]\\n \\n     def sql_insert(self, sentence):\\n-        self.cursor.execute(sentence)\\n+        if type(sentence) is str:\\n+        \\tself.cursor.execute(sentence)\\n+    \\telse:\\n+        \\tself.cursor.execute(sentence[0], sentence[1])\\n         self.conn.commit()\\n         return True\\n \\n     def prop_sentences_stats(self, type, vId = None):\\n         return {\\n-            \\'get_data\\' : \"SELECT victims.*, geo.*, victims.ip AS ip_local, COUNT(clicks.id) FROM victims INNER JOIN geo ON victims.id = geo.id LEFT JOIN clicks ON clicks.id = victims.id GROUP BY victims.id ORDER BY victims.time DESC\",\\n-            \\'all_networks\\' : \"SELECT networks.* FROM networks ORDER BY id\",\\n-            \\'get_preview\\' : \"SELECT victims.*, geo.*, victims.ip AS ip_local FROM victims INNER JOIN geo ON victims.id = geo.id WHERE victims.id = \\'%s\\'\" % (vId),\\n-            \\'id_networks\\' : \"SELECT networks.* FROM networks WHERE id = \\'%s\\'\" % (vId),\\n-            \\'get_requests\\' : \"SELECT requests.*, geo.ip FROM requests INNER JOIN geo on geo.id = requests.user_id ORDER BY requests.date DESC, requests.id \",\\n-            \\'get_sessions\\' : \"SELECT COUNT(*) AS Total FROM networks\",\\n-            \\'get_clicks\\' : \"SELECT COUNT(*) AS Total FROM clicks\",\\n-            \\'get_online\\' : \"SELECT COUNT(*) AS Total FROM victims WHERE status = \\'%s\\'\" % (\\'online\\')\\n+        \\t\\'get_data\\' : \"SELECT victims.*, geo.*, victims.ip AS ip_local, COUNT(clicks.id) FROM victims INNER JOIN geo ON victims.id = geo.id LEFT JOIN clicks ON clicks.id = victims.id GROUP BY victims.id ORDER BY victims.time DESC\",\\n+        \\t\\'all_networks\\' : \"SELECT networks.* FROM networks ORDER BY id\",\\n+        \\t\\'get_preview\\' : (\"SELECT victims.*, geo.*, victims.ip AS ip_local FROM victims INNER JOIN geo ON victims.id = geo.id WHERE victims.id = ?\" , vId),\\n+        \\t\\'id_networks\\' : (\"SELECT networks.* FROM networks WHERE id = ?\", vId),\\n+        \\t\\'get_requests\\' : \"SELECT requests.*, geo.ip FROM requests INNER JOIN geo on geo.id = requests.user_id ORDER BY requests.date DESC, requests.id \",\\n+        \\t\\'get_sessions\\' : \"SELECT COUNT(*) AS Total FROM networks\",\\n+        \\t\\'get_clicks\\' : \"SELECT COUNT(*) AS Total FROM clicks\",\\n+        \\t\\'get_online\\' : (\"SELECT COUNT(*) AS Total FROM victims WHERE status = ?\", vId)\\n         }.get(type, False)\\n \\n     def sentences_stats(self, type, vId = None):\\n-        return self.sql_execute(self.prop_sentences_stats(type, vId))\\n+    \\treturn self.sql_execute(self.prop_sentences_stats(type, vId))\\n \\n     def prop_sentences_victim(self, type, data = None):\\n         if type == \\'count_victim\\':\\n-            return \"SELECT COUNT(*) AS C FROM victims WHERE id = \\'%s\\'\" % (data)\\n+        \\tt = (data,)\\n+        \\treturn (\"SELECT COUNT(*) AS C FROM victims WHERE id = ?\" , t)\\n         elif type == \\'count_times\\':\\n-            return \"SELECT COUNT(*) AS C FROM clicks WHERE id = \\'%s\\'\" % (data)\\n+        \\tt = (data,)\\n+        \\treturn (\"SELECT COUNT(*) AS C FROM clicks WHERE id = ?\" , t)\\n         elif type == \\'update_victim\\':\\n-            return \"UPDATE victims SET ip = \\'%s\\', date = \\'%s\\', bVersion = \\'%s\\', browser = \\'%s\\', device = \\'%s\\', ports = \\'%s\\', time = \\'%s\\', cpu = \\'%s\\', status = \\'%s\\' WHERE id = \\'%s\\'\" % (data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\', data[1])\\n+        \\tt = (data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\', data[1],)\\n+        \\treturn (\"UPDATE victims SET ip = ?, date = ?, bVersion = ?, browser = ?, device = ?, ports = ?, time = ?, cpu = ?, status = ? WHERE id = ?\", t)\\n         elif type == \\'update_victim_geo\\':\\n-            return \"UPDATE geo SET city = \\'%s\\', country_code = \\'%s\\', country_name = \\'%s\\', ip = \\'%s\\', latitude = \\'%s\\', longitude = \\'%s\\', metro_code = \\'%s\\', region_code = \\'%s\\', region_name = \\'%s\\', time_zone = \\'%s\\', zip_code = \\'%s\\', isp = \\'%s\\', ua=\\'%s\\' WHERE id = \\'%s\\'\" % (data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua, data[1])\\n+        \\tt = (data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua, data[1],)\\n+        \\treturn (\"UPDATE geo SET city = ?, country_code = ?, country_name = ?, ip = ?, latitude = ?, longitude = ?, metro_code = ?, region_code = ?, region_name = ?, time_zone = ?, zip_code = ?, isp = ?, ua=? WHERE id = ?\", t)\\n         elif type == \\'insert_victim\\':\\n-            return \"INSERT INTO victims(id, ip, date, bVersion, browser, device, ports, time, cpu, status) VALUES(\\'%s\\',\\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\')\" % (data[1], data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\')\\n+        \\tt = (data[1], data[0].ip, data[0].date, data[0].version, data[0].browser, data[0].device, data[0].ports, data[2], data[0].cpu, \\'online\\',)\\n+        \\treturn (\"INSERT INTO victims(id, ip, date, bVersion, browser, device, ports, time, cpu, status) VALUES(?,?, ?,?, ?,?, ?, ?, ?, ?)\", t)\\n         elif type == \\'insert_victim_geo\\':\\n-            return \"INSERT INTO geo(id, city, country_code, country_name, ip, latitude, longitude, metro_code, region_code, region_name, time_zone, zip_code, isp, ua) VALUES(\\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\', \\'%s\\')\"  % (data[1], data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua)\\n+        \\tt = (data[1], data[0].city, data[0].country_code, data[0].country_name, data[0].ip, data[0].latitude, data[0].longitude, data[0].metro_code, data[0].region_code, data[0].region_name, data[0].time_zone, data[0].zip_code, data[0].isp, data[0].ua,)\\n+        \\treturn (\"INSERT INTO geo(id, city, country_code, country_name, ip, latitude, longitude, metro_code, region_code, region_name, time_zone, zip_code, isp, ua) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\" , t)\\n         elif type == \\'count_victim_network\\':\\n-            return \"SELECT COUNT(*) AS C FROM networks WHERE id = \\'%s\\' AND network = \\'%s\\'\" % (data[0], data[1])\\n+        \\treturn (\"SELECT COUNT(*) AS C FROM networks WHERE id = ? AND network = ?\", (data[0], data[1],))\\n         elif type == \\'delete_networks\\':\\n-            return \"DELETE FROM networks WHERE id = \\'%s\\'\" % (data[0])\\n+        \\treturn (\"DELETE FROM networks WHERE id = ?\", (data[0],))\\n         elif type == \\'update_network\\':\\n-            return \"UPDATE networks SET date = \\'%s\\' WHERE id = \\'%s\\' AND network = \\'%s\\'\" % (data[2], data[0], data[1])\\n+        \\treturn (\"UPDATE networks SET date = ? WHERE id = ? AND network = ?\" , (data[2], data[0], data[1],))\\n         elif type == \\'insert_networks\\':\\n-            return \"INSERT INTO networks(id, public_ip, ip, network, date) VALUES(\\'%s\\',\\'%s\\', \\'%s\\', \\'%s\\',\\'%s\\')\" % (data[0], data[1], data[2], data[3], data[4])\\n+        \\tt = (data[0], data[1], data[2], data[3], data[4],)\\n+        \\treturn (\"INSERT INTO networks(id, public_ip, ip, network, date) VALUES(?,?, ?, ?,?)\" , t)\\n         elif type == \\'insert_requests\\':\\n-            return \"INSERT INTO requests(id, user_id, site, fid, name, value, date) VALUES(\\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\', \\'%s\\',\\'%s\\', \\'%s\\')\" % (data[0].sId, data[0].id, data[0].site, data[0].fid, data[0].name, data[0].value, data[1])\\n+        \\tt = (data[0].sId, data[0].id, data[0].site, data[0].fid, data[0].name, data[0].value, data[1],)\\n+        \\treturn (\"INSERT INTO requests(id, user_id, site, fid, name, value, date) VALUES(?, ?,?, ?, ?,?, ?)\" , t)\\n         elif type == \\'insert_click\\':\\n-            return \"INSERT INTO clicks(id, site, date) VALUES(\\'%s\\', \\'%s\\',\\'%s\\')\" % (data[0], data[1], data[2])\\n+        \\treturn (\"INSERT INTO clicks(id, site, date) VALUES(?, ?,?)\", (data[0], data[1], data[2],))\\n         elif type == \\'report_online\\':\\n-            return \"UPDATE victims SET status = \\'%s\\' WHERE id = \\'%s\\'\" % (\\'online\\', data[0])\\n+        \\treturn (\"UPDATE victims SET status = ? WHERE id = ?\" , (\\'online\\', data[0],))\\n         elif type == \\'clean_online\\':\\n-            return \"UPDATE victims SET status = \\'%s\\' \" % (\\'offline\\')\\n+        \\treturn (\"UPDATE victims SET status = ? \", (\\'offline\\',))\\n         elif type == \\'disconnect_victim\\':\\n-            return \"UPDATE victims SET status = \\'%s\\' WHERE id = \\'%s\\'\" % (\\'offline\\', data)\\n+        \\treturn (\"UPDATE victims SET status = ? WHERE id = ?\" , (\\'offline\\', data,))\\n         else:\\n-            return False\\n+        \\treturn False\\n \\n     def sentences_victim(self, type, data = None, sRun = 1, column = 0):\\n         if sRun == 2:\\n-            return self.sql_insert(self.prop_sentences_victim(type, data))\\n+        \\treturn self.sql_insert(self.prop_sentences_victim(type, data))\\n         elif sRun == 3:\\n-            return self.sql_one_row(self.prop_sentences_victim(type, data), column)\\n+        \\treturn self.sql_one_row(self.prop_sentences_victim(type, data), column)\\n         else:\\n-            return self.sql_execute(self.prop_sentences_victim(type, data))\\n+        \\treturn self.sql_execute(self.prop_sentences_victim(type, data))\\n \\n     def __del__(self):\\n         self.conn.close()\\n\\\\ No newline at end of file', '@@ -97,5 +97,5 @@ def registerRequest():\\n     @app.route(\"/tping\", methods=[\"POST\"])\\n     def receivePing():\\n         vrequest = request.form[\\'id\\']\\n-        db.sentences_victim(\\'report_online\\', [vrequest])\\n+        db.sentences_victim(\\'report_online\\', [vrequest], 2)\\n         return json.dumps({\\'status\\' : \\'OK\\', \\'vId\\' : vrequest});', '@@ -47,21 +47,22 @@ def home_get_dat():\\n     d = db.sentences_stats(\\'get_data\\')\\n     n = db.sentences_stats(\\'all_networks\\')\\n \\n-    (\\'clean_online\\')\\n     rows = db.sentences_stats(\\'get_clicks\\')\\n     c = rows[0][0]\\n     rows = db.sentences_stats(\\'get_sessions\\')\\n     s = rows[0][0]\\n-    rows = db.sentences_stats(\\'get_online\\')\\n+    vId = (\\'online\\', )\\n+    rows = db.sentences_stats(\\'get_online\\', vId)\\n     o = rows[0][0]\\n \\n     return json.dumps({\\'status\\' : \\'OK\\', \\'d\\' : d, \\'n\\' : n, \\'c\\' : c, \\'s\\' : s, \\'o\\' : o});\\n \\n @app.route(\"/get_preview\", methods=[\"POST\"])\\n def home_get_preview():\\n     vId = request.form[\\'vId\\']\\n-    d = db.sentences_stats(\\'get_preview\\', vId)\\n-    n = db.sentences_stats(\\'id_networks\\', vId)\\n+    t = (vId,)\\n+    d = db.sentences_stats(\\'get_preview\\', t)\\n+    n = db.sentences_stats(\\'id_networks\\', t)\\n     return json.dumps({\\'status\\' : \\'OK\\', \\'vId\\' : vId, \\'d\\' : d, \\'n\\' : n});\\n \\n @app.route(\"/get_title\", methods=[\"POST\"])'], 'file': ['core/db.py', 'core/victim.py', 'core/stats.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('77f27b8d-d25a-4321-bf35-7e874a736363'), UUID('b4fe97f9-bd83-4555-9122-4116e334e0d5'), UUID('d77bdaa4-cba5-4d3b-b8f3-5101e8ca6b01')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 101, in clear_file_content\n",
      "    new_content = remove_comments(content)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 71, in remove_comments\n",
      "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 541, in _generate_tokens_from_c_tokenizer\n",
      "    raise e from None\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 537, in _generate_tokens_from_c_tokenizer\n",
      "    for info in it:\n",
      "  File \"<string>\", line 7\n",
      "    return self.cursor.fetchall()\n",
      "                                 ^\n",
      "TabError: inconsistent use of tabs and spaces in indentation\n",
      " 15%|â–ˆâ–Œ        | 275/1800 [03:39<12:53,  1.97it/s]ERROR:src.process_code_changes:Error processing commit 50919d47212066c75f03ee7a5332ecf2d584b98e\n",
      "ERROR:src.process_code_changes:{'repo': 'janeczku/calibre-web', 'vulnerability_id': '2021-25965', 'commit': '50919d47212066c75f03ee7a5332ecf2d584b98e', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -49,7 +49,7 @@\\n     from cps.kobo import kobo, get_kobo_activated\\n     from cps.kobo_auth import kobo_auth\\n     kobo_available = get_kobo_activated()\\n-except ImportError:\\n+except (ImportError, AttributeError):   # Catch also error for not installed flask-wtf (missing csrf decorator)\\n     kobo_available = False\\n \\n try:', '@@ -118,6 +118,7 @@\\n {% endblock %}\\n {% block body %}\\n <h2 class=\"{{page}}\">{{_(title)}}</h2>\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"col-xs-12 col-sm-12\">\\n         <div class=\"row\">\\n           <div class=\"btn btn-default disabled\" id=\"user_delete_selection\" aria-disabled=\"true\">{{_(\\'Remove Selections\\')}}</div>', '@@ -214,6 +214,7 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n         <div class=\"custom_columns\">\\n           <p>\\n           <form id=\"have_read_form\" action=\"{{ url_for(\\'web.toggle_read\\', book_id=entry.id)}}\" method=\"POST\">\\n+            <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n             <label class=\"block-label\">\\n               <input id=\"have_read_cb\" data-checked=\"{{_(\\'Mark As Unread\\')}}\" data-unchecked=\"{{_(\\'Mark As Read\\')}}\" type=\"checkbox\" {% if have_read %}checked{% endif %} >\\n               <span>{{_(\\'Read\\')}}</span>\\n@@ -223,6 +224,7 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n           {% if g.user.check_visibility(32768) %}\\n           <p>\\n             <form id=\"archived_form\" action=\"{{ url_for(\\'web.toggle_archived\\', book_id=entry.id)}}\" method=\"POST\">\\n+              <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n               <label class=\"block-label\">\\n                 <input id=\"archived_cb\" data-checked=\"{{_(\\'Restore from archive\\')}}\" data-unchecked=\"{{_(\\'Add to archive\\')}}\" type=\"checkbox\" {% if is_archived %}checked{% endif %} >\\n                 <span>{{_(\\'Archived\\')}}</span>', \"@@ -29,6 +29,10 @@\\n import babel, pytz, requests, sqlalchemy\\n import werkzeug, flask, flask_login, flask_principal, jinja2\\n from flask_babel import gettext as _\\n+try:\\n+    from flask_wtf import __version__ as flaskwtf_version\\n+except ImportError:\\n+    flaskwtf_version = _(u'not installed')\\n \\n from . import db, calibre_db, converter, uploader, server, isoLanguages, constants\\n from .render_template import render_title_template\\n@@ -75,6 +79,7 @@\\n     Flask=flask.__version__,\\n     Flask_Login=flask_loginVersion,\\n     Flask_Principal=flask_principal.__version__,\\n+    Flask_WTF=flaskwtf_version,\\n     Werkzeug=werkzeug.__version__,\\n     Babel=babel.__version__,\\n     Jinja2=jinja2.__version__,\\n@@ -84,14 +89,14 @@\\n     SQLite=sqlite3.sqlite_version,\\n     iso639=isoLanguages.__version__,\\n     pytz=pytz.__version__,\\n-    Unidecode = unidecode_version,\\n-    Scholarly = scholarly_version,\\n-    Flask_SimpleLDAP =  u'installed' if bool(services.ldap) else None,\\n-    python_LDAP = services.ldapVersion if bool(services.ldapVersion) else None,\\n-    Goodreads = u'installed' if bool(services.goodreads_support) else None,\\n-    jsonschema = services.SyncToken.__version__  if bool(services.SyncToken) else None,\\n-    flask_dance = flask_danceVersion,\\n-    greenlet = greenlet_Version\\n+    Unidecode=unidecode_version,\\n+    Scholarly=scholarly_version,\\n+    Flask_SimpleLDAP=u'installed' if bool(services.ldap) else None,\\n+    python_LDAP=services.ldapVersion if bool(services.ldapVersion) else None,\\n+    Goodreads=u'installed' if bool(services.goodreads_support) else None,\\n+    jsonschema=services.SyncToken.__version__ if bool(services.SyncToken) else None,\\n+    flask_dance=flask_danceVersion,\\n+    greenlet=greenlet_Version\\n )\\n _VERSIONS.update(uploader.get_versions())\\n \", '@@ -23,7 +23,6 @@ if ($(\".tiny_editor\").length) {\\n \\n $(\".datepicker\").datepicker({\\n     format: \"yyyy-mm-dd\",\\n-    language: language\\n }).on(\"change\", function () {\\n     // Show localized date over top of the standard YYYY-MM-DD date\\n     var pubDate;', '@@ -3,6 +3,7 @@\\n <div class=\"discover\">\\n   <h1>{{title}}</h1>\\n   <form role=\"form\" method=\"POST\" autocomplete=\"off\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"col-md-10 col-lg-8\">\\n     {% if new_user or ( g.user and content.name != \"Guest\" and g.user.role_admin() ) %}\\n     <div class=\"form-group required\">', '@@ -112,6 +112,14 @@ $(\"#btn-upload\").change(function() {\\n     $(\"#form-upload\").submit();\\n });\\n \\n+$(\"#form-upload\").uploadprogress({\\n+    redirect_url: getPath() + \"/\", //\"{{ url_for(\\'web.index\\')}}\",\\n+    uploadedMsg: $(\"#form-upload\").data(\"message\"), //\"{{_(\\'Upload done, processing, please wait...\\')}}\",\\n+    modalTitle: $(\"#form-upload\").data(\"title\"), //\"{{_(\\'Uploading...\\')}}\",\\n+    modalFooter: $(\"#form-upload\").data(\"footer\"), //\"{{_(\\'Close\\')}}\",\\n+    modalTitleFailed: $(\"#form-upload\").data(\"failed\") //\"{{_(\\'Error\\')}}\"\\n+});\\n+\\n $(document).ready(function() {\\n   var inp = $(\\'#query\\').first()\\n   if (inp.length) {\\n@@ -223,6 +231,16 @@ $(function() {\\n     var preFilters = $.Callbacks();\\n     $.ajaxPrefilter(preFilters.fire);\\n \\n+    // equip all post requests with csrf_token\\n+    var csrftoken = $(\"input[name=\\'csrf_token\\']\").val();\\n+    $.ajaxSetup({\\n+        beforeSend: function(xhr, settings) {\\n+            if (!/^(GET|HEAD|OPTIONS|TRACE)$/i.test(settings.type) && !this.crossDomain) {\\n+                xhr.setRequestHeader(\"X-CSRFToken\", csrftoken)\\n+            }\\n+        }\\n+    });\\n+\\n     function restartTimer() {\\n         $(\"#spinner\").addClass(\"hidden\");\\n         $(\"#RestartDialog\").modal(\"hide\");\\n@@ -576,7 +594,7 @@ $(function() {\\n             method:\"post\",\\n             dataType: \"json\",\\n             url: window.location.pathname + \"/../../ajax/simulatedbchange\",\\n-            data: {config_calibre_dir: $(\"#config_calibre_dir\").val()},\\n+            data: {config_calibre_dir: $(\"#config_calibre_dir\").val(), csrf_token: $(\"input[name=\\'csrf_token\\']\").val()},\\n             success: function success(data) {\\n                 if ( data.change ) {\\n                     if ( data.valid ) {\\n@@ -712,7 +730,7 @@ $(function() {\\n             method:\"post\",\\n             contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n-            url: window.location.pathname + \"/../ajax/view\",\\n+            url: getPath() + \"/ajax/view\",\\n             data: \"{\\\\\"series\\\\\": {\\\\\"series_view\\\\\": \\\\\"\"+ view +\"\\\\\"}}\",\\n             success: function success() {\\n                 location.reload();', '@@ -8,6 +8,7 @@\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n   <form role=\"form\" method=\"POST\" class=\"col-md-10 col-lg-6\" action=\"{{ url_for(\\'admin.db_configuration\\') }}\" autocomplete=\"off\">\\n+       <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n        <label for=\"config_calibre_dir\">{{_(\\'Location of Calibre Database\\')}}</label>\\n        <div class=\"form-group required input-group\">\\n         <input type=\"text\" class=\"form-control\" id=\"config_calibre_dir\" name=\"config_calibre_dir\" value=\"{% if config.config_calibre_dir != None %}{{ config.config_calibre_dir }}{% endif %}\" autocomplete=\"off\">', '@@ -3,6 +3,7 @@\\n <h1 class=\"{{page}}\">{{title}}</h1>\\n <div class=\"col-md-10 col-lg-6\">\\n   <form role=\"form\" id=\"search\" action=\"{{ url_for(\\'web.advanced_search_form\\') }}\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group\">\\n       <label for=\"book_title\">{{_(\\'Book Title\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" name=\"book_title\" id=\"book_title\" value=\"\">', '@@ -6,8 +6,9 @@\\n {% block body %}\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n-  <form role=\"form\" method=\"POST\" autocomplete=\"off\" >\\n-<div class=\"panel-group class=\"col-md-10 col-lg-6\">\\n+<form role=\"form\" method=\"POST\" autocomplete=\"off\" >\\n+<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n+<div class=\"panel-group\" class=\"col-md-10 col-lg-6\">\\n   <div class=\"panel panel-default\">\\n     <div class=\"panel-heading\">\\n       <h4 class=\"panel-title\">', '@@ -61,7 +61,7 @@\\n               {% if g.user.role_upload() or g.user.role_admin()%}\\n                 {% if g.allow_upload %}\\n                   <li>\\n-                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'editbook.upload\\') }}\" method=\"post\" enctype=\"multipart/form-data\">\\n+                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'editbook.upload\\') }}\" data-title=\"{{_(\\'Uploading...\\')}}\" data-footer=\"{{_(\\'Close\\')}}\" data-failed=\"{{_(\\'Error\\')}}\" data-message=\"{{_(\\'Upload done, processing, please wait...\\')}}\" method=\"post\" enctype=\"multipart/form-data\">\\n                       <div class=\"form-group\">\\n                         <span class=\"btn btn-default btn-file\">{{_(\\'Upload\\')}}<input id=\"btn-upload\" name=\"btn-upload\"\\n                         type=\"file\" accept=\"{% for format in accept %}.{% if format != \\'\\'%}{{format}}{% else %}*{% endif %}{{ \\',\\' if not loop.last }}{% endfor %}\" multiple></span>\\n@@ -200,17 +200,6 @@ <h4 class=\"modal-title\" id=\"bookDetailsModalLabel\">{{_(\\'Book Details\\')}}</h4>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/libs/plugins.js\\') }}\"></script>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/libs/jquery.form.min.js\\') }}\"></script>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/uploadprogress.js\\') }}\"> </script>\\n-    <script type=\"text/javascript\">\\n-        $(function() {\\n-            $(\"#form-upload\").uploadprogress({\\n-              redirect_url: \"{{ url_for(\\'web.index\\')}}\",\\n-              uploadedMsg: \"{{_(\\'Upload done, processing, please wait...\\')}}\",\\n-              modalTitle: \"{{_(\\'Uploading...\\')}}\",\\n-              modalFooter: \"{{_(\\'Close\\')}}\",\\n-              modalTitleFailed: \"{{_(\\'Error\\')}}\"\\n-            });\\n-        });\\n-    </script>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/main.js\\') }}\"></script>\\n     {% if g.current_theme == 1 %}\\n       <script src=\"{{ url_for(\\'static\\', filename=\\'js/libs/jquery.visible.min.js\\') }}\"></script>', '@@ -1,5 +1,5 @@\\n <!DOCTYPE html>\\n-<html class=\"http-error\" lang=\"{{ g.user.locale }}\">\\n+<html class=\"http-error\">\\n   <head>\\n     <title>{{ instance }} | HTTP Error ({{ error_code }})</title>\\n     <meta charset=\"utf-8\">', '@@ -3,6 +3,7 @@\\n <div class=\"discover\">\\n   <h1>{{title}}</h1>\\n   <form role=\"form\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group\">\\n       <label for=\"title\">{{_(\\'Title\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" name=\"title\" id=\"title\" value=\"{{ shelf.name if shelf.name != None }}\">', '@@ -7,6 +7,7 @@\\n <div class=\"discover\">\\n   <h1>{{title}}</h1>\\n   <form role=\"form\" class=\"col-md-10 col-lg-6\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     {% if feature_support[\\'gmail\\'] %}\\n     <div class=\"form-group\">\\n       <label for=\"config_email_type\">{{_(\\'Choose Server Type\\')}}</label>\\n@@ -72,6 +73,7 @@ <h1>{{title}}</h1>\\n   <div class=\"col-md-10 col-lg-6\">\\n     <h2>{{_(\\'Allowed Domains (Whitelist)\\')}}</h2>\\n     <form id=\"domain_add_allow\" action=\"{{ url_for(\\'admin.add_domain\\',allow=1)}}\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group required\">\\n       <label for=\"domainname_allow\">{{_(\\'Add Domain\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" name=\"domainname\" id=\"domainname_allow\" >\\n@@ -98,11 +100,12 @@ <h2>{{_(\\'Denied Domains (Blacklist)\\')}}</h2>\\n       </thead>\\n     </table>\\n     <form id=\"domain_add_deny\" action=\"{{ url_for(\\'admin.add_domain\\',allow=0)}}\" method=\"POST\">\\n-    <div class=\"form-group required\">\\n-      <label for=\"domainname_deny\">{{_(\\'Add Domain\\')}}</label>\\n-      <input type=\"text\" class=\"form-control\" name=\"domainname\" id=\"domainname_deny\" >\\n-    </div>\\n-    <button id=\"domain_deny_submit\" class=\"btn btn-default\">{{_(\\'Add\\')}}</button>\\n+      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n+      <div class=\"form-group required\">\\n+        <label for=\"domainname_deny\">{{_(\\'Add Domain\\')}}</label>\\n+        <input type=\"text\" class=\"form-control\" name=\"domainname\" id=\"domainname_deny\" >\\n+      </div>\\n+      <button id=\"domain_deny_submit\" class=\"btn btn-default\">{{_(\\'Add\\')}}</button>\\n     </form>\\n     </div>\\n ', '@@ -20,6 +20,7 @@\\n {% endblock %}\\n {% block body %}\\n <h2 class=\"{{page}}\">{{_(title)}}</h2>\\n+      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n       <div class=\"col-xs-12 col-sm-6\">\\n         <div class=\"row form-group\">\\n           <div class=\"btn btn-default disabled\" id=\"merge_books\" aria-disabled=\"true\">{{_(\\'Merge selected books\\')}}</div>', '@@ -20,6 +20,7 @@ <h1 class=\"{{page}}\">{{_(title)}}</h1>\\n       </div>\\n \\n       {% if data == \"series\" %}\\n+      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n       <button class=\"update-view btn btn-primary\" data-target=\"series_view\" id=\"grid-button\" data-view=\"grid\">Grid</button>\\n       {% endif %}\\n     </div>', '@@ -47,7 +47,8 @@\\n from sqlalchemy.sql import select\\n import requests\\n \\n-from . import config, logger, kobo_auth, db, calibre_db, helper, shelf as shelf_lib, ub\\n+\\n+from . import config, logger, kobo_auth, db, calibre_db, helper, shelf as shelf_lib, ub, csrf\\n from .constants import sqlalchemy_version2\\n from .helper import get_download_link\\n from .services import SyncToken as SyncToken\\n@@ -505,7 +506,7 @@ def get_metadata(book):\\n \\n     return metadata\\n \\n-\\n+@csrf.exempt\\n @kobo.route(\"/v1/library/tags\", methods=[\"POST\", \"DELETE\"])\\n @requires_kobo_auth\\n # Creates a Shelf with the given items, and returns the shelf\\'s uuid.\\n@@ -595,6 +596,7 @@ def add_items_to_shelf(items, shelf):\\n     return items_unknown_to_calibre\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/library/tags/<tag_id>/items\", methods=[\"POST\"])\\n @requires_kobo_auth\\n def HandleTagAddItem(tag_id):\\n@@ -624,6 +626,7 @@ def HandleTagAddItem(tag_id):\\n     return make_response(\\'\\', 201)\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/library/tags/<tag_id>/items/delete\", methods=[\"POST\"])\\n @requires_kobo_auth\\n def HandleTagRemoveItem(tag_id):\\n@@ -983,6 +986,7 @@ def HandleUnimplementedRequest(dummy=None):\\n \\n \\n # TODO: Implement the following routes\\n+@csrf.exempt\\n @kobo.route(\"/v1/user/loyalty/<dummy>\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/user/profile\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/user/wishlist\", methods=[\"GET\", \"POST\"])\\n@@ -993,6 +997,7 @@ def HandleUserRequest(dummy=None):\\n     return redirect_or_proxy_request()\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/products/<dummy>/prices\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/products/<dummy>/recommendations\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/products/<dummy>/nextread\", methods=[\"GET\", \"POST\"])\\n@@ -1026,6 +1031,7 @@ def make_calibre_web_auth_response():\\n     )\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/auth/device\", methods=[\"POST\"])\\n @requires_kobo_auth\\n def HandleAuthRequest():', '@@ -3,6 +3,7 @@\\n <div class=\"well col-sm-6 col-sm-offset-2\">\\n   <h2 style=\"margin-top: 0\">{{_(\\'Register New Account\\')}}</h2>\\n   <form method=\"POST\" role=\"form\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     {% if not config.config_register_email %}\\n     <div class=\"form-group required\">\\n       <label for=\"name\">{{_(\\'Username\\')}}</label>', '@@ -23,6 +23,7 @@\\n {%  if source_formats|length > 0 and conversion_formats|length > 0 %}\\n   <div class=\"text-center more-stuff\"><h4>{{_(\\'Convert book format:\\')}}</h4>\\n       <form class=\"padded-bottom\" action=\"{{ url_for(\\'editbook.convert_bookformat\\', book_id=book.id) }}\" method=\"post\" id=\"book_convert_frm\">\\n+          <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n           <div class=\"form-group\">\\n               <div class=\"text-left\">\\n                   <label class=\"control-label\" for=\"book_format_from\">{{_(\\'Convert from:\\')}}</label>', '@@ -8,6 +8,7 @@\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n <form role=\"form\" method=\"POST\" autocomplete=\"off\">\\n+<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n <div class=\"panel-group col-md-10 col-lg-8\">\\n   <div class=\"panel panel-default\">\\n     <div class=\"panel-heading\">', '@@ -4,6 +4,7 @@\\n   <h2 style=\"margin-top: 0\">{{_(\\'Login\\')}}</h2>\\n   <form method=\"POST\" role=\"form\">\\n     <input type=\"hidden\" name=\"next\" value=\"{{next_url}}\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group\">\\n       <label for=\"username\">{{_(\\'Username\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" id=\"username\" name=\"username\" placeholder=\"{{_(\\'Username\\')}}\">', '@@ -84,14 +84,13 @@\\n \\n @app.after_request\\n def add_security_headers(resp):\\n-    resp.headers[\\'Content-Security-Policy\\'] = \"default-src \\'self\\' \\'unsafe-inline\\' \\'unsafe-eval\\';\"\\n+    resp.headers[\\'Content-Security-Policy\\'] = \"default-src \\'self\\' \\'unsafe-inline\\' \\'unsafe-eval\\'; img-src \\'self\\' data:\"\\n     if request.endpoint == \"editbook.edit_book\":\\n-        resp.headers[\\'Content-Security-Policy\\'] += \"img-src * data:\"\\n+        resp.headers[\\'Content-Security-Policy\\'] += \" *\"\\n     resp.headers[\\'X-Content-Type-Options\\'] = \\'nosniff\\'\\n     resp.headers[\\'X-Frame-Options\\'] = \\'SAMEORIGIN\\'\\n     resp.headers[\\'X-XSS-Protection\\'] = \\'1; mode=block\\'\\n     resp.headers[\\'Strict-Transport-Security\\'] = \\'max-age=31536000; includeSubDomains\\'\\n-    # log.debug(request.full_path)\\n     return resp\\n \\n web = Blueprint(\\'web\\', __name__)'], 'file': ['cps.py', 'cps/templates/user_table.html', 'cps/templates/detail.html', 'cps/about.py', 'cps/static/js/edit_books.js', 'cps/templates/user_edit.html', 'cps/static/js/main.js', 'cps/templates/config_db.html', 'cps/templates/search_form.html', 'cps/templates/config_view_edit.html', 'cps/templates/layout.html', 'cps/templates/http_error.html', 'cps/templates/shelf_edit.html', 'cps/templates/email_edit.html', 'cps/templates/book_table.html', 'cps/templates/list.html', 'cps/kobo.py', 'cps/templates/register.html', 'cps/templates/book_edit.html', 'cps/templates/config_edit.html', 'cps/templates/login.html', 'cps/web.py'], 'language': ['Python', 'HTML', 'HTML', 'Python', 'JavaScript/TypeScript', 'HTML', 'JavaScript/TypeScript', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'Python', 'HTML', 'HTML', 'HTML', 'HTML', 'Python'], 'temp_id': [UUID('9ebe90d7-5500-4aa2-b37a-1bcd340d629c'), UUID('356ce7b2-7586-4e00-8c76-88442be5761b'), UUID('4b6868cd-2666-4991-b8cb-cfb5352044c8'), UUID('e30badcb-7ea3-42db-a702-35428b408bbe'), UUID('c075e2bb-db74-49cd-9eed-76648481d54a'), UUID('e4a41919-9d11-465f-ae87-5955bc3478d7'), UUID('370ec36a-9976-45d6-b5e0-ff7d12b22948'), UUID('086aad70-3a18-4676-8f45-0cd970c68ecc'), UUID('3cd3d5e8-8257-49bb-ab95-3052d0b6e374'), UUID('69fb9b96-e516-4ccb-9c6a-ac71b51dc9f4'), UUID('6fba77c6-2456-4bad-a5e9-7c677ad74f79'), UUID('60eca2e8-df4c-4dbc-b0a2-b29888c76041'), UUID('04717fd5-a4be-4377-9af3-29fb38a44e71'), UUID('cdfc1282-5f32-46f8-91e3-ec14508169f1'), UUID('e1c5f5d8-e409-4504-b6d3-113690e6883d'), UUID('094b0416-3970-44fc-8d5a-997b75c254a3'), UUID('9243765c-a7ee-4235-825e-978cef95680b'), UUID('9017cb19-bbea-4dde-85c2-81a577292bf4'), UUID('33a938ca-7d13-4cf3-8632-67d3d0e4f755'), UUID('b0d9a71e-9a45-455e-9d70-2d7bd22a2469'), UUID('2b7299f5-dbb8-446d-8bc6-18c48be1e89e'), UUID('140da049-fd29-487e-8336-2831bc8176ec')]}\n",
      "ERROR:root:Error in {'repo': 'janeczku/calibre-web', 'vulnerability_id': '2021-25965', 'commit': '50919d47212066c75f03ee7a5332ecf2d584b98e', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -49,7 +49,7 @@\\n     from cps.kobo import kobo, get_kobo_activated\\n     from cps.kobo_auth import kobo_auth\\n     kobo_available = get_kobo_activated()\\n-except ImportError:\\n+except (ImportError, AttributeError):   # Catch also error for not installed flask-wtf (missing csrf decorator)\\n     kobo_available = False\\n \\n try:', '@@ -118,6 +118,7 @@\\n {% endblock %}\\n {% block body %}\\n <h2 class=\"{{page}}\">{{_(title)}}</h2>\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"col-xs-12 col-sm-12\">\\n         <div class=\"row\">\\n           <div class=\"btn btn-default disabled\" id=\"user_delete_selection\" aria-disabled=\"true\">{{_(\\'Remove Selections\\')}}</div>', '@@ -214,6 +214,7 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n         <div class=\"custom_columns\">\\n           <p>\\n           <form id=\"have_read_form\" action=\"{{ url_for(\\'web.toggle_read\\', book_id=entry.id)}}\" method=\"POST\">\\n+            <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n             <label class=\"block-label\">\\n               <input id=\"have_read_cb\" data-checked=\"{{_(\\'Mark As Unread\\')}}\" data-unchecked=\"{{_(\\'Mark As Read\\')}}\" type=\"checkbox\" {% if have_read %}checked{% endif %} >\\n               <span>{{_(\\'Read\\')}}</span>\\n@@ -223,6 +224,7 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n           {% if g.user.check_visibility(32768) %}\\n           <p>\\n             <form id=\"archived_form\" action=\"{{ url_for(\\'web.toggle_archived\\', book_id=entry.id)}}\" method=\"POST\">\\n+              <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n               <label class=\"block-label\">\\n                 <input id=\"archived_cb\" data-checked=\"{{_(\\'Restore from archive\\')}}\" data-unchecked=\"{{_(\\'Add to archive\\')}}\" type=\"checkbox\" {% if is_archived %}checked{% endif %} >\\n                 <span>{{_(\\'Archived\\')}}</span>', \"@@ -29,6 +29,10 @@\\n import babel, pytz, requests, sqlalchemy\\n import werkzeug, flask, flask_login, flask_principal, jinja2\\n from flask_babel import gettext as _\\n+try:\\n+    from flask_wtf import __version__ as flaskwtf_version\\n+except ImportError:\\n+    flaskwtf_version = _(u'not installed')\\n \\n from . import db, calibre_db, converter, uploader, server, isoLanguages, constants\\n from .render_template import render_title_template\\n@@ -75,6 +79,7 @@\\n     Flask=flask.__version__,\\n     Flask_Login=flask_loginVersion,\\n     Flask_Principal=flask_principal.__version__,\\n+    Flask_WTF=flaskwtf_version,\\n     Werkzeug=werkzeug.__version__,\\n     Babel=babel.__version__,\\n     Jinja2=jinja2.__version__,\\n@@ -84,14 +89,14 @@\\n     SQLite=sqlite3.sqlite_version,\\n     iso639=isoLanguages.__version__,\\n     pytz=pytz.__version__,\\n-    Unidecode = unidecode_version,\\n-    Scholarly = scholarly_version,\\n-    Flask_SimpleLDAP =  u'installed' if bool(services.ldap) else None,\\n-    python_LDAP = services.ldapVersion if bool(services.ldapVersion) else None,\\n-    Goodreads = u'installed' if bool(services.goodreads_support) else None,\\n-    jsonschema = services.SyncToken.__version__  if bool(services.SyncToken) else None,\\n-    flask_dance = flask_danceVersion,\\n-    greenlet = greenlet_Version\\n+    Unidecode=unidecode_version,\\n+    Scholarly=scholarly_version,\\n+    Flask_SimpleLDAP=u'installed' if bool(services.ldap) else None,\\n+    python_LDAP=services.ldapVersion if bool(services.ldapVersion) else None,\\n+    Goodreads=u'installed' if bool(services.goodreads_support) else None,\\n+    jsonschema=services.SyncToken.__version__ if bool(services.SyncToken) else None,\\n+    flask_dance=flask_danceVersion,\\n+    greenlet=greenlet_Version\\n )\\n _VERSIONS.update(uploader.get_versions())\\n \", '@@ -23,7 +23,6 @@ if ($(\".tiny_editor\").length) {\\n \\n $(\".datepicker\").datepicker({\\n     format: \"yyyy-mm-dd\",\\n-    language: language\\n }).on(\"change\", function () {\\n     // Show localized date over top of the standard YYYY-MM-DD date\\n     var pubDate;', '@@ -3,6 +3,7 @@\\n <div class=\"discover\">\\n   <h1>{{title}}</h1>\\n   <form role=\"form\" method=\"POST\" autocomplete=\"off\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"col-md-10 col-lg-8\">\\n     {% if new_user or ( g.user and content.name != \"Guest\" and g.user.role_admin() ) %}\\n     <div class=\"form-group required\">', '@@ -112,6 +112,14 @@ $(\"#btn-upload\").change(function() {\\n     $(\"#form-upload\").submit();\\n });\\n \\n+$(\"#form-upload\").uploadprogress({\\n+    redirect_url: getPath() + \"/\", //\"{{ url_for(\\'web.index\\')}}\",\\n+    uploadedMsg: $(\"#form-upload\").data(\"message\"), //\"{{_(\\'Upload done, processing, please wait...\\')}}\",\\n+    modalTitle: $(\"#form-upload\").data(\"title\"), //\"{{_(\\'Uploading...\\')}}\",\\n+    modalFooter: $(\"#form-upload\").data(\"footer\"), //\"{{_(\\'Close\\')}}\",\\n+    modalTitleFailed: $(\"#form-upload\").data(\"failed\") //\"{{_(\\'Error\\')}}\"\\n+});\\n+\\n $(document).ready(function() {\\n   var inp = $(\\'#query\\').first()\\n   if (inp.length) {\\n@@ -223,6 +231,16 @@ $(function() {\\n     var preFilters = $.Callbacks();\\n     $.ajaxPrefilter(preFilters.fire);\\n \\n+    // equip all post requests with csrf_token\\n+    var csrftoken = $(\"input[name=\\'csrf_token\\']\").val();\\n+    $.ajaxSetup({\\n+        beforeSend: function(xhr, settings) {\\n+            if (!/^(GET|HEAD|OPTIONS|TRACE)$/i.test(settings.type) && !this.crossDomain) {\\n+                xhr.setRequestHeader(\"X-CSRFToken\", csrftoken)\\n+            }\\n+        }\\n+    });\\n+\\n     function restartTimer() {\\n         $(\"#spinner\").addClass(\"hidden\");\\n         $(\"#RestartDialog\").modal(\"hide\");\\n@@ -576,7 +594,7 @@ $(function() {\\n             method:\"post\",\\n             dataType: \"json\",\\n             url: window.location.pathname + \"/../../ajax/simulatedbchange\",\\n-            data: {config_calibre_dir: $(\"#config_calibre_dir\").val()},\\n+            data: {config_calibre_dir: $(\"#config_calibre_dir\").val(), csrf_token: $(\"input[name=\\'csrf_token\\']\").val()},\\n             success: function success(data) {\\n                 if ( data.change ) {\\n                     if ( data.valid ) {\\n@@ -712,7 +730,7 @@ $(function() {\\n             method:\"post\",\\n             contentType: \"application/json; charset=utf-8\",\\n             dataType: \"json\",\\n-            url: window.location.pathname + \"/../ajax/view\",\\n+            url: getPath() + \"/ajax/view\",\\n             data: \"{\\\\\"series\\\\\": {\\\\\"series_view\\\\\": \\\\\"\"+ view +\"\\\\\"}}\",\\n             success: function success() {\\n                 location.reload();', '@@ -8,6 +8,7 @@\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n   <form role=\"form\" method=\"POST\" class=\"col-md-10 col-lg-6\" action=\"{{ url_for(\\'admin.db_configuration\\') }}\" autocomplete=\"off\">\\n+       <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n        <label for=\"config_calibre_dir\">{{_(\\'Location of Calibre Database\\')}}</label>\\n        <div class=\"form-group required input-group\">\\n         <input type=\"text\" class=\"form-control\" id=\"config_calibre_dir\" name=\"config_calibre_dir\" value=\"{% if config.config_calibre_dir != None %}{{ config.config_calibre_dir }}{% endif %}\" autocomplete=\"off\">', '@@ -3,6 +3,7 @@\\n <h1 class=\"{{page}}\">{{title}}</h1>\\n <div class=\"col-md-10 col-lg-6\">\\n   <form role=\"form\" id=\"search\" action=\"{{ url_for(\\'web.advanced_search_form\\') }}\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group\">\\n       <label for=\"book_title\">{{_(\\'Book Title\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" name=\"book_title\" id=\"book_title\" value=\"\">', '@@ -6,8 +6,9 @@\\n {% block body %}\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n-  <form role=\"form\" method=\"POST\" autocomplete=\"off\" >\\n-<div class=\"panel-group class=\"col-md-10 col-lg-6\">\\n+<form role=\"form\" method=\"POST\" autocomplete=\"off\" >\\n+<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n+<div class=\"panel-group\" class=\"col-md-10 col-lg-6\">\\n   <div class=\"panel panel-default\">\\n     <div class=\"panel-heading\">\\n       <h4 class=\"panel-title\">', '@@ -61,7 +61,7 @@\\n               {% if g.user.role_upload() or g.user.role_admin()%}\\n                 {% if g.allow_upload %}\\n                   <li>\\n-                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'editbook.upload\\') }}\" method=\"post\" enctype=\"multipart/form-data\">\\n+                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'editbook.upload\\') }}\" data-title=\"{{_(\\'Uploading...\\')}}\" data-footer=\"{{_(\\'Close\\')}}\" data-failed=\"{{_(\\'Error\\')}}\" data-message=\"{{_(\\'Upload done, processing, please wait...\\')}}\" method=\"post\" enctype=\"multipart/form-data\">\\n                       <div class=\"form-group\">\\n                         <span class=\"btn btn-default btn-file\">{{_(\\'Upload\\')}}<input id=\"btn-upload\" name=\"btn-upload\"\\n                         type=\"file\" accept=\"{% for format in accept %}.{% if format != \\'\\'%}{{format}}{% else %}*{% endif %}{{ \\',\\' if not loop.last }}{% endfor %}\" multiple></span>\\n@@ -200,17 +200,6 @@ <h4 class=\"modal-title\" id=\"bookDetailsModalLabel\">{{_(\\'Book Details\\')}}</h4>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/libs/plugins.js\\') }}\"></script>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/libs/jquery.form.min.js\\') }}\"></script>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/uploadprogress.js\\') }}\"> </script>\\n-    <script type=\"text/javascript\">\\n-        $(function() {\\n-            $(\"#form-upload\").uploadprogress({\\n-              redirect_url: \"{{ url_for(\\'web.index\\')}}\",\\n-              uploadedMsg: \"{{_(\\'Upload done, processing, please wait...\\')}}\",\\n-              modalTitle: \"{{_(\\'Uploading...\\')}}\",\\n-              modalFooter: \"{{_(\\'Close\\')}}\",\\n-              modalTitleFailed: \"{{_(\\'Error\\')}}\"\\n-            });\\n-        });\\n-    </script>\\n     <script src=\"{{ url_for(\\'static\\', filename=\\'js/main.js\\') }}\"></script>\\n     {% if g.current_theme == 1 %}\\n       <script src=\"{{ url_for(\\'static\\', filename=\\'js/libs/jquery.visible.min.js\\') }}\"></script>', '@@ -1,5 +1,5 @@\\n <!DOCTYPE html>\\n-<html class=\"http-error\" lang=\"{{ g.user.locale }}\">\\n+<html class=\"http-error\">\\n   <head>\\n     <title>{{ instance }} | HTTP Error ({{ error_code }})</title>\\n     <meta charset=\"utf-8\">', '@@ -3,6 +3,7 @@\\n <div class=\"discover\">\\n   <h1>{{title}}</h1>\\n   <form role=\"form\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group\">\\n       <label for=\"title\">{{_(\\'Title\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" name=\"title\" id=\"title\" value=\"{{ shelf.name if shelf.name != None }}\">', '@@ -7,6 +7,7 @@\\n <div class=\"discover\">\\n   <h1>{{title}}</h1>\\n   <form role=\"form\" class=\"col-md-10 col-lg-6\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     {% if feature_support[\\'gmail\\'] %}\\n     <div class=\"form-group\">\\n       <label for=\"config_email_type\">{{_(\\'Choose Server Type\\')}}</label>\\n@@ -72,6 +73,7 @@ <h1>{{title}}</h1>\\n   <div class=\"col-md-10 col-lg-6\">\\n     <h2>{{_(\\'Allowed Domains (Whitelist)\\')}}</h2>\\n     <form id=\"domain_add_allow\" action=\"{{ url_for(\\'admin.add_domain\\',allow=1)}}\" method=\"POST\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group required\">\\n       <label for=\"domainname_allow\">{{_(\\'Add Domain\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" name=\"domainname\" id=\"domainname_allow\" >\\n@@ -98,11 +100,12 @@ <h2>{{_(\\'Denied Domains (Blacklist)\\')}}</h2>\\n       </thead>\\n     </table>\\n     <form id=\"domain_add_deny\" action=\"{{ url_for(\\'admin.add_domain\\',allow=0)}}\" method=\"POST\">\\n-    <div class=\"form-group required\">\\n-      <label for=\"domainname_deny\">{{_(\\'Add Domain\\')}}</label>\\n-      <input type=\"text\" class=\"form-control\" name=\"domainname\" id=\"domainname_deny\" >\\n-    </div>\\n-    <button id=\"domain_deny_submit\" class=\"btn btn-default\">{{_(\\'Add\\')}}</button>\\n+      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n+      <div class=\"form-group required\">\\n+        <label for=\"domainname_deny\">{{_(\\'Add Domain\\')}}</label>\\n+        <input type=\"text\" class=\"form-control\" name=\"domainname\" id=\"domainname_deny\" >\\n+      </div>\\n+      <button id=\"domain_deny_submit\" class=\"btn btn-default\">{{_(\\'Add\\')}}</button>\\n     </form>\\n     </div>\\n ', '@@ -20,6 +20,7 @@\\n {% endblock %}\\n {% block body %}\\n <h2 class=\"{{page}}\">{{_(title)}}</h2>\\n+      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n       <div class=\"col-xs-12 col-sm-6\">\\n         <div class=\"row form-group\">\\n           <div class=\"btn btn-default disabled\" id=\"merge_books\" aria-disabled=\"true\">{{_(\\'Merge selected books\\')}}</div>', '@@ -20,6 +20,7 @@ <h1 class=\"{{page}}\">{{_(title)}}</h1>\\n       </div>\\n \\n       {% if data == \"series\" %}\\n+      <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n       <button class=\"update-view btn btn-primary\" data-target=\"series_view\" id=\"grid-button\" data-view=\"grid\">Grid</button>\\n       {% endif %}\\n     </div>', '@@ -47,7 +47,8 @@\\n from sqlalchemy.sql import select\\n import requests\\n \\n-from . import config, logger, kobo_auth, db, calibre_db, helper, shelf as shelf_lib, ub\\n+\\n+from . import config, logger, kobo_auth, db, calibre_db, helper, shelf as shelf_lib, ub, csrf\\n from .constants import sqlalchemy_version2\\n from .helper import get_download_link\\n from .services import SyncToken as SyncToken\\n@@ -505,7 +506,7 @@ def get_metadata(book):\\n \\n     return metadata\\n \\n-\\n+@csrf.exempt\\n @kobo.route(\"/v1/library/tags\", methods=[\"POST\", \"DELETE\"])\\n @requires_kobo_auth\\n # Creates a Shelf with the given items, and returns the shelf\\'s uuid.\\n@@ -595,6 +596,7 @@ def add_items_to_shelf(items, shelf):\\n     return items_unknown_to_calibre\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/library/tags/<tag_id>/items\", methods=[\"POST\"])\\n @requires_kobo_auth\\n def HandleTagAddItem(tag_id):\\n@@ -624,6 +626,7 @@ def HandleTagAddItem(tag_id):\\n     return make_response(\\'\\', 201)\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/library/tags/<tag_id>/items/delete\", methods=[\"POST\"])\\n @requires_kobo_auth\\n def HandleTagRemoveItem(tag_id):\\n@@ -983,6 +986,7 @@ def HandleUnimplementedRequest(dummy=None):\\n \\n \\n # TODO: Implement the following routes\\n+@csrf.exempt\\n @kobo.route(\"/v1/user/loyalty/<dummy>\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/user/profile\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/user/wishlist\", methods=[\"GET\", \"POST\"])\\n@@ -993,6 +997,7 @@ def HandleUserRequest(dummy=None):\\n     return redirect_or_proxy_request()\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/products/<dummy>/prices\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/products/<dummy>/recommendations\", methods=[\"GET\", \"POST\"])\\n @kobo.route(\"/v1/products/<dummy>/nextread\", methods=[\"GET\", \"POST\"])\\n@@ -1026,6 +1031,7 @@ def make_calibre_web_auth_response():\\n     )\\n \\n \\n+@csrf.exempt\\n @kobo.route(\"/v1/auth/device\", methods=[\"POST\"])\\n @requires_kobo_auth\\n def HandleAuthRequest():', '@@ -3,6 +3,7 @@\\n <div class=\"well col-sm-6 col-sm-offset-2\">\\n   <h2 style=\"margin-top: 0\">{{_(\\'Register New Account\\')}}</h2>\\n   <form method=\"POST\" role=\"form\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     {% if not config.config_register_email %}\\n     <div class=\"form-group required\">\\n       <label for=\"name\">{{_(\\'Username\\')}}</label>', '@@ -23,6 +23,7 @@\\n {%  if source_formats|length > 0 and conversion_formats|length > 0 %}\\n   <div class=\"text-center more-stuff\"><h4>{{_(\\'Convert book format:\\')}}</h4>\\n       <form class=\"padded-bottom\" action=\"{{ url_for(\\'editbook.convert_bookformat\\', book_id=book.id) }}\" method=\"post\" id=\"book_convert_frm\">\\n+          <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n           <div class=\"form-group\">\\n               <div class=\"text-left\">\\n                   <label class=\"control-label\" for=\"book_format_from\">{{_(\\'Convert from:\\')}}</label>', '@@ -8,6 +8,7 @@\\n <div class=\"discover\">\\n   <h2>{{title}}</h2>\\n <form role=\"form\" method=\"POST\" autocomplete=\"off\">\\n+<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n <div class=\"panel-group col-md-10 col-lg-8\">\\n   <div class=\"panel panel-default\">\\n     <div class=\"panel-heading\">', '@@ -4,6 +4,7 @@\\n   <h2 style=\"margin-top: 0\">{{_(\\'Login\\')}}</h2>\\n   <form method=\"POST\" role=\"form\">\\n     <input type=\"hidden\" name=\"next\" value=\"{{next_url}}\">\\n+    <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n     <div class=\"form-group\">\\n       <label for=\"username\">{{_(\\'Username\\')}}</label>\\n       <input type=\"text\" class=\"form-control\" id=\"username\" name=\"username\" placeholder=\"{{_(\\'Username\\')}}\">', '@@ -84,14 +84,13 @@\\n \\n @app.after_request\\n def add_security_headers(resp):\\n-    resp.headers[\\'Content-Security-Policy\\'] = \"default-src \\'self\\' \\'unsafe-inline\\' \\'unsafe-eval\\';\"\\n+    resp.headers[\\'Content-Security-Policy\\'] = \"default-src \\'self\\' \\'unsafe-inline\\' \\'unsafe-eval\\'; img-src \\'self\\' data:\"\\n     if request.endpoint == \"editbook.edit_book\":\\n-        resp.headers[\\'Content-Security-Policy\\'] += \"img-src * data:\"\\n+        resp.headers[\\'Content-Security-Policy\\'] += \" *\"\\n     resp.headers[\\'X-Content-Type-Options\\'] = \\'nosniff\\'\\n     resp.headers[\\'X-Frame-Options\\'] = \\'SAMEORIGIN\\'\\n     resp.headers[\\'X-XSS-Protection\\'] = \\'1; mode=block\\'\\n     resp.headers[\\'Strict-Transport-Security\\'] = \\'max-age=31536000; includeSubDomains\\'\\n-    # log.debug(request.full_path)\\n     return resp\\n \\n web = Blueprint(\\'web\\', __name__)'], 'file': ['cps.py', 'cps/templates/user_table.html', 'cps/templates/detail.html', 'cps/about.py', 'cps/static/js/edit_books.js', 'cps/templates/user_edit.html', 'cps/static/js/main.js', 'cps/templates/config_db.html', 'cps/templates/search_form.html', 'cps/templates/config_view_edit.html', 'cps/templates/layout.html', 'cps/templates/http_error.html', 'cps/templates/shelf_edit.html', 'cps/templates/email_edit.html', 'cps/templates/book_table.html', 'cps/templates/list.html', 'cps/kobo.py', 'cps/templates/register.html', 'cps/templates/book_edit.html', 'cps/templates/config_edit.html', 'cps/templates/login.html', 'cps/web.py'], 'language': ['Python', 'HTML', 'HTML', 'Python', 'JavaScript/TypeScript', 'HTML', 'JavaScript/TypeScript', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'HTML', 'Python', 'HTML', 'HTML', 'HTML', 'HTML', 'Python'], 'temp_id': [UUID('9ebe90d7-5500-4aa2-b37a-1bcd340d629c'), UUID('356ce7b2-7586-4e00-8c76-88442be5761b'), UUID('4b6868cd-2666-4991-b8cb-cfb5352044c8'), UUID('e30badcb-7ea3-42db-a702-35428b408bbe'), UUID('c075e2bb-db74-49cd-9eed-76648481d54a'), UUID('e4a41919-9d11-465f-ae87-5955bc3478d7'), UUID('370ec36a-9976-45d6-b5e0-ff7d12b22948'), UUID('086aad70-3a18-4676-8f45-0cd970c68ecc'), UUID('3cd3d5e8-8257-49bb-ab95-3052d0b6e374'), UUID('69fb9b96-e516-4ccb-9c6a-ac71b51dc9f4'), UUID('6fba77c6-2456-4bad-a5e9-7c677ad74f79'), UUID('60eca2e8-df4c-4dbc-b0a2-b29888c76041'), UUID('04717fd5-a4be-4377-9af3-29fb38a44e71'), UUID('cdfc1282-5f32-46f8-91e3-ec14508169f1'), UUID('e1c5f5d8-e409-4504-b6d3-113690e6883d'), UUID('094b0416-3970-44fc-8d5a-997b75c254a3'), UUID('9243765c-a7ee-4235-825e-978cef95680b'), UUID('9017cb19-bbea-4dde-85c2-81a577292bf4'), UUID('33a938ca-7d13-4cf3-8632-67d3d0e4f755'), UUID('b0d9a71e-9a45-455e-9d70-2d7bd22a2469'), UUID('2b7299f5-dbb8-446d-8bc6-18c48be1e89e'), UUID('140da049-fd29-487e-8336-2831bc8176ec')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:0: except (ImportError, AttributeError):                                                                             \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:0: except (ImportError, AttributeError):   # Catch also error for not installed flask-wtf (missing csrf decorator)\n",
      " 15%|â–ˆâ–Œ        | 276/1800 [03:39<13:12,  1.92it/s]ERROR:src.process_code_changes:Error processing commit 8876b07abde0c8d2a4974f79b60562b6d0193db9\n",
      "ERROR:src.process_code_changes:{'repo': 'lyft/confidant', 'vulnerability_id': '2024-45793', 'commit': '8876b07abde0c8d2a4974f79b60562b6d0193db9', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -76,6 +76,7 @@ def get_iam_roles_list():\\n \\n \\n @blueprint.route(\\'/v1/services\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_service_list():\\n     \"\"\"\\n@@ -164,10 +165,12 @@ def get_service_list():\\n             services_response = ServicesResponse.from_services(\\n                 Service.data_type_date_index.query(\\'service\\'),\\n             )\\n+\\n         return services_response_schema.dumps(services_response)\\n \\n \\n @blueprint.route(\\'/v1/services/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_service(id):\\n     \\'\\'\\'\\n@@ -334,6 +337,7 @@ def get_service(id):\\n \\n \\n @blueprint.route(\\'/v1/archive/services/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_service_revisions(id):\\n     \"\"\"\\n@@ -414,6 +418,7 @@ def get_archive_service_revisions(id):\\n \\n \\n @blueprint.route(\\'/v1/archive/services\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_service_list():\\n     \"\"\"\\n@@ -492,6 +497,7 @@ def get_archive_service_list():\\n \\n \\n @blueprint.route(\\'/v1/services/<id>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode\\n@@ -701,6 +707,7 @@ def map_service_credentials(id):\\n \\n \\n @blueprint.route(\\'/v1/services/<id>/<to_revision>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode', '@@ -1,6 +1,8 @@\\n+from functools import wraps\\n import importlib\\n import pytz\\n from datetime import datetime\\n+from flask import make_response\\n \\n \\n def dict_deep_update(a, b):\\n@@ -54,3 +56,28 @@ def utcnow():\\n     \"\"\"\\n     now = datetime.utcnow()\\n     return now.replace(tzinfo=pytz.utc)\\n+\\n+\\n+def prevent_xss_decorator(func):\\n+    \"\"\"\\n+    Prevents XSS attacks:\\n+     1. Set content type to be application/json\\n+     2. Set Content Security Policy (already specified at app level)\\n+     3. Enable XSS Protection\\n+     4. Prevent MIME Type Sniffing\\n+     5. Limit Referrer Information\\n+    \"\"\"\\n+    @wraps(func)\\n+    def wrapper(*args, **kwargs):\\n+        # Call the original function to get the response\\n+        pre_xss_response = func(*args, **kwargs)\\n+\\n+        # Apply XSS prevention\\n+        response = make_response(pre_xss_response)\\n+        response.headers[\\'Content-Type\\'] = \\'application/json\\'\\n+        response.headers[\\'X-XSS-Protection\\'] = \\'1; mode=block\\'\\n+        response.headers[\\'X-Content-Type-Options\\'] = \\'nosniff\\'\\n+        response.headers[\\'Referrer-Policy\\'] = \\'no-referrer\\'\\n+\\n+        return response\\n+    return wrapper', '@@ -36,6 +36,7 @@\\n \\n \\n @blueprint.route(\\'/v1/credentials\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_credential_list():\\n     \"\"\"\\n@@ -132,6 +133,7 @@ def get_credential_list():\\n \\n \\n @blueprint.route(\\'/v1/credentials/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_credential(id):\\n     \"\"\"\\n@@ -369,6 +371,7 @@ def diff_credential(id, old_revision, new_revision):\\n \\n \\n @blueprint.route(\\'/v1/archive/credentials/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_credential_revisions(id):\\n     \"\"\"\\n@@ -451,6 +454,7 @@ def get_archive_credential_revisions(id):\\n \\n \\n @blueprint.route(\\'/v1/archive/credentials\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_credential_list():\\n     \"\"\"\\n@@ -534,6 +538,7 @@ def get_archive_credential_list():\\n \\n \\n @blueprint.route(\\'/v1/credentials\\', methods=[\\'POST\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode\\n@@ -727,6 +732,7 @@ def get_credential_dependencies(id):\\n \\n \\n @blueprint.route(\\'/v1/credentials/<id>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode\\n@@ -952,6 +958,7 @@ def update_credential(id):\\n \\n \\n @blueprint.route(\\'/v1/credentials/<id>/<to_revision>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode'], 'file': ['confidant/routes/services.py', 'confidant/utils/misc.py', 'confidant/routes/credentials.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('82345cad-6423-418b-aa0b-388239e60f18'), UUID('2a73ba7d-ba19-41a1-abe9-5ee664010c98'), UUID('7c6b364b-84ff-42d9-9b41-6dbe6d3a6ed4')]}\n",
      "ERROR:root:Error in {'repo': 'lyft/confidant', 'vulnerability_id': '2024-45793', 'commit': '8876b07abde0c8d2a4974f79b60562b6d0193db9', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -76,6 +76,7 @@ def get_iam_roles_list():\\n \\n \\n @blueprint.route(\\'/v1/services\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_service_list():\\n     \"\"\"\\n@@ -164,10 +165,12 @@ def get_service_list():\\n             services_response = ServicesResponse.from_services(\\n                 Service.data_type_date_index.query(\\'service\\'),\\n             )\\n+\\n         return services_response_schema.dumps(services_response)\\n \\n \\n @blueprint.route(\\'/v1/services/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_service(id):\\n     \\'\\'\\'\\n@@ -334,6 +337,7 @@ def get_service(id):\\n \\n \\n @blueprint.route(\\'/v1/archive/services/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_service_revisions(id):\\n     \"\"\"\\n@@ -414,6 +418,7 @@ def get_archive_service_revisions(id):\\n \\n \\n @blueprint.route(\\'/v1/archive/services\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_service_list():\\n     \"\"\"\\n@@ -492,6 +497,7 @@ def get_archive_service_list():\\n \\n \\n @blueprint.route(\\'/v1/services/<id>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode\\n@@ -701,6 +707,7 @@ def map_service_credentials(id):\\n \\n \\n @blueprint.route(\\'/v1/services/<id>/<to_revision>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode', '@@ -1,6 +1,8 @@\\n+from functools import wraps\\n import importlib\\n import pytz\\n from datetime import datetime\\n+from flask import make_response\\n \\n \\n def dict_deep_update(a, b):\\n@@ -54,3 +56,28 @@ def utcnow():\\n     \"\"\"\\n     now = datetime.utcnow()\\n     return now.replace(tzinfo=pytz.utc)\\n+\\n+\\n+def prevent_xss_decorator(func):\\n+    \"\"\"\\n+    Prevents XSS attacks:\\n+     1. Set content type to be application/json\\n+     2. Set Content Security Policy (already specified at app level)\\n+     3. Enable XSS Protection\\n+     4. Prevent MIME Type Sniffing\\n+     5. Limit Referrer Information\\n+    \"\"\"\\n+    @wraps(func)\\n+    def wrapper(*args, **kwargs):\\n+        # Call the original function to get the response\\n+        pre_xss_response = func(*args, **kwargs)\\n+\\n+        # Apply XSS prevention\\n+        response = make_response(pre_xss_response)\\n+        response.headers[\\'Content-Type\\'] = \\'application/json\\'\\n+        response.headers[\\'X-XSS-Protection\\'] = \\'1; mode=block\\'\\n+        response.headers[\\'X-Content-Type-Options\\'] = \\'nosniff\\'\\n+        response.headers[\\'Referrer-Policy\\'] = \\'no-referrer\\'\\n+\\n+        return response\\n+    return wrapper', '@@ -36,6 +36,7 @@\\n \\n \\n @blueprint.route(\\'/v1/credentials\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_credential_list():\\n     \"\"\"\\n@@ -132,6 +133,7 @@ def get_credential_list():\\n \\n \\n @blueprint.route(\\'/v1/credentials/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_credential(id):\\n     \"\"\"\\n@@ -369,6 +371,7 @@ def diff_credential(id, old_revision, new_revision):\\n \\n \\n @blueprint.route(\\'/v1/archive/credentials/<id>\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_credential_revisions(id):\\n     \"\"\"\\n@@ -451,6 +454,7 @@ def get_archive_credential_revisions(id):\\n \\n \\n @blueprint.route(\\'/v1/archive/credentials\\', methods=[\\'GET\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n def get_archive_credential_list():\\n     \"\"\"\\n@@ -534,6 +538,7 @@ def get_archive_credential_list():\\n \\n \\n @blueprint.route(\\'/v1/credentials\\', methods=[\\'POST\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode\\n@@ -727,6 +732,7 @@ def get_credential_dependencies(id):\\n \\n \\n @blueprint.route(\\'/v1/credentials/<id>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode\\n@@ -952,6 +958,7 @@ def update_credential(id):\\n \\n \\n @blueprint.route(\\'/v1/credentials/<id>/<to_revision>\\', methods=[\\'PUT\\'])\\n+@misc.prevent_xss_decorator\\n @authnz.require_auth\\n @authnz.require_csrf_token\\n @maintenance.check_maintenance_mode'], 'file': ['confidant/routes/services.py', 'confidant/utils/misc.py', 'confidant/routes/credentials.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('82345cad-6423-418b-aa0b-388239e60f18'), UUID('2a73ba7d-ba19-41a1-abe9-5ee664010c98'), UUID('7c6b364b-84ff-42d9-9b41-6dbe6d3a6ed4')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 7:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 7:0: <line number missing in source>\n",
      " 16%|â–ˆâ–Œ        | 279/1800 [03:40<11:36,  2.18it/s]ERROR:src.process_code_changes:Error processing commit e0264a61119d551658d9445af38323ba94fc16db\n",
      "ERROR:src.process_code_changes:{'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': 'e0264a61119d551658d9445af38323ba94fc16db', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -9,6 +9,7 @@\\n import itertools\\n import os\\n import posixpath\\n+import re\\n import shutil\\n import stat\\n import struct\\n@@ -2182,7 +2183,65 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class CompleteDirs(ZipFile):\\n+class SanitizedNames:\\n+    \"\"\"\\n+    ZipFile mix-in to ensure names are sanitized.\\n+    \"\"\"\\n+\\n+    def namelist(self):\\n+        return list(map(self._sanitize, super().namelist()))\\n+\\n+    @staticmethod\\n+    def _sanitize(name):\\n+        r\"\"\"\\n+        Ensure a relative path with posix separators and no dot names.\\n+        Modeled after\\n+        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n+        but provides consistent cross-platform behavior.\\n+        >>> san = SanitizedNames._sanitize\\n+        >>> san(\\'/foo/bar\\')\\n+        \\'foo/bar\\'\\n+        >>> san(\\'//foo.txt\\')\\n+        \\'foo.txt\\'\\n+        >>> san(\\'foo/.././bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'foo../.bar.txt\\')\\n+        \\'foo../.bar.txt\\'\\n+        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n+        \\'D/foo.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n+        \\'server/share/file.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n+        \\'?/GLOBALROOT/Volume3\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n+        \\'PhysicalDrive1/root\\'\\n+        Retain any trailing slash.\\n+        >>> san(\\'abc/\\')\\n+        \\'abc/\\'\\n+        Raises a ValueError if the result is empty.\\n+        >>> san(\\'../..\\')\\n+        Traceback (most recent call last):\\n+        ...\\n+        ValueError: Empty filename\\n+        \"\"\"\\n+\\n+        def allowed(part):\\n+            return part and part not in {\\'..\\', \\'.\\'}\\n+\\n+        # Remove the drive letter.\\n+        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n+        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n+        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n+        parts = clean.split(\\'/\\')\\n+        joined = \\'/\\'.join(filter(allowed, parts))\\n+        if not joined:\\n+            raise ValueError(\"Empty filename\")\\n+        return joined + \\'/\\' * name.endswith(\\'/\\')\\n+\\n+\\n+class CompleteDirs(SanitizedNames, ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('255d079c-a12d-4dc6-9109-7b2627549b4a')]}\n",
      "ERROR:root:Error in {'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': 'e0264a61119d551658d9445af38323ba94fc16db', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -9,6 +9,7 @@\\n import itertools\\n import os\\n import posixpath\\n+import re\\n import shutil\\n import stat\\n import struct\\n@@ -2182,7 +2183,65 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class CompleteDirs(ZipFile):\\n+class SanitizedNames:\\n+    \"\"\"\\n+    ZipFile mix-in to ensure names are sanitized.\\n+    \"\"\"\\n+\\n+    def namelist(self):\\n+        return list(map(self._sanitize, super().namelist()))\\n+\\n+    @staticmethod\\n+    def _sanitize(name):\\n+        r\"\"\"\\n+        Ensure a relative path with posix separators and no dot names.\\n+        Modeled after\\n+        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n+        but provides consistent cross-platform behavior.\\n+        >>> san = SanitizedNames._sanitize\\n+        >>> san(\\'/foo/bar\\')\\n+        \\'foo/bar\\'\\n+        >>> san(\\'//foo.txt\\')\\n+        \\'foo.txt\\'\\n+        >>> san(\\'foo/.././bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'foo../.bar.txt\\')\\n+        \\'foo../.bar.txt\\'\\n+        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n+        \\'D/foo.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n+        \\'server/share/file.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n+        \\'?/GLOBALROOT/Volume3\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n+        \\'PhysicalDrive1/root\\'\\n+        Retain any trailing slash.\\n+        >>> san(\\'abc/\\')\\n+        \\'abc/\\'\\n+        Raises a ValueError if the result is empty.\\n+        >>> san(\\'../..\\')\\n+        Traceback (most recent call last):\\n+        ...\\n+        ValueError: Empty filename\\n+        \"\"\"\\n+\\n+        def allowed(part):\\n+            return part and part not in {\\'..\\', \\'.\\'}\\n+\\n+        # Remove the drive letter.\\n+        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n+        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n+        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n+        parts = clean.split(\\'/\\')\\n+        joined = \\'/\\'.join(filter(allowed, parts))\\n+        if not joined:\\n+            raise ValueError(\"Empty filename\")\\n+        return joined + \\'/\\' * name.endswith(\\'/\\')\\n+\\n+\\n+class CompleteDirs(SanitizedNames, ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('255d079c-a12d-4dc6-9109-7b2627549b4a')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 27:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 64:0: <line number missing in source>\n",
      " 16%|â–ˆâ–Œ        | 288/1800 [03:41<07:43,  3.26it/s]ERROR:src.process_code_changes:Error processing commit b01da7aab5cd02129941d2a900e6e5e3b5f7d4fb\n",
      "ERROR:src.process_code_changes:{'repo': 'python-rope/rope', 'vulnerability_id': '2014-3539', 'commit': 'b01da7aab5cd02129941d2a900e6e5e3b5f7d4fb', 'commit_source': 'github', 'cwe_id': ['CWE-502'], 'patch': [\"@@ -3,14 +3,17 @@ def __rope_start_everything():\\n     import sys\\n     import socket\\n     try:\\n-        import pickle\\n-    except ImportError:\\n         import cPickle as pickle\\n+    except ImportError:\\n+        import pickle\\n     import marshal\\n     import inspect\\n     import types\\n     import threading\\n     import rope.base.utils.pycompat as pycompat\\n+    import base64\\n+    import hashlib\\n+    import hmac\\n \\n     class _MessageSender(object):\\n \\n@@ -19,15 +22,19 @@ def send_data(self, data):\\n \\n     class _SocketSender(_MessageSender):\\n \\n-        def __init__(self, port):\\n+        def __init__(self, port, key):\\n             s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n             s.connect(('127.0.0.1', port))\\n             self.my_file = s.makefile('wb')\\n+            self.key = base64.b64decode(key)\\n \\n         def send_data(self, data):\\n             if not self.my_file.closed:\\n-                pickle.dump(data, self.my_file)\\n-\\n+                pickled_data = base64.b64encode(\\n+                    pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\\n+                dgst = hmac.new(self.key, pickled_data, hashlib.sha256).digest()\\n+                self.my_file.write(base64.b64encode(dgst) + b':' +\\n+                                   pickled_data + b'\\\\n')\\n         def close(self):\\n             self.my_file.close()\\n \\n@@ -58,8 +65,9 @@ class _FunctionCallDataSender(object):\\n \\n         def __init__(self, send_info, project_root):\\n             self.project_root = project_root\\n-            if send_info.isdigit():\\n-                self.sender = _SocketSender(int(send_info))\\n+            if send_info[0].isdigit():\\n+                port, key = send_info.split(':', 1)\\n+                self.sender = _SocketSender(int(port), key)\\n             else:\\n                 self.sender = _FileSender(send_info)\\n \", '@@ -1,7 +1,10 @@\\n+import base64\\n+import hashlib\\n+import hmac\\n try:\\n-    import pickle\\n-except ImportError:\\n     import cPickle as pickle\\n+except ImportError:\\n+    import pickle\\n import marshal\\n import os\\n import socket\\n@@ -11,6 +14,28 @@\\n import threading\\n \\n \\n+def _compat_compare_digest(a, b):\\n+    \"\"\"Implementation of hmac.compare_digest for python < 2.7.7.\\n+\\n+    This function uses an approach designed to prevent timing analysis by\\n+    avoiding content-based short circuiting behaviour, making it appropriate\\n+    for cryptography.\\n+    \"\"\"\\n+    if len(a) != len(b):\\n+        return False\\n+    # Computes the bitwise difference of all characters in the two strings\\n+    # before returning whether or not they are equal.\\n+    difference = 0\\n+    for (a_char, b_char) in zip(a, b):\\n+        difference |= ord(a_char) ^ ord(b_char)\\n+    return difference == 0\\n+\\n+try:\\n+    from hmac import compare_digest\\n+except ImportError:\\n+    compare_digest = _compat_compare_digest\\n+\\n+\\n class PythonFileRunner(object):\\n     \"\"\"A class for running python project files\"\"\"\\n \\n@@ -114,24 +139,55 @@ class _SocketReceiver(_MessageReceiver):\\n     def __init__(self):\\n         self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n         self.data_port = 3037\\n+        self.key = os.urandom(32)\\n+\\n         while self.data_port < 4000:\\n             try:\\n-                self.server_socket.bind((\\'\\', self.data_port))\\n+                self.server_socket.bind((\\'localhost\\', self.data_port))\\n                 break\\n             except socket.error:\\n                 self.data_port += 1\\n         self.server_socket.listen(1)\\n \\n     def get_send_info(self):\\n-        return str(self.data_port)\\n+        return \\'%d:%s\\' % (self.data_port,\\n+                          base64.b64encode(self.key).decode(\\'utf-8\\'))\\n \\n     def receive_data(self):\\n         conn, addr = self.server_socket.accept()\\n         self.server_socket.close()\\n         my_file = conn.makefile(\\'rb\\')\\n         while True:\\n+            # Received messages must meet the following criteria:\\n+            # 1. Must be contained on a single line.\\n+            # 2. Must be prefixed with a base64 encoded sha256 message digest \\n+            #    of the base64 encoded pickle data.\\n+            # 3. Message digest must be computed using the correct key.\\n+            #\\n+            # Any messages received that do not meet these criteria will never\\n+            # be unpickled and will be dropped silently.\\n             try:\\n-                yield pickle.load(my_file)\\n+                buf = my_file.readline()\\n+                if len(buf) == 0:\\n+                    break\\n+\\n+                try:\\n+                    digest_end = buf.index(b\\':\\')\\n+                    buf_digest = base64.b64decode(buf[:digest_end])\\n+                    buf_data = buf[digest_end + 1:-1]\\n+                    decoded_buf_data = base64.b64decode(buf_data)\\n+                except:\\n+                    # Corrupted data; the payload cannot be trusted and just has\\n+                    # to be dropped. See CVE-2014-3539.\\n+                    continue\\n+\\n+                digest = hmac.new(self.key, buf_data, hashlib.sha256).digest()\\n+                if not compare_digest(buf_digest, digest):\\n+                    # Signature mismatch; the payload cannot be trusted and just\\n+                    # has to be dropped. See CVE-2014-3539.\\n+                    continue\\n+\\n+                yield pickle.loads(decoded_buf_data)\\n             except EOFError:\\n                 break\\n         my_file.close()', '@@ -11,9 +11,9 @@\\n from rope.base.resources import File, Folder, _ResourceMatcher\\n \\n try:\\n-    import pickle\\n-except ImportError:\\n     import cPickle as pickle\\n+except ImportError:\\n+    import pickle\\n \\n \\n class _Project(object):'], 'file': ['rope/base/oi/runmod.py', 'rope/base/oi/doa.py', 'rope/base/project.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('1f26e76f-e760-4532-a790-6cf2f5b69975'), UUID('e755e544-a587-4780-b03c-6ead2875318f'), UUID('3ef76174-26af-42c7-9792-0c4c8f53be13')]}\n",
      "ERROR:root:Error in {'repo': 'python-rope/rope', 'vulnerability_id': '2014-3539', 'commit': 'b01da7aab5cd02129941d2a900e6e5e3b5f7d4fb', 'commit_source': 'github', 'cwe_id': ['CWE-502'], 'patch': [\"@@ -3,14 +3,17 @@ def __rope_start_everything():\\n     import sys\\n     import socket\\n     try:\\n-        import pickle\\n-    except ImportError:\\n         import cPickle as pickle\\n+    except ImportError:\\n+        import pickle\\n     import marshal\\n     import inspect\\n     import types\\n     import threading\\n     import rope.base.utils.pycompat as pycompat\\n+    import base64\\n+    import hashlib\\n+    import hmac\\n \\n     class _MessageSender(object):\\n \\n@@ -19,15 +22,19 @@ def send_data(self, data):\\n \\n     class _SocketSender(_MessageSender):\\n \\n-        def __init__(self, port):\\n+        def __init__(self, port, key):\\n             s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n             s.connect(('127.0.0.1', port))\\n             self.my_file = s.makefile('wb')\\n+            self.key = base64.b64decode(key)\\n \\n         def send_data(self, data):\\n             if not self.my_file.closed:\\n-                pickle.dump(data, self.my_file)\\n-\\n+                pickled_data = base64.b64encode(\\n+                    pickle.dumps(data, pickle.HIGHEST_PROTOCOL))\\n+                dgst = hmac.new(self.key, pickled_data, hashlib.sha256).digest()\\n+                self.my_file.write(base64.b64encode(dgst) + b':' +\\n+                                   pickled_data + b'\\\\n')\\n         def close(self):\\n             self.my_file.close()\\n \\n@@ -58,8 +65,9 @@ class _FunctionCallDataSender(object):\\n \\n         def __init__(self, send_info, project_root):\\n             self.project_root = project_root\\n-            if send_info.isdigit():\\n-                self.sender = _SocketSender(int(send_info))\\n+            if send_info[0].isdigit():\\n+                port, key = send_info.split(':', 1)\\n+                self.sender = _SocketSender(int(port), key)\\n             else:\\n                 self.sender = _FileSender(send_info)\\n \", '@@ -1,7 +1,10 @@\\n+import base64\\n+import hashlib\\n+import hmac\\n try:\\n-    import pickle\\n-except ImportError:\\n     import cPickle as pickle\\n+except ImportError:\\n+    import pickle\\n import marshal\\n import os\\n import socket\\n@@ -11,6 +14,28 @@\\n import threading\\n \\n \\n+def _compat_compare_digest(a, b):\\n+    \"\"\"Implementation of hmac.compare_digest for python < 2.7.7.\\n+\\n+    This function uses an approach designed to prevent timing analysis by\\n+    avoiding content-based short circuiting behaviour, making it appropriate\\n+    for cryptography.\\n+    \"\"\"\\n+    if len(a) != len(b):\\n+        return False\\n+    # Computes the bitwise difference of all characters in the two strings\\n+    # before returning whether or not they are equal.\\n+    difference = 0\\n+    for (a_char, b_char) in zip(a, b):\\n+        difference |= ord(a_char) ^ ord(b_char)\\n+    return difference == 0\\n+\\n+try:\\n+    from hmac import compare_digest\\n+except ImportError:\\n+    compare_digest = _compat_compare_digest\\n+\\n+\\n class PythonFileRunner(object):\\n     \"\"\"A class for running python project files\"\"\"\\n \\n@@ -114,24 +139,55 @@ class _SocketReceiver(_MessageReceiver):\\n     def __init__(self):\\n         self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n         self.data_port = 3037\\n+        self.key = os.urandom(32)\\n+\\n         while self.data_port < 4000:\\n             try:\\n-                self.server_socket.bind((\\'\\', self.data_port))\\n+                self.server_socket.bind((\\'localhost\\', self.data_port))\\n                 break\\n             except socket.error:\\n                 self.data_port += 1\\n         self.server_socket.listen(1)\\n \\n     def get_send_info(self):\\n-        return str(self.data_port)\\n+        return \\'%d:%s\\' % (self.data_port,\\n+                          base64.b64encode(self.key).decode(\\'utf-8\\'))\\n \\n     def receive_data(self):\\n         conn, addr = self.server_socket.accept()\\n         self.server_socket.close()\\n         my_file = conn.makefile(\\'rb\\')\\n         while True:\\n+            # Received messages must meet the following criteria:\\n+            # 1. Must be contained on a single line.\\n+            # 2. Must be prefixed with a base64 encoded sha256 message digest \\n+            #    of the base64 encoded pickle data.\\n+            # 3. Message digest must be computed using the correct key.\\n+            #\\n+            # Any messages received that do not meet these criteria will never\\n+            # be unpickled and will be dropped silently.\\n             try:\\n-                yield pickle.load(my_file)\\n+                buf = my_file.readline()\\n+                if len(buf) == 0:\\n+                    break\\n+\\n+                try:\\n+                    digest_end = buf.index(b\\':\\')\\n+                    buf_digest = base64.b64decode(buf[:digest_end])\\n+                    buf_data = buf[digest_end + 1:-1]\\n+                    decoded_buf_data = base64.b64decode(buf_data)\\n+                except:\\n+                    # Corrupted data; the payload cannot be trusted and just has\\n+                    # to be dropped. See CVE-2014-3539.\\n+                    continue\\n+\\n+                digest = hmac.new(self.key, buf_data, hashlib.sha256).digest()\\n+                if not compare_digest(buf_digest, digest):\\n+                    # Signature mismatch; the payload cannot be trusted and just\\n+                    # has to be dropped. See CVE-2014-3539.\\n+                    continue\\n+\\n+                yield pickle.loads(decoded_buf_data)\\n             except EOFError:\\n                 break\\n         my_file.close()', '@@ -11,9 +11,9 @@\\n from rope.base.resources import File, Folder, _ResourceMatcher\\n \\n try:\\n-    import pickle\\n-except ImportError:\\n     import cPickle as pickle\\n+except ImportError:\\n+    import pickle\\n \\n \\n class _Project(object):'], 'file': ['rope/base/oi/runmod.py', 'rope/base/oi/doa.py', 'rope/base/project.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('1f26e76f-e760-4532-a790-6cf2f5b69975'), UUID('e755e544-a587-4780-b03c-6ead2875318f'), UUID('3ef76174-26af-42c7-9792-0c4c8f53be13')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 4:0: except ImportError:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 4:0: except ImportError:\n",
      " 16%|â–ˆâ–‹        | 293/1800 [03:42<06:36,  3.80it/s]ERROR:src.process_code_changes:Error processing commit c4f9677f8790a58eaa1953bac286cca75a5f580e\n",
      "ERROR:src.process_code_changes:{'repo': 'IncludeSecurity/safeurl-python', 'vulnerability_id': 'GHSA-373w-rj84-pv6x', 'commit': 'c4f9677f8790a58eaa1953bac286cca75a5f580e', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -197,7 +197,7 @@ def isInList(self, lst, type_, value):\\n \\n         if type_ == \"domain\":\\n             for domain in dst:\\n-                if domain.lower() == value.lower():\\n+                if domain.lower().strip(\".\") == value.lower().strip(\".\"):\\n                     return True\\n             return False\\n         else:', '@@ -68,3 +68,17 @@\\n     print(\"Error:\", sys.exc_info())\\n \\n \\n+# fqdn\\n+try:\\n+    sc = safeurl.SafeURL()\\n+\\n+    opt = safeurl.Options()\\n+    opt.setList(\"blacklist\", [\"example.com\"], \"domain\")\\n+    sc.setOptions(opt)\\n+\\n+    res = sc.execute(\"https://example.com.\")\\n+\\n+except:\\n+    print(\"Error:\", sys.exc_info())\\n+\\n+'], 'file': ['safeurl/safeurl.py', 'safeurl/safeurl_tests.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('cf18ae35-6220-4bac-a6bc-f4144c5e9f35'), UUID('e56c06af-53f4-44d1-9459-17cd8b781452')]}\n",
      "ERROR:root:Error in {'repo': 'IncludeSecurity/safeurl-python', 'vulnerability_id': 'GHSA-373w-rj84-pv6x', 'commit': 'c4f9677f8790a58eaa1953bac286cca75a5f580e', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -197,7 +197,7 @@ def isInList(self, lst, type_, value):\\n \\n         if type_ == \"domain\":\\n             for domain in dst:\\n-                if domain.lower() == value.lower():\\n+                if domain.lower().strip(\".\") == value.lower().strip(\".\"):\\n                     return True\\n             return False\\n         else:', '@@ -68,3 +68,17 @@\\n     print(\"Error:\", sys.exc_info())\\n \\n \\n+# fqdn\\n+try:\\n+    sc = safeurl.SafeURL()\\n+\\n+    opt = safeurl.Options()\\n+    opt.setList(\"blacklist\", [\"example.com\"], \"domain\")\\n+    sc.setOptions(opt)\\n+\\n+    res = sc.execute(\"https://example.com.\")\\n+\\n+except:\\n+    print(\"Error:\", sys.exc_info())\\n+\\n+'], 'file': ['safeurl/safeurl.py', 'safeurl/safeurl_tests.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('cf18ae35-6220-4bac-a6bc-f4144c5e9f35'), UUID('e56c06af-53f4-44d1-9459-17cd8b781452')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:0: except:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:0: except:\n",
      " 18%|â–ˆâ–Š        | 322/1800 [03:47<04:43,  5.21it/s]ERROR:src.process_code_changes:Error processing commit 0a58bba59cd275bab8e0ae58bf4b359fbc5eb74a\n",
      "ERROR:src.process_code_changes:{'repo': 'jumpserver/jumpserver', 'vulnerability_id': '2023-42442', 'commit': '0a58bba59cd275bab8e0ae58bf4b359fbc5eb74a', 'commit_source': 'github', 'cwe_id': ['CWE-287'], 'patch': [\"@@ -1,13 +1,15 @@\\n from rest_framework import permissions\\n+\\n from common.utils import get_logger\\n \\n logger = get_logger(__file__)\\n \\n-\\n __all__ = ['IsSessionAssignee']\\n \\n \\n-class IsSessionAssignee(permissions.BasePermission):\\n+class IsSessionAssignee(permissions.IsAuthenticated):\\n+    def has_permission(self, request, view):\\n+        return False\\n \\n     def has_object_permission(self, request, view, obj):\\n         try:\", '@@ -16,6 +16,8 @@ def allow_access(private_file):\\n     path_base = path_list[1] if len(path_list) > 1 else None\\n     path_perm = path_perms_map.get(path_base, None)\\n \\n+    if \"..\" in request_path:\\n+        return False\\n     if not path_perm:\\n         return False\\n     if path_perm == \\'*\\' or request.user.has_perms([path_perm]):', '@@ -1,12 +1,12 @@\\n from rest_framework import permissions\\n \\n \\n-class IsAssignee(permissions.BasePermission):\\n+class IsAssignee(permissions.IsAuthenticated):\\n     def has_object_permission(self, request, view, obj):\\n         return obj.has_current_assignee(request.user)\\n \\n \\n-class IsApplicant(permissions.BasePermission):\\n+class IsApplicant(permissions.IsAuthenticated):\\n \\n     def has_object_permission(self, request, view, obj):\\n         return obj.applicant == request.user', '@@ -12,7 +12,7 @@\\n from orgs.utils import tmp_to_root_org\\n \\n \\n-class IsValidUser(permissions.IsAuthenticated, permissions.BasePermission):\\n+class IsValidUser(permissions.IsAuthenticated):\\n     \"\"\"Allows access to valid user, is active and not expired\"\"\"\\n \\n     def has_permission(self, request, view):', \"@@ -1,6 +1,5 @@\\n from rest_framework import permissions\\n \\n-from rbac.builtin import BuiltinRole\\n from .utils import is_auth_password_time_valid\\n \\n \\n@@ -11,7 +10,7 @@ def has_permission(self, request, view):\\n             and is_auth_password_time_valid(request.session)\\n \\n \\n-class UserObjectPermission(permissions.BasePermission):\\n+class UserObjectPermission(permissions.IsAuthenticated):\\n \\n     def has_object_permission(self, request, view, obj):\\n         if view.action not in ['update', 'partial_update', 'destroy']:\"], 'file': ['apps/terminal/permissions.py', 'apps/jumpserver/rewriting/storage/permissions.py', 'apps/tickets/permissions/ticket.py', 'apps/common/permissions.py', 'apps/users/permissions.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('c955ad59-083e-47ab-82c4-32bdf5f15cb2'), UUID('8a418f7b-2b07-4d1f-9879-dcacb7e6f5c7'), UUID('9a64ba56-3bc7-4ffa-a435-444735b94b14'), UUID('70b0fe0d-a14f-49c2-89ac-6d1078d85ddc'), UUID('51a26fcf-12e3-4b5d-abd5-71f07afe33cf')]}\n",
      "ERROR:root:Error in {'repo': 'jumpserver/jumpserver', 'vulnerability_id': '2023-42442', 'commit': '0a58bba59cd275bab8e0ae58bf4b359fbc5eb74a', 'commit_source': 'github', 'cwe_id': ['CWE-287'], 'patch': [\"@@ -1,13 +1,15 @@\\n from rest_framework import permissions\\n+\\n from common.utils import get_logger\\n \\n logger = get_logger(__file__)\\n \\n-\\n __all__ = ['IsSessionAssignee']\\n \\n \\n-class IsSessionAssignee(permissions.BasePermission):\\n+class IsSessionAssignee(permissions.IsAuthenticated):\\n+    def has_permission(self, request, view):\\n+        return False\\n \\n     def has_object_permission(self, request, view, obj):\\n         try:\", '@@ -16,6 +16,8 @@ def allow_access(private_file):\\n     path_base = path_list[1] if len(path_list) > 1 else None\\n     path_perm = path_perms_map.get(path_base, None)\\n \\n+    if \"..\" in request_path:\\n+        return False\\n     if not path_perm:\\n         return False\\n     if path_perm == \\'*\\' or request.user.has_perms([path_perm]):', '@@ -1,12 +1,12 @@\\n from rest_framework import permissions\\n \\n \\n-class IsAssignee(permissions.BasePermission):\\n+class IsAssignee(permissions.IsAuthenticated):\\n     def has_object_permission(self, request, view, obj):\\n         return obj.has_current_assignee(request.user)\\n \\n \\n-class IsApplicant(permissions.BasePermission):\\n+class IsApplicant(permissions.IsAuthenticated):\\n \\n     def has_object_permission(self, request, view, obj):\\n         return obj.applicant == request.user', '@@ -12,7 +12,7 @@\\n from orgs.utils import tmp_to_root_org\\n \\n \\n-class IsValidUser(permissions.IsAuthenticated, permissions.BasePermission):\\n+class IsValidUser(permissions.IsAuthenticated):\\n     \"\"\"Allows access to valid user, is active and not expired\"\"\"\\n \\n     def has_permission(self, request, view):', \"@@ -1,6 +1,5 @@\\n from rest_framework import permissions\\n \\n-from rbac.builtin import BuiltinRole\\n from .utils import is_auth_password_time_valid\\n \\n \\n@@ -11,7 +10,7 @@ def has_permission(self, request, view):\\n             and is_auth_password_time_valid(request.session)\\n \\n \\n-class UserObjectPermission(permissions.BasePermission):\\n+class UserObjectPermission(permissions.IsAuthenticated):\\n \\n     def has_object_permission(self, request, view, obj):\\n         if view.action not in ['update', 'partial_update', 'destroy']:\"], 'file': ['apps/terminal/permissions.py', 'apps/jumpserver/rewriting/storage/permissions.py', 'apps/tickets/permissions/ticket.py', 'apps/common/permissions.py', 'apps/users/permissions.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('c955ad59-083e-47ab-82c4-32bdf5f15cb2'), UUID('8a418f7b-2b07-4d1f-9879-dcacb7e6f5c7'), UUID('9a64ba56-3bc7-4ffa-a435-444735b94b14'), UUID('70b0fe0d-a14f-49c2-89ac-6d1078d85ddc'), UUID('51a26fcf-12e3-4b5d-abd5-71f07afe33cf')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: class IsApplicant(permissions.IsAuthenticated):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: class IsApplicant(permissions.IsAuthenticated):\n",
      " 19%|â–ˆâ–‰        | 349/1800 [03:47<02:20, 10.34it/s]ERROR:src.process_code_changes:Error processing commit 2406780831618405a13113377a784f3102465f40\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-3292', 'commit': '2406780831618405a13113377a784f3102465f40', 'commit_source': 'github', 'cwe_id': ['CWE-524'], 'patch': ['@@ -30,14 +30,6 @@\\n from rdiffweb.core.rdw_helpers import unquote_url\\n \\n \\n-def empty():\\n-    @cherrypy.expose\\n-    def handler():\\n-        return None\\n-\\n-    return handler\\n-\\n-\\n def poppath(*args, **kwargs):\\n     \"\"\"\\n     A decorator for _cp_dispatch\\n@@ -122,6 +114,7 @@ def static(path):\\n     @cherrypy.expose\\n     @cherrypy.tools.auth_form(on=False)\\n     @cherrypy.tools.sessions(on=False)\\n+    @cherrypy.tools.secure_headers(on=False)\\n     def handler(*args, **kwargs):\\n         if cherrypy.request.method not in (\\'GET\\', \\'HEAD\\'):\\n             return None', '@@ -31,28 +31,43 @@\\n     http.cookies.Morsel._reserved[\\'samesite\\'] = \\'SameSite\\'\\n \\n \\n-def set_headers():\\n+def set_headers(\\n+    xfo=\\'DENY\\',\\n+    no_cache=True,\\n+    referrer=\\'same-origin\\',\\n+    nosniff=True,\\n+    xxp=\\'1; mode=block\\',\\n+    csp=\"default-src \\'self\\'; style-src \\'self\\' \\'unsafe-inline\\'; script-src \\'self\\' \\'unsafe-inline\\'\",\\n+):\\n     \"\"\"\\n     This tool provide CSRF mitigation.\\n \\n     * Define X-Frame-Options = DENY\\n     * Define Cookies SameSite=Lax\\n     * Define Cookies Secure when https is detected\\n     * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\\n+    * Define Cache-Control by default\\n+    * Define Referrer-Policy to \\'same-origin\\'\\n \\n     Ref.:\\n     https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html\\n     https://cheatsheetseries.owasp.org/cheatsheets/Clickjacking_Defense_Cheat_Sheet.html\\n     \"\"\"\\n-    if cherrypy.request.method in [\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\']:\\n-        # Check if Origin matches our target.\\n-        origin = cherrypy.request.headers.get(\\'Origin\\', None)\\n-        if origin and not origin.startswith(cherrypy.request.base):\\n+    request = cherrypy.request\\n+    response = cherrypy.serving.response\\n+\\n+    # Check if Origin matches our target.\\n+    if request.method in [\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\']:\\n+        origin = request.headers.get(\\'Origin\\', None)\\n+        if origin and not origin.startswith(request.base):\\n             raise cherrypy.HTTPError(403, \\'Unexpected Origin header\\')\\n \\n-    response = cherrypy.serving.response\\n+    # Check if https is enabled\\n+    https = request.base.startswith(\\'https\\')\\n+\\n     # Define X-Frame-Options to avoid Clickjacking\\n-    response.headers[\\'X-Frame-Options\\'] = \\'DENY\\'\\n+    if xfo:\\n+        response.headers[\\'X-Frame-Options\\'] = xfo\\n \\n     # Enforce security on cookies\\n     cookie = response.cookie.get(\\'session_id\\', None)\\n@@ -61,10 +76,34 @@ def set_headers():\\n         # https://github.com/cherrypy/cherrypy/issues/1767\\n         # Force SameSite to Lax\\n         cookie[\\'samesite\\'] = \\'Lax\\'\\n-        # Check if https is enabled\\n-        https = cherrypy.request.base.startswith(\\'https\\')\\n         if https:\\n             cookie[\\'secure\\'] = 1\\n \\n+    # Add Cache-Control to avoid storing sensible information in Browser cache.\\n+    if no_cache:\\n+        response.headers[\\'Cache-control\\'] = \\'no-cache, no-store, must-revalidate, max-age=0\\'\\n+        response.headers[\\'Pragma\\'] = \\'no-cache\\'\\n+        response.headers[\\'Expires\\'] = \\'0\\'\\n+\\n+    # Add Referrer-Policy\\n+    if referrer:\\n+        response.headers[\\'Referrer-Policy\\'] = referrer\\n+\\n+    # Add X-Content-Type-Options to avoid browser to \"sniff\" to content-type\\n+    if nosniff:\\n+        response.headers[\\'X-Content-Type-Options\\'] = \\'nosniff\\'\\n+\\n+    # Add X-XSS-Protection to enabled XSS protection\\n+    if xxp:\\n+        response.headers[\\'X-XSS-Protection\\'] = xxp\\n+\\n+    # Add Content-Security-Policy\\n+    if csp:\\n+        response.headers[\\'Content-Security-Policy\\'] = csp\\n+\\n+    # Add Strict-Transport-Security to force https use.\\n+    if https:\\n+        response.headers[\\'Strict-Transport-Security\\'] = \"max-age=31536000; includeSubDomains\"\\n+\\n \\n cherrypy.tools.secure_headers = cherrypy.Tool(\\'before_request_body\\', set_headers, priority=71)'], 'file': ['rdiffweb/controller/dispatch.py', 'rdiffweb/tools/secure_headers.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b524462f-c419-4b0f-8fdc-7d93941d5bbd'), UUID('34e81144-c979-440a-b536-ea91c649a784')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-3292', 'commit': '2406780831618405a13113377a784f3102465f40', 'commit_source': 'github', 'cwe_id': ['CWE-524'], 'patch': ['@@ -30,14 +30,6 @@\\n from rdiffweb.core.rdw_helpers import unquote_url\\n \\n \\n-def empty():\\n-    @cherrypy.expose\\n-    def handler():\\n-        return None\\n-\\n-    return handler\\n-\\n-\\n def poppath(*args, **kwargs):\\n     \"\"\"\\n     A decorator for _cp_dispatch\\n@@ -122,6 +114,7 @@ def static(path):\\n     @cherrypy.expose\\n     @cherrypy.tools.auth_form(on=False)\\n     @cherrypy.tools.sessions(on=False)\\n+    @cherrypy.tools.secure_headers(on=False)\\n     def handler(*args, **kwargs):\\n         if cherrypy.request.method not in (\\'GET\\', \\'HEAD\\'):\\n             return None', '@@ -31,28 +31,43 @@\\n     http.cookies.Morsel._reserved[\\'samesite\\'] = \\'SameSite\\'\\n \\n \\n-def set_headers():\\n+def set_headers(\\n+    xfo=\\'DENY\\',\\n+    no_cache=True,\\n+    referrer=\\'same-origin\\',\\n+    nosniff=True,\\n+    xxp=\\'1; mode=block\\',\\n+    csp=\"default-src \\'self\\'; style-src \\'self\\' \\'unsafe-inline\\'; script-src \\'self\\' \\'unsafe-inline\\'\",\\n+):\\n     \"\"\"\\n     This tool provide CSRF mitigation.\\n \\n     * Define X-Frame-Options = DENY\\n     * Define Cookies SameSite=Lax\\n     * Define Cookies Secure when https is detected\\n     * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\\n+    * Define Cache-Control by default\\n+    * Define Referrer-Policy to \\'same-origin\\'\\n \\n     Ref.:\\n     https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html\\n     https://cheatsheetseries.owasp.org/cheatsheets/Clickjacking_Defense_Cheat_Sheet.html\\n     \"\"\"\\n-    if cherrypy.request.method in [\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\']:\\n-        # Check if Origin matches our target.\\n-        origin = cherrypy.request.headers.get(\\'Origin\\', None)\\n-        if origin and not origin.startswith(cherrypy.request.base):\\n+    request = cherrypy.request\\n+    response = cherrypy.serving.response\\n+\\n+    # Check if Origin matches our target.\\n+    if request.method in [\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\']:\\n+        origin = request.headers.get(\\'Origin\\', None)\\n+        if origin and not origin.startswith(request.base):\\n             raise cherrypy.HTTPError(403, \\'Unexpected Origin header\\')\\n \\n-    response = cherrypy.serving.response\\n+    # Check if https is enabled\\n+    https = request.base.startswith(\\'https\\')\\n+\\n     # Define X-Frame-Options to avoid Clickjacking\\n-    response.headers[\\'X-Frame-Options\\'] = \\'DENY\\'\\n+    if xfo:\\n+        response.headers[\\'X-Frame-Options\\'] = xfo\\n \\n     # Enforce security on cookies\\n     cookie = response.cookie.get(\\'session_id\\', None)\\n@@ -61,10 +76,34 @@ def set_headers():\\n         # https://github.com/cherrypy/cherrypy/issues/1767\\n         # Force SameSite to Lax\\n         cookie[\\'samesite\\'] = \\'Lax\\'\\n-        # Check if https is enabled\\n-        https = cherrypy.request.base.startswith(\\'https\\')\\n         if https:\\n             cookie[\\'secure\\'] = 1\\n \\n+    # Add Cache-Control to avoid storing sensible information in Browser cache.\\n+    if no_cache:\\n+        response.headers[\\'Cache-control\\'] = \\'no-cache, no-store, must-revalidate, max-age=0\\'\\n+        response.headers[\\'Pragma\\'] = \\'no-cache\\'\\n+        response.headers[\\'Expires\\'] = \\'0\\'\\n+\\n+    # Add Referrer-Policy\\n+    if referrer:\\n+        response.headers[\\'Referrer-Policy\\'] = referrer\\n+\\n+    # Add X-Content-Type-Options to avoid browser to \"sniff\" to content-type\\n+    if nosniff:\\n+        response.headers[\\'X-Content-Type-Options\\'] = \\'nosniff\\'\\n+\\n+    # Add X-XSS-Protection to enabled XSS protection\\n+    if xxp:\\n+        response.headers[\\'X-XSS-Protection\\'] = xxp\\n+\\n+    # Add Content-Security-Policy\\n+    if csp:\\n+        response.headers[\\'Content-Security-Policy\\'] = csp\\n+\\n+    # Add Strict-Transport-Security to force https use.\\n+    if https:\\n+        response.headers[\\'Strict-Transport-Security\\'] = \"max-age=31536000; includeSubDomains\"\\n+\\n \\n cherrypy.tools.secure_headers = cherrypy.Tool(\\'before_request_body\\', set_headers, priority=71)'], 'file': ['rdiffweb/controller/dispatch.py', 'rdiffweb/tools/secure_headers.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b524462f-c419-4b0f-8fdc-7d93941d5bbd'), UUID('34e81144-c979-440a-b536-ea91c649a784')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 20%|â–ˆâ–‰        | 353/1800 [03:50<04:04,  5.91it/s]ERROR:src.process_code_changes:Error processing commit 497ea861f481c6a3c52fe2aed9d0df1b6c99e9d7\n",
      "ERROR:src.process_code_changes:{'repo': 'frappe/frappe', 'vulnerability_id': '2022-23058', 'commit': '497ea861f481c6a3c52fe2aed9d0df1b6c99e9d7', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -1,81 +0,0 @@\\n-# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n-# MIT License. See license.txt\\n-\\n-from __future__ import unicode_literals\\n-import json, inspect\\n-import frappe\\n-from frappe import _\\n-from frappe.utils import cint\\n-from six import text_type, string_types\\n-\\n-@frappe.whitelist()\\n-def runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\\n-\\t\"\"\"run controller method - old style\"\"\"\\n-\\tif not args: args = arg or \"\"\\n-\\n-\\tif dt: # not called from a doctype (from a page)\\n-\\t\\tif not dn: dn = dt # single\\n-\\t\\tdoc = frappe.get_doc(dt, dn)\\n-\\n-\\telse:\\n-\\t\\tdoc = frappe.get_doc(json.loads(docs))\\n-\\t\\tdoc._original_modified = doc.modified\\n-\\t\\tdoc.check_if_latest()\\n-\\n-\\tif not doc.has_permission(\"read\"):\\n-\\t\\tfrappe.msgprint(_(\"Not permitted\"), raise_exception = True)\\n-\\n-\\tif doc:\\n-\\t\\ttry:\\n-\\t\\t\\targs = json.loads(args)\\n-\\t\\texcept ValueError:\\n-\\t\\t\\targs = args\\n-\\n-\\t\\ttry:\\n-\\t\\t\\tfnargs, varargs, varkw, defaults = inspect.getargspec(getattr(doc, method))\\n-\\t\\texcept ValueError:\\n-\\t\\t\\tfnargs = inspect.getfullargspec(getattr(doc, method)).args\\n-\\t\\t\\tvarargs = inspect.getfullargspec(getattr(doc, method)).varargs\\n-\\t\\t\\tvarkw = inspect.getfullargspec(getattr(doc, method)).varkw\\n-\\t\\t\\tdefaults = inspect.getfullargspec(getattr(doc, method)).defaults\\n-\\n-\\t\\tif not fnargs or (len(fnargs)==1 and fnargs[0]==\"self\"):\\n-\\t\\t\\tr = doc.run_method(method)\\n-\\n-\\t\\telif \"args\" in fnargs or not isinstance(args, dict):\\n-\\t\\t\\tr = doc.run_method(method, args)\\n-\\n-\\t\\telse:\\n-\\t\\t\\tr = doc.run_method(method, **args)\\n-\\n-\\t\\tif r:\\n-\\t\\t\\t#build output as csv\\n-\\t\\t\\tif cint(frappe.form_dict.get(\\'as_csv\\')):\\n-\\t\\t\\t\\tmake_csv_output(r, doc.doctype)\\n-\\t\\t\\telse:\\n-\\t\\t\\t\\tfrappe.response[\\'message\\'] = r\\n-\\n-\\t\\tfrappe.response.docs.append(doc)\\n-\\n-def make_csv_output(res, dt):\\n-\\t\"\"\"send method response as downloadable CSV file\"\"\"\\n-\\timport frappe\\n-\\n-\\tfrom six import StringIO\\n-\\timport csv\\n-\\n-\\tf = StringIO()\\n-\\twriter = csv.writer(f)\\n-\\tfor r in res:\\n-\\t\\trow = []\\n-\\t\\tfor v in r:\\n-\\t\\t\\tif isinstance(v, string_types):\\n-\\t\\t\\t\\tv = v.encode(\"utf-8\")\\n-\\t\\t\\trow.append(v)\\n-\\t\\twriter.writerow(row)\\n-\\n-\\tf.seek(0)\\n-\\n-\\tfrappe.response[\\'result\\'] = text_type(f.read(), \\'utf-8\\')\\n-\\tfrappe.response[\\'type\\'] = \\'csv\\'\\n-\\tfrappe.response[\\'doctype\\'] = dt.replace(\\' \\',\\'\\')', \"@@ -118,6 +118,7 @@ def update_status(self):\\n \\tdef is_completed(self):\\n \\t\\treturn self.end_date and getdate(self.end_date) < getdate(today())\\n \\n+\\t@frappe.whitelist()\\n \\tdef get_auto_repeat_schedule(self):\\n \\t\\tschedule_details = []\\n \\t\\tstart_date = getdate(self.start_date)\\n@@ -328,6 +329,7 @@ def send_notification(self, new_doc):\\n \\t\\tmake(doctype=new_doc.doctype, name=new_doc.name, recipients=recipients,\\n \\t\\t\\tsubject=subject, content=message, attachments=attachments, send_email=1)\\n \\n+\\t@frappe.whitelist()\\n \\tdef fetch_linked_contacts(self):\\n \\t\\tif self.reference_doctype and self.reference_document:\\n \\t\\t\\tres = get_contacts_linking_to(self.reference_doctype, self.reference_document, fields=['email_id'])\", \"@@ -52,6 +52,7 @@ def on_trash(self):\\n \\t\\t\\treference_log.reverted = 0\\n \\t\\t\\treference_log.save()\\n \\n+\\t@frappe.whitelist()\\n \\tdef revert(self, reason, ignore_permissions=False):\\n \\t\\tif not ignore_permissions:\\n \\t\\t\\tfrappe.only_for('System Manager')\", '@@ -55,7 +55,7 @@ frappe.call = function(opts) {\\n \\t\\targs.cmd = opts.module+\\'.page.\\'+opts.page+\\'.\\'+opts.page+\\'.\\'+opts.method;\\n \\t} else if(opts.doc) {\\n \\t\\t$.extend(args, {\\n-\\t\\t\\tcmd: \"runserverobj\",\\n+\\t\\t\\tcmd: \"run_doc_method\",\\n \\t\\t\\tdocs: frappe.get_doc(opts.doc.doctype, opts.doc.name),\\n \\t\\t\\tmethod: opts.method,\\n \\t\\t\\targs: opts.args,', '@@ -18,6 +18,7 @@ class BlogPost(WebsiteGenerator):\\n \\t\\torder_by = \"published_on desc\"\\n \\t)\\n \\n+\\t@frappe.whitelist()\\n \\tdef make_route(self):\\n \\t\\tif not self.route:\\n \\t\\t\\treturn frappe.db.get_value(\\'Blog Category\\', self.blog_category,', \"@@ -8,6 +8,7 @@\\n from frappe.model.document import Document\\n \\n class RolePermissionforPageandReport(Document):\\n+\\t@frappe.whitelist()\\n \\tdef set_report_page_data(self):\\n \\t\\tself.set_custom_roles()\\n \\t\\tself.check_prepared_report_disabled()\\n@@ -35,12 +36,14 @@ def get_standard_roles(self):\\n \\t\\tdoc = frappe.get_doc(doctype, docname)\\n \\t\\treturn doc.roles\\n \\n+\\t@frappe.whitelist()\\n \\tdef reset_roles(self):\\n \\t\\troles = self.get_standard_roles()\\n \\t\\tself.set('roles', roles)\\n \\t\\tself.update_custom_roles()\\n \\t\\tself.update_disable_prepared_report()\\n \\n+\\t@frappe.whitelist()\\n \\tdef update_report_page_data(self):\\n \\t\\tself.update_custom_roles()\\n \\t\\tself.update_disable_prepared_report()\", '@@ -58,6 +58,7 @@ def on_trash(self):\\n \\tdef get_columns(self):\\n \\t\\treturn [d.as_dict(no_default_fields = True) for d in self.columns]\\n \\n+\\t@frappe.whitelist()\\n \\tdef set_doctype_roles(self):\\n \\t\\tif not self.get(\\'roles\\') and self.is_standard == \\'No\\':\\n \\t\\t\\tmeta = frappe.get_meta(self.ref_doctype)\\n@@ -304,7 +305,7 @@ def build_data_dict(self, result, columns):\\n \\n \\t\\treturn data\\n \\n-\\t@Document.whitelist\\n+\\t@frappe.whitelist()\\n \\tdef toggle_disable(self, disable):\\n \\t\\tself.db_set(\"disabled\", cint(disable))\\n ', '@@ -4,7 +4,7 @@\\n from __future__ import unicode_literals, print_function\\n import frappe\\n import time\\n-from frappe import _, msgprint\\n+from frappe import _, msgprint, is_whitelisted\\n from frappe.utils import flt, cstr, now, get_datetime_str, file_lock, date_diff\\n from frappe.model.base_document import BaseDocument, get_controller\\n from frappe.model.naming import set_new_name\\n@@ -126,10 +126,10 @@ def __init__(self, *args, **kwargs):\\n \\t\\t\\traise ValueError(\\'Illegal arguments\\')\\n \\n \\t@staticmethod\\n-\\tdef whitelist(f):\\n+\\tdef whitelist(fn):\\n \\t\\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\\n-\\t\\tf.whitelisted = True\\n-\\t\\treturn f\\n+\\t\\tfrappe.whitelist()(fn)\\n+\\t\\treturn fn\\n \\n \\tdef reload(self):\\n \\t\\t\"\"\"Reload document from database\"\"\"\\n@@ -1148,12 +1148,12 @@ def composer(self, *args, **kwargs):\\n \\n \\t\\treturn composer\\n \\n-\\tdef is_whitelisted(self, method):\\n-\\t\\tfn = getattr(self, method, None)\\n+\\tdef is_whitelisted(self, method_name):\\n+\\t\\tmethod = getattr(self, method_name, None)\\n \\t\\tif not fn:\\n-\\t\\t\\traise NotFound(\"Method {0} not found\".format(method))\\n-\\t\\telif not getattr(fn, \"whitelisted\", False):\\n-\\t\\t\\traise Forbidden(\"Method {0} not whitelisted\".format(method))\\n+\\t\\t\\traise NotFound(\"Method {0} not found\".format(method_name))\\n+\\n+\\t\\tis_whitelisted(getattr(method, \\'__func__\\', method))\\n \\n \\tdef validate_value(self, fieldname, condition, val2, doc=None, raise_exception=None):\\n \\t\\t\"\"\"Check that value of fieldname should be \\'condition\\' val2', '@@ -34,7 +34,7 @@ frappe.ui.form.ControlButton = frappe.ui.form.ControlData.extend({\\n \\t\\tvar me = this;\\n \\t\\tif(this.frm && this.frm.docname) {\\n \\t\\t\\tfrappe.call({\\n-\\t\\t\\t\\tmethod: \"runserverobj\",\\n+\\t\\t\\t\\tmethod: \"run_doc_method\",\\n \\t\\t\\t\\targs: {\\'docs\\': this.frm.doc, \\'method\\': this.df.options },\\n \\t\\t\\t\\tbtn: this.$input,\\n \\t\\t\\t\\tcallback: function(r) {', '@@ -24,6 +24,7 @@ def on_update(self):\\n \\t\\tfrappe.db.sql(\"delete from tabSingles where doctype=\\'Customize Form\\'\")\\n \\t\\tfrappe.db.sql(\"delete from `tabCustomize Form Field`\")\\n \\n+\\t@frappe.whitelist()\\n \\tdef fetch_to_customize(self):\\n \\t\\tself.clear_existing_doc()\\n \\t\\tif not self.doc_type:\\n@@ -133,6 +134,7 @@ def clear_existing_doc(self):\\n \\t\\tself.doc_type = doc_type\\n \\t\\tself.name = \"Customize Form\"\\n \\n+\\t@frappe.whitelist()\\n \\tdef save_customization(self):\\n \\t\\tif not self.doc_type:\\n \\t\\t\\treturn\\n@@ -448,6 +450,7 @@ def validate_fieldtype_length(self):\\n \\n \\t\\tself.flags.update_db = True\\n \\n+\\t@frappe.whitelist()\\n \\tdef reset_to_defaults(self):\\n \\t\\tif not self.doc_type:\\n \\t\\t\\treturn', '@@ -29,6 +29,7 @@ def test_send(self, doctype=\"Lead\"):\\n \\t\\tself.queue_all(test_email=True)\\n \\t\\tfrappe.msgprint(_(\"Test email sent to {0}\").format(self.test_email_id))\\n \\n+\\t@frappe.whitelist()\\n \\tdef send_emails(self):\\n \\t\\t\"\"\"send emails to leads and customers\"\"\"\\n \\t\\tif self.email_sent:', \"@@ -19,6 +19,7 @@ def add_item(self, item):\\n \\t\\t\\tself.append('menu', item)\\n \\t\\t\\treturn True\\n \\n+\\t@frappe.whitelist()\\n \\tdef reset(self):\\n \\t\\t'''Restore defaults'''\\n \\t\\tself.menu = []\", \"@@ -6,8 +6,7 @@\\n import frappe, json\\n from frappe.utils import cstr, unique, cint\\n from frappe.permissions import has_permission\\n-from frappe.handler import is_whitelisted\\n-from frappe import _\\n+from frappe import _, is_whitelisted\\n from six import string_types\\n import re\\n import wrapt\\n@@ -221,4 +220,4 @@ def validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\\n \\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\\n \\t\\treturn []\\n \\n-\\treturn fn(**kwargs)\\n\\\\ No newline at end of file\\n+\\treturn fn(**kwargs)\", '@@ -2,17 +2,22 @@\\n # MIT License. See license.txt\\n \\n from __future__ import unicode_literals\\n+\\n+from werkzeug.wrappers import Response\\n+from six import text_type, string_types, StringIO\\n+\\n import frappe\\n-from frappe import _\\n import frappe.utils\\n import frappe.sessions\\n import frappe.desk.form.run_method\\n-from frappe.utils.response import build_response\\n-from frappe.api import validate_auth\\n+\\n from frappe.utils import cint\\n+from frappe.api import validate_auth\\n+from frappe import _, is_whitelisted\\n+from frappe.utils.response import build_response\\n+from frappe.utils.csvutils import build_csv_response\\n from frappe.core.doctype.server_script.server_script_utils import run_server_script_api\\n-from werkzeug.wrappers import Response\\n-from six import string_types\\n+\\n \\n ALLOWED_MIMETYPES = (\\'image/png\\', \\'image/jpeg\\', \\'application/pdf\\', \\'application/msword\\',\\n \\t\\t\\t\\'application/vnd.openxmlformats-officedocument.wordprocessingml.document\\',\\n@@ -64,8 +69,9 @@ def execute_cmd(cmd, from_async=False):\\n \\tif from_async:\\n \\t\\tmethod = method.queue\\n \\n-\\tis_whitelisted(method)\\n-\\tis_valid_http_method(method)\\n+\\tif method != run_doc_method:\\n+\\t\\tis_whitelisted(method)\\n+\\t\\tis_valid_http_method(method)\\n \\n \\treturn frappe.call(method, **frappe.form_dict)\\n \\n@@ -75,31 +81,10 @@ def is_valid_http_method(method):\\n \\tif http_method not in frappe.allowed_http_methods_for_whitelisted_func[method]:\\n \\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n \\n-def is_whitelisted(method):\\n-\\t# check if whitelisted\\n-\\tif frappe.session[\\'user\\'] == \\'Guest\\':\\n-\\t\\tif (method not in frappe.guest_methods):\\n-\\t\\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n-\\n-\\t\\tif method not in frappe.xss_safe_methods:\\n-\\t\\t\\t# strictly sanitize form_dict\\n-\\t\\t\\t# escapes html characters like <> except for predefined tags like a, b, ul etc.\\n-\\t\\t\\tfor key, value in frappe.form_dict.items():\\n-\\t\\t\\t\\tif isinstance(value, string_types):\\n-\\t\\t\\t\\t\\tfrappe.form_dict[key] = frappe.utils.sanitize_html(value)\\n-\\n-\\telse:\\n-\\t\\tif not method in frappe.whitelisted:\\n-\\t\\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n-\\n @frappe.whitelist(allow_guest=True)\\n def version():\\n \\treturn frappe.__version__\\n \\n-@frappe.whitelist()\\n-def runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\\n-\\tfrappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)\\n-\\n @frappe.whitelist(allow_guest=True)\\n def logout():\\n \\tfrappe.local.login_manager.logout()\\n@@ -112,15 +97,6 @@ def web_logout():\\n \\tfrappe.respond_as_web_page(_(\"Logged Out\"), _(\"You have been successfully logged out\"),\\n \\t\\tindicator_color=\\'green\\')\\n \\n-@frappe.whitelist(allow_guest=True)\\n-def run_custom_method(doctype, name, custom_method):\\n-\\t\"\"\"cmd=run_custom_method&doctype={doctype}&name={name}&custom_method={custom_method}\"\"\"\\n-\\tdoc = frappe.get_doc(doctype, name)\\n-\\tif getattr(doc, custom_method, frappe._dict()).is_whitelisted:\\n-\\t\\tfrappe.call(getattr(doc, custom_method), **frappe.local.form_dict)\\n-\\telse:\\n-\\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n-\\n @frappe.whitelist()\\n def uploadfile():\\n \\tret = None\\n@@ -222,6 +198,65 @@ def get_attr(cmd):\\n \\tfrappe.log(\"method:\" + cmd)\\n \\treturn method\\n \\n-@frappe.whitelist(allow_guest = True)\\n+@frappe.whitelist(allow_guest=True)\\n def ping():\\n \\treturn \"pong\"\\n+\\n+@frappe.whitelist()\\n+def run_doc_method(method, docs=None, dt=None, dn=None, arg=None, args=None):\\n+\\t\"\"\"run controller method - old style\"\"\"\\n+\\timport json, inspect\\n+\\n+\\tif not args: args = arg or \"\"\\n+\\n+\\tif dt: # not called from a doctype (from a page)\\n+\\t\\tif not dn: dn = dt # single\\n+\\t\\tdoc = frappe.get_doc(dt, dn)\\n+\\n+\\telse:\\n+\\t\\tdoc = frappe.get_doc(json.loads(docs))\\n+\\t\\tdoc._original_modified = doc.modified\\n+\\t\\tdoc.check_if_latest()\\n+\\n+\\tif not doc.has_permission(\"read\"):\\n+\\t\\tfrappe.msgprint(_(\"Not permitted\"), raise_exception = True)\\n+\\n+\\tif not doc:\\n+\\t\\treturn\\n+\\n+\\ttry:\\n+\\t\\targs = json.loads(args)\\n+\\texcept ValueError:\\n+\\t\\targs = args\\n+\\n+\\tmethod_obj = getattr(doc, method)\\n+\\tis_whitelisted(getattr(method_obj, \\'__func__\\', method_obj))\\n+\\n+\\ttry:\\n+\\t\\tfnargs = inspect.getargspec(method_obj)[0]\\n+\\texcept ValueError:\\n+\\t\\tfnargs = inspect.getfullargspec(method_obj).args\\n+\\n+\\tif not fnargs or (len(fnargs)==1 and fnargs[0]==\"self\"):\\n+\\t\\tr = doc.run_method(method)\\n+\\n+\\telif \"args\" in fnargs or not isinstance(args, dict):\\n+\\t\\tr = doc.run_method(method, args)\\n+\\n+\\telse:\\n+\\t\\tr = doc.run_method(method, **args)\\n+\\n+\\tfrappe.response.docs.append(doc)\\n+\\n+\\tif not r:\\n+\\t\\treturn\\n+\\n+\\t# build output as csv\\n+\\tif cint(frappe.form_dict.get(\\'as_csv\\')):\\n+\\t\\tbuild_csv_response(r, doc.doctype.replace(\\' \\', \\'\\'))\\n+\\t\\treturn\\n+\\n+\\tfrappe.response[\\'message\\'] = r\\n+\\n+# for backwards compatibility\\n+runserverobj = run_doc_method', '@@ -10,6 +10,7 @@\\n from frappe.data_migration.doctype.data_migration_mapping.data_migration_mapping import get_source_value\\n \\n class DataMigrationRun(Document):\\n+\\t@frappe.whitelist()\\n \\tdef run(self):\\n \\t\\tself.begin()\\n \\t\\tif self.total_pages > 0:', '@@ -98,6 +98,7 @@ def generate_theme_if_not_exist(self):\\n \\t\\telse:\\n \\t\\t\\tself.generate_bootstrap_theme()\\n \\n+\\t@frappe.whitelist()\\n \\tdef set_as_default(self):\\n \\t\\tself.generate_bootstrap_theme()\\n \\t\\tself.save()\\n@@ -106,6 +107,7 @@ def set_as_default(self):\\n \\t\\twebsite_settings.ignore_validate = True\\n \\t\\twebsite_settings.save()\\n \\n+\\t@frappe.whitelist()\\n \\tdef get_apps(self):\\n \\t\\tfrom frappe.utils.change_log import get_versions\\n \\t\\tapps = get_versions()'], 'file': ['frappe/desk/form/run_method.py', 'frappe/automation/doctype/auto_repeat/auto_repeat.py', 'frappe/social/doctype/energy_point_log/energy_point_log.py', 'frappe/public/js/frappe/request.js', 'frappe/website/doctype/blog_post/blog_post.py', 'frappe/core/doctype/role_permission_for_page_and_report/role_permission_for_page_and_report.py', 'frappe/core/doctype/report/report.py', 'frappe/model/document.py', 'frappe/public/js/frappe/form/controls/button.js', 'frappe/custom/doctype/customize_form/customize_form.py', 'frappe/email/doctype/newsletter/newsletter.py', 'frappe/website/doctype/portal_settings/portal_settings.py', 'frappe/desk/search.py', 'frappe/handler.py', 'frappe/data_migration/doctype/data_migration_run/data_migration_run.py', 'frappe/website/doctype/website_theme/website_theme.py'], 'language': ['Python', 'Python', 'Python', 'JavaScript/TypeScript', 'Python', 'Python', 'Python', 'Python', 'JavaScript/TypeScript', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('ff26e55c-ca35-46cd-ab53-95db55666435'), UUID('2cbac80f-7357-4ba7-b9b7-dccf21d04562'), UUID('81ceb5c5-8052-4acd-8b00-18f5c9a4030c'), UUID('9c39dc4d-6545-4e85-b0c2-994e6c590524'), UUID('4b3b453f-9459-4571-820b-6a7778e313ab'), UUID('60b02c1f-5327-460d-bcd4-60afb17083d4'), UUID('70a1fd6d-6609-4462-b5a0-483f3948b711'), UUID('d7039d07-c23b-4444-bb93-a17061693c6c'), UUID('3e410016-33e1-48da-8e15-59b1a0f81170'), UUID('f10dacf8-18db-47c4-a0c4-a98e11c25ad7'), UUID('ece14ea1-7aee-4cdf-8e28-54450cfc1905'), UUID('136f5a12-c65b-44c6-b3e9-b8c9930e1790'), UUID('fad363f7-31ab-4d21-a275-cd2ca822d1b3'), UUID('d3ffa394-25e4-4013-83a3-8d27430f3fd9'), UUID('e8cce007-103c-4aeb-aac5-cac517a4ae57'), UUID('13fbd257-7a9b-48de-b417-307ffc5d8518')]}\n",
      "ERROR:root:Error in {'repo': 'frappe/frappe', 'vulnerability_id': '2022-23058', 'commit': '497ea861f481c6a3c52fe2aed9d0df1b6c99e9d7', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -1,81 +0,0 @@\\n-# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors\\n-# MIT License. See license.txt\\n-\\n-from __future__ import unicode_literals\\n-import json, inspect\\n-import frappe\\n-from frappe import _\\n-from frappe.utils import cint\\n-from six import text_type, string_types\\n-\\n-@frappe.whitelist()\\n-def runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\\n-\\t\"\"\"run controller method - old style\"\"\"\\n-\\tif not args: args = arg or \"\"\\n-\\n-\\tif dt: # not called from a doctype (from a page)\\n-\\t\\tif not dn: dn = dt # single\\n-\\t\\tdoc = frappe.get_doc(dt, dn)\\n-\\n-\\telse:\\n-\\t\\tdoc = frappe.get_doc(json.loads(docs))\\n-\\t\\tdoc._original_modified = doc.modified\\n-\\t\\tdoc.check_if_latest()\\n-\\n-\\tif not doc.has_permission(\"read\"):\\n-\\t\\tfrappe.msgprint(_(\"Not permitted\"), raise_exception = True)\\n-\\n-\\tif doc:\\n-\\t\\ttry:\\n-\\t\\t\\targs = json.loads(args)\\n-\\t\\texcept ValueError:\\n-\\t\\t\\targs = args\\n-\\n-\\t\\ttry:\\n-\\t\\t\\tfnargs, varargs, varkw, defaults = inspect.getargspec(getattr(doc, method))\\n-\\t\\texcept ValueError:\\n-\\t\\t\\tfnargs = inspect.getfullargspec(getattr(doc, method)).args\\n-\\t\\t\\tvarargs = inspect.getfullargspec(getattr(doc, method)).varargs\\n-\\t\\t\\tvarkw = inspect.getfullargspec(getattr(doc, method)).varkw\\n-\\t\\t\\tdefaults = inspect.getfullargspec(getattr(doc, method)).defaults\\n-\\n-\\t\\tif not fnargs or (len(fnargs)==1 and fnargs[0]==\"self\"):\\n-\\t\\t\\tr = doc.run_method(method)\\n-\\n-\\t\\telif \"args\" in fnargs or not isinstance(args, dict):\\n-\\t\\t\\tr = doc.run_method(method, args)\\n-\\n-\\t\\telse:\\n-\\t\\t\\tr = doc.run_method(method, **args)\\n-\\n-\\t\\tif r:\\n-\\t\\t\\t#build output as csv\\n-\\t\\t\\tif cint(frappe.form_dict.get(\\'as_csv\\')):\\n-\\t\\t\\t\\tmake_csv_output(r, doc.doctype)\\n-\\t\\t\\telse:\\n-\\t\\t\\t\\tfrappe.response[\\'message\\'] = r\\n-\\n-\\t\\tfrappe.response.docs.append(doc)\\n-\\n-def make_csv_output(res, dt):\\n-\\t\"\"\"send method response as downloadable CSV file\"\"\"\\n-\\timport frappe\\n-\\n-\\tfrom six import StringIO\\n-\\timport csv\\n-\\n-\\tf = StringIO()\\n-\\twriter = csv.writer(f)\\n-\\tfor r in res:\\n-\\t\\trow = []\\n-\\t\\tfor v in r:\\n-\\t\\t\\tif isinstance(v, string_types):\\n-\\t\\t\\t\\tv = v.encode(\"utf-8\")\\n-\\t\\t\\trow.append(v)\\n-\\t\\twriter.writerow(row)\\n-\\n-\\tf.seek(0)\\n-\\n-\\tfrappe.response[\\'result\\'] = text_type(f.read(), \\'utf-8\\')\\n-\\tfrappe.response[\\'type\\'] = \\'csv\\'\\n-\\tfrappe.response[\\'doctype\\'] = dt.replace(\\' \\',\\'\\')', \"@@ -118,6 +118,7 @@ def update_status(self):\\n \\tdef is_completed(self):\\n \\t\\treturn self.end_date and getdate(self.end_date) < getdate(today())\\n \\n+\\t@frappe.whitelist()\\n \\tdef get_auto_repeat_schedule(self):\\n \\t\\tschedule_details = []\\n \\t\\tstart_date = getdate(self.start_date)\\n@@ -328,6 +329,7 @@ def send_notification(self, new_doc):\\n \\t\\tmake(doctype=new_doc.doctype, name=new_doc.name, recipients=recipients,\\n \\t\\t\\tsubject=subject, content=message, attachments=attachments, send_email=1)\\n \\n+\\t@frappe.whitelist()\\n \\tdef fetch_linked_contacts(self):\\n \\t\\tif self.reference_doctype and self.reference_document:\\n \\t\\t\\tres = get_contacts_linking_to(self.reference_doctype, self.reference_document, fields=['email_id'])\", \"@@ -52,6 +52,7 @@ def on_trash(self):\\n \\t\\t\\treference_log.reverted = 0\\n \\t\\t\\treference_log.save()\\n \\n+\\t@frappe.whitelist()\\n \\tdef revert(self, reason, ignore_permissions=False):\\n \\t\\tif not ignore_permissions:\\n \\t\\t\\tfrappe.only_for('System Manager')\", '@@ -55,7 +55,7 @@ frappe.call = function(opts) {\\n \\t\\targs.cmd = opts.module+\\'.page.\\'+opts.page+\\'.\\'+opts.page+\\'.\\'+opts.method;\\n \\t} else if(opts.doc) {\\n \\t\\t$.extend(args, {\\n-\\t\\t\\tcmd: \"runserverobj\",\\n+\\t\\t\\tcmd: \"run_doc_method\",\\n \\t\\t\\tdocs: frappe.get_doc(opts.doc.doctype, opts.doc.name),\\n \\t\\t\\tmethod: opts.method,\\n \\t\\t\\targs: opts.args,', '@@ -18,6 +18,7 @@ class BlogPost(WebsiteGenerator):\\n \\t\\torder_by = \"published_on desc\"\\n \\t)\\n \\n+\\t@frappe.whitelist()\\n \\tdef make_route(self):\\n \\t\\tif not self.route:\\n \\t\\t\\treturn frappe.db.get_value(\\'Blog Category\\', self.blog_category,', \"@@ -8,6 +8,7 @@\\n from frappe.model.document import Document\\n \\n class RolePermissionforPageandReport(Document):\\n+\\t@frappe.whitelist()\\n \\tdef set_report_page_data(self):\\n \\t\\tself.set_custom_roles()\\n \\t\\tself.check_prepared_report_disabled()\\n@@ -35,12 +36,14 @@ def get_standard_roles(self):\\n \\t\\tdoc = frappe.get_doc(doctype, docname)\\n \\t\\treturn doc.roles\\n \\n+\\t@frappe.whitelist()\\n \\tdef reset_roles(self):\\n \\t\\troles = self.get_standard_roles()\\n \\t\\tself.set('roles', roles)\\n \\t\\tself.update_custom_roles()\\n \\t\\tself.update_disable_prepared_report()\\n \\n+\\t@frappe.whitelist()\\n \\tdef update_report_page_data(self):\\n \\t\\tself.update_custom_roles()\\n \\t\\tself.update_disable_prepared_report()\", '@@ -58,6 +58,7 @@ def on_trash(self):\\n \\tdef get_columns(self):\\n \\t\\treturn [d.as_dict(no_default_fields = True) for d in self.columns]\\n \\n+\\t@frappe.whitelist()\\n \\tdef set_doctype_roles(self):\\n \\t\\tif not self.get(\\'roles\\') and self.is_standard == \\'No\\':\\n \\t\\t\\tmeta = frappe.get_meta(self.ref_doctype)\\n@@ -304,7 +305,7 @@ def build_data_dict(self, result, columns):\\n \\n \\t\\treturn data\\n \\n-\\t@Document.whitelist\\n+\\t@frappe.whitelist()\\n \\tdef toggle_disable(self, disable):\\n \\t\\tself.db_set(\"disabled\", cint(disable))\\n ', '@@ -4,7 +4,7 @@\\n from __future__ import unicode_literals, print_function\\n import frappe\\n import time\\n-from frappe import _, msgprint\\n+from frappe import _, msgprint, is_whitelisted\\n from frappe.utils import flt, cstr, now, get_datetime_str, file_lock, date_diff\\n from frappe.model.base_document import BaseDocument, get_controller\\n from frappe.model.naming import set_new_name\\n@@ -126,10 +126,10 @@ def __init__(self, *args, **kwargs):\\n \\t\\t\\traise ValueError(\\'Illegal arguments\\')\\n \\n \\t@staticmethod\\n-\\tdef whitelist(f):\\n+\\tdef whitelist(fn):\\n \\t\\t\"\"\"Decorator: Whitelist method to be called remotely via REST API.\"\"\"\\n-\\t\\tf.whitelisted = True\\n-\\t\\treturn f\\n+\\t\\tfrappe.whitelist()(fn)\\n+\\t\\treturn fn\\n \\n \\tdef reload(self):\\n \\t\\t\"\"\"Reload document from database\"\"\"\\n@@ -1148,12 +1148,12 @@ def composer(self, *args, **kwargs):\\n \\n \\t\\treturn composer\\n \\n-\\tdef is_whitelisted(self, method):\\n-\\t\\tfn = getattr(self, method, None)\\n+\\tdef is_whitelisted(self, method_name):\\n+\\t\\tmethod = getattr(self, method_name, None)\\n \\t\\tif not fn:\\n-\\t\\t\\traise NotFound(\"Method {0} not found\".format(method))\\n-\\t\\telif not getattr(fn, \"whitelisted\", False):\\n-\\t\\t\\traise Forbidden(\"Method {0} not whitelisted\".format(method))\\n+\\t\\t\\traise NotFound(\"Method {0} not found\".format(method_name))\\n+\\n+\\t\\tis_whitelisted(getattr(method, \\'__func__\\', method))\\n \\n \\tdef validate_value(self, fieldname, condition, val2, doc=None, raise_exception=None):\\n \\t\\t\"\"\"Check that value of fieldname should be \\'condition\\' val2', '@@ -34,7 +34,7 @@ frappe.ui.form.ControlButton = frappe.ui.form.ControlData.extend({\\n \\t\\tvar me = this;\\n \\t\\tif(this.frm && this.frm.docname) {\\n \\t\\t\\tfrappe.call({\\n-\\t\\t\\t\\tmethod: \"runserverobj\",\\n+\\t\\t\\t\\tmethod: \"run_doc_method\",\\n \\t\\t\\t\\targs: {\\'docs\\': this.frm.doc, \\'method\\': this.df.options },\\n \\t\\t\\t\\tbtn: this.$input,\\n \\t\\t\\t\\tcallback: function(r) {', '@@ -24,6 +24,7 @@ def on_update(self):\\n \\t\\tfrappe.db.sql(\"delete from tabSingles where doctype=\\'Customize Form\\'\")\\n \\t\\tfrappe.db.sql(\"delete from `tabCustomize Form Field`\")\\n \\n+\\t@frappe.whitelist()\\n \\tdef fetch_to_customize(self):\\n \\t\\tself.clear_existing_doc()\\n \\t\\tif not self.doc_type:\\n@@ -133,6 +134,7 @@ def clear_existing_doc(self):\\n \\t\\tself.doc_type = doc_type\\n \\t\\tself.name = \"Customize Form\"\\n \\n+\\t@frappe.whitelist()\\n \\tdef save_customization(self):\\n \\t\\tif not self.doc_type:\\n \\t\\t\\treturn\\n@@ -448,6 +450,7 @@ def validate_fieldtype_length(self):\\n \\n \\t\\tself.flags.update_db = True\\n \\n+\\t@frappe.whitelist()\\n \\tdef reset_to_defaults(self):\\n \\t\\tif not self.doc_type:\\n \\t\\t\\treturn', '@@ -29,6 +29,7 @@ def test_send(self, doctype=\"Lead\"):\\n \\t\\tself.queue_all(test_email=True)\\n \\t\\tfrappe.msgprint(_(\"Test email sent to {0}\").format(self.test_email_id))\\n \\n+\\t@frappe.whitelist()\\n \\tdef send_emails(self):\\n \\t\\t\"\"\"send emails to leads and customers\"\"\"\\n \\t\\tif self.email_sent:', \"@@ -19,6 +19,7 @@ def add_item(self, item):\\n \\t\\t\\tself.append('menu', item)\\n \\t\\t\\treturn True\\n \\n+\\t@frappe.whitelist()\\n \\tdef reset(self):\\n \\t\\t'''Restore defaults'''\\n \\t\\tself.menu = []\", \"@@ -6,8 +6,7 @@\\n import frappe, json\\n from frappe.utils import cstr, unique, cint\\n from frappe.permissions import has_permission\\n-from frappe.handler import is_whitelisted\\n-from frappe import _\\n+from frappe import _, is_whitelisted\\n from six import string_types\\n import re\\n import wrapt\\n@@ -221,4 +220,4 @@ def validate_and_sanitize_search_inputs(fn, instance, args, kwargs):\\n \\tif kwargs['doctype'] and not frappe.db.exists('DocType', kwargs['doctype']):\\n \\t\\treturn []\\n \\n-\\treturn fn(**kwargs)\\n\\\\ No newline at end of file\\n+\\treturn fn(**kwargs)\", '@@ -2,17 +2,22 @@\\n # MIT License. See license.txt\\n \\n from __future__ import unicode_literals\\n+\\n+from werkzeug.wrappers import Response\\n+from six import text_type, string_types, StringIO\\n+\\n import frappe\\n-from frappe import _\\n import frappe.utils\\n import frappe.sessions\\n import frappe.desk.form.run_method\\n-from frappe.utils.response import build_response\\n-from frappe.api import validate_auth\\n+\\n from frappe.utils import cint\\n+from frappe.api import validate_auth\\n+from frappe import _, is_whitelisted\\n+from frappe.utils.response import build_response\\n+from frappe.utils.csvutils import build_csv_response\\n from frappe.core.doctype.server_script.server_script_utils import run_server_script_api\\n-from werkzeug.wrappers import Response\\n-from six import string_types\\n+\\n \\n ALLOWED_MIMETYPES = (\\'image/png\\', \\'image/jpeg\\', \\'application/pdf\\', \\'application/msword\\',\\n \\t\\t\\t\\'application/vnd.openxmlformats-officedocument.wordprocessingml.document\\',\\n@@ -64,8 +69,9 @@ def execute_cmd(cmd, from_async=False):\\n \\tif from_async:\\n \\t\\tmethod = method.queue\\n \\n-\\tis_whitelisted(method)\\n-\\tis_valid_http_method(method)\\n+\\tif method != run_doc_method:\\n+\\t\\tis_whitelisted(method)\\n+\\t\\tis_valid_http_method(method)\\n \\n \\treturn frappe.call(method, **frappe.form_dict)\\n \\n@@ -75,31 +81,10 @@ def is_valid_http_method(method):\\n \\tif http_method not in frappe.allowed_http_methods_for_whitelisted_func[method]:\\n \\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n \\n-def is_whitelisted(method):\\n-\\t# check if whitelisted\\n-\\tif frappe.session[\\'user\\'] == \\'Guest\\':\\n-\\t\\tif (method not in frappe.guest_methods):\\n-\\t\\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n-\\n-\\t\\tif method not in frappe.xss_safe_methods:\\n-\\t\\t\\t# strictly sanitize form_dict\\n-\\t\\t\\t# escapes html characters like <> except for predefined tags like a, b, ul etc.\\n-\\t\\t\\tfor key, value in frappe.form_dict.items():\\n-\\t\\t\\t\\tif isinstance(value, string_types):\\n-\\t\\t\\t\\t\\tfrappe.form_dict[key] = frappe.utils.sanitize_html(value)\\n-\\n-\\telse:\\n-\\t\\tif not method in frappe.whitelisted:\\n-\\t\\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n-\\n @frappe.whitelist(allow_guest=True)\\n def version():\\n \\treturn frappe.__version__\\n \\n-@frappe.whitelist()\\n-def runserverobj(method, docs=None, dt=None, dn=None, arg=None, args=None):\\n-\\tfrappe.desk.form.run_method.runserverobj(method, docs=docs, dt=dt, dn=dn, arg=arg, args=args)\\n-\\n @frappe.whitelist(allow_guest=True)\\n def logout():\\n \\tfrappe.local.login_manager.logout()\\n@@ -112,15 +97,6 @@ def web_logout():\\n \\tfrappe.respond_as_web_page(_(\"Logged Out\"), _(\"You have been successfully logged out\"),\\n \\t\\tindicator_color=\\'green\\')\\n \\n-@frappe.whitelist(allow_guest=True)\\n-def run_custom_method(doctype, name, custom_method):\\n-\\t\"\"\"cmd=run_custom_method&doctype={doctype}&name={name}&custom_method={custom_method}\"\"\"\\n-\\tdoc = frappe.get_doc(doctype, name)\\n-\\tif getattr(doc, custom_method, frappe._dict()).is_whitelisted:\\n-\\t\\tfrappe.call(getattr(doc, custom_method), **frappe.local.form_dict)\\n-\\telse:\\n-\\t\\tfrappe.throw(_(\"Not permitted\"), frappe.PermissionError)\\n-\\n @frappe.whitelist()\\n def uploadfile():\\n \\tret = None\\n@@ -222,6 +198,65 @@ def get_attr(cmd):\\n \\tfrappe.log(\"method:\" + cmd)\\n \\treturn method\\n \\n-@frappe.whitelist(allow_guest = True)\\n+@frappe.whitelist(allow_guest=True)\\n def ping():\\n \\treturn \"pong\"\\n+\\n+@frappe.whitelist()\\n+def run_doc_method(method, docs=None, dt=None, dn=None, arg=None, args=None):\\n+\\t\"\"\"run controller method - old style\"\"\"\\n+\\timport json, inspect\\n+\\n+\\tif not args: args = arg or \"\"\\n+\\n+\\tif dt: # not called from a doctype (from a page)\\n+\\t\\tif not dn: dn = dt # single\\n+\\t\\tdoc = frappe.get_doc(dt, dn)\\n+\\n+\\telse:\\n+\\t\\tdoc = frappe.get_doc(json.loads(docs))\\n+\\t\\tdoc._original_modified = doc.modified\\n+\\t\\tdoc.check_if_latest()\\n+\\n+\\tif not doc.has_permission(\"read\"):\\n+\\t\\tfrappe.msgprint(_(\"Not permitted\"), raise_exception = True)\\n+\\n+\\tif not doc:\\n+\\t\\treturn\\n+\\n+\\ttry:\\n+\\t\\targs = json.loads(args)\\n+\\texcept ValueError:\\n+\\t\\targs = args\\n+\\n+\\tmethod_obj = getattr(doc, method)\\n+\\tis_whitelisted(getattr(method_obj, \\'__func__\\', method_obj))\\n+\\n+\\ttry:\\n+\\t\\tfnargs = inspect.getargspec(method_obj)[0]\\n+\\texcept ValueError:\\n+\\t\\tfnargs = inspect.getfullargspec(method_obj).args\\n+\\n+\\tif not fnargs or (len(fnargs)==1 and fnargs[0]==\"self\"):\\n+\\t\\tr = doc.run_method(method)\\n+\\n+\\telif \"args\" in fnargs or not isinstance(args, dict):\\n+\\t\\tr = doc.run_method(method, args)\\n+\\n+\\telse:\\n+\\t\\tr = doc.run_method(method, **args)\\n+\\n+\\tfrappe.response.docs.append(doc)\\n+\\n+\\tif not r:\\n+\\t\\treturn\\n+\\n+\\t# build output as csv\\n+\\tif cint(frappe.form_dict.get(\\'as_csv\\')):\\n+\\t\\tbuild_csv_response(r, doc.doctype.replace(\\' \\', \\'\\'))\\n+\\t\\treturn\\n+\\n+\\tfrappe.response[\\'message\\'] = r\\n+\\n+# for backwards compatibility\\n+runserverobj = run_doc_method', '@@ -10,6 +10,7 @@\\n from frappe.data_migration.doctype.data_migration_mapping.data_migration_mapping import get_source_value\\n \\n class DataMigrationRun(Document):\\n+\\t@frappe.whitelist()\\n \\tdef run(self):\\n \\t\\tself.begin()\\n \\t\\tif self.total_pages > 0:', '@@ -98,6 +98,7 @@ def generate_theme_if_not_exist(self):\\n \\t\\telse:\\n \\t\\t\\tself.generate_bootstrap_theme()\\n \\n+\\t@frappe.whitelist()\\n \\tdef set_as_default(self):\\n \\t\\tself.generate_bootstrap_theme()\\n \\t\\tself.save()\\n@@ -106,6 +107,7 @@ def set_as_default(self):\\n \\t\\twebsite_settings.ignore_validate = True\\n \\t\\twebsite_settings.save()\\n \\n+\\t@frappe.whitelist()\\n \\tdef get_apps(self):\\n \\t\\tfrom frappe.utils.change_log import get_versions\\n \\t\\tapps = get_versions()'], 'file': ['frappe/desk/form/run_method.py', 'frappe/automation/doctype/auto_repeat/auto_repeat.py', 'frappe/social/doctype/energy_point_log/energy_point_log.py', 'frappe/public/js/frappe/request.js', 'frappe/website/doctype/blog_post/blog_post.py', 'frappe/core/doctype/role_permission_for_page_and_report/role_permission_for_page_and_report.py', 'frappe/core/doctype/report/report.py', 'frappe/model/document.py', 'frappe/public/js/frappe/form/controls/button.js', 'frappe/custom/doctype/customize_form/customize_form.py', 'frappe/email/doctype/newsletter/newsletter.py', 'frappe/website/doctype/portal_settings/portal_settings.py', 'frappe/desk/search.py', 'frappe/handler.py', 'frappe/data_migration/doctype/data_migration_run/data_migration_run.py', 'frappe/website/doctype/website_theme/website_theme.py'], 'language': ['Python', 'Python', 'Python', 'JavaScript/TypeScript', 'Python', 'Python', 'Python', 'Python', 'JavaScript/TypeScript', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('ff26e55c-ca35-46cd-ab53-95db55666435'), UUID('2cbac80f-7357-4ba7-b9b7-dccf21d04562'), UUID('81ceb5c5-8052-4acd-8b00-18f5c9a4030c'), UUID('9c39dc4d-6545-4e85-b0c2-994e6c590524'), UUID('4b3b453f-9459-4571-820b-6a7778e313ab'), UUID('60b02c1f-5327-460d-bcd4-60afb17083d4'), UUID('70a1fd6d-6609-4462-b5a0-483f3948b711'), UUID('d7039d07-c23b-4444-bb93-a17061693c6c'), UUID('3e410016-33e1-48da-8e15-59b1a0f81170'), UUID('f10dacf8-18db-47c4-a0c4-a98e11c25ad7'), UUID('ece14ea1-7aee-4cdf-8e28-54450cfc1905'), UUID('136f5a12-c65b-44c6-b3e9-b8c9930e1790'), UUID('fad363f7-31ab-4d21-a275-cd2ca822d1b3'), UUID('d3ffa394-25e4-4013-83a3-8d27430f3fd9'), UUID('e8cce007-103c-4aeb-aac5-cac517a4ae57'), UUID('13fbd257-7a9b-48de-b417-307ffc5d8518')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:  @frappe.whitelist()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: \t@frappe.whitelist()\n",
      " 20%|â–ˆâ–‰        | 355/1800 [03:51<04:46,  5.05it/s]ERROR:src.process_code_changes:Error processing commit 3f3082e0a67851cde26a48da3d1f4b75d8aa07ec\n",
      "ERROR:src.process_code_changes:{'repo': 'lxml/lxml', 'vulnerability_id': '2014-3146', 'commit': '3f3082e0a67851cde26a48da3d1f4b75d8aa07ec', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -38,7 +38,7 @@ def pygments_directive(name, arguments, options, content, lineno,\\n                        content_offset, block_text, state, state_machine):\\n     try:\\n         lexer = get_lexer_by_name(arguments[0])\\n-    except ValueError, e:\\n+    except ValueError:\\n         # no lexer found - use the text one instead of an exception\\n         lexer = TextLexer()\\n     # take an arbitrary option if more than one is given', \"@@ -24,12 +24,16 @@ build_wheel() {\\n             -w /io/$WHEELHOUSE\\n }\\n \\n-assert_importable() {\\n+run_tests() {\\n     # Install packages and test\\n     for PYBIN in /opt/python/*/bin/; do\\n         ${PYBIN}/pip install $PACKAGE --no-index -f /io/$WHEELHOUSE\\n \\n+        # check import as a quick test\\n         (cd $HOME; ${PYBIN}/python -c 'import lxml.etree, lxml.objectify')\\n+\\n+        # run tests\\n+        (cd $HOME; ${PYBIN}/python /io/test.py)\\n     done\\n }\\n \\n@@ -74,5 +78,5 @@ show_wheels() {\\n prepare_system\\n build_wheels\\n repair_wheels\\n-assert_importable\\n+run_tests\\n show_wheels\", '@@ -8,9 +8,10 @@\\n import copy\\n try:\\n     from urlparse import urlsplit\\n+    from urllib import unquote_plus\\n except ImportError:\\n     # Python 3\\n-    from urllib.parse import urlsplit\\n+    from urllib.parse import urlsplit, unquote_plus\\n from lxml import etree\\n from lxml.html import defs\\n from lxml.html import fromstring, XHTML_NAMESPACE\\n@@ -477,7 +478,7 @@ def _kill_elements(self, doc, condition, iterate=None):\\n \\n     def _remove_javascript_link(self, link):\\n         # links like \"j a v a s c r i p t:\" might be interpreted in IE\\n-        new = _substitute_whitespace(\\'\\', link)\\n+        new = _substitute_whitespace(\\'\\', unquote_plus(link))\\n         if _is_javascript_scheme(new):\\n             # FIXME: should this be None to delete?\\n             return \\'\\''], 'file': ['doc/rest2html.py', 'tools/manylinux/build-wheels.sh', 'src/lxml/html/clean.py'], 'language': ['Python', 'Shell', 'Python'], 'temp_id': [UUID('48493502-c418-46e7-8758-306ebcfff68b'), UUID('e1388039-3808-43af-8030-2d0ca95450e0'), UUID('e93a0005-c52b-4111-9345-aab1972d80b5')]}\n",
      "ERROR:root:Error in {'repo': 'lxml/lxml', 'vulnerability_id': '2014-3146', 'commit': '3f3082e0a67851cde26a48da3d1f4b75d8aa07ec', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -38,7 +38,7 @@ def pygments_directive(name, arguments, options, content, lineno,\\n                        content_offset, block_text, state, state_machine):\\n     try:\\n         lexer = get_lexer_by_name(arguments[0])\\n-    except ValueError, e:\\n+    except ValueError:\\n         # no lexer found - use the text one instead of an exception\\n         lexer = TextLexer()\\n     # take an arbitrary option if more than one is given', \"@@ -24,12 +24,16 @@ build_wheel() {\\n             -w /io/$WHEELHOUSE\\n }\\n \\n-assert_importable() {\\n+run_tests() {\\n     # Install packages and test\\n     for PYBIN in /opt/python/*/bin/; do\\n         ${PYBIN}/pip install $PACKAGE --no-index -f /io/$WHEELHOUSE\\n \\n+        # check import as a quick test\\n         (cd $HOME; ${PYBIN}/python -c 'import lxml.etree, lxml.objectify')\\n+\\n+        # run tests\\n+        (cd $HOME; ${PYBIN}/python /io/test.py)\\n     done\\n }\\n \\n@@ -74,5 +78,5 @@ show_wheels() {\\n prepare_system\\n build_wheels\\n repair_wheels\\n-assert_importable\\n+run_tests\\n show_wheels\", '@@ -8,9 +8,10 @@\\n import copy\\n try:\\n     from urlparse import urlsplit\\n+    from urllib import unquote_plus\\n except ImportError:\\n     # Python 3\\n-    from urllib.parse import urlsplit\\n+    from urllib.parse import urlsplit, unquote_plus\\n from lxml import etree\\n from lxml.html import defs\\n from lxml.html import fromstring, XHTML_NAMESPACE\\n@@ -477,7 +478,7 @@ def _kill_elements(self, doc, condition, iterate=None):\\n \\n     def _remove_javascript_link(self, link):\\n         # links like \"j a v a s c r i p t:\" might be interpreted in IE\\n-        new = _substitute_whitespace(\\'\\', link)\\n+        new = _substitute_whitespace(\\'\\', unquote_plus(link))\\n         if _is_javascript_scheme(new):\\n             # FIXME: should this be None to delete?\\n             return \\'\\''], 'file': ['doc/rest2html.py', 'tools/manylinux/build-wheels.sh', 'src/lxml/html/clean.py'], 'language': ['Python', 'Shell', 'Python'], 'temp_id': [UUID('48493502-c418-46e7-8758-306ebcfff68b'), UUID('e1388039-3808-43af-8030-2d0ca95450e0'), UUID('e93a0005-c52b-4111-9345-aab1972d80b5')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     from urllib.parse import urlsplit, unquote_plus\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     from urllib.parse import urlsplit, unquote_plus\n",
      " 20%|â–ˆâ–ˆ        | 361/1800 [03:52<04:10,  5.73it/s]ERROR:src.process_code_changes:Error processing commit feef0d7b11d86aed29bf98c21526088117964d85\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2023-4138', 'commit': 'feef0d7b11d86aed29bf98c21526088117964d85', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': [\"@@ -134,6 +134,7 @@ def populate_obj(self, userobj):\\n \\r\\n class PagePrefNotification(Controller):\\r\\n     @cherrypy.expose\\r\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\r\\n     def default(self, **kwargs):\\r\\n         # Process the parameters.\\r\\n         report_form = ReportForm(obj=self.app.currentuser)\\r\"], 'file': ['rdiffweb/controller/page_pref_notification.py'], 'language': ['Python'], 'temp_id': [UUID('1d7aa2d9-bd08-4ca0-8b18-ecfe1aa882d8')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2023-4138', 'commit': 'feef0d7b11d86aed29bf98c21526088117964d85', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': [\"@@ -134,6 +134,7 @@ def populate_obj(self, userobj):\\n \\r\\n class PagePrefNotification(Controller):\\r\\n     @cherrypy.expose\\r\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\r\\n     def default(self, **kwargs):\\r\\n         # Process the parameters.\\r\\n         report_form = ReportForm(obj=self.app.currentuser)\\r\"], 'file': ['rdiffweb/controller/page_pref_notification.py'], 'language': ['Python'], 'temp_id': [UUID('1d7aa2d9-bd08-4ca0-8b18-ecfe1aa882d8')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 21%|â–ˆâ–ˆ        | 374/1800 [03:54<04:36,  5.15it/s]ERROR:src.process_code_changes:Error processing commit 45bb47533f7abb5479618ae7f6a809215700dcb2\n",
      "ERROR:src.process_code_changes:{'repo': 'lmfit/asteval', 'vulnerability_id': '2025-24359', 'commit': '45bb47533f7abb5479618ae7f6a809215700dcb2', 'commit_source': 'github', 'cwe_id': ['CWE-134', 'CWE-134'], 'patch': ['@@ -13,6 +13,7 @@\\n from tokenize import ENCODING as tk_ENCODING\\n from tokenize import NAME as tk_NAME\\n from tokenize import tokenize as generate_tokens\\n+from string import Formatter\\n \\n builtins = __builtins__\\n if not isinstance(builtins, dict):\\n@@ -33,6 +34,14 @@\\n except ImportError:\\n     pass\\n \\n+# This is a necessary API but it\\'s undocumented and moved around\\n+# between Python releases\\n+try:\\n+    from _string import formatter_field_name_split\\n+except ImportError:\\n+    formatter_field_name_split = lambda \\\\\\n+        x: x._formatter_field_name_split()\\n+\\n \\n \\n MAX_EXPONENT = 10000\\n@@ -59,7 +68,7 @@\\n                 \\'__getattribute__\\', \\'__subclasshook__\\', \\'__new__\\',\\n                 \\'__init__\\', \\'func_globals\\', \\'func_code\\', \\'func_closure\\',\\n                 \\'im_class\\', \\'im_func\\', \\'im_self\\', \\'gi_code\\', \\'gi_frame\\',\\n-                \\'f_locals\\', \\'__asteval__\\')\\n+                \\'f_locals\\', \\'__asteval__\\',\\'mro\\')\\n \\n # unsafe attributes for particular objects, by type\\n UNSAFE_ATTRS_DTYPES = {str: (\\'format\\', \\'format_map\\')}\\n@@ -266,6 +275,45 @@ def safe_lshift(arg1, arg2):\\n              ast.UAdd: lambda a: +a,\\n              ast.USub: lambda a: -a}\\n \\n+# Safe version of getattr\\n+\\n+def safe_getattr(obj, attr, raise_exc, node):\\n+    \"\"\"safe version of getattr\"\"\"\\n+    unsafe = (attr in UNSAFE_ATTRS or\\n+            (attr.startswith(\\'__\\') and attr.endswith(\\'__\\')))\\n+    if not unsafe:\\n+        for dtype, attrlist in UNSAFE_ATTRS_DTYPES.items():\\n+            unsafe = (isinstance(obj, dtype) or obj is dtype) and attr in attrlist\\n+            if unsafe:\\n+                break\\n+    if unsafe:\\n+        msg = f\"no safe attribute \\'{attr}\\' for {repr(obj)}\"\\n+        raise_exc(node, exc=AttributeError, msg=msg)\\n+    else:\\n+        try:\\n+            return getattr(obj, attr)\\n+        except AttributeError:\\n+            pass\\n+\\n+class SafeFormatter(Formatter):\\n+    def __init__(self, raise_exc, node):\\n+        self.raise_exc = raise_exc\\n+        self.node = node\\n+        super().__init__()\\n+\\n+    def get_field(self, field_name, args, kwargs):\\n+        first, rest = formatter_field_name_split(field_name)\\n+        obj = self.get_value(first, args, kwargs)\\n+        for is_attr, i in rest:\\n+            if is_attr:\\n+                obj = safe_getattr(obj, i, self.raise_exc, self.node)\\n+            else:\\n+                obj = obj[i]\\n+        return obj, first\\n+    \\n+def safe_format(_string, raise_exc, node, *args, **kwargs):\\n+    formatter = SafeFormatter(raise_exc, node)\\n+    return formatter.vformat(_string, args, kwargs)                    \\n \\n def valid_symbol_name(name):\\n     \"\"\"Determine whether the input symbol name is a valid name.', '@@ -44,9 +44,9 @@\\n import time\\n from sys import exc_info, stderr, stdout\\n \\n-from .astutils import (HAS_NUMPY, UNSAFE_ATTRS, UNSAFE_ATTRS_DTYPES,\\n+from .astutils import (HAS_NUMPY,\\n                        ExceptionHolder, ReturnedNone, Empty, make_symbol_table,\\n-                       numpy, op2func, valid_symbol_name, Procedure)\\n+                       numpy, op2func, safe_getattr, safe_format, valid_symbol_name, Procedure)\\n \\n ALL_NODES = [\\'arg\\', \\'assert\\', \\'assign\\', \\'attribute\\', \\'augassign\\', \\'binop\\',\\n              \\'boolop\\', \\'break\\', \\'bytes\\', \\'call\\', \\'compare\\', \\'constant\\',\\n@@ -513,7 +513,7 @@ def on_formattedvalue(self, node): # (\\'value\\', \\'conversion\\', \\'format_spec\\')\\n         fmt = \\'{__fstring__}\\'\\n         if node.format_spec is not None:\\n             fmt = f\\'{{__fstring__:{self.run(node.format_spec)}}}\\'\\n-        return fmt.format(__fstring__=val)\\n+        return safe_format(fmt, self.raise_exception, node, __fstring__=val)\\n \\n     def _getsym(self, node):\\n         val = self.symtable.get(node.id, ReturnedNone)\\n@@ -573,22 +573,8 @@ def on_attribute(self, node):    # (\\'value\\', \\'attr\\', \\'ctx\\')\\n         sym = self.run(node.value)\\n         if ctx == ast.Del:\\n             return delattr(sym, node.attr)\\n-        #\\n-        unsafe = (node.attr in UNSAFE_ATTRS or\\n-                 (node.attr.startswith(\\'__\\') and node.attr.endswith(\\'__\\')))\\n-        if not unsafe:\\n-            for dtype, attrlist in UNSAFE_ATTRS_DTYPES.items():\\n-                unsafe = isinstance(sym, dtype) and node.attr in attrlist\\n-                if unsafe:\\n-                    break\\n-        if unsafe:\\n-            msg = f\"no safe attribute \\'{node.attr}\\' for {repr(sym)}\"\\n-            self.raise_exception(node, exc=AttributeError, msg=msg)\\n-        else:\\n-            try:\\n-                return getattr(sym, node.attr)\\n-            except AttributeError:\\n-                pass\\n+        \\n+        return safe_getattr(sym, node.attr, self.raise_exception, node)\\n \\n \\n     def on_assign(self, node):    # (\\'targets\\', \\'value\\')'], 'file': ['asteval/astutils.py', 'asteval/asteval.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('4fa60ab6-5cd7-48e6-a300-00be09adde1b'), UUID('b3766827-471a-4b8a-bef3-eb4000f527fe')]}\n",
      "ERROR:root:Error in {'repo': 'lmfit/asteval', 'vulnerability_id': '2025-24359', 'commit': '45bb47533f7abb5479618ae7f6a809215700dcb2', 'commit_source': 'github', 'cwe_id': ['CWE-134', 'CWE-134'], 'patch': ['@@ -13,6 +13,7 @@\\n from tokenize import ENCODING as tk_ENCODING\\n from tokenize import NAME as tk_NAME\\n from tokenize import tokenize as generate_tokens\\n+from string import Formatter\\n \\n builtins = __builtins__\\n if not isinstance(builtins, dict):\\n@@ -33,6 +34,14 @@\\n except ImportError:\\n     pass\\n \\n+# This is a necessary API but it\\'s undocumented and moved around\\n+# between Python releases\\n+try:\\n+    from _string import formatter_field_name_split\\n+except ImportError:\\n+    formatter_field_name_split = lambda \\\\\\n+        x: x._formatter_field_name_split()\\n+\\n \\n \\n MAX_EXPONENT = 10000\\n@@ -59,7 +68,7 @@\\n                 \\'__getattribute__\\', \\'__subclasshook__\\', \\'__new__\\',\\n                 \\'__init__\\', \\'func_globals\\', \\'func_code\\', \\'func_closure\\',\\n                 \\'im_class\\', \\'im_func\\', \\'im_self\\', \\'gi_code\\', \\'gi_frame\\',\\n-                \\'f_locals\\', \\'__asteval__\\')\\n+                \\'f_locals\\', \\'__asteval__\\',\\'mro\\')\\n \\n # unsafe attributes for particular objects, by type\\n UNSAFE_ATTRS_DTYPES = {str: (\\'format\\', \\'format_map\\')}\\n@@ -266,6 +275,45 @@ def safe_lshift(arg1, arg2):\\n              ast.UAdd: lambda a: +a,\\n              ast.USub: lambda a: -a}\\n \\n+# Safe version of getattr\\n+\\n+def safe_getattr(obj, attr, raise_exc, node):\\n+    \"\"\"safe version of getattr\"\"\"\\n+    unsafe = (attr in UNSAFE_ATTRS or\\n+            (attr.startswith(\\'__\\') and attr.endswith(\\'__\\')))\\n+    if not unsafe:\\n+        for dtype, attrlist in UNSAFE_ATTRS_DTYPES.items():\\n+            unsafe = (isinstance(obj, dtype) or obj is dtype) and attr in attrlist\\n+            if unsafe:\\n+                break\\n+    if unsafe:\\n+        msg = f\"no safe attribute \\'{attr}\\' for {repr(obj)}\"\\n+        raise_exc(node, exc=AttributeError, msg=msg)\\n+    else:\\n+        try:\\n+            return getattr(obj, attr)\\n+        except AttributeError:\\n+            pass\\n+\\n+class SafeFormatter(Formatter):\\n+    def __init__(self, raise_exc, node):\\n+        self.raise_exc = raise_exc\\n+        self.node = node\\n+        super().__init__()\\n+\\n+    def get_field(self, field_name, args, kwargs):\\n+        first, rest = formatter_field_name_split(field_name)\\n+        obj = self.get_value(first, args, kwargs)\\n+        for is_attr, i in rest:\\n+            if is_attr:\\n+                obj = safe_getattr(obj, i, self.raise_exc, self.node)\\n+            else:\\n+                obj = obj[i]\\n+        return obj, first\\n+    \\n+def safe_format(_string, raise_exc, node, *args, **kwargs):\\n+    formatter = SafeFormatter(raise_exc, node)\\n+    return formatter.vformat(_string, args, kwargs)                    \\n \\n def valid_symbol_name(name):\\n     \"\"\"Determine whether the input symbol name is a valid name.', '@@ -44,9 +44,9 @@\\n import time\\n from sys import exc_info, stderr, stdout\\n \\n-from .astutils import (HAS_NUMPY, UNSAFE_ATTRS, UNSAFE_ATTRS_DTYPES,\\n+from .astutils import (HAS_NUMPY,\\n                        ExceptionHolder, ReturnedNone, Empty, make_symbol_table,\\n-                       numpy, op2func, valid_symbol_name, Procedure)\\n+                       numpy, op2func, safe_getattr, safe_format, valid_symbol_name, Procedure)\\n \\n ALL_NODES = [\\'arg\\', \\'assert\\', \\'assign\\', \\'attribute\\', \\'augassign\\', \\'binop\\',\\n              \\'boolop\\', \\'break\\', \\'bytes\\', \\'call\\', \\'compare\\', \\'constant\\',\\n@@ -513,7 +513,7 @@ def on_formattedvalue(self, node): # (\\'value\\', \\'conversion\\', \\'format_spec\\')\\n         fmt = \\'{__fstring__}\\'\\n         if node.format_spec is not None:\\n             fmt = f\\'{{__fstring__:{self.run(node.format_spec)}}}\\'\\n-        return fmt.format(__fstring__=val)\\n+        return safe_format(fmt, self.raise_exception, node, __fstring__=val)\\n \\n     def _getsym(self, node):\\n         val = self.symtable.get(node.id, ReturnedNone)\\n@@ -573,22 +573,8 @@ def on_attribute(self, node):    # (\\'value\\', \\'attr\\', \\'ctx\\')\\n         sym = self.run(node.value)\\n         if ctx == ast.Del:\\n             return delattr(sym, node.attr)\\n-        #\\n-        unsafe = (node.attr in UNSAFE_ATTRS or\\n-                 (node.attr.startswith(\\'__\\') and node.attr.endswith(\\'__\\')))\\n-        if not unsafe:\\n-            for dtype, attrlist in UNSAFE_ATTRS_DTYPES.items():\\n-                unsafe = isinstance(sym, dtype) and node.attr in attrlist\\n-                if unsafe:\\n-                    break\\n-        if unsafe:\\n-            msg = f\"no safe attribute \\'{node.attr}\\' for {repr(sym)}\"\\n-            self.raise_exception(node, exc=AttributeError, msg=msg)\\n-        else:\\n-            try:\\n-                return getattr(sym, node.attr)\\n-            except AttributeError:\\n-                pass\\n+        \\n+        return safe_getattr(sym, node.attr, self.raise_exception, node)\\n \\n \\n     def on_assign(self, node):    # (\\'targets\\', \\'value\\')'], 'file': ['asteval/astutils.py', 'asteval/asteval.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('4fa60ab6-5cd7-48e6-a300-00be09adde1b'), UUID('b3766827-471a-4b8a-bef3-eb4000f527fe')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: except ImportError:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: except ImportError:\n",
      " 21%|â–ˆâ–ˆ        | 379/1800 [04:03<13:48,  1.72it/s]ERROR:src.process_code_changes:Error processing commit ccdce2ccb8207b82501af3c03f50abc0f819b469\n",
      "ERROR:src.process_code_changes:{'repo': 'pretix/pretix', 'vulnerability_id': '2023-44463', 'commit': 'ccdce2ccb8207b82501af3c03f50abc0f819b469', 'commit_source': 'github', 'cwe_id': ['CWE-290'], 'patch': [\"@@ -188,13 +188,13 @@\\n \\n CSRF_TRUSTED_ORIGINS = [urlparse(SITE_URL).scheme + '://' + urlparse(SITE_URL).hostname]\\n \\n-TRUST_X_FORWARDED_FOR = config.get('pretix', 'trust_x_forwarded_for', fallback=False)\\n-USE_X_FORWARDED_HOST = config.get('pretix', 'trust_x_forwarded_host', fallback=False)\\n+TRUST_X_FORWARDED_FOR = config.getboolean('pretix', 'trust_x_forwarded_for', fallback=False)\\n+USE_X_FORWARDED_HOST = config.getboolean('pretix', 'trust_x_forwarded_host', fallback=False)\\n \\n \\n REQUEST_ID_HEADER = config.get('pretix', 'request_id_header', fallback=False)\\n \\n-if config.get('pretix', 'trust_x_forwarded_proto', fallback=False):\\n+if config.getboolean('pretix', 'trust_x_forwarded_proto', fallback=False):\\n     SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')\\n \\n PRETIX_PLUGINS_DEFAULT = config.get('pretix', 'plugins_default',\"], 'file': ['src/pretix/settings.py'], 'language': ['Python'], 'temp_id': [UUID('0b88a6e2-aad1-422e-9b86-77a6b31417fd')]}\n",
      "ERROR:root:Error in {'repo': 'pretix/pretix', 'vulnerability_id': '2023-44463', 'commit': 'ccdce2ccb8207b82501af3c03f50abc0f819b469', 'commit_source': 'github', 'cwe_id': ['CWE-290'], 'patch': [\"@@ -188,13 +188,13 @@\\n \\n CSRF_TRUSTED_ORIGINS = [urlparse(SITE_URL).scheme + '://' + urlparse(SITE_URL).hostname]\\n \\n-TRUST_X_FORWARDED_FOR = config.get('pretix', 'trust_x_forwarded_for', fallback=False)\\n-USE_X_FORWARDED_HOST = config.get('pretix', 'trust_x_forwarded_host', fallback=False)\\n+TRUST_X_FORWARDED_FOR = config.getboolean('pretix', 'trust_x_forwarded_for', fallback=False)\\n+USE_X_FORWARDED_HOST = config.getboolean('pretix', 'trust_x_forwarded_host', fallback=False)\\n \\n \\n REQUEST_ID_HEADER = config.get('pretix', 'request_id_header', fallback=False)\\n \\n-if config.get('pretix', 'trust_x_forwarded_proto', fallback=False):\\n+if config.getboolean('pretix', 'trust_x_forwarded_proto', fallback=False):\\n     SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')\\n \\n PRETIX_PLUGINS_DEFAULT = config.get('pretix', 'plugins_default',\"], 'file': ['src/pretix/settings.py'], 'language': ['Python'], 'temp_id': [UUID('0b88a6e2-aad1-422e-9b86-77a6b31417fd')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0: <line number missing in source>\n",
      " 22%|â–ˆâ–ˆâ–       | 391/1800 [04:04<08:05,  2.90it/s]ERROR:src.process_code_changes:Error processing commit 44abb197120a843cce5b5fe8276e4a44b8bb2f48\n",
      "ERROR:src.process_code_changes:{'repo': 'django-helpdesk/django-helpdesk', 'vulnerability_id': '2021-3945', 'commit': '44abb197120a843cce5b5fe8276e4a44b8bb2f48', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -1,8 +1,8 @@\\n-from celery.decorators import task\\n+from celery import shared_task\\n \\n from .email import process_email\\n \\n \\n-@task()\\n+@shared_task\\n def helpdesk_process_email():\\n     process_email()', '@@ -330,7 +330,11 @@ <h5 class=\"mb-0\">\\n         function get_url(row) {\\n             return \"{% url \\'helpdesk:view\\' 1234 %}\".replace(/1234/, row.id.toString());\\n         }\\n-\\n+        \\n+        function htmlEntities(str) {\\n+            return String(str).replace(/&/g, \\'&amp;\\').replace(/</g, \\'&lt;\\').replace(/>/g, \\'&gt;\\').replace(/\"/g, \\'&quot;\\');\\n+        }\\n+        \\n         $(document).ready(function () {\\n             // Ticket DataTable Initialization\\n             $(\\'#ticketTable\\').DataTable({\\n@@ -366,7 +370,7 @@ <h5 class=\"mb-0\">\\n                             if (type === \\'display\\') {\\n                                 data = \\'<div class=\"tickettitle\"><a href=\"\\' + get_url(row) + \\'\" >\\' +\\n                                     row.id + \\'. \\' +\\n-                                    row.title + \\'</a></div>\\';\\n+                                    htmlEntities(row.title) + \\'</a></div>\\';\\n                             }\\n                             return data\\n                         }', \"@@ -145,6 +145,7 @@ def process_attachments(followup, attached_files):\\n                 'application/octet-stream',\\n                 size=attached.size,\\n             )\\n+            att.full_clean()\\n             att.save()\\n \\n             if attached.size < max_email_attachment_size:\"], 'file': ['helpdesk/tasks.py', 'helpdesk/templates/helpdesk/ticket_list.html', 'helpdesk/lib.py'], 'language': ['Python', 'HTML', 'Python'], 'temp_id': [UUID('8291b797-ae74-49e2-b278-52b1009d9d34'), UUID('eeffe742-769a-4a7d-a83f-b7d59a1a841c'), UUID('f7cb671c-9ebe-4c94-b918-963bea3dee8a')]}\n",
      "ERROR:root:Error in {'repo': 'django-helpdesk/django-helpdesk', 'vulnerability_id': '2021-3945', 'commit': '44abb197120a843cce5b5fe8276e4a44b8bb2f48', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -1,8 +1,8 @@\\n-from celery.decorators import task\\n+from celery import shared_task\\n \\n from .email import process_email\\n \\n \\n-@task()\\n+@shared_task\\n def helpdesk_process_email():\\n     process_email()', '@@ -330,7 +330,11 @@ <h5 class=\"mb-0\">\\n         function get_url(row) {\\n             return \"{% url \\'helpdesk:view\\' 1234 %}\".replace(/1234/, row.id.toString());\\n         }\\n-\\n+        \\n+        function htmlEntities(str) {\\n+            return String(str).replace(/&/g, \\'&amp;\\').replace(/</g, \\'&lt;\\').replace(/>/g, \\'&gt;\\').replace(/\"/g, \\'&quot;\\');\\n+        }\\n+        \\n         $(document).ready(function () {\\n             // Ticket DataTable Initialization\\n             $(\\'#ticketTable\\').DataTable({\\n@@ -366,7 +370,7 @@ <h5 class=\"mb-0\">\\n                             if (type === \\'display\\') {\\n                                 data = \\'<div class=\"tickettitle\"><a href=\"\\' + get_url(row) + \\'\" >\\' +\\n                                     row.id + \\'. \\' +\\n-                                    row.title + \\'</a></div>\\';\\n+                                    htmlEntities(row.title) + \\'</a></div>\\';\\n                             }\\n                             return data\\n                         }', \"@@ -145,6 +145,7 @@ def process_attachments(followup, attached_files):\\n                 'application/octet-stream',\\n                 size=attached.size,\\n             )\\n+            att.full_clean()\\n             att.save()\\n \\n             if attached.size < max_email_attachment_size:\"], 'file': ['helpdesk/tasks.py', 'helpdesk/templates/helpdesk/ticket_list.html', 'helpdesk/lib.py'], 'language': ['Python', 'HTML', 'Python'], 'temp_id': [UUID('8291b797-ae74-49e2-b278-52b1009d9d34'), UUID('eeffe742-769a-4a7d-a83f-b7d59a1a841c'), UUID('f7cb671c-9ebe-4c94-b918-963bea3dee8a')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      " 22%|â–ˆâ–ˆâ–       | 394/1800 [04:05<07:37,  3.08it/s]Following Github server redirection from /repos/kiorky/SOAPpy to /repositories/1393322\n",
      "INFO:github.Requester:Following Github server redirection from /repos/kiorky/SOAPpy to /repositories/1393322\n",
      "ERROR:src.process_code_changes:Error processing commit a38656817c8ce7d02e117b1308328419a5d1560f\n",
      "ERROR:src.process_code_changes:{'repo': 'kiorky/SOAPpy', 'vulnerability_id': '2014-3243', 'commit': 'a38656817c8ce7d02e117b1308328419a5d1560f', 'commit_source': 'github', 'cwe_id': ['CWE-119'], 'patch': ['@@ -188,8 +188,9 @@ def unregisterObject(self, object, namespace = \\'\\', path = \\'\\'):\\n             if namespace[0] == \":\": namespace = namespace[1:]\\n \\n         del self.objmap[namespace]\\n-        \\n+\\n class SOAPRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\\n+    ignore_ext = True\\n     def version_string(self):\\n         return \\'<a href=\"http://pywebsvcs.sf.net\">\\' + \\\\\\n             \\'SOAPpy \\' + __version__ + \\'</a> (Python \\' + \\\\\\n@@ -204,7 +205,7 @@ def date_time_string(self):\\n \\n     def do_POST(self):\\n         global _contexts\\n-        \\n+\\n         status = 500\\n         try:\\n             if self.server.config.dumpHeadersIn:\\n@@ -226,7 +227,7 @@ def do_POST(self):\\n                 debugFooter(s)\\n \\n             (r, header, body, attrs) = \\\\\\n-                parseSOAPRPC(data, header = 1, body = 1, attrs = 1)\\n+                parseSOAPRPC(data, header = 1, body = 1, attrs = 1, ignore_ext=self.ignore_ext)\\n \\n             method = r._name\\n             args   = r._aslist()\\n@@ -252,8 +253,8 @@ def do_POST(self):\\n             ordered_args = {}\\n             named_args   = {}\\n \\n-            if Config.specialArgs: \\n-                \\n+            if Config.specialArgs:\\n+\\n                 for (k,v) in  kw.items():\\n \\n                     if k[0]==\"v\":\\n@@ -271,13 +272,13 @@ def do_POST(self):\\n             # if r._ns is specified use it, if not check for\\n             # a path, if it\\'s specified convert it and use it as the\\n             # namespace. If both are specified, use r._ns.\\n-            \\n+\\n             ns = r._ns\\n \\n             if len(self.path) > 1 and not ns:\\n                 ns = self.path.replace(\"/\", \":\")\\n                 if ns[0] == \":\": ns = ns[1:]\\n-            \\n+\\n             # authorization method\\n             a = None\\n \\n@@ -291,9 +292,9 @@ def do_POST(self):\\n             #print \\'<-> Argument Matching Yielded:\\'\\n             #print \\'<-> Ordered Arguments:\\' + str(ordered_args)\\n             #print \\'<-> Named Arguments  :\\' + str(named_args)\\n-             \\n+\\n             resp = \"\"\\n-            \\n+\\n             # For fault messages\\n             if ns:\\n                 nsmethod = \"%s:%s\" % (ns, method)\\n@@ -318,7 +319,7 @@ def do_POST(self):\\n                     # there are none, because the split will return\\n                     # [method]\\n                     f = self.server.objmap[ns]\\n-                    \\n+\\n                     # Look for the authorization method\\n                     if self.server.config.authMethod != None:\\n                         authmethod = self.server.config.authMethod\\n@@ -359,7 +360,7 @@ def do_POST(self):\\n                     if \"SOAPAction\".lower() not in self.headers.keys() or \\\\\\n                        self.headers[\"SOAPAction\"] == \"\\\\\"\\\\\"\":\\n                         self.headers[\"SOAPAction\"] = method\\n-                        \\n+\\n                     thread_id = thread.get_ident()\\n                     _contexts[thread_id] = SOAPContext(header, body,\\n                                                        attrs, data,\\n@@ -374,11 +375,11 @@ def do_POST(self):\\n                             raise faultType(\"%s:Server\" % NS.ENV_T,\\n                                             \"Authorization failed.\",\\n                                             \"%s\" % nsmethod)\\n-                    \\n+\\n                     # If it\\'s wrapped, some special action may be needed\\n                     if isinstance(f, MethodSig):\\n                         c = None\\n-                    \\n+\\n                         if f.context:  # retrieve context object\\n                             c = _contexts[thread_id]\\n \\n@@ -389,9 +390,9 @@ def do_POST(self):\\n                         elif f.keywords:\\n                             # This is lame, but have to de-unicode\\n                             # keywords\\n-                            \\n+\\n                             strkw = {}\\n-                            \\n+\\n                             for (k, v) in kw.items():\\n                                 strkw[str(k)] = v\\n                             if c:\\n@@ -408,7 +409,7 @@ def do_POST(self):\\n                         else:\\n                             fr = apply(f, args, {})\\n \\n-                    \\n+\\n                     if type(fr) == type(self) and \\\\\\n                         isinstance(fr, voidType):\\n                         resp = buildSOAP(kw = {\\'%sResponse\\' % method: fr},\\n@@ -423,7 +424,7 @@ def do_POST(self):\\n                     # Clean up _contexts\\n                     if _contexts.has_key(thread_id):\\n                         del _contexts[thread_id]\\n-                        \\n+\\n                 except Exception, e:\\n                     import traceback\\n                     info = sys.exc_info()\\n@@ -558,7 +559,7 @@ def do_POST(self):\\n                 self.connection.shutdown(1)\\n \\n         def do_GET(self):\\n-            \\n+\\n             #print \\'command        \\', self.command\\n             #print \\'path           \\', self.path\\n             #print \\'request_version\\', self.request_version\\n@@ -567,29 +568,29 @@ def do_GET(self):\\n             #print \\'   maintype\\', self.headers.maintype\\n             #print \\'   subtype \\', self.headers.subtype\\n             #print \\'   params  \\', self.headers.plist\\n-            \\n+\\n             path = self.path.lower()\\n             if path.endswith(\\'wsdl\\'):\\n                 method = \\'wsdl\\'\\n                 function = namespace = None\\n                 if self.server.funcmap.has_key(namespace) \\\\\\n                         and self.server.funcmap[namespace].has_key(method):\\n                     function = self.server.funcmap[namespace][method]\\n-                else: \\n+                else:\\n                     if namespace in self.server.objmap.keys():\\n                         function = self.server.objmap[namespace]\\n                         l = method.split(\".\")\\n                         for i in l:\\n                             function = getattr(function, i)\\n-            \\n+\\n                 if function:\\n                     self.send_response(200)\\n                     self.send_header(\"Content-type\", \\'text/plain\\')\\n                     self.end_headers()\\n                     response = apply(function, ())\\n                     self.wfile.write(str(response))\\n                     return\\n-            \\n+\\n             # return error\\n             self.send_response(200)\\n             self.send_header(\"Content-type\", \\'text/html\\')\\n@@ -614,13 +615,17 @@ def do_GET(self):\\n \\n </body>\\'\\'\\')\\n \\n-            \\n+\\n     def log_message(self, format, *args):\\n         if self.server.log:\\n             BaseHTTPServer.BaseHTTPRequestHandler.\\\\\\n                 log_message (self, format, *args)\\n \\n \\n+class SOAPInsecureRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\\n+    \\'\\'\\'Request handler that does load POSTed doctypes\\'\\'\\'\\n+    ignore_ext = False\\n+\\n \\n class SOAPServer(SOAPServerBase, SocketServer.TCPServer):\\n \\n@@ -679,28 +684,32 @@ def __init__(self, addr = (\\'localhost\\', 8000),\\n if hasattr(socket, \"AF_UNIX\"):\\n \\n     class SOAPUnixSocketServer(SOAPServerBase, SocketServer.UnixStreamServer):\\n-    \\n+\\n         def __init__(self, addr = 8000,\\n             RequestHandler = SOAPRequestHandler, log = 0, encoding = \\'UTF-8\\',\\n             config = Config, namespace = None, ssl_context = None):\\n-    \\n+\\n             # Test the encoding, raising an exception if it\\'s not known\\n             if encoding != None:\\n                 \\'\\'.encode(encoding)\\n-    \\n+\\n             if ssl_context != None and not config.SSLserver:\\n                 raise AttributeError, \\\\\\n                     \"SSL server not supported by this Python installation\"\\n-    \\n+\\n             self.namespace          = namespace\\n             self.objmap             = {}\\n             self.funcmap            = {}\\n             self.ssl_context        = ssl_context\\n             self.encoding           = encoding\\n             self.config             = config\\n             self.log                = log\\n-    \\n+\\n             self.allow_reuse_address= 1\\n-    \\n+\\n             SocketServer.UnixStreamServer.__init__(self, str(addr), RequestHandler)\\n-    \\n+\\n+\\n+\\n+\\n+', '@@ -1,4 +1,5 @@\\n # SOAPpy modules\\n+import traceback\\n from Config    import Config\\n from Types     import *\\n from NS        import NS\\n@@ -7,6 +8,10 @@\\n import string\\n import xml.sax\\n from wstools.XMLname import fromXMLname\\n+try:\\n+    from cStringIO import StringIO\\n+except ImportError:\\n+    from StringIO import StringIO\\n \\n try: from M2Crypto import SSL\\n except: pass\\n@@ -93,7 +98,7 @@ def toStr( name ):\\n             elif prefix:\\n                tag = prefix + \":\" + tag\\n             return tag\\n-        \\n+\\n         # Workaround two sax bugs\\n         if name[0] == None and name[1][0] == \\' \\':\\n             name = (None, name[1][1:])\\n@@ -127,7 +132,7 @@ def toStr( name ):\\n         elif self._next == \"\":\\n             raise Error, \"expected nothing, \" \\\\\\n                   \"got `%s\\'\" % toStr( name )\\n-                  \\n+\\n \\n         if len(self._stack) == 2:\\n             rules = self._rules\\n@@ -275,7 +280,7 @@ def endElementNS(self, name, qname):\\n                         null = 1\\n \\n                 # check for nil=1, but watch out for string values\\n-                try:                \\n+                try:\\n                     null = int(null)\\n                 except ValueError, e:\\n                     if not e[0].startswith(\"invalid literal for int()\"):\\n@@ -312,7 +317,7 @@ def endElementNS(self, name, qname):\\n             #print \"cur.kind=\", cur.kind\\n             #print \"cur.rules=\", cur.rules\\n             #print \"\\\\n\"\\n-                        \\n+\\n \\n             if cur.rules != None:\\n                 rule = cur.rules\\n@@ -374,7 +379,7 @@ def endElementNS(self, name, qname):\\n #                 print \"ns:\", ns\\n #                 print \"attrs:\", attrs\\n #                 print \"kind:\", kind\\n-                \\n+\\n \\n                 if kind == None:\\n                     # If the current item\\'s container is an array, it will\\n@@ -863,7 +868,7 @@ def convertToBasicTypes(self, d, t, attrs, config=Config):\\n #         print \"   attrs=\", attrs\\n #         print \"   t[0]=\", t[0]\\n #         print \"   t[1]=\", t[1]\\n-            \\n+\\n #         print \"   in?\", t[0] in NS.EXSD_L\\n \\n         if t[0] in NS.EXSD_L:\\n@@ -933,11 +938,11 @@ def convertToBasicTypes(self, d, t, attrs, config=Config):\\n                     elif d == 0:\\n                         if type(self.zerofloatre) == StringType:\\n                             self.zerofloatre = re.compile(self.zerofloatre)\\n-    \\n+\\n                         if self.zerofloatre.search(s):\\n                             raise UnderflowError, \"invalid %s: %s\" % (t[1], s)\\n                 return d\\n-            \\n+\\n             if t[1] in (\"dateTime\", \"date\", \"timeInstant\", \"time\"):\\n                 return self.convertDateTime(d, t[1])\\n             if t[1] == \"decimal\":\\n@@ -1031,30 +1036,37 @@ def convertToBasicTypes(self, d, t, attrs, config=Config):\\n ################################################################################\\n # call to SOAPParser that keeps all of the info\\n ################################################################################\\n-def _parseSOAP(xml_str, rules = None):\\n-    try:\\n-        from cStringIO import StringIO\\n-    except ImportError:\\n-        from StringIO import StringIO\\n+class EmptyEntityResolver(xml.sax.handler.EntityResolver):\\n+    def resolveEntity(self, publicId, systemId):\\n+        return StringIO(\"<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\")\\n+\\n+\\n+def _parseSOAP(xml_str, rules = None, ignore_ext=None):\\n+    if ignore_ext is None:\\n+        ignore_ext = False\\n \\n     parser = xml.sax.make_parser()\\n-    t = SOAPParser(rules = rules)\\n+    t = SOAPParser(rules=rules)\\n     parser.setContentHandler(t)\\n     e = xml.sax.handler.ErrorHandler()\\n     parser.setErrorHandler(e)\\n \\n     inpsrc = xml.sax.xmlreader.InputSource()\\n     inpsrc.setByteStream(StringIO(xml_str))\\n \\n+    # disable by default  entity loading on posted content\\n+    if ignore_ext:\\n+        parser.setEntityResolver(EmptyEntityResolver())\\n     # turn on namespace mangeling\\n-    parser.setFeature(xml.sax.handler.feature_namespaces,1)\\n+    parser.setFeature(xml.sax.handler.feature_namespaces, 1)\\n \\n     try:\\n         parser.parse(inpsrc)\\n     except xml.sax.SAXParseException, e:\\n         parser._parser = None\\n+        print traceback.format_exc()\\n         raise e\\n-    \\n+\\n     return t\\n \\n ################################################################################\\n@@ -1068,9 +1080,9 @@ def parseSOAP(xml_str, attrs = 0):\\n     return t.body\\n \\n \\n-def parseSOAPRPC(xml_str, header = 0, body = 0, attrs = 0, rules = None):\\n+def parseSOAPRPC(xml_str, header = 0, body = 0, attrs = 0, rules = None, ignore_ext=None):\\n \\n-    t = _parseSOAP(xml_str, rules = rules)\\n+    t = _parseSOAP(xml_str, rules = rules, ignore_ext=ignore_ext)\\n     p = t.body[0]\\n \\n     # Empty string, for RPC this translates into a void\\n@@ -1080,7 +1092,7 @@ def parseSOAPRPC(xml_str, header = 0, body = 0, attrs = 0, rules = None):\\n             if k[0] != \"_\":\\n                 name = k\\n         p = structType(name)\\n-        \\n+\\n     if header or body or attrs:\\n         ret = (p,)\\n         if header : ret += (t.header,)'], 'file': ['src/SOAPpy/Server.py', 'src/SOAPpy/Parser.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('8c0140e8-bf97-4639-bb04-4c078966a11b'), UUID('ffd8c7af-51c8-431f-a947-c5d55c86f6ae')]}\n",
      "ERROR:root:Error in {'repo': 'kiorky/SOAPpy', 'vulnerability_id': '2014-3243', 'commit': 'a38656817c8ce7d02e117b1308328419a5d1560f', 'commit_source': 'github', 'cwe_id': ['CWE-119'], 'patch': ['@@ -188,8 +188,9 @@ def unregisterObject(self, object, namespace = \\'\\', path = \\'\\'):\\n             if namespace[0] == \":\": namespace = namespace[1:]\\n \\n         del self.objmap[namespace]\\n-        \\n+\\n class SOAPRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\\n+    ignore_ext = True\\n     def version_string(self):\\n         return \\'<a href=\"http://pywebsvcs.sf.net\">\\' + \\\\\\n             \\'SOAPpy \\' + __version__ + \\'</a> (Python \\' + \\\\\\n@@ -204,7 +205,7 @@ def date_time_string(self):\\n \\n     def do_POST(self):\\n         global _contexts\\n-        \\n+\\n         status = 500\\n         try:\\n             if self.server.config.dumpHeadersIn:\\n@@ -226,7 +227,7 @@ def do_POST(self):\\n                 debugFooter(s)\\n \\n             (r, header, body, attrs) = \\\\\\n-                parseSOAPRPC(data, header = 1, body = 1, attrs = 1)\\n+                parseSOAPRPC(data, header = 1, body = 1, attrs = 1, ignore_ext=self.ignore_ext)\\n \\n             method = r._name\\n             args   = r._aslist()\\n@@ -252,8 +253,8 @@ def do_POST(self):\\n             ordered_args = {}\\n             named_args   = {}\\n \\n-            if Config.specialArgs: \\n-                \\n+            if Config.specialArgs:\\n+\\n                 for (k,v) in  kw.items():\\n \\n                     if k[0]==\"v\":\\n@@ -271,13 +272,13 @@ def do_POST(self):\\n             # if r._ns is specified use it, if not check for\\n             # a path, if it\\'s specified convert it and use it as the\\n             # namespace. If both are specified, use r._ns.\\n-            \\n+\\n             ns = r._ns\\n \\n             if len(self.path) > 1 and not ns:\\n                 ns = self.path.replace(\"/\", \":\")\\n                 if ns[0] == \":\": ns = ns[1:]\\n-            \\n+\\n             # authorization method\\n             a = None\\n \\n@@ -291,9 +292,9 @@ def do_POST(self):\\n             #print \\'<-> Argument Matching Yielded:\\'\\n             #print \\'<-> Ordered Arguments:\\' + str(ordered_args)\\n             #print \\'<-> Named Arguments  :\\' + str(named_args)\\n-             \\n+\\n             resp = \"\"\\n-            \\n+\\n             # For fault messages\\n             if ns:\\n                 nsmethod = \"%s:%s\" % (ns, method)\\n@@ -318,7 +319,7 @@ def do_POST(self):\\n                     # there are none, because the split will return\\n                     # [method]\\n                     f = self.server.objmap[ns]\\n-                    \\n+\\n                     # Look for the authorization method\\n                     if self.server.config.authMethod != None:\\n                         authmethod = self.server.config.authMethod\\n@@ -359,7 +360,7 @@ def do_POST(self):\\n                     if \"SOAPAction\".lower() not in self.headers.keys() or \\\\\\n                        self.headers[\"SOAPAction\"] == \"\\\\\"\\\\\"\":\\n                         self.headers[\"SOAPAction\"] = method\\n-                        \\n+\\n                     thread_id = thread.get_ident()\\n                     _contexts[thread_id] = SOAPContext(header, body,\\n                                                        attrs, data,\\n@@ -374,11 +375,11 @@ def do_POST(self):\\n                             raise faultType(\"%s:Server\" % NS.ENV_T,\\n                                             \"Authorization failed.\",\\n                                             \"%s\" % nsmethod)\\n-                    \\n+\\n                     # If it\\'s wrapped, some special action may be needed\\n                     if isinstance(f, MethodSig):\\n                         c = None\\n-                    \\n+\\n                         if f.context:  # retrieve context object\\n                             c = _contexts[thread_id]\\n \\n@@ -389,9 +390,9 @@ def do_POST(self):\\n                         elif f.keywords:\\n                             # This is lame, but have to de-unicode\\n                             # keywords\\n-                            \\n+\\n                             strkw = {}\\n-                            \\n+\\n                             for (k, v) in kw.items():\\n                                 strkw[str(k)] = v\\n                             if c:\\n@@ -408,7 +409,7 @@ def do_POST(self):\\n                         else:\\n                             fr = apply(f, args, {})\\n \\n-                    \\n+\\n                     if type(fr) == type(self) and \\\\\\n                         isinstance(fr, voidType):\\n                         resp = buildSOAP(kw = {\\'%sResponse\\' % method: fr},\\n@@ -423,7 +424,7 @@ def do_POST(self):\\n                     # Clean up _contexts\\n                     if _contexts.has_key(thread_id):\\n                         del _contexts[thread_id]\\n-                        \\n+\\n                 except Exception, e:\\n                     import traceback\\n                     info = sys.exc_info()\\n@@ -558,7 +559,7 @@ def do_POST(self):\\n                 self.connection.shutdown(1)\\n \\n         def do_GET(self):\\n-            \\n+\\n             #print \\'command        \\', self.command\\n             #print \\'path           \\', self.path\\n             #print \\'request_version\\', self.request_version\\n@@ -567,29 +568,29 @@ def do_GET(self):\\n             #print \\'   maintype\\', self.headers.maintype\\n             #print \\'   subtype \\', self.headers.subtype\\n             #print \\'   params  \\', self.headers.plist\\n-            \\n+\\n             path = self.path.lower()\\n             if path.endswith(\\'wsdl\\'):\\n                 method = \\'wsdl\\'\\n                 function = namespace = None\\n                 if self.server.funcmap.has_key(namespace) \\\\\\n                         and self.server.funcmap[namespace].has_key(method):\\n                     function = self.server.funcmap[namespace][method]\\n-                else: \\n+                else:\\n                     if namespace in self.server.objmap.keys():\\n                         function = self.server.objmap[namespace]\\n                         l = method.split(\".\")\\n                         for i in l:\\n                             function = getattr(function, i)\\n-            \\n+\\n                 if function:\\n                     self.send_response(200)\\n                     self.send_header(\"Content-type\", \\'text/plain\\')\\n                     self.end_headers()\\n                     response = apply(function, ())\\n                     self.wfile.write(str(response))\\n                     return\\n-            \\n+\\n             # return error\\n             self.send_response(200)\\n             self.send_header(\"Content-type\", \\'text/html\\')\\n@@ -614,13 +615,17 @@ def do_GET(self):\\n \\n </body>\\'\\'\\')\\n \\n-            \\n+\\n     def log_message(self, format, *args):\\n         if self.server.log:\\n             BaseHTTPServer.BaseHTTPRequestHandler.\\\\\\n                 log_message (self, format, *args)\\n \\n \\n+class SOAPInsecureRequestHandler(BaseHTTPServer.BaseHTTPRequestHandler):\\n+    \\'\\'\\'Request handler that does load POSTed doctypes\\'\\'\\'\\n+    ignore_ext = False\\n+\\n \\n class SOAPServer(SOAPServerBase, SocketServer.TCPServer):\\n \\n@@ -679,28 +684,32 @@ def __init__(self, addr = (\\'localhost\\', 8000),\\n if hasattr(socket, \"AF_UNIX\"):\\n \\n     class SOAPUnixSocketServer(SOAPServerBase, SocketServer.UnixStreamServer):\\n-    \\n+\\n         def __init__(self, addr = 8000,\\n             RequestHandler = SOAPRequestHandler, log = 0, encoding = \\'UTF-8\\',\\n             config = Config, namespace = None, ssl_context = None):\\n-    \\n+\\n             # Test the encoding, raising an exception if it\\'s not known\\n             if encoding != None:\\n                 \\'\\'.encode(encoding)\\n-    \\n+\\n             if ssl_context != None and not config.SSLserver:\\n                 raise AttributeError, \\\\\\n                     \"SSL server not supported by this Python installation\"\\n-    \\n+\\n             self.namespace          = namespace\\n             self.objmap             = {}\\n             self.funcmap            = {}\\n             self.ssl_context        = ssl_context\\n             self.encoding           = encoding\\n             self.config             = config\\n             self.log                = log\\n-    \\n+\\n             self.allow_reuse_address= 1\\n-    \\n+\\n             SocketServer.UnixStreamServer.__init__(self, str(addr), RequestHandler)\\n-    \\n+\\n+\\n+\\n+\\n+', '@@ -1,4 +1,5 @@\\n # SOAPpy modules\\n+import traceback\\n from Config    import Config\\n from Types     import *\\n from NS        import NS\\n@@ -7,6 +8,10 @@\\n import string\\n import xml.sax\\n from wstools.XMLname import fromXMLname\\n+try:\\n+    from cStringIO import StringIO\\n+except ImportError:\\n+    from StringIO import StringIO\\n \\n try: from M2Crypto import SSL\\n except: pass\\n@@ -93,7 +98,7 @@ def toStr( name ):\\n             elif prefix:\\n                tag = prefix + \":\" + tag\\n             return tag\\n-        \\n+\\n         # Workaround two sax bugs\\n         if name[0] == None and name[1][0] == \\' \\':\\n             name = (None, name[1][1:])\\n@@ -127,7 +132,7 @@ def toStr( name ):\\n         elif self._next == \"\":\\n             raise Error, \"expected nothing, \" \\\\\\n                   \"got `%s\\'\" % toStr( name )\\n-                  \\n+\\n \\n         if len(self._stack) == 2:\\n             rules = self._rules\\n@@ -275,7 +280,7 @@ def endElementNS(self, name, qname):\\n                         null = 1\\n \\n                 # check for nil=1, but watch out for string values\\n-                try:                \\n+                try:\\n                     null = int(null)\\n                 except ValueError, e:\\n                     if not e[0].startswith(\"invalid literal for int()\"):\\n@@ -312,7 +317,7 @@ def endElementNS(self, name, qname):\\n             #print \"cur.kind=\", cur.kind\\n             #print \"cur.rules=\", cur.rules\\n             #print \"\\\\n\"\\n-                        \\n+\\n \\n             if cur.rules != None:\\n                 rule = cur.rules\\n@@ -374,7 +379,7 @@ def endElementNS(self, name, qname):\\n #                 print \"ns:\", ns\\n #                 print \"attrs:\", attrs\\n #                 print \"kind:\", kind\\n-                \\n+\\n \\n                 if kind == None:\\n                     # If the current item\\'s container is an array, it will\\n@@ -863,7 +868,7 @@ def convertToBasicTypes(self, d, t, attrs, config=Config):\\n #         print \"   attrs=\", attrs\\n #         print \"   t[0]=\", t[0]\\n #         print \"   t[1]=\", t[1]\\n-            \\n+\\n #         print \"   in?\", t[0] in NS.EXSD_L\\n \\n         if t[0] in NS.EXSD_L:\\n@@ -933,11 +938,11 @@ def convertToBasicTypes(self, d, t, attrs, config=Config):\\n                     elif d == 0:\\n                         if type(self.zerofloatre) == StringType:\\n                             self.zerofloatre = re.compile(self.zerofloatre)\\n-    \\n+\\n                         if self.zerofloatre.search(s):\\n                             raise UnderflowError, \"invalid %s: %s\" % (t[1], s)\\n                 return d\\n-            \\n+\\n             if t[1] in (\"dateTime\", \"date\", \"timeInstant\", \"time\"):\\n                 return self.convertDateTime(d, t[1])\\n             if t[1] == \"decimal\":\\n@@ -1031,30 +1036,37 @@ def convertToBasicTypes(self, d, t, attrs, config=Config):\\n ################################################################################\\n # call to SOAPParser that keeps all of the info\\n ################################################################################\\n-def _parseSOAP(xml_str, rules = None):\\n-    try:\\n-        from cStringIO import StringIO\\n-    except ImportError:\\n-        from StringIO import StringIO\\n+class EmptyEntityResolver(xml.sax.handler.EntityResolver):\\n+    def resolveEntity(self, publicId, systemId):\\n+        return StringIO(\"<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\")\\n+\\n+\\n+def _parseSOAP(xml_str, rules = None, ignore_ext=None):\\n+    if ignore_ext is None:\\n+        ignore_ext = False\\n \\n     parser = xml.sax.make_parser()\\n-    t = SOAPParser(rules = rules)\\n+    t = SOAPParser(rules=rules)\\n     parser.setContentHandler(t)\\n     e = xml.sax.handler.ErrorHandler()\\n     parser.setErrorHandler(e)\\n \\n     inpsrc = xml.sax.xmlreader.InputSource()\\n     inpsrc.setByteStream(StringIO(xml_str))\\n \\n+    # disable by default  entity loading on posted content\\n+    if ignore_ext:\\n+        parser.setEntityResolver(EmptyEntityResolver())\\n     # turn on namespace mangeling\\n-    parser.setFeature(xml.sax.handler.feature_namespaces,1)\\n+    parser.setFeature(xml.sax.handler.feature_namespaces, 1)\\n \\n     try:\\n         parser.parse(inpsrc)\\n     except xml.sax.SAXParseException, e:\\n         parser._parser = None\\n+        print traceback.format_exc()\\n         raise e\\n-    \\n+\\n     return t\\n \\n ################################################################################\\n@@ -1068,9 +1080,9 @@ def parseSOAP(xml_str, attrs = 0):\\n     return t.body\\n \\n \\n-def parseSOAPRPC(xml_str, header = 0, body = 0, attrs = 0, rules = None):\\n+def parseSOAPRPC(xml_str, header = 0, body = 0, attrs = 0, rules = None, ignore_ext=None):\\n \\n-    t = _parseSOAP(xml_str, rules = rules)\\n+    t = _parseSOAP(xml_str, rules = rules, ignore_ext=ignore_ext)\\n     p = t.body[0]\\n \\n     # Empty string, for RPC this translates into a void\\n@@ -1080,7 +1092,7 @@ def parseSOAPRPC(xml_str, header = 0, body = 0, attrs = 0, rules = None):\\n             if k[0] != \"_\":\\n                 name = k\\n         p = structType(name)\\n-        \\n+\\n     if header or body or attrs:\\n         ret = (p,)\\n         if header : ret += (t.header,)'], 'file': ['src/SOAPpy/Server.py', 'src/SOAPpy/Parser.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('8c0140e8-bf97-4639-bb04-4c078966a11b'), UUID('ffd8c7af-51c8-431f-a947-c5d55c86f6ae')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:22:                 print self.raw_requestline.strip()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:22:                 print self.raw_requestline.strip()\n",
      " 22%|â–ˆâ–ˆâ–       | 395/1800 [04:08<11:26,  2.05it/s]ERROR:src.process_code_changes:Error processing commit 5d36d10e493d92e893d7eae595544bcbe9cce1ce\n",
      "ERROR:src.process_code_changes:{'repo': 'cvat-ai/cvat', 'vulnerability_id': '2024-37306', 'commit': '5d36d10e493d92e893d7eae595544bcbe9cce1ce', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -952,14 +952,12 @@ def export(db_instance, request, queue_name):\\n         field_name=StorageType.TARGET\\n     )\\n \\n+    last_instance_update_time = timezone.localtime(db_instance.updated_date)\\n+\\n     queue = django_rq.get_queue(queue_name)\\n     rq_id = f\"export:{obj_type}.id{db_instance.pk}-by-{request.user}\"\\n     rq_job = queue.fetch_job(rq_id)\\n \\n-    last_instance_update_time = timezone.localtime(db_instance.updated_date)\\n-    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\\n-    location = location_conf.get(\\'location\\')\\n-\\n     if rq_job:\\n         rq_request = rq_job.meta.get(\\'request\\', None)\\n         request_time = rq_request.get(\"timestamp\", None) if rq_request else None\\n@@ -968,43 +966,54 @@ def export(db_instance, request, queue_name):\\n             # we have to enqueue dependent jobs after canceling one\\n             rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n             rq_job.delete()\\n-        else:\\n-            if rq_job.is_finished:\\n-                if location == Location.LOCAL:\\n-                    file_path = rq_job.return_value()\\n-\\n-                    if not file_path:\\n-                        return Response(\\'A result for exporting job was not found for finished RQ job\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n-\\n-                    elif not os.path.exists(file_path):\\n-                        return Response(\\'The result file does not exist in export cache\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n-\\n-                    filename = filename or build_backup_file_name(\\n-                        class_name=obj_type,\\n-                        identifier=db_instance.name,\\n-                        timestamp=timestamp,\\n-                        extension=os.path.splitext(file_path)[1]\\n-                    )\\n-\\n-                    if action == \"download\":\\n-                        rq_job.delete()\\n-                        return sendfile(request, file_path, attachment=True,\\n-                            attachment_filename=filename)\\n-\\n-                    return Response(status=status.HTTP_201_CREATED)\\n-\\n-                elif location == Location.CLOUD_STORAGE:\\n-                    rq_job.delete()\\n-                    return Response(status=status.HTTP_200_OK)\\n-                else:\\n-                    raise NotImplementedError()\\n-            elif rq_job.is_failed:\\n-                exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n+            rq_job = None\\n+\\n+    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\\n+    location = location_conf.get(\\'location\\')\\n+\\n+    if action == \"download\":\\n+        if location != Location.LOCAL:\\n+            return Response(\\'Action \"download\" is only supported for a local backup location\\', status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        if not rq_job or not rq_job.is_finished:\\n+            return Response(\\'Backup has not finished\\', status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        file_path = rq_job.return_value()\\n+\\n+        if not file_path:\\n+            return Response(\\'A result for exporting job was not found for finished RQ job\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+\\n+        elif not os.path.exists(file_path):\\n+            return Response(\\'The result file does not exist in export cache\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+\\n+        filename = filename or build_backup_file_name(\\n+            class_name=obj_type,\\n+            identifier=db_instance.name,\\n+            timestamp=timestamp,\\n+            extension=os.path.splitext(file_path)[1]\\n+        )\\n+\\n+        rq_job.delete()\\n+        return sendfile(request, file_path, attachment=True,\\n+            attachment_filename=filename)\\n+\\n+    if rq_job:\\n+        if rq_job.is_finished:\\n+            if location == Location.LOCAL:\\n+                return Response(status=status.HTTP_201_CREATED)\\n+\\n+            elif location == Location.CLOUD_STORAGE:\\n                 rq_job.delete()\\n-                return Response(exc_info,\\n-                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+                return Response(status=status.HTTP_200_OK)\\n             else:\\n-                return Response(status=status.HTTP_202_ACCEPTED)\\n+                raise NotImplementedError()\\n+        elif rq_job.is_failed:\\n+            exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n+            rq_job.delete()\\n+            return Response(exc_info,\\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+        else:\\n+            return Response(status=status.HTTP_202_ACCEPTED)\\n \\n     ttl = dm.views.PROJECT_CACHE_TTL.total_seconds()\\n     user_id = request.user.id', '@@ -7,7 +7,7 @@\\n import os.path as osp\\n from PIL import Image\\n from types import SimpleNamespace\\n-from typing import Optional, Any, Dict, List, cast, Callable\\n+from typing import Optional, Any, Dict, List, cast, Callable, Mapping\\n import traceback\\n import textwrap\\n from copy import copy\\n@@ -76,7 +76,9 @@\\n     build_annotations_file_name,\\n )\\n from cvat.apps.engine import backup\\n-from cvat.apps.engine.mixins import PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\\n+from cvat.apps.engine.mixins import (\\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\\n+)\\n from cvat.apps.engine.location import get_location_configuration, StorageType\\n \\n from . import models, task\\n@@ -206,6 +208,12 @@ def plugins(request):\\n         }\\n         return Response(PluginsSerializer(data).data)\\n \\n+def csrf_workaround_is_needed_for_backup(query_params: Mapping[str, str]) -> bool:\\n+    return query_params.get(\\'action\\') != \\'download\\'\\n+\\n+def csrf_workaround_is_needed_for_export(query_params: Mapping[str, str]) -> bool:\\n+    return \\'format\\' in query_params and query_params.get(\\'action\\') != \\'download\\'\\n+\\n @extend_schema(tags=[\\'projects\\'])\\n @extend_schema_view(\\n     list=extend_schema(\\n@@ -239,7 +247,7 @@ def plugins(request):\\n )\\n class ProjectViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\\n     mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,\\n-    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\\n ):\\n     queryset = models.Project.objects.select_related(\\n         \\'assignee\\', \\'owner\\', \\'target_storage\\', \\'source_storage\\', \\'annotation_guide\\',\\n@@ -347,7 +355,9 @@ def perform_create(self, serializer, **kwargs):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\', \\'POST\\', \\'OPTIONS\\'], serializer_class=None,\\n-        url_path=r\\'dataset/?$\\', parser_classes=_UPLOAD_PARSER_CLASSES)\\n+        url_path=r\\'dataset/?$\\', parser_classes=_UPLOAD_PARSER_CLASSES,\\n+        csrf_workaround_is_needed=lambda qp:\\n+            csrf_workaround_is_needed_for_export(qp) and qp.get(\"action\") != \"import_status\")\\n     def dataset(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n \\n@@ -479,7 +489,8 @@ def upload_finished(self, request):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\'],\\n-        serializer_class=LabeledDataSerializer)\\n+        serializer_class=LabeledDataSerializer,\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def annotations(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n         return self.export_annotations(\\n@@ -511,7 +522,8 @@ def annotations(self, request, pk):\\n             \\'201\\': OpenApiResponse(description=\\'Output backup file is ready for downloading\\'),\\n             \\'202\\': OpenApiResponse(description=\\'Creating a backup file has been started\\'),\\n         })\\n-    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\')\\n+    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\',\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_backup)\\n     def export_backup(self, request, pk=None):\\n         return self.serialize(request, backup.export)\\n \\n@@ -765,7 +777,7 @@ def __call__(self, request, start, stop, db_data):\\n \\n class TaskViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\\n     mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,\\n-    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\\n ):\\n     queryset = Task.objects.select_related(\\n         \\'data\\', \\'assignee\\', \\'owner\\',\\n@@ -880,7 +892,8 @@ def append_backup_chunk(self, request, file_id):\\n             \\'202\\': OpenApiResponse(description=\\'Creating a backup file has been started\\'),\\n             \\'400\\': OpenApiResponse(description=\\'Backup of a task without data is not allowed\\'),\\n         })\\n-    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\')\\n+    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\',\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_backup)\\n     def export_backup(self, request, pk=None):\\n         if self.get_object().data is None:\\n             return Response(\\n@@ -1330,7 +1343,8 @@ def append_data_chunk(self, request, pk, file_id):\\n             \\'204\\': OpenApiResponse(description=\\'The annotation has been deleted\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\', \\'DELETE\\', \\'PUT\\', \\'PATCH\\', \\'POST\\', \\'OPTIONS\\'], url_path=r\\'annotations/?$\\',\\n-        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES)\\n+        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES,\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def annotations(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n         if request.method == \\'GET\\':\\n@@ -1509,7 +1523,7 @@ def metadata(self, request, pk):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\'], serializer_class=None,\\n-        url_path=\\'dataset\\')\\n+        url_path=\\'dataset\\', csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def dataset_export(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n \\n@@ -1584,7 +1598,7 @@ def preview(self, request, pk):\\n )\\n class JobViewSet(viewsets.GenericViewSet, mixins.ListModelMixin, mixins.CreateModelMixin,\\n     mixins.RetrieveModelMixin, PartialUpdateModelMixin, mixins.DestroyModelMixin,\\n-    UploadMixin, AnnotationMixin\\n+    UploadMixin, AnnotationMixin, CsrfWorkaroundMixin\\n ):\\n     queryset = Job.objects.select_related(\\'assignee\\', \\'segment__task__data\\',\\n         \\'segment__task__project\\', \\'segment__task__annotation_guide\\', \\'segment__task__project__annotation_guide\\',\\n@@ -1776,7 +1790,8 @@ def upload_finished(self, request):\\n             \\'204\\': OpenApiResponse(description=\\'The annotation has been deleted\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\', \\'DELETE\\', \\'PUT\\', \\'PATCH\\', \\'POST\\', \\'OPTIONS\\'], url_path=r\\'annotations/?$\\',\\n-        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES)\\n+        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES,\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def annotations(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n         if request.method == \\'GET\\':\\n@@ -1873,7 +1888,7 @@ def append_annotations_chunk(self, request, pk, file_id):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\'], serializer_class=None,\\n-        url_path=\\'dataset\\')\\n+        url_path=\\'dataset\\', csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def dataset_export(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n \\n@@ -2951,110 +2966,130 @@ def _export_annotations(\\n     elif not format_desc.ENABLED:\\n         return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\\n \\n+    instance_update_time = timezone.localtime(db_instance.updated_date)\\n+    if isinstance(db_instance, Project):\\n+        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\\n+        instance_update_time = max(tasks_update + [instance_update_time])\\n+\\n     queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)\\n     rq_job = queue.fetch_job(rq_id)\\n \\n+    if rq_job:\\n+        rq_request = rq_job.meta.get(\\'request\\', None)\\n+        request_time = rq_request.get(\\'timestamp\\', None) if rq_request else None\\n+        if request_time is None or request_time < instance_update_time:\\n+            # The result is outdated, need to restart the export.\\n+            # Cancel the current job.\\n+            # The new attempt will be made after the last existing job.\\n+            # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n+            # we have to enqueue dependent jobs after canceling one.\\n+            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n+            rq_job.delete()\\n+            rq_job = None\\n+\\n     location = location_conf.get(\\'location\\')\\n     if location not in Location.list():\\n         raise serializers.ValidationError(\\n             f\"Unexpected location {location} specified for the request\"\\n         )\\n \\n     cache_ttl = dm.views.get_export_cache_ttl(db_instance)\\n-    instance_update_time = timezone.localtime(db_instance.updated_date)\\n-    if isinstance(db_instance, Project):\\n-        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\\n-        instance_update_time = max(tasks_update + [instance_update_time])\\n \\n     instance_timestamp = datetime.strftime(instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\\n     is_annotation_file = rq_id.startswith(\\'export:annotations\\')\\n \\n+    REQUEST_TIMEOUT = 60\\n+\\n+    if action == \"download\":\\n+        if location != Location.LOCAL:\\n+            return Response(\\'Action \"download\" is only supported for a local export location\\',\\n+                status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        if not rq_job or not rq_job.is_finished:\\n+            return Response(\\'Export has not finished\\', status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        file_path = rq_job.return_value()\\n+\\n+        if not file_path:\\n+            return Response(\\n+                \\'A result for exporting job was not found for finished RQ job\\',\\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR\\n+            )\\n+\\n+        with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):\\n+            if not osp.exists(file_path):\\n+                return Response(\\n+                    \"The exported file has expired, please retry exporting\",\\n+                    status=status.HTTP_404_NOT_FOUND\\n+                )\\n+\\n+            filename = filename or \\\\\\n+                build_annotations_file_name(\\n+                    class_name=db_instance.__class__.__name__,\\n+                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\\n+                    timestamp=instance_timestamp,\\n+                    format_name=format_name,\\n+                    is_annotation_file=is_annotation_file,\\n+                    extension=osp.splitext(file_path)[1]\\n+                )\\n+\\n+            rq_job.delete()\\n+            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\\n+\\n+\\n     if rq_job:\\n-        rq_request = rq_job.meta.get(\\'request\\', None)\\n-        request_time = rq_request.get(\\'timestamp\\', None) if rq_request else None\\n-        if request_time is None or request_time < instance_update_time:\\n-            # The result is outdated, need to restart the export.\\n-            # Cancel the current job.\\n+        if rq_job.is_finished:\\n+            if location == Location.CLOUD_STORAGE:\\n+                rq_job.delete()\\n+                return Response(status=status.HTTP_200_OK)\\n+\\n+            elif location == Location.LOCAL:\\n+                file_path = rq_job.return_value()\\n+\\n+                if not file_path:\\n+                    return Response(\\n+                        \\'A result for exporting job was not found for finished RQ job\\',\\n+                        status=status.HTTP_500_INTERNAL_SERVER_ERROR\\n+                    )\\n+\\n+                with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):\\n+                    if osp.exists(file_path):\\n+                        # Update last update time to prolong the export lifetime\\n+                        # as the last access time is not available on every filesystem\\n+                        os.utime(file_path, None)\\n+\\n+                        return Response(status=status.HTTP_201_CREATED)\\n+                    else:\\n+                        # Cancel and reenqueue the job.\\n+                        # The new attempt will be made after the last existing job.\\n+                        # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n+                        # we have to enqueue dependent jobs after canceling one.\\n+                        rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n+                        rq_job.delete()\\n+            else:\\n+                raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\\n+        elif rq_job.is_failed:\\n+            exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n+            rq_job.delete()\\n+            return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+        elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\\n+            # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.\\n+            # They can be fetched by their specific ids, but are not listed by get_job_ids().\\n+            # Supposedly, this can happen because of the server restarts\\n+            # (potentially, because the redis used for the queue is inmemory).\\n+            # Another potential reason is canceling without enqueueing dependents.\\n+            # Such dependencies are never removed or finished,\\n+            # as there is no TTL for deferred jobs,\\n+            # so the current job can be blocked indefinitely.\\n+\\n+            # Cancel the current job and then reenqueue it, considering the current situation.\\n             # The new attempt will be made after the last existing job.\\n             # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n             # we have to enqueue dependent jobs after canceling one.\\n             rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n             rq_job.delete()\\n         else:\\n-            if rq_job.is_finished:\\n-                if location == Location.CLOUD_STORAGE:\\n-                    rq_job.delete()\\n-                    return Response(status=status.HTTP_200_OK)\\n-\\n-                elif location == Location.LOCAL:\\n-                    file_path = rq_job.return_value()\\n-\\n-                    if not file_path:\\n-                        return Response(\\n-                            \\'A result for exporting job was not found for finished RQ job\\',\\n-                            status=status.HTTP_500_INTERNAL_SERVER_ERROR\\n-                        )\\n-\\n-                    with dm.util.get_export_cache_lock(\\n-                        file_path, ttl=60, # request timeout\\n-                    ):\\n-                        if action == \"download\":\\n-                            if not osp.exists(file_path):\\n-                                return Response(\\n-                                    \"The exported file has expired, please retry exporting\",\\n-                                    status=status.HTTP_404_NOT_FOUND\\n-                                )\\n-\\n-                            filename = filename or \\\\\\n-                                build_annotations_file_name(\\n-                                    class_name=db_instance.__class__.__name__,\\n-                                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\\n-                                    timestamp=instance_timestamp,\\n-                                    format_name=format_name,\\n-                                    is_annotation_file=is_annotation_file,\\n-                                    extension=osp.splitext(file_path)[1]\\n-                                )\\n-\\n-                            rq_job.delete()\\n-                            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\\n-                        else:\\n-                            if osp.exists(file_path):\\n-                                # Update last update time to prolong the export lifetime\\n-                                # as the last access time is not available on every filesystem\\n-                                os.utime(file_path, None)\\n-\\n-                                return Response(status=status.HTTP_201_CREATED)\\n-                            else:\\n-                                # Cancel and reenqueue the job.\\n-                                # The new attempt will be made after the last existing job.\\n-                                # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n-                                # we have to enqueue dependent jobs after canceling one.\\n-                                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n-                                rq_job.delete()\\n-                else:\\n-                    raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\\n-            elif rq_job.is_failed:\\n-                exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n-                rq_job.delete()\\n-                return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n-            elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\\n-                # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.\\n-                # They can be fetched by their specific ids, but are not listed by get_job_ids().\\n-                # Supposedly, this can happen because of the server restarts\\n-                # (potentially, because the redis used for the queue is inmemory).\\n-                # Another potential reason is canceling without enqueueing dependents.\\n-                # Such dependencies are never removed or finished,\\n-                # as there is no TTL for deferred jobs,\\n-                # so the current job can be blocked indefinitely.\\n-\\n-                # Cancel the current job and then reenqueue it, considering the current situation.\\n-                # The new attempt will be made after the last existing job.\\n-                # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n-                # we have to enqueue dependent jobs after canceling one.\\n-                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n-                rq_job.delete()\\n-            else:\\n-                return Response(status=status.HTTP_202_ACCEPTED)\\n+            return Response(status=status.HTTP_202_ACCEPTED)\\n     try:\\n         if request.scheme:\\n             server_address = request.scheme + \\'://\\''], 'file': ['cvat/apps/engine/backup.py', 'cvat/apps/engine/views.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b7beffd8-24eb-414c-bebf-4f1548ea1449'), UUID('4917ff0f-a5fa-48e1-bba1-60c41f200b2f')]}\n",
      "ERROR:root:Error in {'repo': 'cvat-ai/cvat', 'vulnerability_id': '2024-37306', 'commit': '5d36d10e493d92e893d7eae595544bcbe9cce1ce', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -952,14 +952,12 @@ def export(db_instance, request, queue_name):\\n         field_name=StorageType.TARGET\\n     )\\n \\n+    last_instance_update_time = timezone.localtime(db_instance.updated_date)\\n+\\n     queue = django_rq.get_queue(queue_name)\\n     rq_id = f\"export:{obj_type}.id{db_instance.pk}-by-{request.user}\"\\n     rq_job = queue.fetch_job(rq_id)\\n \\n-    last_instance_update_time = timezone.localtime(db_instance.updated_date)\\n-    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\\n-    location = location_conf.get(\\'location\\')\\n-\\n     if rq_job:\\n         rq_request = rq_job.meta.get(\\'request\\', None)\\n         request_time = rq_request.get(\"timestamp\", None) if rq_request else None\\n@@ -968,43 +966,54 @@ def export(db_instance, request, queue_name):\\n             # we have to enqueue dependent jobs after canceling one\\n             rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n             rq_job.delete()\\n-        else:\\n-            if rq_job.is_finished:\\n-                if location == Location.LOCAL:\\n-                    file_path = rq_job.return_value()\\n-\\n-                    if not file_path:\\n-                        return Response(\\'A result for exporting job was not found for finished RQ job\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n-\\n-                    elif not os.path.exists(file_path):\\n-                        return Response(\\'The result file does not exist in export cache\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n-\\n-                    filename = filename or build_backup_file_name(\\n-                        class_name=obj_type,\\n-                        identifier=db_instance.name,\\n-                        timestamp=timestamp,\\n-                        extension=os.path.splitext(file_path)[1]\\n-                    )\\n-\\n-                    if action == \"download\":\\n-                        rq_job.delete()\\n-                        return sendfile(request, file_path, attachment=True,\\n-                            attachment_filename=filename)\\n-\\n-                    return Response(status=status.HTTP_201_CREATED)\\n-\\n-                elif location == Location.CLOUD_STORAGE:\\n-                    rq_job.delete()\\n-                    return Response(status=status.HTTP_200_OK)\\n-                else:\\n-                    raise NotImplementedError()\\n-            elif rq_job.is_failed:\\n-                exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n+            rq_job = None\\n+\\n+    timestamp = datetime.strftime(last_instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\\n+    location = location_conf.get(\\'location\\')\\n+\\n+    if action == \"download\":\\n+        if location != Location.LOCAL:\\n+            return Response(\\'Action \"download\" is only supported for a local backup location\\', status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        if not rq_job or not rq_job.is_finished:\\n+            return Response(\\'Backup has not finished\\', status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        file_path = rq_job.return_value()\\n+\\n+        if not file_path:\\n+            return Response(\\'A result for exporting job was not found for finished RQ job\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+\\n+        elif not os.path.exists(file_path):\\n+            return Response(\\'The result file does not exist in export cache\\', status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+\\n+        filename = filename or build_backup_file_name(\\n+            class_name=obj_type,\\n+            identifier=db_instance.name,\\n+            timestamp=timestamp,\\n+            extension=os.path.splitext(file_path)[1]\\n+        )\\n+\\n+        rq_job.delete()\\n+        return sendfile(request, file_path, attachment=True,\\n+            attachment_filename=filename)\\n+\\n+    if rq_job:\\n+        if rq_job.is_finished:\\n+            if location == Location.LOCAL:\\n+                return Response(status=status.HTTP_201_CREATED)\\n+\\n+            elif location == Location.CLOUD_STORAGE:\\n                 rq_job.delete()\\n-                return Response(exc_info,\\n-                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+                return Response(status=status.HTTP_200_OK)\\n             else:\\n-                return Response(status=status.HTTP_202_ACCEPTED)\\n+                raise NotImplementedError()\\n+        elif rq_job.is_failed:\\n+            exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n+            rq_job.delete()\\n+            return Response(exc_info,\\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+        else:\\n+            return Response(status=status.HTTP_202_ACCEPTED)\\n \\n     ttl = dm.views.PROJECT_CACHE_TTL.total_seconds()\\n     user_id = request.user.id', '@@ -7,7 +7,7 @@\\n import os.path as osp\\n from PIL import Image\\n from types import SimpleNamespace\\n-from typing import Optional, Any, Dict, List, cast, Callable\\n+from typing import Optional, Any, Dict, List, cast, Callable, Mapping\\n import traceback\\n import textwrap\\n from copy import copy\\n@@ -76,7 +76,9 @@\\n     build_annotations_file_name,\\n )\\n from cvat.apps.engine import backup\\n-from cvat.apps.engine.mixins import PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\\n+from cvat.apps.engine.mixins import (\\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\\n+)\\n from cvat.apps.engine.location import get_location_configuration, StorageType\\n \\n from . import models, task\\n@@ -206,6 +208,12 @@ def plugins(request):\\n         }\\n         return Response(PluginsSerializer(data).data)\\n \\n+def csrf_workaround_is_needed_for_backup(query_params: Mapping[str, str]) -> bool:\\n+    return query_params.get(\\'action\\') != \\'download\\'\\n+\\n+def csrf_workaround_is_needed_for_export(query_params: Mapping[str, str]) -> bool:\\n+    return \\'format\\' in query_params and query_params.get(\\'action\\') != \\'download\\'\\n+\\n @extend_schema(tags=[\\'projects\\'])\\n @extend_schema_view(\\n     list=extend_schema(\\n@@ -239,7 +247,7 @@ def plugins(request):\\n )\\n class ProjectViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\\n     mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,\\n-    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\\n ):\\n     queryset = models.Project.objects.select_related(\\n         \\'assignee\\', \\'owner\\', \\'target_storage\\', \\'source_storage\\', \\'annotation_guide\\',\\n@@ -347,7 +355,9 @@ def perform_create(self, serializer, **kwargs):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\', \\'POST\\', \\'OPTIONS\\'], serializer_class=None,\\n-        url_path=r\\'dataset/?$\\', parser_classes=_UPLOAD_PARSER_CLASSES)\\n+        url_path=r\\'dataset/?$\\', parser_classes=_UPLOAD_PARSER_CLASSES,\\n+        csrf_workaround_is_needed=lambda qp:\\n+            csrf_workaround_is_needed_for_export(qp) and qp.get(\"action\") != \"import_status\")\\n     def dataset(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n \\n@@ -479,7 +489,8 @@ def upload_finished(self, request):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\'],\\n-        serializer_class=LabeledDataSerializer)\\n+        serializer_class=LabeledDataSerializer,\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def annotations(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n         return self.export_annotations(\\n@@ -511,7 +522,8 @@ def annotations(self, request, pk):\\n             \\'201\\': OpenApiResponse(description=\\'Output backup file is ready for downloading\\'),\\n             \\'202\\': OpenApiResponse(description=\\'Creating a backup file has been started\\'),\\n         })\\n-    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\')\\n+    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\',\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_backup)\\n     def export_backup(self, request, pk=None):\\n         return self.serialize(request, backup.export)\\n \\n@@ -765,7 +777,7 @@ def __call__(self, request, start, stop, db_data):\\n \\n class TaskViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,\\n     mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,\\n-    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin\\n+    PartialUpdateModelMixin, UploadMixin, AnnotationMixin, SerializeMixin, CsrfWorkaroundMixin\\n ):\\n     queryset = Task.objects.select_related(\\n         \\'data\\', \\'assignee\\', \\'owner\\',\\n@@ -880,7 +892,8 @@ def append_backup_chunk(self, request, file_id):\\n             \\'202\\': OpenApiResponse(description=\\'Creating a backup file has been started\\'),\\n             \\'400\\': OpenApiResponse(description=\\'Backup of a task without data is not allowed\\'),\\n         })\\n-    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\')\\n+    @action(methods=[\\'GET\\'], detail=True, url_path=\\'backup\\',\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_backup)\\n     def export_backup(self, request, pk=None):\\n         if self.get_object().data is None:\\n             return Response(\\n@@ -1330,7 +1343,8 @@ def append_data_chunk(self, request, pk, file_id):\\n             \\'204\\': OpenApiResponse(description=\\'The annotation has been deleted\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\', \\'DELETE\\', \\'PUT\\', \\'PATCH\\', \\'POST\\', \\'OPTIONS\\'], url_path=r\\'annotations/?$\\',\\n-        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES)\\n+        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES,\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def annotations(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n         if request.method == \\'GET\\':\\n@@ -1509,7 +1523,7 @@ def metadata(self, request, pk):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\'], serializer_class=None,\\n-        url_path=\\'dataset\\')\\n+        url_path=\\'dataset\\', csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def dataset_export(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n \\n@@ -1584,7 +1598,7 @@ def preview(self, request, pk):\\n )\\n class JobViewSet(viewsets.GenericViewSet, mixins.ListModelMixin, mixins.CreateModelMixin,\\n     mixins.RetrieveModelMixin, PartialUpdateModelMixin, mixins.DestroyModelMixin,\\n-    UploadMixin, AnnotationMixin\\n+    UploadMixin, AnnotationMixin, CsrfWorkaroundMixin\\n ):\\n     queryset = Job.objects.select_related(\\'assignee\\', \\'segment__task__data\\',\\n         \\'segment__task__project\\', \\'segment__task__annotation_guide\\', \\'segment__task__project__annotation_guide\\',\\n@@ -1776,7 +1790,8 @@ def upload_finished(self, request):\\n             \\'204\\': OpenApiResponse(description=\\'The annotation has been deleted\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\', \\'DELETE\\', \\'PUT\\', \\'PATCH\\', \\'POST\\', \\'OPTIONS\\'], url_path=r\\'annotations/?$\\',\\n-        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES)\\n+        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES,\\n+        csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def annotations(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n         if request.method == \\'GET\\':\\n@@ -1873,7 +1888,7 @@ def append_annotations_chunk(self, request, pk, file_id):\\n             \\'405\\': OpenApiResponse(description=\\'Format is not available\\'),\\n         })\\n     @action(detail=True, methods=[\\'GET\\'], serializer_class=None,\\n-        url_path=\\'dataset\\')\\n+        url_path=\\'dataset\\', csrf_workaround_is_needed=csrf_workaround_is_needed_for_export)\\n     def dataset_export(self, request, pk):\\n         self._object = self.get_object() # force call of check_object_permissions()\\n \\n@@ -2951,110 +2966,130 @@ def _export_annotations(\\n     elif not format_desc.ENABLED:\\n         return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)\\n \\n+    instance_update_time = timezone.localtime(db_instance.updated_date)\\n+    if isinstance(db_instance, Project):\\n+        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\\n+        instance_update_time = max(tasks_update + [instance_update_time])\\n+\\n     queue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)\\n     rq_job = queue.fetch_job(rq_id)\\n \\n+    if rq_job:\\n+        rq_request = rq_job.meta.get(\\'request\\', None)\\n+        request_time = rq_request.get(\\'timestamp\\', None) if rq_request else None\\n+        if request_time is None or request_time < instance_update_time:\\n+            # The result is outdated, need to restart the export.\\n+            # Cancel the current job.\\n+            # The new attempt will be made after the last existing job.\\n+            # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n+            # we have to enqueue dependent jobs after canceling one.\\n+            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n+            rq_job.delete()\\n+            rq_job = None\\n+\\n     location = location_conf.get(\\'location\\')\\n     if location not in Location.list():\\n         raise serializers.ValidationError(\\n             f\"Unexpected location {location} specified for the request\"\\n         )\\n \\n     cache_ttl = dm.views.get_export_cache_ttl(db_instance)\\n-    instance_update_time = timezone.localtime(db_instance.updated_date)\\n-    if isinstance(db_instance, Project):\\n-        tasks_update = list(map(lambda db_task: timezone.localtime(db_task.updated_date), db_instance.tasks.all()))\\n-        instance_update_time = max(tasks_update + [instance_update_time])\\n \\n     instance_timestamp = datetime.strftime(instance_update_time, \"%Y_%m_%d_%H_%M_%S\")\\n     is_annotation_file = rq_id.startswith(\\'export:annotations\\')\\n \\n+    REQUEST_TIMEOUT = 60\\n+\\n+    if action == \"download\":\\n+        if location != Location.LOCAL:\\n+            return Response(\\'Action \"download\" is only supported for a local export location\\',\\n+                status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        if not rq_job or not rq_job.is_finished:\\n+            return Response(\\'Export has not finished\\', status=status.HTTP_400_BAD_REQUEST)\\n+\\n+        file_path = rq_job.return_value()\\n+\\n+        if not file_path:\\n+            return Response(\\n+                \\'A result for exporting job was not found for finished RQ job\\',\\n+                status=status.HTTP_500_INTERNAL_SERVER_ERROR\\n+            )\\n+\\n+        with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):\\n+            if not osp.exists(file_path):\\n+                return Response(\\n+                    \"The exported file has expired, please retry exporting\",\\n+                    status=status.HTTP_404_NOT_FOUND\\n+                )\\n+\\n+            filename = filename or \\\\\\n+                build_annotations_file_name(\\n+                    class_name=db_instance.__class__.__name__,\\n+                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\\n+                    timestamp=instance_timestamp,\\n+                    format_name=format_name,\\n+                    is_annotation_file=is_annotation_file,\\n+                    extension=osp.splitext(file_path)[1]\\n+                )\\n+\\n+            rq_job.delete()\\n+            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\\n+\\n+\\n     if rq_job:\\n-        rq_request = rq_job.meta.get(\\'request\\', None)\\n-        request_time = rq_request.get(\\'timestamp\\', None) if rq_request else None\\n-        if request_time is None or request_time < instance_update_time:\\n-            # The result is outdated, need to restart the export.\\n-            # Cancel the current job.\\n+        if rq_job.is_finished:\\n+            if location == Location.CLOUD_STORAGE:\\n+                rq_job.delete()\\n+                return Response(status=status.HTTP_200_OK)\\n+\\n+            elif location == Location.LOCAL:\\n+                file_path = rq_job.return_value()\\n+\\n+                if not file_path:\\n+                    return Response(\\n+                        \\'A result for exporting job was not found for finished RQ job\\',\\n+                        status=status.HTTP_500_INTERNAL_SERVER_ERROR\\n+                    )\\n+\\n+                with dm.util.get_export_cache_lock(file_path, ttl=REQUEST_TIMEOUT):\\n+                    if osp.exists(file_path):\\n+                        # Update last update time to prolong the export lifetime\\n+                        # as the last access time is not available on every filesystem\\n+                        os.utime(file_path, None)\\n+\\n+                        return Response(status=status.HTTP_201_CREATED)\\n+                    else:\\n+                        # Cancel and reenqueue the job.\\n+                        # The new attempt will be made after the last existing job.\\n+                        # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n+                        # we have to enqueue dependent jobs after canceling one.\\n+                        rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n+                        rq_job.delete()\\n+            else:\\n+                raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\\n+        elif rq_job.is_failed:\\n+            exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n+            rq_job.delete()\\n+            return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n+        elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\\n+            # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.\\n+            # They can be fetched by their specific ids, but are not listed by get_job_ids().\\n+            # Supposedly, this can happen because of the server restarts\\n+            # (potentially, because the redis used for the queue is inmemory).\\n+            # Another potential reason is canceling without enqueueing dependents.\\n+            # Such dependencies are never removed or finished,\\n+            # as there is no TTL for deferred jobs,\\n+            # so the current job can be blocked indefinitely.\\n+\\n+            # Cancel the current job and then reenqueue it, considering the current situation.\\n             # The new attempt will be made after the last existing job.\\n             # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n             # we have to enqueue dependent jobs after canceling one.\\n             rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n             rq_job.delete()\\n         else:\\n-            if rq_job.is_finished:\\n-                if location == Location.CLOUD_STORAGE:\\n-                    rq_job.delete()\\n-                    return Response(status=status.HTTP_200_OK)\\n-\\n-                elif location == Location.LOCAL:\\n-                    file_path = rq_job.return_value()\\n-\\n-                    if not file_path:\\n-                        return Response(\\n-                            \\'A result for exporting job was not found for finished RQ job\\',\\n-                            status=status.HTTP_500_INTERNAL_SERVER_ERROR\\n-                        )\\n-\\n-                    with dm.util.get_export_cache_lock(\\n-                        file_path, ttl=60, # request timeout\\n-                    ):\\n-                        if action == \"download\":\\n-                            if not osp.exists(file_path):\\n-                                return Response(\\n-                                    \"The exported file has expired, please retry exporting\",\\n-                                    status=status.HTTP_404_NOT_FOUND\\n-                                )\\n-\\n-                            filename = filename or \\\\\\n-                                build_annotations_file_name(\\n-                                    class_name=db_instance.__class__.__name__,\\n-                                    identifier=db_instance.name if isinstance(db_instance, (Task, Project)) else db_instance.id,\\n-                                    timestamp=instance_timestamp,\\n-                                    format_name=format_name,\\n-                                    is_annotation_file=is_annotation_file,\\n-                                    extension=osp.splitext(file_path)[1]\\n-                                )\\n-\\n-                            rq_job.delete()\\n-                            return sendfile(request, file_path, attachment=True, attachment_filename=filename)\\n-                        else:\\n-                            if osp.exists(file_path):\\n-                                # Update last update time to prolong the export lifetime\\n-                                # as the last access time is not available on every filesystem\\n-                                os.utime(file_path, None)\\n-\\n-                                return Response(status=status.HTTP_201_CREATED)\\n-                            else:\\n-                                # Cancel and reenqueue the job.\\n-                                # The new attempt will be made after the last existing job.\\n-                                # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n-                                # we have to enqueue dependent jobs after canceling one.\\n-                                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n-                                rq_job.delete()\\n-                else:\\n-                    raise NotImplementedError(f\"Export to {location} location is not implemented yet\")\\n-            elif rq_job.is_failed:\\n-                exc_info = rq_job.meta.get(\\'formatted_exception\\', str(rq_job.exc_info))\\n-                rq_job.delete()\\n-                return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\\n-            elif rq_job.is_deferred and rq_id not in queue.deferred_job_registry.get_job_ids():\\n-                # Sometimes jobs can depend on outdated jobs in the deferred jobs registry.\\n-                # They can be fetched by their specific ids, but are not listed by get_job_ids().\\n-                # Supposedly, this can happen because of the server restarts\\n-                # (potentially, because the redis used for the queue is inmemory).\\n-                # Another potential reason is canceling without enqueueing dependents.\\n-                # Such dependencies are never removed or finished,\\n-                # as there is no TTL for deferred jobs,\\n-                # so the current job can be blocked indefinitely.\\n-\\n-                # Cancel the current job and then reenqueue it, considering the current situation.\\n-                # The new attempt will be made after the last existing job.\\n-                # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER\\n-                # we have to enqueue dependent jobs after canceling one.\\n-                rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)\\n-                rq_job.delete()\\n-            else:\\n-                return Response(status=status.HTTP_202_ACCEPTED)\\n+            return Response(status=status.HTTP_202_ACCEPTED)\\n     try:\\n         if request.scheme:\\n             server_address = request.scheme + \\'://\\''], 'file': ['cvat/apps/engine/backup.py', 'cvat/apps/engine/views.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b7beffd8-24eb-414c-bebf-4f1548ea1449'), UUID('4917ff0f-a5fa-48e1-bba1-60c41f200b2f')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 386:0:         serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES,\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 386:0:         serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES,\n",
      " 23%|â–ˆâ–ˆâ–       | 418/1800 [04:11<05:54,  3.90it/s]ERROR:src.process_code_changes:Error processing commit f2e0818bc97bfbeba83f6abbb07909a8debcad77\n",
      "ERROR:src.process_code_changes:{'repo': 'openstack/python-keystoneclient', 'vulnerability_id': '2013-2013', 'commit': 'f2e0818bc97bfbeba83f6abbb07909a8debcad77', 'commit_source': 'github', 'cwe_id': ['CWE-200'], 'patch': ['@@ -1,5 +1,7 @@\\n-import uuid\\n+import getpass\\n import hashlib\\n+import sys\\n+import uuid\\n \\n import prettytable\\n \\n@@ -128,3 +130,22 @@ def hash_signed_token(signed_text):\\n     hash_ = hashlib.md5()\\n     hash_.update(signed_text)\\n     return hash_.hexdigest()\\n+\\n+\\n+def prompt_for_password():\\n+    \"\"\"\\n+     Prompt user for password if not provided so the password\\n+     doesn\\'t show up in the bash history.\\n+    \"\"\"\\n+    if not (hasattr(sys.stdin, \\'isatty\\') and sys.stdin.isatty()):\\n+        # nothing to do\\n+        return\\n+\\n+    while True:\\n+        try:\\n+            new_passwd = getpass.getpass(\\'New Password: \\')\\n+            rep_passwd = getpass.getpass(\\'Repeat New Password: \\')\\n+            if new_passwd == rep_passwd:\\n+                return new_passwd\\n+        except EOFError:\\n+            return', '@@ -17,6 +17,7 @@\\n \\n import argparse\\n import getpass\\n+import sys\\n \\n from keystoneclient.v2_0 import client\\n from keystoneclient import utils\\n@@ -103,14 +104,19 @@ def do_user_update(kc, args):\\n         print \\'Unable to update user: %s\\' % e\\n \\n \\n-@utils.arg(\\'--pass\\', metavar=\\'<password>\\', dest=\\'passwd\\', required=True,\\n+@utils.arg(\\'--pass\\', metavar=\\'<password>\\', dest=\\'passwd\\', required=False,\\n            help=\\'Desired new password\\')\\n @utils.arg(\\'user\\', metavar=\\'<user>\\',\\n            help=\\'Name or ID of user to update password\\')\\n def do_user_password_update(kc, args):\\n     \"\"\"Update user password\"\"\"\\n     user = utils.find_resource(kc.users, args.user)\\n-    kc.users.update_password(user, args.passwd)\\n+    new_passwd = args.passwd or utils.prompt_for_password()\\n+    if new_passwd is None:\\n+        msg = (\"\\\\nPlease specify password using the --pass option \"\\n+               \"or using the prompt\")\\n+        sys.exit(msg)\\n+    kc.users.update_password(user, new_passwd)\\n \\n \\n @utils.arg(\\'--current-password\\', metavar=\\'<current-password>\\','], 'file': ['keystoneclient/utils.py', 'keystoneclient/v2_0/shell.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('3f37626c-6ad9-4ef8-a9b6-ca6f2e50985b'), UUID('511e0e22-75b3-4519-9557-15e6cffdf534')]}\n",
      "ERROR:root:Error in {'repo': 'openstack/python-keystoneclient', 'vulnerability_id': '2013-2013', 'commit': 'f2e0818bc97bfbeba83f6abbb07909a8debcad77', 'commit_source': 'github', 'cwe_id': ['CWE-200'], 'patch': ['@@ -1,5 +1,7 @@\\n-import uuid\\n+import getpass\\n import hashlib\\n+import sys\\n+import uuid\\n \\n import prettytable\\n \\n@@ -128,3 +130,22 @@ def hash_signed_token(signed_text):\\n     hash_ = hashlib.md5()\\n     hash_.update(signed_text)\\n     return hash_.hexdigest()\\n+\\n+\\n+def prompt_for_password():\\n+    \"\"\"\\n+     Prompt user for password if not provided so the password\\n+     doesn\\'t show up in the bash history.\\n+    \"\"\"\\n+    if not (hasattr(sys.stdin, \\'isatty\\') and sys.stdin.isatty()):\\n+        # nothing to do\\n+        return\\n+\\n+    while True:\\n+        try:\\n+            new_passwd = getpass.getpass(\\'New Password: \\')\\n+            rep_passwd = getpass.getpass(\\'Repeat New Password: \\')\\n+            if new_passwd == rep_passwd:\\n+                return new_passwd\\n+        except EOFError:\\n+            return', '@@ -17,6 +17,7 @@\\n \\n import argparse\\n import getpass\\n+import sys\\n \\n from keystoneclient.v2_0 import client\\n from keystoneclient import utils\\n@@ -103,14 +104,19 @@ def do_user_update(kc, args):\\n         print \\'Unable to update user: %s\\' % e\\n \\n \\n-@utils.arg(\\'--pass\\', metavar=\\'<password>\\', dest=\\'passwd\\', required=True,\\n+@utils.arg(\\'--pass\\', metavar=\\'<password>\\', dest=\\'passwd\\', required=False,\\n            help=\\'Desired new password\\')\\n @utils.arg(\\'user\\', metavar=\\'<user>\\',\\n            help=\\'Name or ID of user to update password\\')\\n def do_user_password_update(kc, args):\\n     \"\"\"Update user password\"\"\"\\n     user = utils.find_resource(kc.users, args.user)\\n-    kc.users.update_password(user, args.passwd)\\n+    new_passwd = args.passwd or utils.prompt_for_password()\\n+    if new_passwd is None:\\n+        msg = (\"\\\\nPlease specify password using the --pass option \"\\n+               \"or using the prompt\")\\n+        sys.exit(msg)\\n+    kc.users.update_password(user, new_passwd)\\n \\n \\n @utils.arg(\\'--current-password\\', metavar=\\'<current-password>\\','], 'file': ['keystoneclient/utils.py', 'keystoneclient/v2_0/shell.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('3f37626c-6ad9-4ef8-a9b6-ca6f2e50985b'), UUID('511e0e22-75b3-4519-9557-15e6cffdf534')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 101, in clear_file_content\n",
      "    new_content = remove_comments(content)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 71, in remove_comments\n",
      "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 543, in _generate_tokens_from_c_tokenizer\n",
      "    raise TokenError(msg, (e.lineno, e.offset)) from None\n",
      "tokenize.TokenError: ('unexpected EOF in multi-line statement', (11, 0))\n",
      " 24%|â–ˆâ–ˆâ–       | 432/1800 [04:42<26:49,  1.18s/it]ERROR:src.process_code_changes:Error processing commit b95f083efffa56831cd41d8ed536aeb0b6038fa3\n",
      "ERROR:src.process_code_changes:{'repo': 'zenml-io/zenml', 'vulnerability_id': '2024-2035', 'commit': 'b95f083efffa56831cd41d8ed536aeb0b6038fa3', 'commit_source': 'github', 'cwe_id': ['CWE-1220'], 'patch': ['@@ -233,7 +233,10 @@ def verify_permission_for_model(model: AnyResponse, action: Action) -> None:\\n     batch_verify_permissions_for_models(models=[model], action=action)\\n \\n \\n-def batch_verify_permissions(resources: Set[Resource], action: Action) -> None:\\n+def batch_verify_permissions(\\n+    resources: Set[Resource],\\n+    action: Action,\\n+) -> None:\\n     \"\"\"Batch permission verification.\\n \\n     Args:', '@@ -27,7 +27,7 @@\\n     ENV_ZENML_SERVER,\\n )\\n from zenml.enums import ServerProviderType\\n-from zenml.exceptions import OAuthError\\n+from zenml.exceptions import IllegalOperationError, OAuthError\\n from zenml.logger import get_logger\\n from zenml.plugins.plugin_flavor_registry import PluginFlavorRegistry\\n from zenml.zen_server.deploy.deployment import ServerDeployment\\n@@ -396,3 +396,34 @@ def get_ip_location(ip_address: str) -> Tuple[str, str, str]:\\n     except Exception:\\n         logger.exception(f\"Could not get IP location for {ip_address}.\")\\n         return \"\", \"\", \"\"\\n+\\n+\\n+def verify_admin_status_if_no_rbac(\\n+    admin_status: Optional[bool],\\n+    action: Optional[str] = None,\\n+) -> None:\\n+    \"\"\"Validate the admin status for sensitive requests.\\n+\\n+    Only add this check in endpoints meant for admin use only.\\n+\\n+    Args:\\n+        admin_status: Whether the user is an admin or not. This is only used\\n+            if explicitly specified in the call and even if passed will be\\n+            ignored, if RBAC is enabled.\\n+        action: The action that is being performed, used for output only.\\n+\\n+    Raises:\\n+        IllegalOperationError: If the admin status is not valid.\\n+    \"\"\"\\n+    if not server_config().rbac_enabled:\\n+        if not action:\\n+            action = \"this action\"\\n+        else:\\n+            action = f\"`{action.strip(\\'`\\')}`\"\\n+\\n+        if admin_status is False:\\n+            raise IllegalOperationError(\\n+                message=f\"Only admin users can perform {action} \"\\n+                \"without RBAC enabled.\",\\n+            )\\n+    return', '@@ -7514,6 +7514,14 @@ def update_user(\\n                 user_id, session=session, service_account=False\\n             )\\n \\n+            if (\\n+                existing_user.name == self._default_user_name\\n+                and user_update.is_admin is False\\n+            ):\\n+                raise IllegalOperationError(\\n+                    \"The default user\\'s admin status cannot be removed.\"\\n+                )\\n+\\n             if (\\n                 user_update.name is not None\\n                 and user_update.name != existing_user.name\\n@@ -7604,6 +7612,7 @@ def _get_or_create_default_user(self) -> UserResponse:\\n                     name=default_user_name,\\n                     active=True,\\n                     password=password,\\n+                    is_admin=True,\\n                 )\\n             )\\n ', '@@ -77,6 +77,7 @@ class UserSchema(NamedSchema, table=True):\\n     hub_token: Optional[str] = Field(nullable=True)\\n     email_opted_in: Optional[bool] = Field(nullable=True)\\n     external_user_id: Optional[UUID] = Field(nullable=True)\\n+    is_admin: bool = Field(default=False)\\n \\n     stacks: List[\"StackSchema\"] = Relationship(back_populates=\"user\")\\n     components: List[\"StackComponentSchema\"] = Relationship(\\n@@ -167,6 +168,7 @@ def from_user_request(cls, model: UserRequest) -> \"UserSchema\":\\n             email_opted_in=model.email_opted_in,\\n             email=model.email,\\n             is_service_account=False,\\n+            is_admin=model.is_admin,\\n         )\\n \\n     @classmethod\\n@@ -189,6 +191,7 @@ def from_service_account_request(\\n             is_service_account=True,\\n             email_opted_in=False,\\n             full_name=\"\",\\n+            is_admin=False,\\n         )\\n \\n     def update_user(self, user_update: UserUpdate) -> \"UserSchema\":\\n@@ -271,6 +274,7 @@ def to_model(\\n                 is_service_account=self.is_service_account,\\n                 created=self.created,\\n                 updated=self.updated,\\n+                is_admin=self.is_admin,\\n             ),\\n             metadata=metadata,\\n         )', '@@ -147,6 +147,7 @@ def to_user_model(self) -> \"UserResponse\":\\n                 email_opted_in=False,\\n                 created=self.created,\\n                 updated=self.updated,\\n+                is_admin=False,\\n             ),\\n             metadata=UserResponseMetadata(\\n                 description=self.description,', '@@ -25,6 +25,7 @@ class ExternalUserModel(BaseModel):\\n     id: UUID\\n     email: str\\n     name: Optional[str] = None\\n+    is_admin: bool = False\\n \\n     class Config:\\n         \"\"\"Pydantic configuration.\"\"\"', '@@ -59,6 +59,7 @@\\n     handle_exceptions,\\n     make_dependable,\\n     server_config,\\n+    verify_admin_status_if_no_rbac,\\n     zen_store,\\n )\\n \\n@@ -112,6 +113,9 @@ def list_users(\\n     if allowed_ids is not None:\\n         # Make sure users can see themselves\\n         allowed_ids.add(auth_context.user.id)\\n+    else:\\n+        if not auth_context.user.is_admin and not server_config().rbac_enabled:\\n+            allowed_ids = {auth_context.user.id}\\n \\n     user_filter_model.configure_rbac(\\n         authenticated_user_id=auth_context.user.id, id=allowed_ids\\n@@ -139,14 +143,15 @@ def list_users(\\n     @handle_exceptions\\n     def create_user(\\n         user: UserRequest,\\n-        _: AuthContext = Security(authorize),\\n+        auth_context: AuthContext = Security(authorize),\\n     ) -> UserResponse:\\n         \"\"\"Creates a user.\\n \\n         # noqa: DAR401\\n \\n         Args:\\n             user: User to create.\\n+            auth_context: Authentication context.\\n \\n         Returns:\\n             The created user.\\n@@ -163,6 +168,10 @@ def create_user(\\n         else:\\n             user.active = True\\n \\n+        verify_admin_status_if_no_rbac(\\n+            auth_context.user.is_admin, \"create user\"\\n+        )\\n+\\n         new_user = verify_permissions_and_create_entity(\\n             request_model=user,\\n             resource_type=ResourceType.USER,\\n@@ -202,7 +211,13 @@ def get_user(\\n         user_name_or_id=user_name_or_id, hydrate=hydrate\\n     )\\n     if user.id != auth_context.user.id:\\n-        verify_permission_for_model(user, action=Action.READ)\\n+        verify_admin_status_if_no_rbac(\\n+            auth_context.user.is_admin, \"get other user\"\\n+        )\\n+        verify_permission_for_model(\\n+            user,\\n+            action=Action.READ,\\n+        )\\n \\n     return dehydrate_response_model(user)\\n \\n@@ -235,11 +250,32 @@ def update_user(\\n \\n         Returns:\\n             The updated user.\\n+\\n+        Raises:\\n+            IllegalOperationError: if the user tries change admin status,\\n+                while not an admin\\n         \"\"\"\\n         user = zen_store().get_user(user_name_or_id)\\n         if user.id != auth_context.user.id:\\n-            verify_permission_for_model(user, action=Action.UPDATE)\\n+            verify_admin_status_if_no_rbac(\\n+                auth_context.user.is_admin, \"update other user\"\\n+            )\\n+            verify_permission_for_model(\\n+                user,\\n+                action=Action.UPDATE,\\n+            )\\n+        if (\\n+            user_update.is_admin is not None\\n+            and user.is_admin != user_update.is_admin\\n+            and not auth_context.user.is_admin\\n+        ):\\n+            raise IllegalOperationError(\\n+                \"Only admins can change the admin status of other users.\"\\n+            )\\n \\n+        user_update.activation_token = user.activation_token\\n+        if not auth_context.user.is_admin or user.id == auth_context.user.id:\\n+            user_update.active = user.active\\n         updated_user = zen_store().update_user(\\n             user_id=user.id,\\n             user_update=user_update,\\n@@ -279,6 +315,7 @@ def activate_user(\\n         )\\n         user_update.active = True\\n         user_update.activation_token = None\\n+        user_update.is_admin = user.is_admin\\n         return zen_store().update_user(\\n             user_id=user.id, user_update=user_update\\n         )\\n@@ -305,10 +342,21 @@ def deactivate_user(\\n \\n         Returns:\\n             The generated activation token.\\n+\\n+        Raises:\\n+            IllegalOperationError: if the user is trying to deactivate\\n+                themselves.\\n         \"\"\"\\n         user = zen_store().get_user(user_name_or_id)\\n-        if user.id != auth_context.user.id:\\n-            verify_permission_for_model(user, action=Action.UPDATE)\\n+        if user.id == auth_context.user.id:\\n+            raise IllegalOperationError(\"Cannot deactivate yourself.\")\\n+        verify_admin_status_if_no_rbac(\\n+            auth_context.user.is_admin, \"deactivate user\"\\n+        )\\n+        verify_permission_for_model(\\n+            user,\\n+            action=Action.UPDATE,\\n+        )\\n \\n         user_update = UserUpdate(\\n             name=user.name,\\n@@ -354,7 +402,13 @@ def delete_user(\\n                 \"administrator.\"\\n             )\\n         else:\\n-            verify_permission_for_model(user, action=Action.DELETE)\\n+            verify_admin_status_if_no_rbac(\\n+                auth_context.user.is_admin, \"delete user\"\\n+            )\\n+            verify_permission_for_model(\\n+                user,\\n+                action=Action.DELETE,\\n+            )\\n \\n         zen_store().delete_user(user_name_or_id=user_name_or_id)\\n \\n@@ -402,7 +456,6 @@ def email_opt_in_response(\\n                     email=user_response.email,\\n                     source=\"zenml server\",\\n                 )\\n-\\n             updated_user = zen_store().update_user(\\n                 user_id=user.id, user_update=user_update\\n             )\\n@@ -460,6 +513,11 @@ def update_myself(\\n         Returns:\\n             The updated user.\\n         \"\"\"\\n+        current_user = zen_store().get_user(auth_context.user.id)\\n+        user.activation_token = current_user.activation_token\\n+        user.active = current_user.active\\n+        user.is_admin = current_user.is_admin\\n+\\n         updated_user = zen_store().update_user(\\n             user_id=auth_context.user.id, user_update=user\\n         )', '@@ -588,6 +588,7 @@ def authenticate_external_user(external_access_token: str) -> AuthContext:\\n                 email_opted_in=True,\\n                 active=True,\\n                 email=external_user.email,\\n+                is_admin=external_user.is_admin,\\n             ),\\n         )\\n     except KeyError:\\n@@ -603,6 +604,7 @@ def authenticate_external_user(external_access_token: str) -> AuthContext:\\n                 email_opted_in=True,\\n                 active=True,\\n                 email=external_user.email,\\n+                is_admin=external_user.is_admin,\\n             )\\n         )\\n ', '@@ -26,7 +26,7 @@\\n )\\n from uuid import UUID\\n \\n-from pydantic import Field, root_validator\\n+from pydantic import BaseModel, Field, root_validator\\n \\n from zenml.constants import STR_FIELD_MAX_LENGTH\\n from zenml.models.v2.base.base import (\\n@@ -35,40 +35,23 @@\\n     BaseRequest,\\n     BaseResponseMetadata,\\n     BaseResponseResources,\\n+    BaseZenModel,\\n )\\n from zenml.models.v2.base.filter import AnyQuery, BaseFilter\\n-from zenml.models.v2.base.update import update_model\\n \\n if TYPE_CHECKING:\\n     from passlib.context import CryptContext\\n \\n     from zenml.models.v2.base.filter import AnySchema\\n \\n-# ------------------ Request Model ------------------\\n-\\n+# ------------------ Base Model ------------------\\n \\n-class UserRequest(BaseRequest):\\n-    \"\"\"Request model for users.\"\"\"\\n \\n-    # Analytics fields for user request models\\n-    ANALYTICS_FIELDS: ClassVar[List[str]] = [\\n-        \"name\",\\n-        \"full_name\",\\n-        \"active\",\\n-        \"email_opted_in\",\\n-    ]\\n+class UserBase(BaseModel):\\n+    \"\"\"Base model for users.\"\"\"\\n \\n     # Fields\\n-    name: str = Field(\\n-        title=\"The unique username for the account.\",\\n-        max_length=STR_FIELD_MAX_LENGTH,\\n-    )\\n-    full_name: str = Field(\\n-        default=\"\",\\n-        title=\"The full name for the account owner. Only relevant for user \"\\n-        \"accounts.\",\\n-        max_length=STR_FIELD_MAX_LENGTH,\\n-    )\\n+\\n     email: Optional[str] = Field(\\n         default=None,\\n         title=\"The email address associated with the account.\",\\n@@ -99,17 +82,6 @@ class UserRequest(BaseRequest):\\n         default=None,\\n         title=\"The external user ID associated with the account.\",\\n     )\\n-    active: bool = Field(default=False, title=\"Whether the account is active.\")\\n-\\n-    class Config:\\n-        \"\"\"Pydantic configuration class.\"\"\"\\n-\\n-        # Validate attributes when assigning them\\n-        validate_assignment = True\\n-\\n-        # Forbid extra attributes to prevent unexpected behavior\\n-        extra = \"forbid\"\\n-        underscore_attrs_are_private = True\\n \\n     @classmethod\\n     def _get_crypt_context(cls) -> \"CryptContext\":\\n@@ -165,13 +137,71 @@ def generate_activation_token(self) -> str:\\n         return self.activation_token\\n \\n \\n+# ------------------ Request Model ------------------\\n+\\n+\\n+class UserRequest(UserBase, BaseRequest):\\n+    \"\"\"Request model for users.\"\"\"\\n+\\n+    # Analytics fields for user request models\\n+    ANALYTICS_FIELDS: ClassVar[List[str]] = [\\n+        \"name\",\\n+        \"full_name\",\\n+        \"active\",\\n+        \"email_opted_in\",\\n+    ]\\n+\\n+    name: str = Field(\\n+        title=\"The unique username for the account.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n+    full_name: str = Field(\\n+        default=\"\",\\n+        title=\"The full name for the account owner. Only relevant for user \"\\n+        \"accounts.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n+    is_admin: bool = Field(\\n+        title=\"Whether the account is an administrator.\",\\n+    )\\n+    active: bool = Field(default=False, title=\"Whether the account is active.\")\\n+\\n+    class Config:\\n+        \"\"\"Pydantic configuration class.\"\"\"\\n+\\n+        # Validate attributes when assigning them\\n+        validate_assignment = True\\n+\\n+        # Forbid extra attributes to prevent unexpected behavior\\n+        extra = \"forbid\"\\n+        underscore_attrs_are_private = True\\n+\\n+\\n # ------------------ Update Model ------------------\\n \\n \\n-@update_model\\n-class UserUpdate(UserRequest):\\n+class UserUpdate(UserBase, BaseZenModel):\\n     \"\"\"Update model for users.\"\"\"\\n \\n+    name: Optional[str] = Field(\\n+        title=\"The unique username for the account.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+        default=None,\\n+    )\\n+    full_name: Optional[str] = Field(\\n+        default=None,\\n+        title=\"The full name for the account owner. Only relevant for user \"\\n+        \"accounts.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n+    is_admin: Optional[bool] = Field(\\n+        default=None,\\n+        title=\"Whether the account is an administrator.\",\\n+    )\\n+    active: Optional[bool] = Field(\\n+        default=None, title=\"Whether the account is active.\"\\n+    )\\n+\\n     @root_validator\\n     def user_email_updates(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n         \"\"\"Validate that the UserUpdateModel conforms to the email-opt-in-flow.\\n@@ -231,6 +261,9 @@ class UserResponseBody(BaseDatedResponseBody):\\n     is_service_account: bool = Field(\\n         title=\"Indicates whether this is a service account or a user account.\"\\n     )\\n+    is_admin: bool = Field(\\n+        title=\"Whether the account is an administrator.\",\\n+    )\\n \\n \\n class UserResponseMetadata(BaseResponseMetadata):\\n@@ -340,6 +373,15 @@ def is_service_account(self) -> bool:\\n         \"\"\"\\n         return self.get_body().is_service_account\\n \\n+    @property\\n+    def is_admin(self) -> bool:\\n+        \"\"\"The `is_admin` property.\\n+\\n+        Returns:\\n+            Whether the user is an admin.\\n+        \"\"\"\\n+        return self.get_body().is_admin\\n+\\n     @property\\n     def email(self) -> Optional[str]:\\n         \"\"\"The `email` property.', '@@ -59,7 +59,10 @@ def verify_permissions_and_create_entity(\\n                 \"different user.\"\\n             )\\n \\n-    verify_permission(resource_type=resource_type, action=Action.CREATE)\\n+    verify_permission(\\n+        resource_type=resource_type,\\n+        action=Action.CREATE,\\n+    )\\n     return create_method(request_model)\\n \\n ', '@@ -691,18 +691,22 @@ def create_user(\\n         self,\\n         name: str,\\n         password: Optional[str] = None,\\n+        is_admin: bool = False,\\n     ) -> UserResponse:\\n         \"\"\"Create a new user.\\n \\n         Args:\\n             name: The name of the user.\\n             password: The password of the user. If not provided, the user will\\n                 be created with empty password.\\n+            is_admin: Whether the user should be an admin.\\n \\n         Returns:\\n             The model of the created user.\\n         \"\"\"\\n-        user = UserRequest(name=name, password=password or None)\\n+        user = UserRequest(\\n+            name=name, password=password or None, is_admin=is_admin\\n+        )\\n         user.active = (\\n             password != \"\" if self.zen_store.type != StoreType.REST else True\\n         )\\n@@ -801,6 +805,8 @@ def update_user(\\n         updated_email: Optional[str] = None,\\n         updated_email_opt_in: Optional[bool] = None,\\n         updated_hub_token: Optional[str] = None,\\n+        updated_password: Optional[str] = None,\\n+        updated_is_admin: Optional[bool] = None,\\n     ) -> UserResponse:\\n         \"\"\"Update a user.\\n \\n@@ -811,6 +817,8 @@ def update_user(\\n             updated_email: The new email of the user.\\n             updated_email_opt_in: The new email opt-in status of the user.\\n             updated_hub_token: Update the hub token\\n+            updated_password: The new password of the user.\\n+            updated_is_admin: Whether the user should be an admin.\\n \\n         Returns:\\n             The updated user.\\n@@ -830,6 +838,10 @@ def update_user(\\n             user_update.email_opted_in = updated_email_opt_in\\n         if updated_hub_token is not None:\\n             user_update.hub_token = updated_hub_token\\n+        if updated_password is not None:\\n+            user_update.password = updated_password\\n+        if updated_is_admin is not None:\\n+            user_update.is_admin = updated_is_admin\\n \\n         return self.zen_store.update_user(\\n             user_id=user.id, user_update=user_update', '@@ -120,15 +120,27 @@ def list_users(ctx: click.Context, **kwargs: Any) -> None:\\n     required=False,\\n     type=str,\\n )\\n+@click.option(\\n+    \"--is_admin\",\\n+    is_flag=True,\\n+    help=(\\n+        \"Whether the user should be an admin. If not specified, the user will \"\\n+        \"be a regular user.\"\\n+    ),\\n+    required=False,\\n+    default=False,\\n+)\\n def create_user(\\n     user_name: str,\\n     password: Optional[str] = None,\\n+    is_admin: bool = False,\\n ) -> None:\\n     \"\"\"Create a new user.\\n \\n     Args:\\n         user_name: The name of the user to create.\\n         password: The password of the user to create.\\n+        is_admin: Whether the user should be an admin.\\n     \"\"\"\\n     client = Client()\\n     if not password:\\n@@ -146,7 +158,9 @@ def create_user(\\n             )\\n \\n     try:\\n-        new_user = client.create_user(name=user_name, password=password)\\n+        new_user = client.create_user(\\n+            name=user_name, password=password, is_admin=is_admin\\n+        )\\n \\n         cli_utils.declare(f\"Created user \\'{new_user.name}\\'.\")\\n     except EntityExistsError as err:\\n@@ -162,8 +176,7 @@ def create_user(\\n \\n @user.command(\\n     \"update\",\\n-    help=\"Update user information through the cli. All attributes \"\\n-    \"except for password can be updated through the cli.\",\\n+    help=\"Update user information through the cli.\",\\n )\\n @click.argument(\"user_name_or_id\", type=str, required=True)\\n @click.option(\\n@@ -191,26 +204,80 @@ def create_user(\\n     required=False,\\n     help=\"New user email.\",\\n )\\n+@click.option(\\n+    \"--password\",\\n+    \"-p\",\\n+    \"updated_password\",\\n+    type=str,\\n+    required=False,\\n+    help=\"New user password.\",\\n+)\\n+@click.option(\\n+    \"--admin\",\\n+    \"-a\",\\n+    \"make_admin\",\\n+    is_flag=True,\\n+    required=False,\\n+    default=None,\\n+    help=\"Whether the user should be an admin.\",\\n+)\\n+@click.option(\\n+    \"--user\",\\n+    \"-u\",\\n+    \"make_user\",\\n+    is_flag=True,\\n+    required=False,\\n+    default=None,\\n+    help=\"Whether the user should be a regular user.\",\\n+)\\n def update_user(\\n     user_name_or_id: str,\\n     updated_name: Optional[str] = None,\\n     updated_full_name: Optional[str] = None,\\n     updated_email: Optional[str] = None,\\n+    updated_password: Optional[str] = None,\\n+    make_admin: Optional[bool] = None,\\n+    make_user: Optional[bool] = None,\\n ) -> None:\\n-    \"\"\"Create a new user.\\n+    \"\"\"Update an existing user.\\n \\n     Args:\\n         user_name_or_id: The name of the user to create.\\n         updated_name: The name of the user to create.\\n         updated_full_name: The name of the user to create.\\n         updated_email: The name of the user to create.\\n+        updated_password: The name of the user to create.\\n+        make_admin: Whether the user should be an admin.\\n+        make_user: Whether the user should be a regular user.\\n     \"\"\"\\n+    if make_admin is not None and make_user is not None:\\n+        cli_utils.error(\\n+            \"Cannot set both --admin and --user flags as these are mutually exclusive.\"\\n+        )\\n     try:\\n+        current_user = Client().get_user(\\n+            user_name_or_id, allow_name_prefix_match=False\\n+        )\\n+        if current_user.is_admin and make_user:\\n+            confirmation = cli_utils.confirmation(\\n+                f\"Currently user `{current_user.name}` is an admin. Are you sure you want to make them a regular user?\"\\n+            )\\n+            if not confirmation:\\n+                cli_utils.declare(\"User update canceled.\")\\n+                return\\n+\\n+        updated_is_admin = None\\n+        if make_admin is True:\\n+            updated_is_admin = True\\n+        elif make_user is True:\\n+            updated_is_admin = False\\n         Client().update_user(\\n             name_id_or_prefix=user_name_or_id,\\n             updated_name=updated_name,\\n             updated_full_name=updated_full_name,\\n             updated_email=updated_email,\\n+            updated_password=updated_password,\\n+            updated_is_admin=updated_is_admin,\\n         )\\n     except (KeyError, IllegalOperationError) as err:\\n         cli_utils.error(str(err))', '@@ -0,0 +1,56 @@\\n+\"\"\"admin users [1a9a9d2a836d].\\n+\\n+Revision ID: 1a9a9d2a836d\\n+Revises: 0.55.5\\n+Create Date: 2024-03-04 15:48:16.580871\\n+\\n+\"\"\"\\n+\\n+import sqlalchemy as sa\\n+import sqlmodel\\n+from alembic import op\\n+\\n+# revision identifiers, used by Alembic.\\n+revision = \"1a9a9d2a836d\"\\n+down_revision = \"0.55.5\"\\n+branch_labels = None\\n+depends_on = None\\n+\\n+\\n+def upgrade() -> None:\\n+    \"\"\"Upgrade database schema and/or data, creating a new revision.\"\"\"\\n+    # ### commands auto generated by Alembic - please adjust! ###\\n+    bind = op.get_bind()\\n+    session = sqlmodel.Session(bind=bind)\\n+\\n+    with op.batch_alter_table(\"user\", schema=None) as batch_op:\\n+        batch_op.add_column(\\n+            sa.Column(\\n+                \"is_admin\",\\n+                sa.Boolean(),\\n+                nullable=False,\\n+                server_default=sa.sql.expression.false(),\\n+            )\\n+        )\\n+\\n+    # during migration we treat all users as admin for backward compatibility\\n+    # this should be adjusted by server admins after upgrade\\n+    session.execute(\\n+        sa.text(\\n+            \"\"\"\\n+            UPDATE user\\n+            SET is_admin = true\\n+            WHERE NOT is_service_account AND external_user_id IS NULL\\n+            \"\"\"\\n+        )\\n+    )\\n+    # ### end Alembic commands ###\\n+\\n+\\n+def downgrade() -> None:\\n+    \"\"\"Downgrade database schema and/or data back to the previous revision.\"\"\"\\n+    # ### commands auto generated by Alembic - please adjust! ###\\n+    with op.batch_alter_table(\"user\", schema=None) as batch_op:\\n+        batch_op.drop_column(\"is_admin\")\\n+\\n+    # ### end Alembic commands ###'], 'file': ['src/zenml/zen_server/rbac/utils.py', 'src/zenml/zen_server/utils.py', 'src/zenml/zen_stores/sql_zen_store.py', 'src/zenml/zen_stores/schemas/user_schemas.py', 'src/zenml/models/v2/core/service_account.py', 'src/zenml/models/v2/misc/external_user.py', 'src/zenml/zen_server/routers/users_endpoints.py', 'src/zenml/zen_server/auth.py', 'src/zenml/models/v2/core/user.py', 'src/zenml/zen_server/rbac/endpoint_utils.py', 'src/zenml/client.py', 'src/zenml/cli/user_management.py', 'src/zenml/zen_stores/migrations/versions/1a9a9d2a836d_admin_users.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('402f4ff2-5a4a-4015-b63e-b4b4c90d97ff'), UUID('b61cca97-bfb1-4df1-ab50-2edde8655463'), UUID('0b914dfd-4889-4ea1-b16c-6b50ab16b96a'), UUID('f836e36a-1f43-4977-8938-4e13a2990a90'), UUID('d2f4f481-fedb-457c-a9c8-71ec5a632b85'), UUID('824440cc-13f6-46c0-863d-b192ec2362cb'), UUID('fed00c3b-56a2-40c1-9c05-219588a413cf'), UUID('223e6df7-4d06-44a0-a85e-94066084934c'), UUID('f4d6e392-248c-4049-90c6-e6d0e14c0e8a'), UUID('d1bc9877-6163-446a-b3ef-6586fe48956f'), UUID('906d1816-dff4-4c4f-b066-ba759b03db8e'), UUID('413a9482-9ac1-4ac0-84d7-a64ad27c7ef2'), UUID('5977a70a-6fd7-4b79-b180-974697dc534a')]}\n",
      "ERROR:root:Error in {'repo': 'zenml-io/zenml', 'vulnerability_id': '2024-2035', 'commit': 'b95f083efffa56831cd41d8ed536aeb0b6038fa3', 'commit_source': 'github', 'cwe_id': ['CWE-1220'], 'patch': ['@@ -233,7 +233,10 @@ def verify_permission_for_model(model: AnyResponse, action: Action) -> None:\\n     batch_verify_permissions_for_models(models=[model], action=action)\\n \\n \\n-def batch_verify_permissions(resources: Set[Resource], action: Action) -> None:\\n+def batch_verify_permissions(\\n+    resources: Set[Resource],\\n+    action: Action,\\n+) -> None:\\n     \"\"\"Batch permission verification.\\n \\n     Args:', '@@ -27,7 +27,7 @@\\n     ENV_ZENML_SERVER,\\n )\\n from zenml.enums import ServerProviderType\\n-from zenml.exceptions import OAuthError\\n+from zenml.exceptions import IllegalOperationError, OAuthError\\n from zenml.logger import get_logger\\n from zenml.plugins.plugin_flavor_registry import PluginFlavorRegistry\\n from zenml.zen_server.deploy.deployment import ServerDeployment\\n@@ -396,3 +396,34 @@ def get_ip_location(ip_address: str) -> Tuple[str, str, str]:\\n     except Exception:\\n         logger.exception(f\"Could not get IP location for {ip_address}.\")\\n         return \"\", \"\", \"\"\\n+\\n+\\n+def verify_admin_status_if_no_rbac(\\n+    admin_status: Optional[bool],\\n+    action: Optional[str] = None,\\n+) -> None:\\n+    \"\"\"Validate the admin status for sensitive requests.\\n+\\n+    Only add this check in endpoints meant for admin use only.\\n+\\n+    Args:\\n+        admin_status: Whether the user is an admin or not. This is only used\\n+            if explicitly specified in the call and even if passed will be\\n+            ignored, if RBAC is enabled.\\n+        action: The action that is being performed, used for output only.\\n+\\n+    Raises:\\n+        IllegalOperationError: If the admin status is not valid.\\n+    \"\"\"\\n+    if not server_config().rbac_enabled:\\n+        if not action:\\n+            action = \"this action\"\\n+        else:\\n+            action = f\"`{action.strip(\\'`\\')}`\"\\n+\\n+        if admin_status is False:\\n+            raise IllegalOperationError(\\n+                message=f\"Only admin users can perform {action} \"\\n+                \"without RBAC enabled.\",\\n+            )\\n+    return', '@@ -7514,6 +7514,14 @@ def update_user(\\n                 user_id, session=session, service_account=False\\n             )\\n \\n+            if (\\n+                existing_user.name == self._default_user_name\\n+                and user_update.is_admin is False\\n+            ):\\n+                raise IllegalOperationError(\\n+                    \"The default user\\'s admin status cannot be removed.\"\\n+                )\\n+\\n             if (\\n                 user_update.name is not None\\n                 and user_update.name != existing_user.name\\n@@ -7604,6 +7612,7 @@ def _get_or_create_default_user(self) -> UserResponse:\\n                     name=default_user_name,\\n                     active=True,\\n                     password=password,\\n+                    is_admin=True,\\n                 )\\n             )\\n ', '@@ -77,6 +77,7 @@ class UserSchema(NamedSchema, table=True):\\n     hub_token: Optional[str] = Field(nullable=True)\\n     email_opted_in: Optional[bool] = Field(nullable=True)\\n     external_user_id: Optional[UUID] = Field(nullable=True)\\n+    is_admin: bool = Field(default=False)\\n \\n     stacks: List[\"StackSchema\"] = Relationship(back_populates=\"user\")\\n     components: List[\"StackComponentSchema\"] = Relationship(\\n@@ -167,6 +168,7 @@ def from_user_request(cls, model: UserRequest) -> \"UserSchema\":\\n             email_opted_in=model.email_opted_in,\\n             email=model.email,\\n             is_service_account=False,\\n+            is_admin=model.is_admin,\\n         )\\n \\n     @classmethod\\n@@ -189,6 +191,7 @@ def from_service_account_request(\\n             is_service_account=True,\\n             email_opted_in=False,\\n             full_name=\"\",\\n+            is_admin=False,\\n         )\\n \\n     def update_user(self, user_update: UserUpdate) -> \"UserSchema\":\\n@@ -271,6 +274,7 @@ def to_model(\\n                 is_service_account=self.is_service_account,\\n                 created=self.created,\\n                 updated=self.updated,\\n+                is_admin=self.is_admin,\\n             ),\\n             metadata=metadata,\\n         )', '@@ -147,6 +147,7 @@ def to_user_model(self) -> \"UserResponse\":\\n                 email_opted_in=False,\\n                 created=self.created,\\n                 updated=self.updated,\\n+                is_admin=False,\\n             ),\\n             metadata=UserResponseMetadata(\\n                 description=self.description,', '@@ -25,6 +25,7 @@ class ExternalUserModel(BaseModel):\\n     id: UUID\\n     email: str\\n     name: Optional[str] = None\\n+    is_admin: bool = False\\n \\n     class Config:\\n         \"\"\"Pydantic configuration.\"\"\"', '@@ -59,6 +59,7 @@\\n     handle_exceptions,\\n     make_dependable,\\n     server_config,\\n+    verify_admin_status_if_no_rbac,\\n     zen_store,\\n )\\n \\n@@ -112,6 +113,9 @@ def list_users(\\n     if allowed_ids is not None:\\n         # Make sure users can see themselves\\n         allowed_ids.add(auth_context.user.id)\\n+    else:\\n+        if not auth_context.user.is_admin and not server_config().rbac_enabled:\\n+            allowed_ids = {auth_context.user.id}\\n \\n     user_filter_model.configure_rbac(\\n         authenticated_user_id=auth_context.user.id, id=allowed_ids\\n@@ -139,14 +143,15 @@ def list_users(\\n     @handle_exceptions\\n     def create_user(\\n         user: UserRequest,\\n-        _: AuthContext = Security(authorize),\\n+        auth_context: AuthContext = Security(authorize),\\n     ) -> UserResponse:\\n         \"\"\"Creates a user.\\n \\n         # noqa: DAR401\\n \\n         Args:\\n             user: User to create.\\n+            auth_context: Authentication context.\\n \\n         Returns:\\n             The created user.\\n@@ -163,6 +168,10 @@ def create_user(\\n         else:\\n             user.active = True\\n \\n+        verify_admin_status_if_no_rbac(\\n+            auth_context.user.is_admin, \"create user\"\\n+        )\\n+\\n         new_user = verify_permissions_and_create_entity(\\n             request_model=user,\\n             resource_type=ResourceType.USER,\\n@@ -202,7 +211,13 @@ def get_user(\\n         user_name_or_id=user_name_or_id, hydrate=hydrate\\n     )\\n     if user.id != auth_context.user.id:\\n-        verify_permission_for_model(user, action=Action.READ)\\n+        verify_admin_status_if_no_rbac(\\n+            auth_context.user.is_admin, \"get other user\"\\n+        )\\n+        verify_permission_for_model(\\n+            user,\\n+            action=Action.READ,\\n+        )\\n \\n     return dehydrate_response_model(user)\\n \\n@@ -235,11 +250,32 @@ def update_user(\\n \\n         Returns:\\n             The updated user.\\n+\\n+        Raises:\\n+            IllegalOperationError: if the user tries change admin status,\\n+                while not an admin\\n         \"\"\"\\n         user = zen_store().get_user(user_name_or_id)\\n         if user.id != auth_context.user.id:\\n-            verify_permission_for_model(user, action=Action.UPDATE)\\n+            verify_admin_status_if_no_rbac(\\n+                auth_context.user.is_admin, \"update other user\"\\n+            )\\n+            verify_permission_for_model(\\n+                user,\\n+                action=Action.UPDATE,\\n+            )\\n+        if (\\n+            user_update.is_admin is not None\\n+            and user.is_admin != user_update.is_admin\\n+            and not auth_context.user.is_admin\\n+        ):\\n+            raise IllegalOperationError(\\n+                \"Only admins can change the admin status of other users.\"\\n+            )\\n \\n+        user_update.activation_token = user.activation_token\\n+        if not auth_context.user.is_admin or user.id == auth_context.user.id:\\n+            user_update.active = user.active\\n         updated_user = zen_store().update_user(\\n             user_id=user.id,\\n             user_update=user_update,\\n@@ -279,6 +315,7 @@ def activate_user(\\n         )\\n         user_update.active = True\\n         user_update.activation_token = None\\n+        user_update.is_admin = user.is_admin\\n         return zen_store().update_user(\\n             user_id=user.id, user_update=user_update\\n         )\\n@@ -305,10 +342,21 @@ def deactivate_user(\\n \\n         Returns:\\n             The generated activation token.\\n+\\n+        Raises:\\n+            IllegalOperationError: if the user is trying to deactivate\\n+                themselves.\\n         \"\"\"\\n         user = zen_store().get_user(user_name_or_id)\\n-        if user.id != auth_context.user.id:\\n-            verify_permission_for_model(user, action=Action.UPDATE)\\n+        if user.id == auth_context.user.id:\\n+            raise IllegalOperationError(\"Cannot deactivate yourself.\")\\n+        verify_admin_status_if_no_rbac(\\n+            auth_context.user.is_admin, \"deactivate user\"\\n+        )\\n+        verify_permission_for_model(\\n+            user,\\n+            action=Action.UPDATE,\\n+        )\\n \\n         user_update = UserUpdate(\\n             name=user.name,\\n@@ -354,7 +402,13 @@ def delete_user(\\n                 \"administrator.\"\\n             )\\n         else:\\n-            verify_permission_for_model(user, action=Action.DELETE)\\n+            verify_admin_status_if_no_rbac(\\n+                auth_context.user.is_admin, \"delete user\"\\n+            )\\n+            verify_permission_for_model(\\n+                user,\\n+                action=Action.DELETE,\\n+            )\\n \\n         zen_store().delete_user(user_name_or_id=user_name_or_id)\\n \\n@@ -402,7 +456,6 @@ def email_opt_in_response(\\n                     email=user_response.email,\\n                     source=\"zenml server\",\\n                 )\\n-\\n             updated_user = zen_store().update_user(\\n                 user_id=user.id, user_update=user_update\\n             )\\n@@ -460,6 +513,11 @@ def update_myself(\\n         Returns:\\n             The updated user.\\n         \"\"\"\\n+        current_user = zen_store().get_user(auth_context.user.id)\\n+        user.activation_token = current_user.activation_token\\n+        user.active = current_user.active\\n+        user.is_admin = current_user.is_admin\\n+\\n         updated_user = zen_store().update_user(\\n             user_id=auth_context.user.id, user_update=user\\n         )', '@@ -588,6 +588,7 @@ def authenticate_external_user(external_access_token: str) -> AuthContext:\\n                 email_opted_in=True,\\n                 active=True,\\n                 email=external_user.email,\\n+                is_admin=external_user.is_admin,\\n             ),\\n         )\\n     except KeyError:\\n@@ -603,6 +604,7 @@ def authenticate_external_user(external_access_token: str) -> AuthContext:\\n                 email_opted_in=True,\\n                 active=True,\\n                 email=external_user.email,\\n+                is_admin=external_user.is_admin,\\n             )\\n         )\\n ', '@@ -26,7 +26,7 @@\\n )\\n from uuid import UUID\\n \\n-from pydantic import Field, root_validator\\n+from pydantic import BaseModel, Field, root_validator\\n \\n from zenml.constants import STR_FIELD_MAX_LENGTH\\n from zenml.models.v2.base.base import (\\n@@ -35,40 +35,23 @@\\n     BaseRequest,\\n     BaseResponseMetadata,\\n     BaseResponseResources,\\n+    BaseZenModel,\\n )\\n from zenml.models.v2.base.filter import AnyQuery, BaseFilter\\n-from zenml.models.v2.base.update import update_model\\n \\n if TYPE_CHECKING:\\n     from passlib.context import CryptContext\\n \\n     from zenml.models.v2.base.filter import AnySchema\\n \\n-# ------------------ Request Model ------------------\\n-\\n+# ------------------ Base Model ------------------\\n \\n-class UserRequest(BaseRequest):\\n-    \"\"\"Request model for users.\"\"\"\\n \\n-    # Analytics fields for user request models\\n-    ANALYTICS_FIELDS: ClassVar[List[str]] = [\\n-        \"name\",\\n-        \"full_name\",\\n-        \"active\",\\n-        \"email_opted_in\",\\n-    ]\\n+class UserBase(BaseModel):\\n+    \"\"\"Base model for users.\"\"\"\\n \\n     # Fields\\n-    name: str = Field(\\n-        title=\"The unique username for the account.\",\\n-        max_length=STR_FIELD_MAX_LENGTH,\\n-    )\\n-    full_name: str = Field(\\n-        default=\"\",\\n-        title=\"The full name for the account owner. Only relevant for user \"\\n-        \"accounts.\",\\n-        max_length=STR_FIELD_MAX_LENGTH,\\n-    )\\n+\\n     email: Optional[str] = Field(\\n         default=None,\\n         title=\"The email address associated with the account.\",\\n@@ -99,17 +82,6 @@ class UserRequest(BaseRequest):\\n         default=None,\\n         title=\"The external user ID associated with the account.\",\\n     )\\n-    active: bool = Field(default=False, title=\"Whether the account is active.\")\\n-\\n-    class Config:\\n-        \"\"\"Pydantic configuration class.\"\"\"\\n-\\n-        # Validate attributes when assigning them\\n-        validate_assignment = True\\n-\\n-        # Forbid extra attributes to prevent unexpected behavior\\n-        extra = \"forbid\"\\n-        underscore_attrs_are_private = True\\n \\n     @classmethod\\n     def _get_crypt_context(cls) -> \"CryptContext\":\\n@@ -165,13 +137,71 @@ def generate_activation_token(self) -> str:\\n         return self.activation_token\\n \\n \\n+# ------------------ Request Model ------------------\\n+\\n+\\n+class UserRequest(UserBase, BaseRequest):\\n+    \"\"\"Request model for users.\"\"\"\\n+\\n+    # Analytics fields for user request models\\n+    ANALYTICS_FIELDS: ClassVar[List[str]] = [\\n+        \"name\",\\n+        \"full_name\",\\n+        \"active\",\\n+        \"email_opted_in\",\\n+    ]\\n+\\n+    name: str = Field(\\n+        title=\"The unique username for the account.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n+    full_name: str = Field(\\n+        default=\"\",\\n+        title=\"The full name for the account owner. Only relevant for user \"\\n+        \"accounts.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n+    is_admin: bool = Field(\\n+        title=\"Whether the account is an administrator.\",\\n+    )\\n+    active: bool = Field(default=False, title=\"Whether the account is active.\")\\n+\\n+    class Config:\\n+        \"\"\"Pydantic configuration class.\"\"\"\\n+\\n+        # Validate attributes when assigning them\\n+        validate_assignment = True\\n+\\n+        # Forbid extra attributes to prevent unexpected behavior\\n+        extra = \"forbid\"\\n+        underscore_attrs_are_private = True\\n+\\n+\\n # ------------------ Update Model ------------------\\n \\n \\n-@update_model\\n-class UserUpdate(UserRequest):\\n+class UserUpdate(UserBase, BaseZenModel):\\n     \"\"\"Update model for users.\"\"\"\\n \\n+    name: Optional[str] = Field(\\n+        title=\"The unique username for the account.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+        default=None,\\n+    )\\n+    full_name: Optional[str] = Field(\\n+        default=None,\\n+        title=\"The full name for the account owner. Only relevant for user \"\\n+        \"accounts.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n+    is_admin: Optional[bool] = Field(\\n+        default=None,\\n+        title=\"Whether the account is an administrator.\",\\n+    )\\n+    active: Optional[bool] = Field(\\n+        default=None, title=\"Whether the account is active.\"\\n+    )\\n+\\n     @root_validator\\n     def user_email_updates(cls, values: Dict[str, Any]) -> Dict[str, Any]:\\n         \"\"\"Validate that the UserUpdateModel conforms to the email-opt-in-flow.\\n@@ -231,6 +261,9 @@ class UserResponseBody(BaseDatedResponseBody):\\n     is_service_account: bool = Field(\\n         title=\"Indicates whether this is a service account or a user account.\"\\n     )\\n+    is_admin: bool = Field(\\n+        title=\"Whether the account is an administrator.\",\\n+    )\\n \\n \\n class UserResponseMetadata(BaseResponseMetadata):\\n@@ -340,6 +373,15 @@ def is_service_account(self) -> bool:\\n         \"\"\"\\n         return self.get_body().is_service_account\\n \\n+    @property\\n+    def is_admin(self) -> bool:\\n+        \"\"\"The `is_admin` property.\\n+\\n+        Returns:\\n+            Whether the user is an admin.\\n+        \"\"\"\\n+        return self.get_body().is_admin\\n+\\n     @property\\n     def email(self) -> Optional[str]:\\n         \"\"\"The `email` property.', '@@ -59,7 +59,10 @@ def verify_permissions_and_create_entity(\\n                 \"different user.\"\\n             )\\n \\n-    verify_permission(resource_type=resource_type, action=Action.CREATE)\\n+    verify_permission(\\n+        resource_type=resource_type,\\n+        action=Action.CREATE,\\n+    )\\n     return create_method(request_model)\\n \\n ', '@@ -691,18 +691,22 @@ def create_user(\\n         self,\\n         name: str,\\n         password: Optional[str] = None,\\n+        is_admin: bool = False,\\n     ) -> UserResponse:\\n         \"\"\"Create a new user.\\n \\n         Args:\\n             name: The name of the user.\\n             password: The password of the user. If not provided, the user will\\n                 be created with empty password.\\n+            is_admin: Whether the user should be an admin.\\n \\n         Returns:\\n             The model of the created user.\\n         \"\"\"\\n-        user = UserRequest(name=name, password=password or None)\\n+        user = UserRequest(\\n+            name=name, password=password or None, is_admin=is_admin\\n+        )\\n         user.active = (\\n             password != \"\" if self.zen_store.type != StoreType.REST else True\\n         )\\n@@ -801,6 +805,8 @@ def update_user(\\n         updated_email: Optional[str] = None,\\n         updated_email_opt_in: Optional[bool] = None,\\n         updated_hub_token: Optional[str] = None,\\n+        updated_password: Optional[str] = None,\\n+        updated_is_admin: Optional[bool] = None,\\n     ) -> UserResponse:\\n         \"\"\"Update a user.\\n \\n@@ -811,6 +817,8 @@ def update_user(\\n             updated_email: The new email of the user.\\n             updated_email_opt_in: The new email opt-in status of the user.\\n             updated_hub_token: Update the hub token\\n+            updated_password: The new password of the user.\\n+            updated_is_admin: Whether the user should be an admin.\\n \\n         Returns:\\n             The updated user.\\n@@ -830,6 +838,10 @@ def update_user(\\n             user_update.email_opted_in = updated_email_opt_in\\n         if updated_hub_token is not None:\\n             user_update.hub_token = updated_hub_token\\n+        if updated_password is not None:\\n+            user_update.password = updated_password\\n+        if updated_is_admin is not None:\\n+            user_update.is_admin = updated_is_admin\\n \\n         return self.zen_store.update_user(\\n             user_id=user.id, user_update=user_update', '@@ -120,15 +120,27 @@ def list_users(ctx: click.Context, **kwargs: Any) -> None:\\n     required=False,\\n     type=str,\\n )\\n+@click.option(\\n+    \"--is_admin\",\\n+    is_flag=True,\\n+    help=(\\n+        \"Whether the user should be an admin. If not specified, the user will \"\\n+        \"be a regular user.\"\\n+    ),\\n+    required=False,\\n+    default=False,\\n+)\\n def create_user(\\n     user_name: str,\\n     password: Optional[str] = None,\\n+    is_admin: bool = False,\\n ) -> None:\\n     \"\"\"Create a new user.\\n \\n     Args:\\n         user_name: The name of the user to create.\\n         password: The password of the user to create.\\n+        is_admin: Whether the user should be an admin.\\n     \"\"\"\\n     client = Client()\\n     if not password:\\n@@ -146,7 +158,9 @@ def create_user(\\n             )\\n \\n     try:\\n-        new_user = client.create_user(name=user_name, password=password)\\n+        new_user = client.create_user(\\n+            name=user_name, password=password, is_admin=is_admin\\n+        )\\n \\n         cli_utils.declare(f\"Created user \\'{new_user.name}\\'.\")\\n     except EntityExistsError as err:\\n@@ -162,8 +176,7 @@ def create_user(\\n \\n @user.command(\\n     \"update\",\\n-    help=\"Update user information through the cli. All attributes \"\\n-    \"except for password can be updated through the cli.\",\\n+    help=\"Update user information through the cli.\",\\n )\\n @click.argument(\"user_name_or_id\", type=str, required=True)\\n @click.option(\\n@@ -191,26 +204,80 @@ def create_user(\\n     required=False,\\n     help=\"New user email.\",\\n )\\n+@click.option(\\n+    \"--password\",\\n+    \"-p\",\\n+    \"updated_password\",\\n+    type=str,\\n+    required=False,\\n+    help=\"New user password.\",\\n+)\\n+@click.option(\\n+    \"--admin\",\\n+    \"-a\",\\n+    \"make_admin\",\\n+    is_flag=True,\\n+    required=False,\\n+    default=None,\\n+    help=\"Whether the user should be an admin.\",\\n+)\\n+@click.option(\\n+    \"--user\",\\n+    \"-u\",\\n+    \"make_user\",\\n+    is_flag=True,\\n+    required=False,\\n+    default=None,\\n+    help=\"Whether the user should be a regular user.\",\\n+)\\n def update_user(\\n     user_name_or_id: str,\\n     updated_name: Optional[str] = None,\\n     updated_full_name: Optional[str] = None,\\n     updated_email: Optional[str] = None,\\n+    updated_password: Optional[str] = None,\\n+    make_admin: Optional[bool] = None,\\n+    make_user: Optional[bool] = None,\\n ) -> None:\\n-    \"\"\"Create a new user.\\n+    \"\"\"Update an existing user.\\n \\n     Args:\\n         user_name_or_id: The name of the user to create.\\n         updated_name: The name of the user to create.\\n         updated_full_name: The name of the user to create.\\n         updated_email: The name of the user to create.\\n+        updated_password: The name of the user to create.\\n+        make_admin: Whether the user should be an admin.\\n+        make_user: Whether the user should be a regular user.\\n     \"\"\"\\n+    if make_admin is not None and make_user is not None:\\n+        cli_utils.error(\\n+            \"Cannot set both --admin and --user flags as these are mutually exclusive.\"\\n+        )\\n     try:\\n+        current_user = Client().get_user(\\n+            user_name_or_id, allow_name_prefix_match=False\\n+        )\\n+        if current_user.is_admin and make_user:\\n+            confirmation = cli_utils.confirmation(\\n+                f\"Currently user `{current_user.name}` is an admin. Are you sure you want to make them a regular user?\"\\n+            )\\n+            if not confirmation:\\n+                cli_utils.declare(\"User update canceled.\")\\n+                return\\n+\\n+        updated_is_admin = None\\n+        if make_admin is True:\\n+            updated_is_admin = True\\n+        elif make_user is True:\\n+            updated_is_admin = False\\n         Client().update_user(\\n             name_id_or_prefix=user_name_or_id,\\n             updated_name=updated_name,\\n             updated_full_name=updated_full_name,\\n             updated_email=updated_email,\\n+            updated_password=updated_password,\\n+            updated_is_admin=updated_is_admin,\\n         )\\n     except (KeyError, IllegalOperationError) as err:\\n         cli_utils.error(str(err))', '@@ -0,0 +1,56 @@\\n+\"\"\"admin users [1a9a9d2a836d].\\n+\\n+Revision ID: 1a9a9d2a836d\\n+Revises: 0.55.5\\n+Create Date: 2024-03-04 15:48:16.580871\\n+\\n+\"\"\"\\n+\\n+import sqlalchemy as sa\\n+import sqlmodel\\n+from alembic import op\\n+\\n+# revision identifiers, used by Alembic.\\n+revision = \"1a9a9d2a836d\"\\n+down_revision = \"0.55.5\"\\n+branch_labels = None\\n+depends_on = None\\n+\\n+\\n+def upgrade() -> None:\\n+    \"\"\"Upgrade database schema and/or data, creating a new revision.\"\"\"\\n+    # ### commands auto generated by Alembic - please adjust! ###\\n+    bind = op.get_bind()\\n+    session = sqlmodel.Session(bind=bind)\\n+\\n+    with op.batch_alter_table(\"user\", schema=None) as batch_op:\\n+        batch_op.add_column(\\n+            sa.Column(\\n+                \"is_admin\",\\n+                sa.Boolean(),\\n+                nullable=False,\\n+                server_default=sa.sql.expression.false(),\\n+            )\\n+        )\\n+\\n+    # during migration we treat all users as admin for backward compatibility\\n+    # this should be adjusted by server admins after upgrade\\n+    session.execute(\\n+        sa.text(\\n+            \"\"\"\\n+            UPDATE user\\n+            SET is_admin = true\\n+            WHERE NOT is_service_account AND external_user_id IS NULL\\n+            \"\"\"\\n+        )\\n+    )\\n+    # ### end Alembic commands ###\\n+\\n+\\n+def downgrade() -> None:\\n+    \"\"\"Downgrade database schema and/or data back to the previous revision.\"\"\"\\n+    # ### commands auto generated by Alembic - please adjust! ###\\n+    with op.batch_alter_table(\"user\", schema=None) as batch_op:\\n+        batch_op.drop_column(\"is_admin\")\\n+\\n+    # ### end Alembic commands ###'], 'file': ['src/zenml/zen_server/rbac/utils.py', 'src/zenml/zen_server/utils.py', 'src/zenml/zen_stores/sql_zen_store.py', 'src/zenml/zen_stores/schemas/user_schemas.py', 'src/zenml/models/v2/core/service_account.py', 'src/zenml/models/v2/misc/external_user.py', 'src/zenml/zen_server/routers/users_endpoints.py', 'src/zenml/zen_server/auth.py', 'src/zenml/models/v2/core/user.py', 'src/zenml/zen_server/rbac/endpoint_utils.py', 'src/zenml/client.py', 'src/zenml/cli/user_management.py', 'src/zenml/zen_stores/migrations/versions/1a9a9d2a836d_admin_users.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('402f4ff2-5a4a-4015-b63e-b4b4c90d97ff'), UUID('b61cca97-bfb1-4df1-ab50-2edde8655463'), UUID('0b914dfd-4889-4ea1-b16c-6b50ab16b96a'), UUID('f836e36a-1f43-4977-8938-4e13a2990a90'), UUID('d2f4f481-fedb-457c-a9c8-71ec5a632b85'), UUID('824440cc-13f6-46c0-863d-b192ec2362cb'), UUID('fed00c3b-56a2-40c1-9c05-219588a413cf'), UUID('223e6df7-4d06-44a0-a85e-94066084934c'), UUID('f4d6e392-248c-4049-90c6-e6d0e14c0e8a'), UUID('d1bc9877-6163-446a-b3ef-6586fe48956f'), UUID('906d1816-dff4-4c4f-b066-ba759b03db8e'), UUID('413a9482-9ac1-4ac0-84d7-a64ad27c7ef2'), UUID('5977a70a-6fd7-4b79-b180-974697dc534a')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 36:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 36:0: <line number missing in source>\n",
      " 26%|â–ˆâ–ˆâ–‹       | 476/1800 [04:52<08:39,  2.55it/s]ERROR:src.process_code_changes:Error processing commit f496f7fa776335ae7825cad2991c9b38923271fc\n",
      "ERROR:src.process_code_changes:{'repo': 'ComposioHQ/composio', 'vulnerability_id': '2024-53526', 'commit': 'f496f7fa776335ae7825cad2991c9b38923271fc', 'commit_source': 'github', 'cwe_id': ['CWE-77'], 'patch': ['@@ -135,6 +135,7 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]\\n ', '@@ -6,6 +6,7 @@\\n from langchain.agents import AgentExecutor, create_openai_functions_agent\\n from langchain_openai import ChatOpenAI\\n \\n+\\n @action(toolname=\"math\", requires=[\"smtplib\"])\\n def multiply(a: int, b: int, c: int) -> int:\\n     \"\"\"', '@@ -133,6 +133,7 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]\\n ', '@@ -180,5 +180,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -121,5 +121,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -162,5 +162,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -187,5 +187,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -328,6 +328,10 @@ def _limit_file_search_response(response: t.Dict) -> t.Dict:\\n         )\\n         self.max_retries = max_retries\\n \\n+        # To be populated by get_tools(), from within subclasses like\\n+        # composio_openai\\'s Toolset.\\n+        self._requested_actions: t.Optional[t.List[str]] = None\\n+\\n     def _validating_connection_ids(\\n         self,\\n         connected_account_ids: t.Dict[AppType, str],\\n@@ -797,6 +801,16 @@ def execute_action(\\n         :return: Output object from the function call\\n         \"\"\"\\n         action = Action(action)\\n+        if (\\n+            self._requested_actions is not None\\n+            and action.slug not in self._requested_actions\\n+        ):\\n+            raise ComposioSDKError(\\n+                f\"Action {action.slug} is being called, but was never requested by the toolset. \"\\n+                \"Make sure that the actions you are trying to execute are requested in your \"\\n+                \"`get_tools()` call.\"\\n+            )\\n+\\n         params = self._serialize_execute_params(param=params)\\n         if processors is not None:\\n             self._merge_processors(processors)\\n@@ -932,6 +946,7 @@ def validate_tools(\\n         # NOTE: This an experimental, can convert to decorator for more convinience\\n         if not apps and not actions and not tags:\\n             return\\n+\\n         self.workspace.check_for_missing_dependencies(\\n             apps=apps,\\n             actions=actions,\\n@@ -945,6 +960,7 @@ def get_action_schemas(\\n         tags: t.Optional[t.Sequence[TagType]] = None,\\n         *,\\n         check_connected_accounts: bool = True,\\n+        _populate_requested: bool = False,\\n     ) -> t.List[ActionModel]:\\n         runtime_actions = t.cast(\\n             t.List[t.Type[LocalAction]],\\n@@ -1010,6 +1026,13 @@ def get_action_schemas(\\n             if item.name == Action.ANTHROPIC_TEXT_EDITOR.slug:\\n                 item.name = \"str_replace_editor\"\\n \\n+        if _populate_requested:\\n+            action_names = [item.name for item in items]\\n+            if self._requested_actions is None:\\n+                self._requested_actions = []\\n+\\n+            self._requested_actions += action_names\\n+\\n         return items\\n \\n     def _process_schema(self, action_item: ActionModel) -> ActionModel:', '@@ -166,5 +166,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -123,5 +123,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -26,8 +26,8 @@ def register_tools(\\n         self,\\r\\n         caller: ConversableAgent,\\r\\n         executor: ConversableAgent,\\r\\n-        apps: t.Optional[t.Sequence[AppType]] = None,\\r\\n         actions: t.Optional[t.Sequence[ActionType]] = None,\\r\\n+        apps: t.Optional[t.Sequence[AppType]] = None,\\r\\n         tags: t.Optional[t.List[TagType]] = None,\\r\\n         entity_id: t.Optional[str] = None,\\r\\n     ) -> None:\\r', '@@ -174,6 +174,7 @@ def get_tools(\\n                     apps=apps,\\n                     tags=tags,\\n                     check_connected_accounts=check_connected_accounts,\\n+                    _populate_requested=True,\\n                 )\\n             ]\\n ', '@@ -216,5 +216,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]'], 'file': ['python/plugins/openai/composio_openai/toolset.py', 'python/examples/miscellaneous/runtime_tools/langchain_math.py', 'python/plugins/claude/composio_claude/toolset.py', 'python/plugins/langchain/composio_langchain/toolset.py', 'python/plugins/lyzr/composio_lyzr/toolset.py', 'python/plugins/llamaindex/composio_llamaindex/toolset.py', 'python/plugins/camel/composio_camel/toolset.py', 'python/composio/tools/toolset.py', 'python/plugins/griptape/composio_griptape/toolset.py', 'python/plugins/phidata/composio_phidata/toolset.py', 'python/plugins/autogen/composio_autogen/toolset.py', 'python/plugins/crew_ai/composio_crewai/toolset.py', 'python/plugins/praisonai/composio_praisonai/toolset.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('cffba566-6011-4313-9538-5ef4582d1e98'), UUID('c2bfe56d-0fd4-4ccb-8a71-5bc7d31a6035'), UUID('78ee8c2c-1bfb-4ed9-9869-3a3ae76c3ca5'), UUID('6c622af7-3cea-4e54-8417-084e6e2d3663'), UUID('80da4e35-ef07-474a-a66b-adf6c3a17a01'), UUID('fff3a27a-632a-4957-ac53-24b328cb064f'), UUID('3c5556db-840c-4d01-8ac2-f077c9b381fb'), UUID('334fd0db-672c-4ecb-ba02-de39c35dbabf'), UUID('64b4149e-77ac-4700-9946-46c6097b2f76'), UUID('56652932-88d4-4470-9aaf-018db257bfc8'), UUID('dc306c03-c88f-4efa-bd2a-387c82b03b2b'), UUID('d3cd9f8a-fe39-4007-9e90-6990d8e2b9ae'), UUID('d07622d9-f76e-4a52-a0a4-dbd9f3a259f3')]}\n",
      "ERROR:root:Error in {'repo': 'ComposioHQ/composio', 'vulnerability_id': '2024-53526', 'commit': 'f496f7fa776335ae7825cad2991c9b38923271fc', 'commit_source': 'github', 'cwe_id': ['CWE-77'], 'patch': ['@@ -135,6 +135,7 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]\\n ', '@@ -6,6 +6,7 @@\\n from langchain.agents import AgentExecutor, create_openai_functions_agent\\n from langchain_openai import ChatOpenAI\\n \\n+\\n @action(toolname=\"math\", requires=[\"smtplib\"])\\n def multiply(a: int, b: int, c: int) -> int:\\n     \"\"\"', '@@ -133,6 +133,7 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]\\n ', '@@ -180,5 +180,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -121,5 +121,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -162,5 +162,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -187,5 +187,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -328,6 +328,10 @@ def _limit_file_search_response(response: t.Dict) -> t.Dict:\\n         )\\n         self.max_retries = max_retries\\n \\n+        # To be populated by get_tools(), from within subclasses like\\n+        # composio_openai\\'s Toolset.\\n+        self._requested_actions: t.Optional[t.List[str]] = None\\n+\\n     def _validating_connection_ids(\\n         self,\\n         connected_account_ids: t.Dict[AppType, str],\\n@@ -797,6 +801,16 @@ def execute_action(\\n         :return: Output object from the function call\\n         \"\"\"\\n         action = Action(action)\\n+        if (\\n+            self._requested_actions is not None\\n+            and action.slug not in self._requested_actions\\n+        ):\\n+            raise ComposioSDKError(\\n+                f\"Action {action.slug} is being called, but was never requested by the toolset. \"\\n+                \"Make sure that the actions you are trying to execute are requested in your \"\\n+                \"`get_tools()` call.\"\\n+            )\\n+\\n         params = self._serialize_execute_params(param=params)\\n         if processors is not None:\\n             self._merge_processors(processors)\\n@@ -932,6 +946,7 @@ def validate_tools(\\n         # NOTE: This an experimental, can convert to decorator for more convinience\\n         if not apps and not actions and not tags:\\n             return\\n+\\n         self.workspace.check_for_missing_dependencies(\\n             apps=apps,\\n             actions=actions,\\n@@ -945,6 +960,7 @@ def get_action_schemas(\\n         tags: t.Optional[t.Sequence[TagType]] = None,\\n         *,\\n         check_connected_accounts: bool = True,\\n+        _populate_requested: bool = False,\\n     ) -> t.List[ActionModel]:\\n         runtime_actions = t.cast(\\n             t.List[t.Type[LocalAction]],\\n@@ -1010,6 +1026,13 @@ def get_action_schemas(\\n             if item.name == Action.ANTHROPIC_TEXT_EDITOR.slug:\\n                 item.name = \"str_replace_editor\"\\n \\n+        if _populate_requested:\\n+            action_names = [item.name for item in items]\\n+            if self._requested_actions is None:\\n+                self._requested_actions = []\\n+\\n+            self._requested_actions += action_names\\n+\\n         return items\\n \\n     def _process_schema(self, action_item: ActionModel) -> ActionModel:', '@@ -166,5 +166,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -123,5 +123,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]', '@@ -26,8 +26,8 @@ def register_tools(\\n         self,\\r\\n         caller: ConversableAgent,\\r\\n         executor: ConversableAgent,\\r\\n-        apps: t.Optional[t.Sequence[AppType]] = None,\\r\\n         actions: t.Optional[t.Sequence[ActionType]] = None,\\r\\n+        apps: t.Optional[t.Sequence[AppType]] = None,\\r\\n         tags: t.Optional[t.List[TagType]] = None,\\r\\n         entity_id: t.Optional[str] = None,\\r\\n     ) -> None:\\r', '@@ -174,6 +174,7 @@ def get_tools(\\n                     apps=apps,\\n                     tags=tags,\\n                     check_connected_accounts=check_connected_accounts,\\n+                    _populate_requested=True,\\n                 )\\n             ]\\n ', '@@ -216,5 +216,6 @@ def get_tools(\\n                 apps=apps,\\n                 tags=tags,\\n                 check_connected_accounts=check_connected_accounts,\\n+                _populate_requested=True,\\n             )\\n         ]'], 'file': ['python/plugins/openai/composio_openai/toolset.py', 'python/examples/miscellaneous/runtime_tools/langchain_math.py', 'python/plugins/claude/composio_claude/toolset.py', 'python/plugins/langchain/composio_langchain/toolset.py', 'python/plugins/lyzr/composio_lyzr/toolset.py', 'python/plugins/llamaindex/composio_llamaindex/toolset.py', 'python/plugins/camel/composio_camel/toolset.py', 'python/composio/tools/toolset.py', 'python/plugins/griptape/composio_griptape/toolset.py', 'python/plugins/phidata/composio_phidata/toolset.py', 'python/plugins/autogen/composio_autogen/toolset.py', 'python/plugins/crew_ai/composio_crewai/toolset.py', 'python/plugins/praisonai/composio_praisonai/toolset.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('cffba566-6011-4313-9538-5ef4582d1e98'), UUID('c2bfe56d-0fd4-4ccb-8a71-5bc7d31a6035'), UUID('78ee8c2c-1bfb-4ed9-9869-3a3ae76c3ca5'), UUID('6c622af7-3cea-4e54-8417-084e6e2d3663'), UUID('80da4e35-ef07-474a-a66b-adf6c3a17a01'), UUID('fff3a27a-632a-4957-ac53-24b328cb064f'), UUID('3c5556db-840c-4d01-8ac2-f077c9b381fb'), UUID('334fd0db-672c-4ecb-ba02-de39c35dbabf'), UUID('64b4149e-77ac-4700-9946-46c6097b2f76'), UUID('56652932-88d4-4470-9aaf-018db257bfc8'), UUID('dc306c03-c88f-4efa-bd2a-387c82b03b2b'), UUID('d3cd9f8a-fe39-4007-9e90-6990d8e2b9ae'), UUID('d07622d9-f76e-4a52-a0a4-dbd9f3a259f3')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 143, in get_changes\n",
      "    local_repo = Repo.clone_from(repo.clone_url, f\"{REPOS_PATH}/{repo_name}\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1541, in clone_from\n",
      "    return cls._clone(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1412, in _clone\n",
      "    finalize_process(proc, stderr=stderr)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/util.py\", line 504, in finalize_process\n",
      "    proc.wait(**kwargs)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/cmd.py\", line 834, in wait\n",
      "    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\n",
      "git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git clone -v -- https://github.com/ComposioHQ/composio.git /Users/somen/repos/ComposioHQ/composio\n",
      "  stderr: 'Cloning into '/Users/somen/repos/ComposioHQ/composio'...\n",
      "POST git-upload-pack (175 bytes)\n",
      "POST git-upload-pack (gzip 12467 to 6110 bytes)\n",
      "error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly: CANCEL (err 8)\n",
      "error: 5198 bytes of body are still expected\n",
      "fetch-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: early EOF\n",
      "fatal: fetch-pack: invalid index-pack output\n",
      "'\n",
      " 27%|â–ˆâ–ˆâ–‹       | 485/1800 [05:53<45:05,  2.06s/it]ERROR:src.process_code_changes:Error processing commit 19318d4c00bb02c4ec3c4f8f15ac2e1dbe8d846c\n",
      "ERROR:src.process_code_changes:{'repo': 'goauthentik/authentik', 'vulnerability_id': '2024-42490', 'commit': '19318d4c00bb02c4ec3c4f8f15ac2e1dbe8d846c', 'commit_source': 'github', 'cwe_id': ['CWE-285', 'CWE-285'], 'patch': ['@@ -23,6 +23,7 @@\\n     KubernetesServiceConnection,\\n     OutpostServiceConnection,\\n )\\n+from authentik.rbac.filters import ObjectFilter\\n \\n \\n class ServiceConnectionSerializer(ModelSerializer, MetaNameSerializer):\\n@@ -88,7 +89,7 @@ def types(self, request: Request) -> Response:\\n         return Response(TypeCreateSerializer(data, many=True).data)\\n \\n     @extend_schema(responses={200: ServiceConnectionStateSerializer(many=False)})\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def state(self, request: Request, pk: str) -> Response:\\n         \"\"\"Get the service connection\\'s state\"\"\"\\n         connection = self.get_object()', '@@ -33,6 +33,7 @@\\n )\\n from authentik.lib.views import bad_request_message\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -277,7 +278,7 @@ def set_background_url(self, request: Request, slug: str):\\n             400: OpenApiResponse(description=\"Flow not applicable\"),\\n         },\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def execute(self, request: Request, slug: str):\\n         \"\"\"Execute flow for current user\"\"\"\\n         # Because we pre-plan the flow here, and not in the planner, we need to manually clear', '@@ -436,6 +436,7 @@ const docsSidebar = {\\n             },\\n             items: [\\n                 \"security/policy\",\\n+                \"security/CVE-2024-42490\",\\n                 \"security/CVE-2024-38371\",\\n                 \"security/CVE-2024-37905\",\\n                 \"security/CVE-2024-23647\",', '@@ -214,6 +214,46 @@ def test_private_key_download(self):\\n         self.assertEqual(200, response.status_code)\\n         self.assertIn(\"Content-Disposition\", response)\\n \\n+    def test_certificate_download_denied(self):\\n+        \"\"\"Test certificate export (download)\"\"\"\\n+        self.client.logout()\\n+        keypair = create_test_cert()\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-certificate\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            )\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-certificate\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            ),\\n+            data={\"download\": True},\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+\\n+    def test_private_key_download_denied(self):\\n+        \"\"\"Test private_key export (download)\"\"\"\\n+        self.client.logout()\\n+        keypair = create_test_cert()\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-private-key\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            )\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-private-key\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            ),\\n+            data={\"download\": True},\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+\\n     def test_used_by(self):\\n         \"\"\"Test used_by endpoint\"\"\"\\n         self.client.force_login(create_test_admin_user())\\n@@ -246,6 +286,26 @@ def test_used_by(self):\\n             ],\\n         )\\n \\n+    def test_used_by_denied(self):\\n+        \"\"\"Test used_by endpoint\"\"\"\\n+        self.client.logout()\\n+        keypair = create_test_cert()\\n+        OAuth2Provider.objects.create(\\n+            name=generate_id(),\\n+            client_id=\"test\",\\n+            client_secret=generate_key(),\\n+            authorization_flow=create_test_flow(),\\n+            redirect_uris=\"http://localhost\",\\n+            signing_key=keypair,\\n+        )\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-used-by\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            )\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+\\n     def test_discovery(self):\\n         \"\"\"Test certificate discovery\"\"\"\\n         name = generate_id()', '@@ -36,6 +36,7 @@\\n from authentik.crypto.models import CertificateKeyPair\\n from authentik.events.models import Event, EventAction\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -266,7 +267,7 @@ def generate(self, request: Request) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_certificate(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs certificate and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()\\n@@ -296,7 +297,7 @@ def view_certificate(self, request: Request, pk: str) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_private_key(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs private key and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()'], 'file': ['authentik/outposts/api/service_connections.py', 'authentik/flows/api/flows.py', 'website/sidebars.js', 'authentik/crypto/tests.py', 'authentik/crypto/api.py'], 'language': ['Python', 'Python', 'JavaScript/TypeScript', 'Python', 'Python'], 'temp_id': [UUID('14fe809f-bbc0-451a-9725-ff50c278de98'), UUID('10dab460-2dc9-4a35-8e14-d00caf13b023'), UUID('fe55b517-54a1-42c8-ae34-b792936ad8ae'), UUID('0a83e060-7bbc-4ad8-8784-b74ae69395ac'), UUID('28054fd7-31c9-4ad9-b743-e658c745a71a')]}\n",
      "ERROR:root:Error in {'repo': 'goauthentik/authentik', 'vulnerability_id': '2024-42490', 'commit': '19318d4c00bb02c4ec3c4f8f15ac2e1dbe8d846c', 'commit_source': 'github', 'cwe_id': ['CWE-285', 'CWE-285'], 'patch': ['@@ -23,6 +23,7 @@\\n     KubernetesServiceConnection,\\n     OutpostServiceConnection,\\n )\\n+from authentik.rbac.filters import ObjectFilter\\n \\n \\n class ServiceConnectionSerializer(ModelSerializer, MetaNameSerializer):\\n@@ -88,7 +89,7 @@ def types(self, request: Request) -> Response:\\n         return Response(TypeCreateSerializer(data, many=True).data)\\n \\n     @extend_schema(responses={200: ServiceConnectionStateSerializer(many=False)})\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def state(self, request: Request, pk: str) -> Response:\\n         \"\"\"Get the service connection\\'s state\"\"\"\\n         connection = self.get_object()', '@@ -33,6 +33,7 @@\\n )\\n from authentik.lib.views import bad_request_message\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -277,7 +278,7 @@ def set_background_url(self, request: Request, slug: str):\\n             400: OpenApiResponse(description=\"Flow not applicable\"),\\n         },\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def execute(self, request: Request, slug: str):\\n         \"\"\"Execute flow for current user\"\"\"\\n         # Because we pre-plan the flow here, and not in the planner, we need to manually clear', '@@ -436,6 +436,7 @@ const docsSidebar = {\\n             },\\n             items: [\\n                 \"security/policy\",\\n+                \"security/CVE-2024-42490\",\\n                 \"security/CVE-2024-38371\",\\n                 \"security/CVE-2024-37905\",\\n                 \"security/CVE-2024-23647\",', '@@ -214,6 +214,46 @@ def test_private_key_download(self):\\n         self.assertEqual(200, response.status_code)\\n         self.assertIn(\"Content-Disposition\", response)\\n \\n+    def test_certificate_download_denied(self):\\n+        \"\"\"Test certificate export (download)\"\"\"\\n+        self.client.logout()\\n+        keypair = create_test_cert()\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-certificate\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            )\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-certificate\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            ),\\n+            data={\"download\": True},\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+\\n+    def test_private_key_download_denied(self):\\n+        \"\"\"Test private_key export (download)\"\"\"\\n+        self.client.logout()\\n+        keypair = create_test_cert()\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-private-key\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            )\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-view-private-key\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            ),\\n+            data={\"download\": True},\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+\\n     def test_used_by(self):\\n         \"\"\"Test used_by endpoint\"\"\"\\n         self.client.force_login(create_test_admin_user())\\n@@ -246,6 +286,26 @@ def test_used_by(self):\\n             ],\\n         )\\n \\n+    def test_used_by_denied(self):\\n+        \"\"\"Test used_by endpoint\"\"\"\\n+        self.client.logout()\\n+        keypair = create_test_cert()\\n+        OAuth2Provider.objects.create(\\n+            name=generate_id(),\\n+            client_id=\"test\",\\n+            client_secret=generate_key(),\\n+            authorization_flow=create_test_flow(),\\n+            redirect_uris=\"http://localhost\",\\n+            signing_key=keypair,\\n+        )\\n+        response = self.client.get(\\n+            reverse(\\n+                \"authentik_api:certificatekeypair-used-by\",\\n+                kwargs={\"pk\": keypair.pk},\\n+            )\\n+        )\\n+        self.assertEqual(403, response.status_code)\\n+\\n     def test_discovery(self):\\n         \"\"\"Test certificate discovery\"\"\"\\n         name = generate_id()', '@@ -36,6 +36,7 @@\\n from authentik.crypto.models import CertificateKeyPair\\n from authentik.events.models import Event, EventAction\\n from authentik.rbac.decorators import permission_required\\n+from authentik.rbac.filters import ObjectFilter\\n \\n LOGGER = get_logger()\\n \\n@@ -266,7 +267,7 @@ def generate(self, request: Request) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_certificate(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs certificate and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()\\n@@ -296,7 +297,7 @@ def view_certificate(self, request: Request, pk: str) -> Response:\\n         ],\\n         responses={200: CertificateDataSerializer(many=False)},\\n     )\\n-    @action(detail=True, pagination_class=None, filter_backends=[])\\n+    @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\\n     def view_private_key(self, request: Request, pk: str) -> Response:\\n         \"\"\"Return certificate-key pairs private key and log access\"\"\"\\n         certificate: CertificateKeyPair = self.get_object()'], 'file': ['authentik/outposts/api/service_connections.py', 'authentik/flows/api/flows.py', 'website/sidebars.js', 'authentik/crypto/tests.py', 'authentik/crypto/api.py'], 'language': ['Python', 'Python', 'JavaScript/TypeScript', 'Python', 'Python'], 'temp_id': [UUID('14fe809f-bbc0-451a-9725-ff50c278de98'), UUID('10dab460-2dc9-4a35-8e14-d00caf13b023'), UUID('fe55b517-54a1-42c8-ae34-b792936ad8ae'), UUID('0a83e060-7bbc-4ad8-8784-b74ae69395ac'), UUID('28054fd7-31c9-4ad9-b743-e658c745a71a')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @action(detail=True, pagination_class=None, filter_backends=[ObjectFilter])\n",
      " 27%|â–ˆâ–ˆâ–‹       | 488/1800 [05:54<40:46,  1.87s/it]ERROR:src.process_code_changes:Error processing commit 64125a24aad228761f38312d44bde4bec7354276\n",
      "ERROR:src.process_code_changes:{'repo': 'kiorky/SOAPpy', 'vulnerability_id': '2014-3243', 'commit': '64125a24aad228761f38312d44bde4bec7354276', 'commit_source': 'github', 'cwe_id': ['CWE-119'], 'patch': ['@@ -1,2 +1,5 @@\\n-__version__=\"0.12.6\"\\n-\\n+try:\\n+    import pkg_resources\\n+    __version__ = pkg_resources.get_distribution(\"SOAPpy\").version\\n+except:\\n+    __version__=\"xxx\"', '@@ -16,13 +16,22 @@\\n try: from M2Crypto import SSL\\n except: pass\\n \\n+from defusedxml import expatreader\\n+from defusedxml.common import DefusedXmlException\\n+\\n+\\n ident = \\'$Id: Parser.py 1497 2010-03-08 06:06:52Z pooryorick $\\'\\n from version import __version__\\n \\n \\n ################################################################################\\n # SOAP Parser\\n ################################################################################\\n+\\n+def make_parser(parser_list=[]):\\n+        return expatreader.create_parser()\\n+\\n+\\n class RefHolder:\\n     def __init__(self, name, frame):\\n         self.name = name\\n@@ -1041,27 +1050,38 @@ def resolveEntity(self, publicId, systemId):\\n         return StringIO(\"<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\")\\n \\n \\n-def _parseSOAP(xml_str, rules = None, ignore_ext=None):\\n+def _parseSOAP(xml_str, rules = None, ignore_ext=None,\\n+               forbid_entities=False, forbid_external=True, forbid_dtd=False):\\n+    inpsrc = xml.sax.xmlreader.InputSource()\\n+    inpsrc.setByteStream(StringIO(xml_str))\\n     if ignore_ext is None:\\n         ignore_ext = False\\n \\n-    parser = xml.sax.make_parser()\\n+    parser = make_parser()\\n     t = SOAPParser(rules=rules)\\n     parser.setContentHandler(t)\\n-    e = xml.sax.handler.ErrorHandler()\\n-    parser.setErrorHandler(e)\\n+    errorHandler = xml.sax.handler.ErrorHandler()\\n+    parser.setErrorHandler(errorHandler)\\n \\n-    inpsrc = xml.sax.xmlreader.InputSource()\\n-    inpsrc.setByteStream(StringIO(xml_str))\\n-\\n-    # disable by default  entity loading on posted content\\n     if ignore_ext:\\n-        parser.setEntityResolver(EmptyEntityResolver())\\n+        # disable by default  entity loading on posted content\\n+        forbid_dtd = True\\n+        forbid_entities = True\\n+        forbid_external = True\\n+    parser.forbid_dtd = forbid_dtd\\n+    parser.forbid_entities = forbid_entities\\n+    parser.forbid_external = forbid_external\\n+    parser.setEntityResolver(EmptyEntityResolver())\\n+\\n     # turn on namespace mangeling\\n     parser.setFeature(xml.sax.handler.feature_namespaces, 1)\\n \\n     try:\\n         parser.parse(inpsrc)\\n+    except DefusedXmlException, e:\\n+        parser._parser = None\\n+        print traceback.format_exc()\\n+        raise e\\n     except xml.sax.SAXParseException, e:\\n         parser._parser = None\\n         print traceback.format_exc()'], 'file': ['src/SOAPpy/version.py', 'src/SOAPpy/Parser.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('98eb67e8-8e68-4014-9cd5-98e94de4d0de'), UUID('9572d2b0-4d51-4853-9f8b-50e803f28c11')]}\n",
      "ERROR:root:Error in {'repo': 'kiorky/SOAPpy', 'vulnerability_id': '2014-3243', 'commit': '64125a24aad228761f38312d44bde4bec7354276', 'commit_source': 'github', 'cwe_id': ['CWE-119'], 'patch': ['@@ -1,2 +1,5 @@\\n-__version__=\"0.12.6\"\\n-\\n+try:\\n+    import pkg_resources\\n+    __version__ = pkg_resources.get_distribution(\"SOAPpy\").version\\n+except:\\n+    __version__=\"xxx\"', '@@ -16,13 +16,22 @@\\n try: from M2Crypto import SSL\\n except: pass\\n \\n+from defusedxml import expatreader\\n+from defusedxml.common import DefusedXmlException\\n+\\n+\\n ident = \\'$Id: Parser.py 1497 2010-03-08 06:06:52Z pooryorick $\\'\\n from version import __version__\\n \\n \\n ################################################################################\\n # SOAP Parser\\n ################################################################################\\n+\\n+def make_parser(parser_list=[]):\\n+        return expatreader.create_parser()\\n+\\n+\\n class RefHolder:\\n     def __init__(self, name, frame):\\n         self.name = name\\n@@ -1041,27 +1050,38 @@ def resolveEntity(self, publicId, systemId):\\n         return StringIO(\"<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\")\\n \\n \\n-def _parseSOAP(xml_str, rules = None, ignore_ext=None):\\n+def _parseSOAP(xml_str, rules = None, ignore_ext=None,\\n+               forbid_entities=False, forbid_external=True, forbid_dtd=False):\\n+    inpsrc = xml.sax.xmlreader.InputSource()\\n+    inpsrc.setByteStream(StringIO(xml_str))\\n     if ignore_ext is None:\\n         ignore_ext = False\\n \\n-    parser = xml.sax.make_parser()\\n+    parser = make_parser()\\n     t = SOAPParser(rules=rules)\\n     parser.setContentHandler(t)\\n-    e = xml.sax.handler.ErrorHandler()\\n-    parser.setErrorHandler(e)\\n+    errorHandler = xml.sax.handler.ErrorHandler()\\n+    parser.setErrorHandler(errorHandler)\\n \\n-    inpsrc = xml.sax.xmlreader.InputSource()\\n-    inpsrc.setByteStream(StringIO(xml_str))\\n-\\n-    # disable by default  entity loading on posted content\\n     if ignore_ext:\\n-        parser.setEntityResolver(EmptyEntityResolver())\\n+        # disable by default  entity loading on posted content\\n+        forbid_dtd = True\\n+        forbid_entities = True\\n+        forbid_external = True\\n+    parser.forbid_dtd = forbid_dtd\\n+    parser.forbid_entities = forbid_entities\\n+    parser.forbid_external = forbid_external\\n+    parser.setEntityResolver(EmptyEntityResolver())\\n+\\n     # turn on namespace mangeling\\n     parser.setFeature(xml.sax.handler.feature_namespaces, 1)\\n \\n     try:\\n         parser.parse(inpsrc)\\n+    except DefusedXmlException, e:\\n+        parser._parser = None\\n+        print traceback.format_exc()\\n+        raise e\\n     except xml.sax.SAXParseException, e:\\n         parser._parser = None\\n         print traceback.format_exc()'], 'file': ['src/SOAPpy/version.py', 'src/SOAPpy/Parser.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('98eb67e8-8e68-4014-9cd5-98e94de4d0de'), UUID('9572d2b0-4d51-4853-9f8b-50e803f28c11')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     __version__ = pkg_resources.get_distribution(\"SOAPpy\").version\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     __version__ = pkg_resources.get_distribution(\"SOAPpy\").version\n",
      " 28%|â–ˆâ–ˆâ–Š       | 498/1800 [05:55<27:01,  1.25s/it]ERROR:src.process_code_changes:Error processing commit f68bbaba47f4474e1da553aa51564a73e1d92a84\n",
      "ERROR:src.process_code_changes:{'repo': 'OnShift/turbogears', 'vulnerability_id': '2019-25101', 'commit': 'f68bbaba47f4474e1da553aa51564a73e1d92a84', 'commit_source': 'github', 'cwe_id': ['CWE-113', 'CWE-113'], 'patch': ['@@ -28,7 +28,7 @@\\n     http://svn.turbogears.org/trunk#egg=turbogears-dev\\n \"\"\"\\n \\n-version = \"1.0.11.8\"\\n+version = \"1.0.11.9\"\\n description = \"Front-to-back, open-source, rapid web development framework\"\\n long_description = __doc__\\n author = \"Kevin Dangoor\"', '@@ -325,7 +325,7 @@ def expose(template=None, validators=None, allow_json=None, html=None,\\n             applied to that arg\\n     @keyparam inputform deprecated. A form object that generates the\\n             input to this method\\n-    @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be \\n+    @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be\\n             used for performance or in case profiling generates errors\\n     \"\"\"\\n     if html:\\n@@ -602,60 +602,67 @@ def check_app_root():\\n     request.app_root = app_root\\n \\n \\n-def get_server_name(): \\n-    \"\"\"Return name of the server this application runs on. \\n- \\n-    Respects \\'Host\\' and \\'X-Forwarded-Host\\' header. \\n- \\n-    See the docstring of the \\'absolute_url\\' function for more information. \\n- \\n-    \"\"\" \\n-    get = config.get \\n-    h = request.headers \\n-    host = get(\\'tg.url_domain\\') or h.get(\\'X-Forwarded-Host\\', h.get(\\'Host\\')) \\n-    if not host: \\n-        host = \\'%s:%s\\' % (get(\\'server.socket_host\\', \\'localhost\\'), \\n-            get(\\'server.socket_port\\', 8080)) \\n-    return host \\n-\\n-\\n-def absolute_url(tgpath=\\'/\\', params=None, **kw): \\n-    \"\"\"Return absolute URL (including schema and host to this server). \\n- \\n-    Tries to account for \\'Host\\' header and reverse proxying \\n-    (\\'X-Forwarded-Host\\'). \\n- \\n-    The host name is determined this way: \\n- \\n-    * If the config setting \\'tg.url_domain\\' is set and non-null, use this value. \\n-    * Else, if the \\'base_url_filter.use_x_forwarded_host\\' config setting is \\n-      True, use the value from the \\'Host\\' or \\'X-Forwarded-Host\\' request header. \\n-    * Else, if config setting \\'base_url_filter.on\\' is True and \\n-      \\'base_url_filter.base_url\\' is non-null, use its value for the host AND \\n-      scheme part of the URL. \\n-    * As a last fallback, use the value of \\'server.socket_host\\' and \\n-      \\'server.socket_port\\' config settings (defaults to \\'localhost:8080\\'). \\n- \\n-    The URL scheme (\\'http\\' or \\'http\\') used is determined in the following way: \\n- \\n-    * If \\'base_url_filter.base_url\\' is used, use the scheme from this URL. \\n-    * If there is a \\'X-Use-SSL\\' request header, use \\'https\\'. \\n-    * Else, if the config setting \\'tg.url_scheme\\' is set, use its value. \\n-    * Else, use the value of \\'cherrypy.request.scheme\\'. \\n- \\n-    \"\"\" \\n-    get = config.get \\n-    use_xfh = get(\\'base_url_filter.use_x_forwarded_host\\', False) \\n-    if request.headers.get(\\'X-Use-SSL\\'): \\n-        scheme = \\'https\\' \\n-    else: \\n-        scheme = get(\\'tg.url_scheme\\') \\n-    if not scheme: \\n-        scheme = request.scheme \\n-    base_url = \\'%s://%s\\' % (scheme, get_server_name()) \\n-    if get(\\'base_url_filter.on\\', False) and not use_xfh: \\n-        base_url = get(\\'base_url_filter.base_url\\').rstrip(\\'/\\') \\n-    return \\'%s%s\\' % (base_url, url(tgpath, params, **kw)) \\n+def get_server_name():\\n+    \"\"\"Return name of the server this application runs on.\\n+\\n+    Respects \\'Host\\' and \\'X-Forwarded-Host\\' header.\\n+\\n+    See the docstring of the \\'absolute_url\\' function for more information.\\n+\\n+    \"\"\"\\n+    get = config.get\\n+    h = request.headers\\n+    host = get(\\'tg.url_domain\\') or h.get(\\'X-Forwarded-Host\\', h.get(\\'Host\\'))\\n+    if not host:\\n+        host = \\'%s:%s\\' % (get(\\'server.socket_host\\', \\'localhost\\'),\\n+            get(\\'server.socket_port\\', 8080))\\n+    return host\\n+\\n+\\n+def absolute_url(tgpath=\\'/\\', params=None, **kw):\\n+    \"\"\"Return absolute URL (including schema and host to this server).\\n+\\n+    Tries to account for \\'Host\\' header and reverse proxying\\n+    (\\'X-Forwarded-Host\\').\\n+\\n+    The host name is determined this way:\\n+\\n+    * If the config setting \\'tg.url_domain\\' is set and non-null, use this value.\\n+    * Else, if the \\'base_url_filter.use_x_forwarded_host\\' config setting is\\n+      True, use the value from the \\'Host\\' or \\'X-Forwarded-Host\\' request header.\\n+    * Else, if config setting \\'base_url_filter.on\\' is True and\\n+      \\'base_url_filter.base_url\\' is non-null, use its value for the host AND\\n+      scheme part of the URL.\\n+    * As a last fallback, use the value of \\'server.socket_host\\' and\\n+      \\'server.socket_port\\' config settings (defaults to \\'localhost:8080\\').\\n+\\n+    The URL scheme (\\'http\\' or \\'http\\') used is determined in the following way:\\n+\\n+    * If \\'base_url_filter.base_url\\' is used, use the scheme from this URL.\\n+    * If there is a \\'X-Use-SSL\\' request header, use \\'https\\'.\\n+    * Else, if the config setting \\'tg.url_scheme\\' is set, use its value.\\n+    * Else, use the value of \\'cherrypy.request.scheme\\'.\\n+\\n+    \"\"\"\\n+    get = config.get\\n+    use_xfh = get(\\'base_url_filter.use_x_forwarded_host\\', False)\\n+    if request.headers.get(\\'X-Use-SSL\\'):\\n+        scheme = \\'https\\'\\n+    else:\\n+        scheme = get(\\'tg.url_scheme\\')\\n+    if not scheme:\\n+        scheme = request.scheme\\n+    base_url = \\'%s://%s\\' % (scheme, get_server_name())\\n+    if get(\\'base_url_filter.on\\', False) and not use_xfh:\\n+        base_url = get(\\'base_url_filter.base_url\\').rstrip(\\'/\\')\\n+    return \\'%s%s\\' % (base_url, url(tgpath, params, **kw))\\n+\\n+\\n+class InvalidRedirectException(Exception):\\n+    \"\"\"\\n+    An invalid redirect url was provided.  Redirects cannot\\n+    include a carriage return (\\\\r) or new line (\\\\n) character.\\n+    \"\"\"\\n \\n \\n def redirect(redirect_path, redirect_params=None, **kw):\\n@@ -673,6 +680,8 @@ def redirect(redirect_path, redirect_params=None, **kw):\\n         if path.startswith(request.app_root):\\n             path = path[len(request.app_root):]\\n         redirect_path = urlparse.urljoin(path, redirect_path)\\n+    if set(redirect_path).intersection({\\'\\\\r\\', \\'\\\\n\\'}):\\n+        raise InvalidRedirectException(\\'Invalid redirect: {}\\'.format(redirect_path))\\n     raise cherrypy.HTTPRedirect(url(tgpath=redirect_path,\\n         tgparams=redirect_params, **kw))\\n '], 'file': ['turbogears/release.py', 'turbogears/controllers.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('60607e73-9bbb-47a2-b4e7-0cf7e8032828'), UUID('9226c222-8cc5-4ba5-a12c-272715bf785d')]}\n",
      "ERROR:root:Error in {'repo': 'OnShift/turbogears', 'vulnerability_id': '2019-25101', 'commit': 'f68bbaba47f4474e1da553aa51564a73e1d92a84', 'commit_source': 'github', 'cwe_id': ['CWE-113', 'CWE-113'], 'patch': ['@@ -28,7 +28,7 @@\\n     http://svn.turbogears.org/trunk#egg=turbogears-dev\\n \"\"\"\\n \\n-version = \"1.0.11.8\"\\n+version = \"1.0.11.9\"\\n description = \"Front-to-back, open-source, rapid web development framework\"\\n long_description = __doc__\\n author = \"Kevin Dangoor\"', '@@ -325,7 +325,7 @@ def expose(template=None, validators=None, allow_json=None, html=None,\\n             applied to that arg\\n     @keyparam inputform deprecated. A form object that generates the\\n             input to this method\\n-    @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be \\n+    @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be\\n             used for performance or in case profiling generates errors\\n     \"\"\"\\n     if html:\\n@@ -602,60 +602,67 @@ def check_app_root():\\n     request.app_root = app_root\\n \\n \\n-def get_server_name(): \\n-    \"\"\"Return name of the server this application runs on. \\n- \\n-    Respects \\'Host\\' and \\'X-Forwarded-Host\\' header. \\n- \\n-    See the docstring of the \\'absolute_url\\' function for more information. \\n- \\n-    \"\"\" \\n-    get = config.get \\n-    h = request.headers \\n-    host = get(\\'tg.url_domain\\') or h.get(\\'X-Forwarded-Host\\', h.get(\\'Host\\')) \\n-    if not host: \\n-        host = \\'%s:%s\\' % (get(\\'server.socket_host\\', \\'localhost\\'), \\n-            get(\\'server.socket_port\\', 8080)) \\n-    return host \\n-\\n-\\n-def absolute_url(tgpath=\\'/\\', params=None, **kw): \\n-    \"\"\"Return absolute URL (including schema and host to this server). \\n- \\n-    Tries to account for \\'Host\\' header and reverse proxying \\n-    (\\'X-Forwarded-Host\\'). \\n- \\n-    The host name is determined this way: \\n- \\n-    * If the config setting \\'tg.url_domain\\' is set and non-null, use this value. \\n-    * Else, if the \\'base_url_filter.use_x_forwarded_host\\' config setting is \\n-      True, use the value from the \\'Host\\' or \\'X-Forwarded-Host\\' request header. \\n-    * Else, if config setting \\'base_url_filter.on\\' is True and \\n-      \\'base_url_filter.base_url\\' is non-null, use its value for the host AND \\n-      scheme part of the URL. \\n-    * As a last fallback, use the value of \\'server.socket_host\\' and \\n-      \\'server.socket_port\\' config settings (defaults to \\'localhost:8080\\'). \\n- \\n-    The URL scheme (\\'http\\' or \\'http\\') used is determined in the following way: \\n- \\n-    * If \\'base_url_filter.base_url\\' is used, use the scheme from this URL. \\n-    * If there is a \\'X-Use-SSL\\' request header, use \\'https\\'. \\n-    * Else, if the config setting \\'tg.url_scheme\\' is set, use its value. \\n-    * Else, use the value of \\'cherrypy.request.scheme\\'. \\n- \\n-    \"\"\" \\n-    get = config.get \\n-    use_xfh = get(\\'base_url_filter.use_x_forwarded_host\\', False) \\n-    if request.headers.get(\\'X-Use-SSL\\'): \\n-        scheme = \\'https\\' \\n-    else: \\n-        scheme = get(\\'tg.url_scheme\\') \\n-    if not scheme: \\n-        scheme = request.scheme \\n-    base_url = \\'%s://%s\\' % (scheme, get_server_name()) \\n-    if get(\\'base_url_filter.on\\', False) and not use_xfh: \\n-        base_url = get(\\'base_url_filter.base_url\\').rstrip(\\'/\\') \\n-    return \\'%s%s\\' % (base_url, url(tgpath, params, **kw)) \\n+def get_server_name():\\n+    \"\"\"Return name of the server this application runs on.\\n+\\n+    Respects \\'Host\\' and \\'X-Forwarded-Host\\' header.\\n+\\n+    See the docstring of the \\'absolute_url\\' function for more information.\\n+\\n+    \"\"\"\\n+    get = config.get\\n+    h = request.headers\\n+    host = get(\\'tg.url_domain\\') or h.get(\\'X-Forwarded-Host\\', h.get(\\'Host\\'))\\n+    if not host:\\n+        host = \\'%s:%s\\' % (get(\\'server.socket_host\\', \\'localhost\\'),\\n+            get(\\'server.socket_port\\', 8080))\\n+    return host\\n+\\n+\\n+def absolute_url(tgpath=\\'/\\', params=None, **kw):\\n+    \"\"\"Return absolute URL (including schema and host to this server).\\n+\\n+    Tries to account for \\'Host\\' header and reverse proxying\\n+    (\\'X-Forwarded-Host\\').\\n+\\n+    The host name is determined this way:\\n+\\n+    * If the config setting \\'tg.url_domain\\' is set and non-null, use this value.\\n+    * Else, if the \\'base_url_filter.use_x_forwarded_host\\' config setting is\\n+      True, use the value from the \\'Host\\' or \\'X-Forwarded-Host\\' request header.\\n+    * Else, if config setting \\'base_url_filter.on\\' is True and\\n+      \\'base_url_filter.base_url\\' is non-null, use its value for the host AND\\n+      scheme part of the URL.\\n+    * As a last fallback, use the value of \\'server.socket_host\\' and\\n+      \\'server.socket_port\\' config settings (defaults to \\'localhost:8080\\').\\n+\\n+    The URL scheme (\\'http\\' or \\'http\\') used is determined in the following way:\\n+\\n+    * If \\'base_url_filter.base_url\\' is used, use the scheme from this URL.\\n+    * If there is a \\'X-Use-SSL\\' request header, use \\'https\\'.\\n+    * Else, if the config setting \\'tg.url_scheme\\' is set, use its value.\\n+    * Else, use the value of \\'cherrypy.request.scheme\\'.\\n+\\n+    \"\"\"\\n+    get = config.get\\n+    use_xfh = get(\\'base_url_filter.use_x_forwarded_host\\', False)\\n+    if request.headers.get(\\'X-Use-SSL\\'):\\n+        scheme = \\'https\\'\\n+    else:\\n+        scheme = get(\\'tg.url_scheme\\')\\n+    if not scheme:\\n+        scheme = request.scheme\\n+    base_url = \\'%s://%s\\' % (scheme, get_server_name())\\n+    if get(\\'base_url_filter.on\\', False) and not use_xfh:\\n+        base_url = get(\\'base_url_filter.base_url\\').rstrip(\\'/\\')\\n+    return \\'%s%s\\' % (base_url, url(tgpath, params, **kw))\\n+\\n+\\n+class InvalidRedirectException(Exception):\\n+    \"\"\"\\n+    An invalid redirect url was provided.  Redirects cannot\\n+    include a carriage return (\\\\r) or new line (\\\\n) character.\\n+    \"\"\"\\n \\n \\n def redirect(redirect_path, redirect_params=None, **kw):\\n@@ -673,6 +680,8 @@ def redirect(redirect_path, redirect_params=None, **kw):\\n         if path.startswith(request.app_root):\\n             path = path[len(request.app_root):]\\n         redirect_path = urlparse.urljoin(path, redirect_path)\\n+    if set(redirect_path).intersection({\\'\\\\r\\', \\'\\\\n\\'}):\\n+        raise InvalidRedirectException(\\'Invalid redirect: {}\\'.format(redirect_path))\\n     raise cherrypy.HTTPRedirect(url(tgpath=redirect_path,\\n         tgparams=redirect_params, **kw))\\n '], 'file': ['turbogears/release.py', 'turbogears/controllers.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('60607e73-9bbb-47a2-b4e7-0cf7e8032828'), UUID('9226c222-8cc5-4ba5-a12c-272715bf785d')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:10: @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:10: @keyparam exclude_from_memory_profiling allows to exclude individual end points from memory profiling. Can be\n",
      " 28%|â–ˆâ–ˆâ–Š       | 503/1800 [05:55<22:09,  1.03s/it]ERROR:src.process_code_changes:Error processing commit fc0b8259e693caa8400fa8b6ac1e494e47ea7798\n",
      "ERROR:src.process_code_changes:{'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': 'fc0b8259e693caa8400fa8b6ac1e494e47ea7798', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -2213,7 +2213,7 @@ def _parents(path):\\n def _ancestry(path):\\n     \"\"\"\\n     Given a path with elements separated by\\n-    posixpath.sep, generate all elements of that path\\n+    posixpath.sep, generate all elements of that path.\\n \\n     >>> list(_ancestry(\\'b/d\\'))\\n     [\\'b/d\\', \\'b\\']\\n@@ -2225,9 +2225,14 @@ def _ancestry(path):\\n     [\\'b\\']\\n     >>> list(_ancestry(\\'\\'))\\n     []\\n+\\n+    Multiple separators are treated like a single.\\n+\\n+    >>> list(_ancestry(\\'//b//d///f//\\'))\\n+    [\\'//b//d///f\\', \\'//b//d\\', \\'//b\\']\\n     \"\"\"\\n     path = path.rstrip(posixpath.sep)\\n-    while path and path != posixpath.sep:\\n+    while path.rstrip(posixpath.sep):\\n         yield path\\n         path, tail = posixpath.split(path)\\n \\n@@ -2244,65 +2249,7 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class SanitizedNames:\\n-    \"\"\"\\n-    ZipFile mix-in to ensure names are sanitized.\\n-    \"\"\"\\n-\\n-    def namelist(self):\\n-        return list(map(self._sanitize, super().namelist()))\\n-\\n-    @staticmethod\\n-    def _sanitize(name):\\n-        r\"\"\"\\n-        Ensure a relative path with posix separators and no dot names.\\n-        Modeled after\\n-        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n-        but provides consistent cross-platform behavior.\\n-        >>> san = SanitizedNames._sanitize\\n-        >>> san(\\'/foo/bar\\')\\n-        \\'foo/bar\\'\\n-        >>> san(\\'//foo.txt\\')\\n-        \\'foo.txt\\'\\n-        >>> san(\\'foo/.././bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'foo../.bar.txt\\')\\n-        \\'foo../.bar.txt\\'\\n-        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n-        \\'D/foo.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n-        \\'server/share/file.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n-        \\'?/GLOBALROOT/Volume3\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n-        \\'PhysicalDrive1/root\\'\\n-        Retain any trailing slash.\\n-        >>> san(\\'abc/\\')\\n-        \\'abc/\\'\\n-        Raises a ValueError if the result is empty.\\n-        >>> san(\\'../..\\')\\n-        Traceback (most recent call last):\\n-        ...\\n-        ValueError: Empty filename\\n-        \"\"\"\\n-\\n-        def allowed(part):\\n-            return part and part not in {\\'..\\', \\'.\\'}\\n-\\n-        # Remove the drive letter.\\n-        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n-        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n-        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n-        parts = clean.split(\\'/\\')\\n-        joined = \\'/\\'.join(filter(allowed, parts))\\n-        if not joined:\\n-            raise ValueError(\"Empty filename\")\\n-        return joined + \\'/\\' * name.endswith(\\'/\\')\\n-\\n-\\n-class CompleteDirs(SanitizedNames, ZipFile):\\n+class CompleteDirs(ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('2f9429c7-5019-4804-a837-8d9f5ac4ae73')]}\n",
      "ERROR:root:Error in {'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': 'fc0b8259e693caa8400fa8b6ac1e494e47ea7798', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -2213,7 +2213,7 @@ def _parents(path):\\n def _ancestry(path):\\n     \"\"\"\\n     Given a path with elements separated by\\n-    posixpath.sep, generate all elements of that path\\n+    posixpath.sep, generate all elements of that path.\\n \\n     >>> list(_ancestry(\\'b/d\\'))\\n     [\\'b/d\\', \\'b\\']\\n@@ -2225,9 +2225,14 @@ def _ancestry(path):\\n     [\\'b\\']\\n     >>> list(_ancestry(\\'\\'))\\n     []\\n+\\n+    Multiple separators are treated like a single.\\n+\\n+    >>> list(_ancestry(\\'//b//d///f//\\'))\\n+    [\\'//b//d///f\\', \\'//b//d\\', \\'//b\\']\\n     \"\"\"\\n     path = path.rstrip(posixpath.sep)\\n-    while path and path != posixpath.sep:\\n+    while path.rstrip(posixpath.sep):\\n         yield path\\n         path, tail = posixpath.split(path)\\n \\n@@ -2244,65 +2249,7 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class SanitizedNames:\\n-    \"\"\"\\n-    ZipFile mix-in to ensure names are sanitized.\\n-    \"\"\"\\n-\\n-    def namelist(self):\\n-        return list(map(self._sanitize, super().namelist()))\\n-\\n-    @staticmethod\\n-    def _sanitize(name):\\n-        r\"\"\"\\n-        Ensure a relative path with posix separators and no dot names.\\n-        Modeled after\\n-        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n-        but provides consistent cross-platform behavior.\\n-        >>> san = SanitizedNames._sanitize\\n-        >>> san(\\'/foo/bar\\')\\n-        \\'foo/bar\\'\\n-        >>> san(\\'//foo.txt\\')\\n-        \\'foo.txt\\'\\n-        >>> san(\\'foo/.././bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'foo../.bar.txt\\')\\n-        \\'foo../.bar.txt\\'\\n-        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n-        \\'D/foo.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n-        \\'server/share/file.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n-        \\'?/GLOBALROOT/Volume3\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n-        \\'PhysicalDrive1/root\\'\\n-        Retain any trailing slash.\\n-        >>> san(\\'abc/\\')\\n-        \\'abc/\\'\\n-        Raises a ValueError if the result is empty.\\n-        >>> san(\\'../..\\')\\n-        Traceback (most recent call last):\\n-        ...\\n-        ValueError: Empty filename\\n-        \"\"\"\\n-\\n-        def allowed(part):\\n-            return part and part not in {\\'..\\', \\'.\\'}\\n-\\n-        # Remove the drive letter.\\n-        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n-        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n-        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n-        parts = clean.split(\\'/\\')\\n-        joined = \\'/\\'.join(filter(allowed, parts))\\n-        if not joined:\\n-            raise ValueError(\"Empty filename\")\\n-        return joined + \\'/\\' * name.endswith(\\'/\\')\\n-\\n-\\n-class CompleteDirs(SanitizedNames, ZipFile):\\n+class CompleteDirs(ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('2f9429c7-5019-4804-a837-8d9f5ac4ae73')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 33:0: <line number missing in source>\n",
      " 28%|â–ˆâ–ˆâ–Š       | 510/1800 [05:57<16:56,  1.27it/s]ERROR:src.process_code_changes:Error processing commit b07b79a1e92cc62574ba0492cce000ef4a7bd25f\n",
      "ERROR:src.process_code_changes:{'repo': '2071174A/vinylmap', 'vulnerability_id': '2015-10056', 'commit': 'b07b79a1e92cc62574ba0492cce000ef4a7bd25f', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -24,12 +24,14 @@ def contact(request):\\n \\n def search(request):\\n \\tcontext_dict = {}\\n-\\tif \\'q\\' in request.GET and request.GET[\\'q\\'] != \\'\\':\\n-\\t\\tq = request.GET[\\'q\\']\\n+\\tq = request.GET[\\'q\\'].replace(\\'%\\', \\'\\').replace(\\'_\\', \\'\\').strip()\\n+\\tif \\'q\\' in request.GET and q != \\'\\':\\n+\\t\\tq = \\'%\\' + q + \\'%\\'\\n \\t\\tcursor = connection.cursor()\\n-\\t\\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like \\'%\" + q + \"%\\' or artist like \\'%\" + q + \"%\\' or label like \\'%\" + q + \"%\\' or cat_no like \\'%\" + q + \"%\\';\")\\n+\\t\\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like %s or artist like %s or label like %s or cat_no like %s;\", [q,q,q,q])\\n \\t\\trec_list=cursor.fetchall()\\n \\t\\t\\n+\\n \\t\\ttotal=len(rec_list)\\n \\t\\tpg=int(request.GET[\\'page\\']) if \\'page\\' in request.GET else 1\\n \\t\\tub=min(pg*12, total)'], 'file': ['recordstoreapp/views.py'], 'language': ['Python'], 'temp_id': [UUID('fc2ee1e5-acc8-4566-b86a-4c4f6ffe682a')]}\n",
      "ERROR:root:Error in {'repo': '2071174A/vinylmap', 'vulnerability_id': '2015-10056', 'commit': 'b07b79a1e92cc62574ba0492cce000ef4a7bd25f', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -24,12 +24,14 @@ def contact(request):\\n \\n def search(request):\\n \\tcontext_dict = {}\\n-\\tif \\'q\\' in request.GET and request.GET[\\'q\\'] != \\'\\':\\n-\\t\\tq = request.GET[\\'q\\']\\n+\\tq = request.GET[\\'q\\'].replace(\\'%\\', \\'\\').replace(\\'_\\', \\'\\').strip()\\n+\\tif \\'q\\' in request.GET and q != \\'\\':\\n+\\t\\tq = \\'%\\' + q + \\'%\\'\\n \\t\\tcursor = connection.cursor()\\n-\\t\\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like \\'%\" + q + \"%\\' or artist like \\'%\" + q + \"%\\' or label like \\'%\" + q + \"%\\' or cat_no like \\'%\" + q + \"%\\';\")\\n+\\t\\tcursor.execute(\"SELECT id,title,artist,cover FROM recordstoreapp_record WHERE title like %s or artist like %s or label like %s or cat_no like %s;\", [q,q,q,q])\\n \\t\\trec_list=cursor.fetchall()\\n \\t\\t\\n+\\n \\t\\ttotal=len(rec_list)\\n \\t\\tpg=int(request.GET[\\'page\\']) if \\'page\\' in request.GET else 1\\n \\t\\tub=min(pg*12, total)'], 'file': ['recordstoreapp/views.py'], 'language': ['Python'], 'temp_id': [UUID('fc2ee1e5-acc8-4566-b86a-4c4f6ffe682a')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 143, in get_changes\n",
      "    local_repo = Repo.clone_from(repo.clone_url, f\"{REPOS_PATH}/{repo_name}\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1541, in clone_from\n",
      "    return cls._clone(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1412, in _clone\n",
      "    finalize_process(proc, stderr=stderr)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/util.py\", line 504, in finalize_process\n",
      "    proc.wait(**kwargs)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/cmd.py\", line 834, in wait\n",
      "    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\n",
      "git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git clone -v -- https://github.com/2071174A/vinylmap.git /Users/somen/repos/2071174A/vinylmap\n",
      "  stderr: 'Cloning into '/Users/somen/repos/2071174A/vinylmap'...\n",
      "POST git-upload-pack (175 bytes)\n",
      "POST git-upload-pack (217 bytes)\n",
      "error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly: CANCEL (err 8)\n",
      "error: 5789 bytes of body are still expected\n",
      "fetch-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: early EOF\n",
      "fatal: fetch-pack: invalid index-pack output\n",
      "'\n",
      " 30%|â–ˆâ–ˆâ–‰       | 539/1800 [06:17<13:57,  1.51it/s]ERROR:src.process_code_changes:Error processing commit 5f861670ef8f38ca8eea52a98672d0e0fabb5368\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-4644', 'commit': '5f861670ef8f38ca8eea52a98672d0e0fabb5368', 'commit_source': 'github', 'cwe_id': ['CWE-601'], 'patch': [\"@@ -85,7 +85,7 @@\\n @cherrypy.tools.currentuser(userobj=lambda username: UserObject.get_user(username))\\n @cherrypy.tools.db()\\n @cherrypy.tools.enrich_session()\\n-@cherrypy.tools.proxy(remote='X-Real-IP')\\n+@cherrypy.tools.proxy(local=None, remote='X-Real-IP')\\n @cherrypy.tools.secure_headers()\\n class Root(LocationsPage):\\n     def __init__(self):\"], 'file': ['rdiffweb/rdw_app.py'], 'language': ['Python'], 'temp_id': [UUID('774a3872-2e34-4be5-b1de-da7b828e7870')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-4644', 'commit': '5f861670ef8f38ca8eea52a98672d0e0fabb5368', 'commit_source': 'github', 'cwe_id': ['CWE-601'], 'patch': [\"@@ -85,7 +85,7 @@\\n @cherrypy.tools.currentuser(userobj=lambda username: UserObject.get_user(username))\\n @cherrypy.tools.db()\\n @cherrypy.tools.enrich_session()\\n-@cherrypy.tools.proxy(remote='X-Real-IP')\\n+@cherrypy.tools.proxy(local=None, remote='X-Real-IP')\\n @cherrypy.tools.secure_headers()\\n class Root(LocationsPage):\\n     def __init__(self):\"], 'file': ['rdiffweb/rdw_app.py'], 'language': ['Python'], 'temp_id': [UUID('774a3872-2e34-4be5-b1de-da7b828e7870')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 569/1800 [06:19<05:49,  3.52it/s]ERROR:src.process_code_changes:Error processing commit bcec1df703cd4a01520a90c3f801cca6f97d9bfd\n",
      "ERROR:src.process_code_changes:{'repo': 'apache/airflow', 'vulnerability_id': '2021-38540', 'commit': 'bcec1df703cd4a01520a90c3f801cca6f97d9bfd', 'commit_source': 'github', 'cwe_id': ['CWE-269', 'CWE-269'], 'patch': ['@@ -3276,7 +3276,6 @@ class VariableModelView(AirflowModelView):\\n         \\'delete\\': \\'delete\\',\\n         \\'action_muldelete\\': \\'delete\\',\\n         \\'action_varexport\\': \\'read\\',\\n-        \\'varimport\\': \\'create\\',\\n     }\\n     base_permissions = [\\n         permissions.ACTION_CAN_CREATE,\\n@@ -3339,6 +3338,7 @@ def action_varexport(self, items):\\n         return response\\n \\n     @expose(\\'/varimport\\', methods=[\"POST\"])\\n+    @auth.has_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_VARIABLE)])\\n     @action_logging\\n     def varimport(self):\\n         \"\"\"Import variables\"\"\"'], 'file': ['airflow/www/views.py'], 'language': ['Python'], 'temp_id': [UUID('da6785b4-a2f8-405e-8f55-1d16dea54294')]}\n",
      "ERROR:root:Error in {'repo': 'apache/airflow', 'vulnerability_id': '2021-38540', 'commit': 'bcec1df703cd4a01520a90c3f801cca6f97d9bfd', 'commit_source': 'github', 'cwe_id': ['CWE-269', 'CWE-269'], 'patch': ['@@ -3276,7 +3276,6 @@ class VariableModelView(AirflowModelView):\\n         \\'delete\\': \\'delete\\',\\n         \\'action_muldelete\\': \\'delete\\',\\n         \\'action_varexport\\': \\'read\\',\\n-        \\'varimport\\': \\'create\\',\\n     }\\n     base_permissions = [\\n         permissions.ACTION_CAN_CREATE,\\n@@ -3339,6 +3338,7 @@ def action_varexport(self, items):\\n         return response\\n \\n     @expose(\\'/varimport\\', methods=[\"POST\"])\\n+    @auth.has_access([(permissions.ACTION_CAN_CREATE, permissions.RESOURCE_VARIABLE)])\\n     @action_logging\\n     def varimport(self):\\n         \"\"\"Import variables\"\"\"'], 'file': ['airflow/www/views.py'], 'language': ['Python'], 'temp_id': [UUID('da6785b4-a2f8-405e-8f55-1d16dea54294')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 578/1800 [06:21<05:25,  3.75it/s]ERROR:src.process_code_changes:Error processing commit a5b15fb0a5be5fdbacba8ff7b2c8759d5e3ba20f\n",
      "ERROR:src.process_code_changes:{'repo': 'PuddingBot/pudding-bot', 'vulnerability_id': '2022-21669', 'commit': 'a5b15fb0a5be5fdbacba8ff7b2c8759d5e3ba20f', 'commit_source': 'github', 'cwe_id': ['CWE-798'], 'patch': ['@@ -49,7 +49,6 @@ def about_command(update: Update, context: CallbackContext) -> None:\\n     update.message.reply_animation(animation=about.ANIMATION,\\r\\n     caption=about.TEXT)\\r\\n \\r\\n-\\r\\n def help_command(update: Update, context: CallbackContext) -> None:\\r\\n     \"\"\"Send a message when the command /help is issued.\"\"\"\\r\\n     update.message.reply_text(\\'Help!\\')\\r\\n@@ -175,13 +174,6 @@ def unmute_command(update: Update, context: CallbackContext) -> None:\\n         # Should wrap this into a common function call \"not_admin(update)\"\\r\\n         update.message.reply_text(messages.PERM_LACK + statuses.IS_ADMIN)\\r\\n \\r\\n-# def get_commands_command(update: Update, context: CallbackContext) -> None:\\r\\n-#     # bot = utils.get_bot() // return a Bot object\\r\\n-#     bot = Bot(\"5016982005:AAG6YJFXVkvyVha7_3cghe8gj-PIGAL9aXE\")\\r\\n-#     commands = bot.get_my_commands()\\r\\n-#     update.message.reply_text(commands)\\r\\n-\\r\\n-\\r\\n def ban_command(update: Update, context: CallbackContext) -> None:\\r\\n     reply = update.message.reply_to_message.from_user\\r\\n     # perms = {\"can_send_messages\": False}\\r\\n@@ -193,7 +185,6 @@ def ban_command(update: Update, context: CallbackContext) -> None:\\n     else: \\r\\n         update.message.reply_text(messages.PERM_LACK + statuses.IS_ADMIN)\\r\\n \\r\\n-\\r\\n def unban_command(update: Update, context: CallbackContext) -> None:\\r\\n     reply = update.message.reply_to_message.from_user\\r\\n     # perms = {\"can_send_messages\": False}\\r', '@@ -0,0 +1,48 @@\\n+#!/usr/bin/env python3\\r\\n+# pylint: disable=C0116,W0613\\r\\n+\\r\\n+from os.path import exists\\r\\n+\\r\\n+HOOKS_FOLDER = \".git/hooks/\"\\r\\n+HOOK = \"pre-commit\"\\r\\n+\\r\\n+def prompt(question, default=\"yes\"):\\r\\n+    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\\r\\n+    if default is None:\\r\\n+        prompt = \" [y/n] \"\\r\\n+    elif default == \"yes\":\\r\\n+        prompt = \" [Y/n] \"\\r\\n+    elif default == \"no\":\\r\\n+        prompt = \" [y/N] \"\\r\\n+    else:\\r\\n+        raise ValueError(\"Invalid default answer: \\'%s\\'\" % default)\\r\\n+\\r\\n+    while True:\\r\\n+        print(question + prompt + \"\\\\r\")\\r\\n+        try:\\r\\n+            choice = input().lower()\\r\\n+            if default is not None and choice == \"\":\\r\\n+                return valid[default]\\r\\n+            elif choice in valid:\\r\\n+                return valid[choice]\\r\\n+            else:\\r\\n+                print(\"Please respond with \\'[y]es\\' or \\'[n]o\\'\")\\r\\n+        except KeyboardInterrupt:\\r\\n+            exit(\"Operation aborted. No files have been changed.\")\\r\\n+\\r\\n+if exists(HOOKS_FOLDER+HOOK):\\r\\n+    overwrite = prompt(\"pre-commit hook already exists. Overwrite?\", None)\\r\\n+    if overwrite == True:\\r\\n+        try:\\r\\n+            with open(HOOK, \\'rb\\') as src, open(HOOKS_FOLDER+HOOK, \\'wb\\') as dst: dst.write(src.read())\\r\\n+            print(f\"{HOOK} inside {HOOKS_FOLDER} was replaced successfully.\")\\r\\n+        except:\\r\\n+            print(f\"{HOOK} could not be written to {HOOKS_FOLDER}.\")\\r\\n+    else:\\r\\n+        print(\"Operation aborted. No files have been changed.\")\\r\\n+else:\\r\\n+    try:\\r\\n+        with open(HOOK, \\'rb\\') as src, open(HOOKS_FOLDER+HOOK, \\'wb\\') as dst: dst.write(src.read())\\r\\n+        print(f\"{HOOK} hook was added successfully.\")\\r\\n+    except:\\r\\n+            print(f\"{HOOK} could not be written to {HOOKS_FOLDER}.\")\\r'], 'file': ['main.py', 'first-run.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('d9be6d81-37c8-45c1-bad7-12f5f28efb9d'), UUID('b587e982-384b-4453-b39c-2f8ac511f0fc')]}\n",
      "ERROR:root:Error in {'repo': 'PuddingBot/pudding-bot', 'vulnerability_id': '2022-21669', 'commit': 'a5b15fb0a5be5fdbacba8ff7b2c8759d5e3ba20f', 'commit_source': 'github', 'cwe_id': ['CWE-798'], 'patch': ['@@ -49,7 +49,6 @@ def about_command(update: Update, context: CallbackContext) -> None:\\n     update.message.reply_animation(animation=about.ANIMATION,\\r\\n     caption=about.TEXT)\\r\\n \\r\\n-\\r\\n def help_command(update: Update, context: CallbackContext) -> None:\\r\\n     \"\"\"Send a message when the command /help is issued.\"\"\"\\r\\n     update.message.reply_text(\\'Help!\\')\\r\\n@@ -175,13 +174,6 @@ def unmute_command(update: Update, context: CallbackContext) -> None:\\n         # Should wrap this into a common function call \"not_admin(update)\"\\r\\n         update.message.reply_text(messages.PERM_LACK + statuses.IS_ADMIN)\\r\\n \\r\\n-# def get_commands_command(update: Update, context: CallbackContext) -> None:\\r\\n-#     # bot = utils.get_bot() // return a Bot object\\r\\n-#     bot = Bot(\"5016982005:AAG6YJFXVkvyVha7_3cghe8gj-PIGAL9aXE\")\\r\\n-#     commands = bot.get_my_commands()\\r\\n-#     update.message.reply_text(commands)\\r\\n-\\r\\n-\\r\\n def ban_command(update: Update, context: CallbackContext) -> None:\\r\\n     reply = update.message.reply_to_message.from_user\\r\\n     # perms = {\"can_send_messages\": False}\\r\\n@@ -193,7 +185,6 @@ def ban_command(update: Update, context: CallbackContext) -> None:\\n     else: \\r\\n         update.message.reply_text(messages.PERM_LACK + statuses.IS_ADMIN)\\r\\n \\r\\n-\\r\\n def unban_command(update: Update, context: CallbackContext) -> None:\\r\\n     reply = update.message.reply_to_message.from_user\\r\\n     # perms = {\"can_send_messages\": False}\\r', '@@ -0,0 +1,48 @@\\n+#!/usr/bin/env python3\\r\\n+# pylint: disable=C0116,W0613\\r\\n+\\r\\n+from os.path import exists\\r\\n+\\r\\n+HOOKS_FOLDER = \".git/hooks/\"\\r\\n+HOOK = \"pre-commit\"\\r\\n+\\r\\n+def prompt(question, default=\"yes\"):\\r\\n+    valid = {\"yes\": True, \"y\": True, \"ye\": True, \"no\": False, \"n\": False}\\r\\n+    if default is None:\\r\\n+        prompt = \" [y/n] \"\\r\\n+    elif default == \"yes\":\\r\\n+        prompt = \" [Y/n] \"\\r\\n+    elif default == \"no\":\\r\\n+        prompt = \" [y/N] \"\\r\\n+    else:\\r\\n+        raise ValueError(\"Invalid default answer: \\'%s\\'\" % default)\\r\\n+\\r\\n+    while True:\\r\\n+        print(question + prompt + \"\\\\r\")\\r\\n+        try:\\r\\n+            choice = input().lower()\\r\\n+            if default is not None and choice == \"\":\\r\\n+                return valid[default]\\r\\n+            elif choice in valid:\\r\\n+                return valid[choice]\\r\\n+            else:\\r\\n+                print(\"Please respond with \\'[y]es\\' or \\'[n]o\\'\")\\r\\n+        except KeyboardInterrupt:\\r\\n+            exit(\"Operation aborted. No files have been changed.\")\\r\\n+\\r\\n+if exists(HOOKS_FOLDER+HOOK):\\r\\n+    overwrite = prompt(\"pre-commit hook already exists. Overwrite?\", None)\\r\\n+    if overwrite == True:\\r\\n+        try:\\r\\n+            with open(HOOK, \\'rb\\') as src, open(HOOKS_FOLDER+HOOK, \\'wb\\') as dst: dst.write(src.read())\\r\\n+            print(f\"{HOOK} inside {HOOKS_FOLDER} was replaced successfully.\")\\r\\n+        except:\\r\\n+            print(f\"{HOOK} could not be written to {HOOKS_FOLDER}.\")\\r\\n+    else:\\r\\n+        print(\"Operation aborted. No files have been changed.\")\\r\\n+else:\\r\\n+    try:\\r\\n+        with open(HOOK, \\'rb\\') as src, open(HOOKS_FOLDER+HOOK, \\'wb\\') as dst: dst.write(src.read())\\r\\n+        print(f\"{HOOK} hook was added successfully.\")\\r\\n+    except:\\r\\n+            print(f\"{HOOK} could not be written to {HOOKS_FOLDER}.\")\\r'], 'file': ['main.py', 'first-run.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('d9be6d81-37c8-45c1-bad7-12f5f28efb9d'), UUID('b587e982-384b-4453-b39c-2f8ac511f0fc')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 42:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 42:0: <line number missing in source>\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 607/1800 [06:41<09:48,  2.03it/s]ERROR:src.process_code_changes:Error processing commit c60b64af5bb8c09189071522a1f6796cb44340b0\n",
      "ERROR:src.process_code_changes:{'repo': 'motioneye-project/motioneye', 'vulnerability_id': '2022-25568', 'commit': 'c60b64af5bb8c09189071522a1f6796cb44340b0', 'commit_source': 'github', 'cwe_id': ['CWE-200'], 'patch': [\"@@ -368,7 +368,7 @@ def _handle_get_config_response(self, camera_id, local_config, resp: utils.GetCo\\n         finished = self.check_finished(cameras, length)\\n         return\\n \\n-    @BaseHandler.auth()\\n+    @BaseHandler.auth(admin=True)\\n     async def list(self):\\n         logging.debug('listing cameras')\\n \"], 'file': ['motioneye/handlers/config.py'], 'language': ['Python'], 'temp_id': [UUID('e930d981-15c8-433f-9d9e-edbe3dba1aa0')]}\n",
      "ERROR:root:Error in {'repo': 'motioneye-project/motioneye', 'vulnerability_id': '2022-25568', 'commit': 'c60b64af5bb8c09189071522a1f6796cb44340b0', 'commit_source': 'github', 'cwe_id': ['CWE-200'], 'patch': [\"@@ -368,7 +368,7 @@ def _handle_get_config_response(self, camera_id, local_config, resp: utils.GetCo\\n         finished = self.check_finished(cameras, length)\\n         return\\n \\n-    @BaseHandler.auth()\\n+    @BaseHandler.auth(admin=True)\\n     async def list(self):\\n         logging.debug('listing cameras')\\n \"], 'file': ['motioneye/handlers/config.py'], 'language': ['Python'], 'temp_id': [UUID('e930d981-15c8-433f-9d9e-edbe3dba1aa0')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 615/1800 [06:42<07:48,  2.53it/s]ERROR:src.process_code_changes:Error processing commit f2de2371c5e13ce1c6fd6f9a1ed3e5d46b93cd7e\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-3174', 'commit': 'f2de2371c5e13ce1c6fd6f9a1ed3e5d46b93cd7e', 'commit_source': 'github', 'cwe_id': ['CWE-614', 'CWE-614'], 'patch': ['@@ -32,12 +32,13 @@\\n     http.cookies.Morsel._reserved[\\'samesite\\'] = \\'SameSite\\'\\n \\n \\n-class CsrfAuth(HandlerTool):\\n+class SecureHeaders(HandlerTool):\\n     \"\"\"\\n     This tool provide CSRF mitigation.\\n \\n     * Define X-Frame-Options = DENY\\n     * Define Cookies SameSite=Lax\\n+    * Define Cookies Secure when https is detected\\n     * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\\n \\n     Ref.:\\n@@ -46,7 +47,7 @@ class CsrfAuth(HandlerTool):\\n     \"\"\"\\n \\n     def __init__(self):\\n-        HandlerTool.__init__(self, self.run, name=\\'csrf\\')\\n+        HandlerTool.__init__(self, self.run, name=\\'secure_headers\\')\\n         # Make sure to run before authform (priority 71)\\n         self._priority = 71\\n \\n@@ -58,12 +59,18 @@ def _set_headers(self):\\n         response = cherrypy.serving.response\\n         # Define X-Frame-Options to avoid Clickjacking\\n         response.headers[\\'X-Frame-Options\\'] = \\'DENY\\'\\n-        # Awaiting bug fix in cherrypy\\n-        # https://github.com/cherrypy/cherrypy/issues/1767\\n-        # Force SameSite to Lax\\n+\\n+        # Enforce security on cookies\\n         cookie = response.cookie.get(\\'session_id\\', None)\\n         if cookie:\\n+            # Awaiting bug fix in cherrypy\\n+            # https://github.com/cherrypy/cherrypy/issues/1767\\n+            # Force SameSite to Lax\\n             cookie[\\'samesite\\'] = \\'Lax\\'\\n+            # Check if https is enabled\\n+            https = cherrypy.request.base.startswith(\\'https\\')\\n+            if https:\\n+                cookie[\\'secure\\'] = 1\\n \\n     def run(self):\\n         if cherrypy.request.method in [\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\']:\\n@@ -73,4 +80,4 @@ def run(self):\\n                 raise cherrypy.HTTPError(403, \\'Unexpected Origin header\\')\\n \\n \\n-cherrypy.tools.csrf = CsrfAuth()\\n+cherrypy.tools.secure_headers = SecureHeaders()', \"@@ -38,7 +38,7 @@\\n import rdiffweb.tools.i18n\\n import rdiffweb.tools.proxy\\n import rdiffweb.tools.ratelimit\\n-import rdiffweb.tools.security\\n+import rdiffweb.tools.secure_headers\\n from rdiffweb.controller import Controller\\n from rdiffweb.controller.api import ApiPage\\n from rdiffweb.controller.dispatch import static  # noqa\\n@@ -63,6 +63,7 @@\\n \\n \\n @cherrypy.tools.proxy()\\n+@cherrypy.tools.secure_headers()\\n class Root(LocationsPage):\\n     def __init__(self):\\n         self.login = LoginPage()\\n@@ -170,7 +171,6 @@ def __init__(self, cfg):\\n                 'tools.auth_form.on': True,\\n                 'tools.currentuser.on': True,\\n                 'tools.currentuser.userobj': lambda username: self.store.get_user(username),\\n-                'tools.csrf.on': True,\\n                 'tools.i18n.on': True,\\n                 'tools.i18n.default': 'en_US',\\n                 'tools.i18n.mo_dir': pkg_resources.resource_filename('rdiffweb', 'locales'),  # @UndefinedVariable\"], 'file': ['rdiffweb/tools/secure_headers.py', 'rdiffweb/rdw_app.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b17790eb-8fc8-4ff1-944d-13a78542a639'), UUID('a04d70f0-5ef0-4ec9-9b86-c64d0802a63d')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-3174', 'commit': 'f2de2371c5e13ce1c6fd6f9a1ed3e5d46b93cd7e', 'commit_source': 'github', 'cwe_id': ['CWE-614', 'CWE-614'], 'patch': ['@@ -32,12 +32,13 @@\\n     http.cookies.Morsel._reserved[\\'samesite\\'] = \\'SameSite\\'\\n \\n \\n-class CsrfAuth(HandlerTool):\\n+class SecureHeaders(HandlerTool):\\n     \"\"\"\\n     This tool provide CSRF mitigation.\\n \\n     * Define X-Frame-Options = DENY\\n     * Define Cookies SameSite=Lax\\n+    * Define Cookies Secure when https is detected\\n     * Validate `Origin` and `Referer` on POST, PUT, PATCH, DELETE\\n \\n     Ref.:\\n@@ -46,7 +47,7 @@ class CsrfAuth(HandlerTool):\\n     \"\"\"\\n \\n     def __init__(self):\\n-        HandlerTool.__init__(self, self.run, name=\\'csrf\\')\\n+        HandlerTool.__init__(self, self.run, name=\\'secure_headers\\')\\n         # Make sure to run before authform (priority 71)\\n         self._priority = 71\\n \\n@@ -58,12 +59,18 @@ def _set_headers(self):\\n         response = cherrypy.serving.response\\n         # Define X-Frame-Options to avoid Clickjacking\\n         response.headers[\\'X-Frame-Options\\'] = \\'DENY\\'\\n-        # Awaiting bug fix in cherrypy\\n-        # https://github.com/cherrypy/cherrypy/issues/1767\\n-        # Force SameSite to Lax\\n+\\n+        # Enforce security on cookies\\n         cookie = response.cookie.get(\\'session_id\\', None)\\n         if cookie:\\n+            # Awaiting bug fix in cherrypy\\n+            # https://github.com/cherrypy/cherrypy/issues/1767\\n+            # Force SameSite to Lax\\n             cookie[\\'samesite\\'] = \\'Lax\\'\\n+            # Check if https is enabled\\n+            https = cherrypy.request.base.startswith(\\'https\\')\\n+            if https:\\n+                cookie[\\'secure\\'] = 1\\n \\n     def run(self):\\n         if cherrypy.request.method in [\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\']:\\n@@ -73,4 +80,4 @@ def run(self):\\n                 raise cherrypy.HTTPError(403, \\'Unexpected Origin header\\')\\n \\n \\n-cherrypy.tools.csrf = CsrfAuth()\\n+cherrypy.tools.secure_headers = SecureHeaders()', \"@@ -38,7 +38,7 @@\\n import rdiffweb.tools.i18n\\n import rdiffweb.tools.proxy\\n import rdiffweb.tools.ratelimit\\n-import rdiffweb.tools.security\\n+import rdiffweb.tools.secure_headers\\n from rdiffweb.controller import Controller\\n from rdiffweb.controller.api import ApiPage\\n from rdiffweb.controller.dispatch import static  # noqa\\n@@ -63,6 +63,7 @@\\n \\n \\n @cherrypy.tools.proxy()\\n+@cherrypy.tools.secure_headers()\\n class Root(LocationsPage):\\n     def __init__(self):\\n         self.login = LoginPage()\\n@@ -170,7 +171,6 @@ def __init__(self, cfg):\\n                 'tools.auth_form.on': True,\\n                 'tools.currentuser.on': True,\\n                 'tools.currentuser.userobj': lambda username: self.store.get_user(username),\\n-                'tools.csrf.on': True,\\n                 'tools.i18n.on': True,\\n                 'tools.i18n.default': 'en_US',\\n                 'tools.i18n.mo_dir': pkg_resources.resource_filename('rdiffweb', 'locales'),  # @UndefinedVariable\"], 'file': ['rdiffweb/tools/secure_headers.py', 'rdiffweb/rdw_app.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b17790eb-8fc8-4ff1-944d-13a78542a639'), UUID('a04d70f0-5ef0-4ec9-9b86-c64d0802a63d')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 618/1800 [06:44<09:26,  2.09it/s]ERROR:src.process_code_changes:Error processing commit 4ad12f204ad0b85580fc32137c647baaff044e95\n",
      "ERROR:src.process_code_changes:{'repo': 'jupyter-lsp/jupyterlab-lsp', 'vulnerability_id': '2024-22415', 'commit': '4ad12f204ad0b85580fc32137c647baaff044e95', 'commit_source': 'github', 'cwe_id': ['CWE-23', 'CWE-23', 'CWE-23', 'CWE-23'], 'patch': ['@@ -2,14 +2,32 @@\\n \"\"\"\\n from typing import Optional, Text\\n \\n+from jupyter_core.utils import ensure_async\\n from jupyter_server.base.handlers import APIHandler\\n-from jupyter_server.base.zmqhandlers import WebSocketHandler, WebSocketMixin\\n from jupyter_server.utils import url_path_join as ujoin\\n+from tornado import web\\n+from tornado.websocket import WebSocketHandler\\n+\\n+try:\\n+    from jupyter_server.auth.decorator import authorized\\n+except ImportError:\\n+\\n+    def authorized(method):  # type: ignore\\n+        \"\"\"A no-op fallback for `jupyter_server 1.x`\"\"\"\\n+        return method\\n+\\n+\\n+try:\\n+    from jupyter_server.base.websocket import WebSocketMixin\\n+except ImportError:\\n+    from jupyter_server.base.zmqhandlers import WebSocketMixin\\n \\n from .manager import LanguageServerManager\\n from .schema import SERVERS_RESPONSE\\n from .specs.utils import censored_spec\\n \\n+AUTH_RESOURCE = \"lsp\"\\n+\\n \\n class BaseHandler(APIHandler):\\n     manager = None  # type: LanguageServerManager\\n@@ -21,10 +39,43 @@ def initialize(self, manager: LanguageServerManager):\\n class LanguageServerWebSocketHandler(  # type: ignore\\n     WebSocketMixin, WebSocketHandler, BaseHandler\\n ):\\n-    \"\"\"Setup tornado websocket to route to language server sessions\"\"\"\\n+    \"\"\"Setup tornado websocket to route to language server sessions.\\n+\\n+    The logic of `get` and `pre_get` methods is derived from jupyter-server ws handlers,\\n+    and should be kept in sync to follow best practice established by upstream; see:\\n+    https://github.com/jupyter-server/jupyter_server/blob/v2.12.5/jupyter_server/services/kernels/websocket.py#L36\\n+    \"\"\"\\n+\\n+    auth_resource = AUTH_RESOURCE\\n \\n     language_server: Optional[Text] = None\\n \\n+    async def pre_get(self):\\n+        \"\"\"Handle a pre_get.\"\"\"\\n+        # authenticate first\\n+        # authenticate the request before opening the websocket\\n+        user = self.current_user\\n+        if user is None:\\n+            self.log.warning(\"Couldn\\'t authenticate WebSocket connection\")\\n+            raise web.HTTPError(403)\\n+\\n+        if not hasattr(self, \"authorizer\"):\\n+            return\\n+\\n+        # authorize the user.\\n+        is_authorized = await ensure_async(\\n+            self.authorizer.is_authorized(self, user, \"execute\", AUTH_RESOURCE)\\n+        )\\n+        if not is_authorized:\\n+            raise web.HTTPError(403)\\n+\\n+    async def get(self, *args, **kwargs):\\n+        \"\"\"Get an event socket.\"\"\"\\n+        await self.pre_get()\\n+        res = super().get(*args, **kwargs)\\n+        if res is not None:\\n+            await res\\n+\\n     async def open(self, language_server):\\n         await self.manager.ready()\\n         self.language_server = language_server\\n@@ -47,11 +98,11 @@ class LanguageServersHandler(BaseHandler):\\n     Response should conform to schema in schema/servers.schema.json\\n     \"\"\"\\n \\n+    auth_resource = AUTH_RESOURCE\\n     validator = SERVERS_RESPONSE\\n \\n-    def initialize(self, *args, **kwargs):\\n-        super().initialize(*args, **kwargs)\\n-\\n+    @web.authenticated\\n+    @authorized\\n     async def get(self):\\n         \"\"\"finish with the JSON representations of the sessions\"\"\"\\n         await self.manager.ready()', '@@ -4,6 +4,7 @@\\n from pathlib import Path\\n \\n import traitlets\\n+from tornado import ioloop\\n \\n from .handlers import add_handlers\\n from .manager import LanguageServerManager\\n@@ -73,4 +74,11 @@ def load_jupyter_server_extension(nbapp):\\n     page_config.update(rootUri=root_uri, virtualDocumentsUri=virtual_documents_uri)\\n \\n     add_handlers(nbapp)\\n-    nbapp.io_loop.call_later(0, initialize, nbapp, virtual_documents_uri)\\n+\\n+    if hasattr(nbapp, \"io_loop\"):\\n+        io_loop = nbapp.io_loop\\n+    else:\\n+        # handle jupyter_server 1.x\\n+        io_loop = ioloop.IOLoop.current()\\n+\\n+    io_loop.call_later(0, initialize, nbapp, virtual_documents_uri)', '@@ -8,7 +8,7 @@\\n from tornado.gen import convert_yielded\\n \\n from .manager import lsp_message_listener\\n-from .paths import file_uri_to_path\\n+from .paths import file_uri_to_path, is_relative\\n from .types import LanguageServerManagerAPI\\n \\n # TODO: make configurable\\n@@ -171,6 +171,11 @@ async def shadow_virtual_documents(scope, message, language_server, manager):\\n             initialized = True\\n \\n         path = file_uri_to_path(uri)\\n+        if not is_relative(shadow_filesystem, path):\\n+            raise ShadowFilesystemError(\\n+                f\"Path {path} is not relative to shadow filesystem root\"\\n+            )\\n+\\n         editable_file = EditableFile(path)\\n \\n         await editable_file.read()', '@@ -1,6 +1,7 @@\\n import os\\n-import pathlib\\n import re\\n+from pathlib import Path\\n+from typing import Union\\n from urllib.parse import unquote, urlparse\\n \\n RE_PATH_ANCHOR = r\"^file://([^/]+|/[A-Z]:)\"\\n@@ -12,7 +13,7 @@ def normalized_uri(root_dir):\\n     Special care must be taken around windows paths: the canonical form of\\n     windows drives and UNC paths is lower case\\n     \"\"\"\\n-    root_uri = pathlib.Path(root_dir).expanduser().resolve().as_uri()\\n+    root_uri = Path(root_dir).expanduser().resolve().as_uri()\\n     root_uri = re.sub(\\n         RE_PATH_ANCHOR, lambda m: \"file://{}\".format(m.group(1).lower()), root_uri\\n     )\\n@@ -33,3 +34,12 @@ def file_uri_to_path(file_uri):\\n     else:\\n         result = file_uri_path_unquoted  # pragma: no cover\\n     return result\\n+\\n+\\n+def is_relative(root: Union[str, Path], path: Union[str, Path]) -> bool:\\n+    \"\"\"Return if path is relative to root\"\"\"\\n+    try:\\n+        Path(path).resolve().relative_to(Path(root).resolve())\\n+        return True\\n+    except ValueError:\\n+        return False', '@@ -47,7 +47,6 @@ def maybe_change_version(self, dry: bool):\\n             self.change_version(new_version=version, dry=dry)\\n \\n     def change_version(self, new_version: str, dry: bool):\\n-\\n         changelog = CHANGELOG.read_text(encoding=\"utf-8\")\\n         if new_version not in changelog:\\n             raise Exception('], 'file': ['python_packages/jupyter_lsp/jupyter_lsp/handlers.py', 'python_packages/jupyter_lsp/jupyter_lsp/serverextension.py', 'python_packages/jupyter_lsp/jupyter_lsp/virtual_documents_shadow.py', 'python_packages/jupyter_lsp/jupyter_lsp/paths.py', 'scripts/bump_versions.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('b75dec18-d113-4e87-8476-b8d8bbada565'), UUID('2680b9da-1319-4a89-af40-3df1fc78746e'), UUID('0c57a2ab-fa6b-4d4a-ba8e-913531ab9961'), UUID('46f02a9f-f533-4d87-83c3-a0b535cee267'), UUID('c4bfef5b-7af1-4dc5-8367-ea8bfbcc2a40')]}\n",
      "ERROR:root:Error in {'repo': 'jupyter-lsp/jupyterlab-lsp', 'vulnerability_id': '2024-22415', 'commit': '4ad12f204ad0b85580fc32137c647baaff044e95', 'commit_source': 'github', 'cwe_id': ['CWE-23', 'CWE-23', 'CWE-23', 'CWE-23'], 'patch': ['@@ -2,14 +2,32 @@\\n \"\"\"\\n from typing import Optional, Text\\n \\n+from jupyter_core.utils import ensure_async\\n from jupyter_server.base.handlers import APIHandler\\n-from jupyter_server.base.zmqhandlers import WebSocketHandler, WebSocketMixin\\n from jupyter_server.utils import url_path_join as ujoin\\n+from tornado import web\\n+from tornado.websocket import WebSocketHandler\\n+\\n+try:\\n+    from jupyter_server.auth.decorator import authorized\\n+except ImportError:\\n+\\n+    def authorized(method):  # type: ignore\\n+        \"\"\"A no-op fallback for `jupyter_server 1.x`\"\"\"\\n+        return method\\n+\\n+\\n+try:\\n+    from jupyter_server.base.websocket import WebSocketMixin\\n+except ImportError:\\n+    from jupyter_server.base.zmqhandlers import WebSocketMixin\\n \\n from .manager import LanguageServerManager\\n from .schema import SERVERS_RESPONSE\\n from .specs.utils import censored_spec\\n \\n+AUTH_RESOURCE = \"lsp\"\\n+\\n \\n class BaseHandler(APIHandler):\\n     manager = None  # type: LanguageServerManager\\n@@ -21,10 +39,43 @@ def initialize(self, manager: LanguageServerManager):\\n class LanguageServerWebSocketHandler(  # type: ignore\\n     WebSocketMixin, WebSocketHandler, BaseHandler\\n ):\\n-    \"\"\"Setup tornado websocket to route to language server sessions\"\"\"\\n+    \"\"\"Setup tornado websocket to route to language server sessions.\\n+\\n+    The logic of `get` and `pre_get` methods is derived from jupyter-server ws handlers,\\n+    and should be kept in sync to follow best practice established by upstream; see:\\n+    https://github.com/jupyter-server/jupyter_server/blob/v2.12.5/jupyter_server/services/kernels/websocket.py#L36\\n+    \"\"\"\\n+\\n+    auth_resource = AUTH_RESOURCE\\n \\n     language_server: Optional[Text] = None\\n \\n+    async def pre_get(self):\\n+        \"\"\"Handle a pre_get.\"\"\"\\n+        # authenticate first\\n+        # authenticate the request before opening the websocket\\n+        user = self.current_user\\n+        if user is None:\\n+            self.log.warning(\"Couldn\\'t authenticate WebSocket connection\")\\n+            raise web.HTTPError(403)\\n+\\n+        if not hasattr(self, \"authorizer\"):\\n+            return\\n+\\n+        # authorize the user.\\n+        is_authorized = await ensure_async(\\n+            self.authorizer.is_authorized(self, user, \"execute\", AUTH_RESOURCE)\\n+        )\\n+        if not is_authorized:\\n+            raise web.HTTPError(403)\\n+\\n+    async def get(self, *args, **kwargs):\\n+        \"\"\"Get an event socket.\"\"\"\\n+        await self.pre_get()\\n+        res = super().get(*args, **kwargs)\\n+        if res is not None:\\n+            await res\\n+\\n     async def open(self, language_server):\\n         await self.manager.ready()\\n         self.language_server = language_server\\n@@ -47,11 +98,11 @@ class LanguageServersHandler(BaseHandler):\\n     Response should conform to schema in schema/servers.schema.json\\n     \"\"\"\\n \\n+    auth_resource = AUTH_RESOURCE\\n     validator = SERVERS_RESPONSE\\n \\n-    def initialize(self, *args, **kwargs):\\n-        super().initialize(*args, **kwargs)\\n-\\n+    @web.authenticated\\n+    @authorized\\n     async def get(self):\\n         \"\"\"finish with the JSON representations of the sessions\"\"\"\\n         await self.manager.ready()', '@@ -4,6 +4,7 @@\\n from pathlib import Path\\n \\n import traitlets\\n+from tornado import ioloop\\n \\n from .handlers import add_handlers\\n from .manager import LanguageServerManager\\n@@ -73,4 +74,11 @@ def load_jupyter_server_extension(nbapp):\\n     page_config.update(rootUri=root_uri, virtualDocumentsUri=virtual_documents_uri)\\n \\n     add_handlers(nbapp)\\n-    nbapp.io_loop.call_later(0, initialize, nbapp, virtual_documents_uri)\\n+\\n+    if hasattr(nbapp, \"io_loop\"):\\n+        io_loop = nbapp.io_loop\\n+    else:\\n+        # handle jupyter_server 1.x\\n+        io_loop = ioloop.IOLoop.current()\\n+\\n+    io_loop.call_later(0, initialize, nbapp, virtual_documents_uri)', '@@ -8,7 +8,7 @@\\n from tornado.gen import convert_yielded\\n \\n from .manager import lsp_message_listener\\n-from .paths import file_uri_to_path\\n+from .paths import file_uri_to_path, is_relative\\n from .types import LanguageServerManagerAPI\\n \\n # TODO: make configurable\\n@@ -171,6 +171,11 @@ async def shadow_virtual_documents(scope, message, language_server, manager):\\n             initialized = True\\n \\n         path = file_uri_to_path(uri)\\n+        if not is_relative(shadow_filesystem, path):\\n+            raise ShadowFilesystemError(\\n+                f\"Path {path} is not relative to shadow filesystem root\"\\n+            )\\n+\\n         editable_file = EditableFile(path)\\n \\n         await editable_file.read()', '@@ -1,6 +1,7 @@\\n import os\\n-import pathlib\\n import re\\n+from pathlib import Path\\n+from typing import Union\\n from urllib.parse import unquote, urlparse\\n \\n RE_PATH_ANCHOR = r\"^file://([^/]+|/[A-Z]:)\"\\n@@ -12,7 +13,7 @@ def normalized_uri(root_dir):\\n     Special care must be taken around windows paths: the canonical form of\\n     windows drives and UNC paths is lower case\\n     \"\"\"\\n-    root_uri = pathlib.Path(root_dir).expanduser().resolve().as_uri()\\n+    root_uri = Path(root_dir).expanduser().resolve().as_uri()\\n     root_uri = re.sub(\\n         RE_PATH_ANCHOR, lambda m: \"file://{}\".format(m.group(1).lower()), root_uri\\n     )\\n@@ -33,3 +34,12 @@ def file_uri_to_path(file_uri):\\n     else:\\n         result = file_uri_path_unquoted  # pragma: no cover\\n     return result\\n+\\n+\\n+def is_relative(root: Union[str, Path], path: Union[str, Path]) -> bool:\\n+    \"\"\"Return if path is relative to root\"\"\"\\n+    try:\\n+        Path(path).resolve().relative_to(Path(root).resolve())\\n+        return True\\n+    except ValueError:\\n+        return False', '@@ -47,7 +47,6 @@ def maybe_change_version(self, dry: bool):\\n             self.change_version(new_version=version, dry=dry)\\n \\n     def change_version(self, new_version: str, dry: bool):\\n-\\n         changelog = CHANGELOG.read_text(encoding=\"utf-8\")\\n         if new_version not in changelog:\\n             raise Exception('], 'file': ['python_packages/jupyter_lsp/jupyter_lsp/handlers.py', 'python_packages/jupyter_lsp/jupyter_lsp/serverextension.py', 'python_packages/jupyter_lsp/jupyter_lsp/virtual_documents_shadow.py', 'python_packages/jupyter_lsp/jupyter_lsp/paths.py', 'scripts/bump_versions.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('b75dec18-d113-4e87-8476-b8d8bbada565'), UUID('2680b9da-1319-4a89-af40-3df1fc78746e'), UUID('0c57a2ab-fa6b-4d4a-ba8e-913531ab9961'), UUID('46f02a9f-f533-4d87-83c3-a0b535cee267'), UUID('c4bfef5b-7af1-4dc5-8367-ea8bfbcc2a40')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 55:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 63:0: <line number missing in source>\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 646/1800 [06:48<04:29,  4.29it/s]ERROR:src.process_code_changes:Error processing commit b78ec09f4582e363f6f449df6f987127e126c311\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-3456', 'commit': 'b78ec09f4582e363f6f449df6f987127e126c311', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': ['@@ -19,8 +19,6 @@\\n from markupsafe import Markup\\r\\n from wtforms.form import Form\\r\\n \\r\\n-SUBMIT_METHODS = {\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\'}\\r\\n-\\r\\n \\r\\n class _ProxyFormdata:\\r\\n     \"\"\"\\r\\n@@ -65,7 +63,7 @@ def is_submitted(self):\\n         Consider the form submitted if there is an active request and\\r\\n         the method is ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\\r\\n         \"\"\"\\r\\n-        return cherrypy.request.method in SUBMIT_METHODS\\r\\n+        return cherrypy.request.method == \\'POST\\'\\r\\n \\r\\n     def validate_on_submit(self):\\r\\n         \"\"\"\\r', '@@ -62,7 +62,7 @@ class LoginPage(Controller):\\n \\n     @cherrypy.expose()\\n     @cherrypy.tools.auth_mfa(on=False)\\n-    @cherrypy.tools.ratelimit()\\n+    @cherrypy.tools.ratelimit(methods=[\\'POST\\'])\\n     def index(self, **kwargs):\\n         \"\"\"\\n         Called by auth_form to generate the /login/ page.', \"@@ -70,7 +70,7 @@ def validate(self, extra_validators=None):\\n \\n class MfaPage(Controller):\\n     @cherrypy.expose()\\n-    @cherrypy.tools.ratelimit()\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def index(self, **kwargs):\\n         form = MfaForm()\\n \", '@@ -1,6 +1,6 @@\\n # -*- coding: utf-8 -*-\\n-# rdiffweb, A web interface to rdiff-backup repositories\\n-# Copyright (C) 2012-2021 rdiffweb contributors\\n+# udb, A web interface to manage IT network\\n+# Copyright (C) 2022 IKUS Software inc.\\n #\\n # This program is free software: you can redistribute it and/or modify\\n # it under the terms of the GNU General Public License as published by\\n@@ -33,13 +33,13 @@ class _DataStore:\\n     def __init__(self, **kwargs):\\n         self._locks = {}\\n \\n-    def get_and_increment(self, token, delay):\\n+    def get_and_increment(self, token, delay, hit=1):\\n         lock = self._locks.setdefault(token, threading.RLock())\\n         with lock:\\n             tracker = self._load(token)\\n             if tracker is None or tracker.timeout < time.time():\\n                 tracker = Tracker(token=token, hits=0, timeout=int(time.time() + delay))\\n-            tracker = tracker._replace(hits=tracker.hits + 1)\\n+            tracker = tracker._replace(hits=tracker.hits + hit)\\n             self._save(tracker)\\n         return tracker.hits\\n \\n@@ -97,7 +97,7 @@ def _load(self, token):\\n                 return pickle.load(f)\\n             finally:\\n                 f.close()\\n-        except (IOError, EOFError):\\n+        except Exception:\\n             # Drop session data if invalid\\n             pass\\n         return None\\n@@ -111,43 +111,74 @@ def _save(self, tracker):\\n             f.close()\\n \\n \\n-def check_ratelimit(delay=60, anonymous_limit=0, registered_limit=0, rate_exceed_status=429, debug=False, **conf):\\n+def check_ratelimit(\\n+    delay=3600, limit=25, return_status=429, logout=False, scope=None, methods=None, debug=False, hit=1, **conf\\n+):\\n     \"\"\"\\n-    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request).\\n+    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request). After 25 request within the same hour.\\n+\\n+    Arguments:\\n+        delay:         Time window for analysis in seconds. Default per hour (3600 seconds)\\n+        limit:         Number of request allowed for an entry point. Default 25\\n+        return_status: HTTP Error code to return.\\n+        logout:        True to logout user when limit is reached\\n+        scope:         if specify, define the scope of rate limit. Default to path_info.\\n+        methods:       if specify, only the methods in the list will be rate limited.\\n+    \"\"\"\\n+    assert delay > 0, \\'invalid delay\\'\\n \\n-    Usage:\\n+    # Check if limit is enabled\\n+    if limit <= 0:\\n+        return\\n \\n-    @cherrypy.tools.ratelimit(on=True, anonymous_limit=5, registered_limit=50, storage_class=FileRateLimit, storage_path=\\'/tmp\\')\\n-    def index(self):\\n-        pass\\n-    \"\"\"\\n+    # Check if this \\'method\\' should be rate limited\\n+    request = cherrypy.request\\n+    if methods is not None and request.method not in methods:\\n+        if debug:\\n+            cherrypy.log(\\n+                \\'skip rate limit for HTTP method %s\\' % (request.method,),\\n+                \\'TOOLS.RATELIMIT\\',\\n+            )\\n+        return\\n \\n     # If datastore is not pass as configuration, create it for the first time.\\n-    datastore = getattr(cherrypy, \\'_ratelimit_datastore\\', None)\\n+    datastore = getattr(cherrypy.request.app, \\'_ratelimit_datastore\\', None)\\n     if datastore is None:\\n         # Create storage using storage class\\n         storage_class = conf.get(\\'storage_class\\', RamRateLimit)\\n         datastore = storage_class(**conf)\\n-        cherrypy._ratelimit_datastore = datastore\\n+        cherrypy.request.app._ratelimit_datastore = datastore\\n \\n     # If user is authenticated, use the username else use the ip address\\n-    token = cherrypy.request.login or cherrypy.request.remote.ip\\n-\\n-    # Get the real limit depending of user login.\\n-    limit = registered_limit if cherrypy.request.login else anonymous_limit\\n-    if limit is None or limit <= 0:\\n-        return\\n+    token = (request.login or request.remote.ip) + \\'.\\' + (scope or request.path_info)\\n \\n     # Get hits count using datastore.\\n-    hits = datastore.get_and_increment(token, delay)\\n+    hits = datastore.get_and_increment(token, delay, hit)\\n     if debug:\\n         cherrypy.log(\\n-            \\'check and increase rate limit for token %s, limit %s, hits %s\\' % (token, limit, hits), \\'TOOLS.RATELIMIT\\'\\n+            \\'check and increase rate limit for scope %s, limit %s, hits %s\\' % (token, limit, hits), \\'TOOLS.RATELIMIT\\'\\n         )\\n \\n     # Verify user has not exceeded rate limit\\n     if limit <= hits:\\n-        raise cherrypy.HTTPError(rate_exceed_status)\\n+        if logout:\\n+            if hasattr(cherrypy, \\'session\\'):\\n+                cherrypy.session.clear()\\n+            raise cherrypy.HTTPRedirect(\"/\")\\n+\\n+        raise cherrypy.HTTPError(return_status)\\n+\\n+\\n+def hit(hit=1):\\n+    \"\"\"\\n+    May be called directly by handlers to add a hit for the given request.\\n+    \"\"\"\\n+    conf = cherrypy.tools.ratelimit._merged_args()\\n+    conf[\\'hit\\'] = hit\\n+    cherrypy.tools.ratelimit.callable(**conf)\\n \\n \\n cherrypy.tools.ratelimit = cherrypy.Tool(\\'before_handler\\', check_ratelimit, priority=60)\\n+\\n+\\n+cherrypy.tools.ratelimit.hit = hit', '@@ -64,9 +64,14 @@ def _checkpassword(realm, username, password):\\n             return True\\n         # Disable password authentication for MFA\\n         if userobj.mfa == UserObject.ENABLED_MFA:\\n+            cherrypy.tools.ratelimit.hit()\\n             return False\\n     # Otherwise validate username password\\n-    return any(cherrypy.engine.publish(\\'login\\', username, password))\\n+    valid = any(cherrypy.engine.publish(\\'login\\', username, password))\\n+    if not valid:\\n+        # When invalid, we need to increase the rate limit.\\n+        cherrypy.tools.ratelimit.hit()\\n+    return valid\\n \\n \\n class ApiCurrentUser(Controller):\\n@@ -99,9 +104,9 @@ def default(self):\\n @cherrypy.tools.auth_basic(realm=\\'rdiffweb\\', checkpassword=_checkpassword, priority=70)\\n @cherrypy.tools.auth_form(on=False)\\n @cherrypy.tools.auth_mfa(on=False)\\n-@cherrypy.tools.sessions(on=False)\\n @cherrypy.tools.i18n(on=False)\\n-@cherrypy.tools.ratelimit()\\n+@cherrypy.tools.ratelimit(scope=\\'rdiffweb-api\\', hit=0, priority=69)\\n+@cherrypy.tools.sessions(on=False)\\n class ApiPage(Controller):\\n     \"\"\"\\n     This class provide a restful API to access some of the rdiffweb resources.', \"@@ -30,10 +30,6 @@ class RevokeSessionForm(CherryForm):\\n     action = StringField(validators=[validators.regexp('delete')])\\n     number = IntegerField(validators=[validators.data_required()])\\n \\n-    @property\\n-    def app(self):\\n-        return cherrypy.request.app\\n-\\n \\n class PagePrefSession(Controller):\\n     @cherrypy.expose\", \"@@ -209,8 +209,8 @@ def __init__(self, cfg):\\n                 'tools.sessions.persistent': False,  # auth_form should update this.\\n                 'tools.auth_form.timeout': cfg.session_persistent_timeout,  # minutes\\n                 'tools.ratelimit.debug': cfg.debug,\\n-                'tools.ratelimit.delay': 60,\\n-                'tools.ratelimit.anonymous_limit': cfg.rate_limit,\\n+                'tools.ratelimit.delay': 3600,\\n+                'tools.ratelimit.limit': cfg.rate_limit,\\n                 'tools.ratelimit.storage_class': rate_limit_storage_class,\\n                 'tools.ratelimit.storage_path': cfg.rate_limit_dir,\\n             },\", \"@@ -30,10 +30,6 @@ class RevokeSessionForm(CherryForm):\\n     action = StringField(validators=[validators.regexp('delete')])\\n     number = IntegerField(validators=[validators.data_required()])\\n \\n-    @property\\n-    def app(self):\\n-        return cherrypy.request.app\\n-\\n \\n @cherrypy.tools.is_admin()\\n class AdminSessionPage(Controller):\", \"@@ -404,8 +404,8 @@ def get_parser():\\n         '--rate-limit',\\n         metavar='LIMIT',\\n         type=int,\\n-        default=30,\\n-        help='maximum number of requests per minute that can be made by an IP address for an unauthenticated connection. When this limit is reached, an HTTP 429 message is returned to the user. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\\n+        default=20,\\n+        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\\n     )\\n \\n     parser.add(\", '@@ -29,10 +29,6 @@\\n from rdiffweb.core.model import UserObject\\n from rdiffweb.tools.i18n import gettext_lazy as _\\n \\n-# Maximum number of password change attempt before logout\\n-CHANGE_PASSWORD_MAX_ATTEMPT = 5\\n-CHANGE_PASSWORD_ATTEMPTS = \\'change_password_attempts\\'\\n-\\n \\n class UserProfileForm(CherryForm):\\n     action = HiddenField(default=\\'set_profile_info\\')\\n@@ -99,23 +95,8 @@ def populate_obj(self, user):\\n         # Check if current password is \"valid\" if Not, rate limit the\\n         # number of attempts and logout user after too many invalid attempts.\\n         if not user.validate_password(self.current.data):\\n-            cherrypy.session[CHANGE_PASSWORD_ATTEMPTS] = cherrypy.session.get(CHANGE_PASSWORD_ATTEMPTS, 0) + 1\\n-            attempts = cherrypy.session[CHANGE_PASSWORD_ATTEMPTS]\\n-            if attempts >= CHANGE_PASSWORD_MAX_ATTEMPT:\\n-                cherrypy.session.clear()\\n-                cherrypy.session.regenerate()\\n-                flash(\\n-                    _(\"You were logged out because you entered the wrong password too many times.\"),\\n-                    level=\\'warning\\',\\n-                )\\n-                raise cherrypy.HTTPRedirect(\\'/login/\\')\\n             self.current.errors = [_(\"Wrong current password.\")]\\n             return False\\n-\\n-        # Clear number of attempts\\n-        if CHANGE_PASSWORD_ATTEMPTS in cherrypy.session:\\n-            del cherrypy.session[CHANGE_PASSWORD_ATTEMPTS]\\n-\\n         try:\\n             user.set_password(self.new.data)\\n             return True\\n@@ -151,6 +132,7 @@ class PagePrefsGeneral(Controller):\\n     \"\"\"\\n \\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=[\\'POST\\'], logout=True)\\n     def default(self, **kwargs):\\n         # Process the parameters.\\n         profile_form = UserProfileForm(obj=self.app.currentuser)'], 'file': ['rdiffweb/controller/form.py', 'rdiffweb/controller/page_login.py', 'rdiffweb/controller/page_mfa.py', 'rdiffweb/tools/ratelimit.py', 'rdiffweb/controller/api.py', 'rdiffweb/controller/page_pref_session.py', 'rdiffweb/rdw_app.py', 'rdiffweb/controller/page_admin_session.py', 'rdiffweb/core/config.py', 'rdiffweb/controller/page_pref_general.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('decf573a-e4a1-4404-8075-0eae63a0a451'), UUID('cb610d4d-950a-42f2-9dc5-151498938e84'), UUID('4c968548-7a72-40cb-a2a3-6df25a4d16e1'), UUID('b81eb96c-d0ad-414a-bfa7-a8ee557f1857'), UUID('f8996275-2244-4e79-aabe-5fb4bba83719'), UUID('bc5c43f7-30cc-419b-8422-ce5b883be1e3'), UUID('7f869a55-eacd-4cb1-b8e2-ffc6949379ee'), UUID('7b01bdb4-265b-4963-a3cd-b4a9903e3664'), UUID('97fb618a-732a-454b-96e0-fcb1baeec18a'), UUID('2501144f-a40e-41fd-9d91-9ef3e988e39d')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-3456', 'commit': 'b78ec09f4582e363f6f449df6f987127e126c311', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': ['@@ -19,8 +19,6 @@\\n from markupsafe import Markup\\r\\n from wtforms.form import Form\\r\\n \\r\\n-SUBMIT_METHODS = {\\'POST\\', \\'PUT\\', \\'PATCH\\', \\'DELETE\\'}\\r\\n-\\r\\n \\r\\n class _ProxyFormdata:\\r\\n     \"\"\"\\r\\n@@ -65,7 +63,7 @@ def is_submitted(self):\\n         Consider the form submitted if there is an active request and\\r\\n         the method is ``POST``, ``PUT``, ``PATCH``, or ``DELETE``.\\r\\n         \"\"\"\\r\\n-        return cherrypy.request.method in SUBMIT_METHODS\\r\\n+        return cherrypy.request.method == \\'POST\\'\\r\\n \\r\\n     def validate_on_submit(self):\\r\\n         \"\"\"\\r', '@@ -62,7 +62,7 @@ class LoginPage(Controller):\\n \\n     @cherrypy.expose()\\n     @cherrypy.tools.auth_mfa(on=False)\\n-    @cherrypy.tools.ratelimit()\\n+    @cherrypy.tools.ratelimit(methods=[\\'POST\\'])\\n     def index(self, **kwargs):\\n         \"\"\"\\n         Called by auth_form to generate the /login/ page.', \"@@ -70,7 +70,7 @@ def validate(self, extra_validators=None):\\n \\n class MfaPage(Controller):\\n     @cherrypy.expose()\\n-    @cherrypy.tools.ratelimit()\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def index(self, **kwargs):\\n         form = MfaForm()\\n \", '@@ -1,6 +1,6 @@\\n # -*- coding: utf-8 -*-\\n-# rdiffweb, A web interface to rdiff-backup repositories\\n-# Copyright (C) 2012-2021 rdiffweb contributors\\n+# udb, A web interface to manage IT network\\n+# Copyright (C) 2022 IKUS Software inc.\\n #\\n # This program is free software: you can redistribute it and/or modify\\n # it under the terms of the GNU General Public License as published by\\n@@ -33,13 +33,13 @@ class _DataStore:\\n     def __init__(self, **kwargs):\\n         self._locks = {}\\n \\n-    def get_and_increment(self, token, delay):\\n+    def get_and_increment(self, token, delay, hit=1):\\n         lock = self._locks.setdefault(token, threading.RLock())\\n         with lock:\\n             tracker = self._load(token)\\n             if tracker is None or tracker.timeout < time.time():\\n                 tracker = Tracker(token=token, hits=0, timeout=int(time.time() + delay))\\n-            tracker = tracker._replace(hits=tracker.hits + 1)\\n+            tracker = tracker._replace(hits=tracker.hits + hit)\\n             self._save(tracker)\\n         return tracker.hits\\n \\n@@ -97,7 +97,7 @@ def _load(self, token):\\n                 return pickle.load(f)\\n             finally:\\n                 f.close()\\n-        except (IOError, EOFError):\\n+        except Exception:\\n             # Drop session data if invalid\\n             pass\\n         return None\\n@@ -111,43 +111,74 @@ def _save(self, tracker):\\n             f.close()\\n \\n \\n-def check_ratelimit(delay=60, anonymous_limit=0, registered_limit=0, rate_exceed_status=429, debug=False, **conf):\\n+def check_ratelimit(\\n+    delay=3600, limit=25, return_status=429, logout=False, scope=None, methods=None, debug=False, hit=1, **conf\\n+):\\n     \"\"\"\\n-    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request).\\n+    Verify the ratelimit. By default return a 429 HTTP error code (Too Many Request). After 25 request within the same hour.\\n+\\n+    Arguments:\\n+        delay:         Time window for analysis in seconds. Default per hour (3600 seconds)\\n+        limit:         Number of request allowed for an entry point. Default 25\\n+        return_status: HTTP Error code to return.\\n+        logout:        True to logout user when limit is reached\\n+        scope:         if specify, define the scope of rate limit. Default to path_info.\\n+        methods:       if specify, only the methods in the list will be rate limited.\\n+    \"\"\"\\n+    assert delay > 0, \\'invalid delay\\'\\n \\n-    Usage:\\n+    # Check if limit is enabled\\n+    if limit <= 0:\\n+        return\\n \\n-    @cherrypy.tools.ratelimit(on=True, anonymous_limit=5, registered_limit=50, storage_class=FileRateLimit, storage_path=\\'/tmp\\')\\n-    def index(self):\\n-        pass\\n-    \"\"\"\\n+    # Check if this \\'method\\' should be rate limited\\n+    request = cherrypy.request\\n+    if methods is not None and request.method not in methods:\\n+        if debug:\\n+            cherrypy.log(\\n+                \\'skip rate limit for HTTP method %s\\' % (request.method,),\\n+                \\'TOOLS.RATELIMIT\\',\\n+            )\\n+        return\\n \\n     # If datastore is not pass as configuration, create it for the first time.\\n-    datastore = getattr(cherrypy, \\'_ratelimit_datastore\\', None)\\n+    datastore = getattr(cherrypy.request.app, \\'_ratelimit_datastore\\', None)\\n     if datastore is None:\\n         # Create storage using storage class\\n         storage_class = conf.get(\\'storage_class\\', RamRateLimit)\\n         datastore = storage_class(**conf)\\n-        cherrypy._ratelimit_datastore = datastore\\n+        cherrypy.request.app._ratelimit_datastore = datastore\\n \\n     # If user is authenticated, use the username else use the ip address\\n-    token = cherrypy.request.login or cherrypy.request.remote.ip\\n-\\n-    # Get the real limit depending of user login.\\n-    limit = registered_limit if cherrypy.request.login else anonymous_limit\\n-    if limit is None or limit <= 0:\\n-        return\\n+    token = (request.login or request.remote.ip) + \\'.\\' + (scope or request.path_info)\\n \\n     # Get hits count using datastore.\\n-    hits = datastore.get_and_increment(token, delay)\\n+    hits = datastore.get_and_increment(token, delay, hit)\\n     if debug:\\n         cherrypy.log(\\n-            \\'check and increase rate limit for token %s, limit %s, hits %s\\' % (token, limit, hits), \\'TOOLS.RATELIMIT\\'\\n+            \\'check and increase rate limit for scope %s, limit %s, hits %s\\' % (token, limit, hits), \\'TOOLS.RATELIMIT\\'\\n         )\\n \\n     # Verify user has not exceeded rate limit\\n     if limit <= hits:\\n-        raise cherrypy.HTTPError(rate_exceed_status)\\n+        if logout:\\n+            if hasattr(cherrypy, \\'session\\'):\\n+                cherrypy.session.clear()\\n+            raise cherrypy.HTTPRedirect(\"/\")\\n+\\n+        raise cherrypy.HTTPError(return_status)\\n+\\n+\\n+def hit(hit=1):\\n+    \"\"\"\\n+    May be called directly by handlers to add a hit for the given request.\\n+    \"\"\"\\n+    conf = cherrypy.tools.ratelimit._merged_args()\\n+    conf[\\'hit\\'] = hit\\n+    cherrypy.tools.ratelimit.callable(**conf)\\n \\n \\n cherrypy.tools.ratelimit = cherrypy.Tool(\\'before_handler\\', check_ratelimit, priority=60)\\n+\\n+\\n+cherrypy.tools.ratelimit.hit = hit', '@@ -64,9 +64,14 @@ def _checkpassword(realm, username, password):\\n             return True\\n         # Disable password authentication for MFA\\n         if userobj.mfa == UserObject.ENABLED_MFA:\\n+            cherrypy.tools.ratelimit.hit()\\n             return False\\n     # Otherwise validate username password\\n-    return any(cherrypy.engine.publish(\\'login\\', username, password))\\n+    valid = any(cherrypy.engine.publish(\\'login\\', username, password))\\n+    if not valid:\\n+        # When invalid, we need to increase the rate limit.\\n+        cherrypy.tools.ratelimit.hit()\\n+    return valid\\n \\n \\n class ApiCurrentUser(Controller):\\n@@ -99,9 +104,9 @@ def default(self):\\n @cherrypy.tools.auth_basic(realm=\\'rdiffweb\\', checkpassword=_checkpassword, priority=70)\\n @cherrypy.tools.auth_form(on=False)\\n @cherrypy.tools.auth_mfa(on=False)\\n-@cherrypy.tools.sessions(on=False)\\n @cherrypy.tools.i18n(on=False)\\n-@cherrypy.tools.ratelimit()\\n+@cherrypy.tools.ratelimit(scope=\\'rdiffweb-api\\', hit=0, priority=69)\\n+@cherrypy.tools.sessions(on=False)\\n class ApiPage(Controller):\\n     \"\"\"\\n     This class provide a restful API to access some of the rdiffweb resources.', \"@@ -30,10 +30,6 @@ class RevokeSessionForm(CherryForm):\\n     action = StringField(validators=[validators.regexp('delete')])\\n     number = IntegerField(validators=[validators.data_required()])\\n \\n-    @property\\n-    def app(self):\\n-        return cherrypy.request.app\\n-\\n \\n class PagePrefSession(Controller):\\n     @cherrypy.expose\", \"@@ -209,8 +209,8 @@ def __init__(self, cfg):\\n                 'tools.sessions.persistent': False,  # auth_form should update this.\\n                 'tools.auth_form.timeout': cfg.session_persistent_timeout,  # minutes\\n                 'tools.ratelimit.debug': cfg.debug,\\n-                'tools.ratelimit.delay': 60,\\n-                'tools.ratelimit.anonymous_limit': cfg.rate_limit,\\n+                'tools.ratelimit.delay': 3600,\\n+                'tools.ratelimit.limit': cfg.rate_limit,\\n                 'tools.ratelimit.storage_class': rate_limit_storage_class,\\n                 'tools.ratelimit.storage_path': cfg.rate_limit_dir,\\n             },\", \"@@ -30,10 +30,6 @@ class RevokeSessionForm(CherryForm):\\n     action = StringField(validators=[validators.regexp('delete')])\\n     number = IntegerField(validators=[validators.data_required()])\\n \\n-    @property\\n-    def app(self):\\n-        return cherrypy.request.app\\n-\\n \\n @cherrypy.tools.is_admin()\\n class AdminSessionPage(Controller):\", \"@@ -404,8 +404,8 @@ def get_parser():\\n         '--rate-limit',\\n         metavar='LIMIT',\\n         type=int,\\n-        default=30,\\n-        help='maximum number of requests per minute that can be made by an IP address for an unauthenticated connection. When this limit is reached, an HTTP 429 message is returned to the user. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\\n+        default=20,\\n+        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\\n     )\\n \\n     parser.add(\", '@@ -29,10 +29,6 @@\\n from rdiffweb.core.model import UserObject\\n from rdiffweb.tools.i18n import gettext_lazy as _\\n \\n-# Maximum number of password change attempt before logout\\n-CHANGE_PASSWORD_MAX_ATTEMPT = 5\\n-CHANGE_PASSWORD_ATTEMPTS = \\'change_password_attempts\\'\\n-\\n \\n class UserProfileForm(CherryForm):\\n     action = HiddenField(default=\\'set_profile_info\\')\\n@@ -99,23 +95,8 @@ def populate_obj(self, user):\\n         # Check if current password is \"valid\" if Not, rate limit the\\n         # number of attempts and logout user after too many invalid attempts.\\n         if not user.validate_password(self.current.data):\\n-            cherrypy.session[CHANGE_PASSWORD_ATTEMPTS] = cherrypy.session.get(CHANGE_PASSWORD_ATTEMPTS, 0) + 1\\n-            attempts = cherrypy.session[CHANGE_PASSWORD_ATTEMPTS]\\n-            if attempts >= CHANGE_PASSWORD_MAX_ATTEMPT:\\n-                cherrypy.session.clear()\\n-                cherrypy.session.regenerate()\\n-                flash(\\n-                    _(\"You were logged out because you entered the wrong password too many times.\"),\\n-                    level=\\'warning\\',\\n-                )\\n-                raise cherrypy.HTTPRedirect(\\'/login/\\')\\n             self.current.errors = [_(\"Wrong current password.\")]\\n             return False\\n-\\n-        # Clear number of attempts\\n-        if CHANGE_PASSWORD_ATTEMPTS in cherrypy.session:\\n-            del cherrypy.session[CHANGE_PASSWORD_ATTEMPTS]\\n-\\n         try:\\n             user.set_password(self.new.data)\\n             return True\\n@@ -151,6 +132,7 @@ class PagePrefsGeneral(Controller):\\n     \"\"\"\\n \\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=[\\'POST\\'], logout=True)\\n     def default(self, **kwargs):\\n         # Process the parameters.\\n         profile_form = UserProfileForm(obj=self.app.currentuser)'], 'file': ['rdiffweb/controller/form.py', 'rdiffweb/controller/page_login.py', 'rdiffweb/controller/page_mfa.py', 'rdiffweb/tools/ratelimit.py', 'rdiffweb/controller/api.py', 'rdiffweb/controller/page_pref_session.py', 'rdiffweb/rdw_app.py', 'rdiffweb/controller/page_admin_session.py', 'rdiffweb/core/config.py', 'rdiffweb/controller/page_pref_general.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('decf573a-e4a1-4404-8075-0eae63a0a451'), UUID('cb610d4d-950a-42f2-9dc5-151498938e84'), UUID('4c968548-7a72-40cb-a2a3-6df25a4d16e1'), UUID('b81eb96c-d0ad-414a-bfa7-a8ee557f1857'), UUID('f8996275-2244-4e79-aabe-5fb4bba83719'), UUID('bc5c43f7-30cc-419b-8422-ce5b883be1e3'), UUID('7f869a55-eacd-4cb1-b8e2-ffc6949379ee'), UUID('7b01bdb4-265b-4963-a3cd-b4a9903e3664'), UUID('97fb618a-732a-454b-96e0-fcb1baeec18a'), UUID('2501144f-a40e-41fd-9d91-9ef3e988e39d')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 659/1800 [06:51<04:33,  4.17it/s]ERROR:src.process_code_changes:Error processing commit 3f3d887a6844ec2db743fee64c9e53e04d39a368\n",
      "ERROR:src.process_code_changes:{'repo': 'django/django', 'vulnerability_id': '2013-1443', 'commit': '3f3d887a6844ec2db743fee64c9e53e04d39a368', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -1,3 +1,4 @@\\n+import functools\\n import hashlib\\n \\n from django.conf import settings\\n@@ -11,10 +12,23 @@\\n \\n \\n UNUSABLE_PASSWORD = \\'!\\'  # This will never be a valid encoded hash\\n+MAXIMUM_PASSWORD_LENGTH = 4096  # The maximum length a password can be to prevent DoS\\n HASHERS = None  # lazily loaded from PASSWORD_HASHERS\\n PREFERRED_HASHER = None  # defaults to first item in PASSWORD_HASHERS\\n \\n \\n+def password_max_length(max_length):\\n+    def inner(fn):\\n+        @functools.wraps(fn)\\n+        def wrapper(self, password, *args, **kwargs):\\n+            if len(password) > max_length:\\n+                raise ValueError(\"Invalid password; Must be less than or equal\"\\n+                                 \" to %d bytes\" % max_length)\\n+            return fn(self, password, *args, **kwargs)\\n+        return wrapper\\n+    return inner\\n+\\n+\\n def is_password_usable(encoded):\\n     return (encoded is not None and encoded != UNUSABLE_PASSWORD)\\n \\n@@ -202,6 +216,7 @@ class PBKDF2PasswordHasher(BasePasswordHasher):\\n     iterations = 10000\\n     digest = hashlib.sha256\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt, iterations=None):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n@@ -211,6 +226,7 @@ def encode(self, password, salt, iterations=None):\\n         hash = hash.encode(\\'base64\\').strip()\\n         return \"%s$%d$%s$%s\" % (self.algorithm, iterations, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, iterations, salt, hash = encoded.split(\\'$\\', 3)\\n         assert algorithm == self.algorithm\\n@@ -256,11 +272,13 @@ def salt(self):\\n         bcrypt = self._load_library()\\n         return bcrypt.gensalt(self.rounds)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         bcrypt = self._load_library()\\n         data = bcrypt.hashpw(password, salt)\\n         return \"%s$%s\" % (self.algorithm, data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, data = encoded.split(\\'$\\', 1)\\n         assert algorithm == self.algorithm\\n@@ -285,12 +303,14 @@ class SHA1PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"sha1\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.sha1(salt + password).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -313,12 +333,14 @@ class MD5PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"md5\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.md5(salt + password).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -349,11 +371,13 @@ class UnsaltedSHA1PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         hash = hashlib.sha1(password).hexdigest()\\n         return \\'sha1$$%s\\' % hash\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         encoded_2 = self.encode(password, \\'\\')\\n         return constant_time_compare(encoded, encoded_2)\\n@@ -383,10 +407,12 @@ class UnsaltedMD5PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         return hashlib.md5(password).hexdigest()\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         if len(encoded) == 37 and encoded.startswith(\\'md5$$\\'):\\n             encoded = encoded[5:]\\n@@ -412,13 +438,15 @@ class CryptPasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return get_random_string(2)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         crypt = self._load_library()\\n         assert len(salt) == 2\\n         data = crypt.crypt(password, salt)\\n         # we don\\'t need to store the salt, but Django used to do this\\n         return \"%s$%s$%s\" % (self.algorithm, \\'\\', data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         crypt = self._load_library()\\n         algorithm, salt, data = encoded.split(\\'$\\', 2)\\n@@ -433,4 +461,3 @@ def safe_summary(self, encoded):\\n             (_(\\'salt\\'), salt),\\n             (_(\\'hash\\'), mask_hash(data, show=3)),\\n         ])\\n-', '@@ -8,7 +8,10 @@\\n \\n from django.contrib.auth import authenticate\\n from django.contrib.auth.models import User\\n-from django.contrib.auth.hashers import UNUSABLE_PASSWORD, is_password_usable, get_hasher\\n+from django.contrib.auth.hashers import (\\n+    MAXIMUM_PASSWORD_LENGTH, UNUSABLE_PASSWORD,\\n+    is_password_usable, get_hasher\\n+)\\n from django.contrib.auth.tokens import default_token_generator\\n from django.contrib.sites.models import get_current_site\\n \\n@@ -70,10 +73,11 @@ class UserCreationForm(forms.ModelForm):\\n             \\'invalid\\': _(\"This value may contain only letters, numbers and \"\\n                          \"@/./+/-/_ characters.\")})\\n     password1 = forms.CharField(label=_(\"Password\"),\\n-        widget=forms.PasswordInput)\\n+        widget=forms.PasswordInput, max_length=MAXIMUM_PASSWORD_LENGTH)\\n     password2 = forms.CharField(label=_(\"Password confirmation\"),\\n         widget=forms.PasswordInput,\\n-        help_text = _(\"Enter the same password as above, for verification.\"))\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+        help_text=_(\"Enter the same password as above, for verification.\"))\\n \\n     class Meta:\\n         model = User\\n@@ -137,7 +141,11 @@ class AuthenticationForm(forms.Form):\\n     username/password logins.\\n     \"\"\"\\n     username = forms.CharField(label=_(\"Username\"), max_length=30)\\n-    password = forms.CharField(label=_(\"Password\"), widget=forms.PasswordInput)\\n+    password = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     error_messages = {\\n         \\'invalid_login\\': _(\"Please enter a correct username and password. \"\\n@@ -250,10 +258,16 @@ class SetPasswordForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    new_password1 = forms.CharField(label=_(\"New password\"),\\n-                                    widget=forms.PasswordInput)\\n-    new_password2 = forms.CharField(label=_(\"New password confirmation\"),\\n-                                    widget=forms.PasswordInput)\\n+    new_password1 = forms.CharField(\\n+        label=_(\"New password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    new_password2 = forms.CharField(\\n+        label=_(\"New password confirmation\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user\\n@@ -284,8 +298,11 @@ class PasswordChangeForm(SetPasswordForm):\\n         \\'password_incorrect\\': _(\"Your old password was entered incorrectly. \"\\n                                 \"Please enter it again.\"),\\n     })\\n-    old_password = forms.CharField(label=_(\"Old password\"),\\n-                                   widget=forms.PasswordInput)\\n+    old_password = forms.CharField(\\n+        label=_(\"Old password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def clean_old_password(self):\\n         \"\"\"\\n@@ -307,10 +324,16 @@ class AdminPasswordChangeForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    password1 = forms.CharField(label=_(\"Password\"),\\n-                                widget=forms.PasswordInput)\\n-    password2 = forms.CharField(label=_(\"Password (again)\"),\\n-                                widget=forms.PasswordInput)\\n+    password1 = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    password2 = forms.CharField(\\n+        label=_(\"Password (again)\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user'], 'file': ['django/contrib/auth/hashers.py', 'django/contrib/auth/forms.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('76aac655-b413-4d74-aba7-75e24968e010'), UUID('1af3bfbb-b646-485a-9633-97cc02d09840')]}\n",
      "ERROR:root:Error in {'repo': 'django/django', 'vulnerability_id': '2013-1443', 'commit': '3f3d887a6844ec2db743fee64c9e53e04d39a368', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -1,3 +1,4 @@\\n+import functools\\n import hashlib\\n \\n from django.conf import settings\\n@@ -11,10 +12,23 @@\\n \\n \\n UNUSABLE_PASSWORD = \\'!\\'  # This will never be a valid encoded hash\\n+MAXIMUM_PASSWORD_LENGTH = 4096  # The maximum length a password can be to prevent DoS\\n HASHERS = None  # lazily loaded from PASSWORD_HASHERS\\n PREFERRED_HASHER = None  # defaults to first item in PASSWORD_HASHERS\\n \\n \\n+def password_max_length(max_length):\\n+    def inner(fn):\\n+        @functools.wraps(fn)\\n+        def wrapper(self, password, *args, **kwargs):\\n+            if len(password) > max_length:\\n+                raise ValueError(\"Invalid password; Must be less than or equal\"\\n+                                 \" to %d bytes\" % max_length)\\n+            return fn(self, password, *args, **kwargs)\\n+        return wrapper\\n+    return inner\\n+\\n+\\n def is_password_usable(encoded):\\n     return (encoded is not None and encoded != UNUSABLE_PASSWORD)\\n \\n@@ -202,6 +216,7 @@ class PBKDF2PasswordHasher(BasePasswordHasher):\\n     iterations = 10000\\n     digest = hashlib.sha256\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt, iterations=None):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n@@ -211,6 +226,7 @@ def encode(self, password, salt, iterations=None):\\n         hash = hash.encode(\\'base64\\').strip()\\n         return \"%s$%d$%s$%s\" % (self.algorithm, iterations, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, iterations, salt, hash = encoded.split(\\'$\\', 3)\\n         assert algorithm == self.algorithm\\n@@ -256,11 +272,13 @@ def salt(self):\\n         bcrypt = self._load_library()\\n         return bcrypt.gensalt(self.rounds)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         bcrypt = self._load_library()\\n         data = bcrypt.hashpw(password, salt)\\n         return \"%s$%s\" % (self.algorithm, data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, data = encoded.split(\\'$\\', 1)\\n         assert algorithm == self.algorithm\\n@@ -285,12 +303,14 @@ class SHA1PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"sha1\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.sha1(salt + password).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -313,12 +333,14 @@ class MD5PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"md5\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.md5(salt + password).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -349,11 +371,13 @@ class UnsaltedSHA1PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         hash = hashlib.sha1(password).hexdigest()\\n         return \\'sha1$$%s\\' % hash\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         encoded_2 = self.encode(password, \\'\\')\\n         return constant_time_compare(encoded, encoded_2)\\n@@ -383,10 +407,12 @@ class UnsaltedMD5PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         return hashlib.md5(password).hexdigest()\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         if len(encoded) == 37 and encoded.startswith(\\'md5$$\\'):\\n             encoded = encoded[5:]\\n@@ -412,13 +438,15 @@ class CryptPasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return get_random_string(2)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         crypt = self._load_library()\\n         assert len(salt) == 2\\n         data = crypt.crypt(password, salt)\\n         # we don\\'t need to store the salt, but Django used to do this\\n         return \"%s$%s$%s\" % (self.algorithm, \\'\\', data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         crypt = self._load_library()\\n         algorithm, salt, data = encoded.split(\\'$\\', 2)\\n@@ -433,4 +461,3 @@ def safe_summary(self, encoded):\\n             (_(\\'salt\\'), salt),\\n             (_(\\'hash\\'), mask_hash(data, show=3)),\\n         ])\\n-', '@@ -8,7 +8,10 @@\\n \\n from django.contrib.auth import authenticate\\n from django.contrib.auth.models import User\\n-from django.contrib.auth.hashers import UNUSABLE_PASSWORD, is_password_usable, get_hasher\\n+from django.contrib.auth.hashers import (\\n+    MAXIMUM_PASSWORD_LENGTH, UNUSABLE_PASSWORD,\\n+    is_password_usable, get_hasher\\n+)\\n from django.contrib.auth.tokens import default_token_generator\\n from django.contrib.sites.models import get_current_site\\n \\n@@ -70,10 +73,11 @@ class UserCreationForm(forms.ModelForm):\\n             \\'invalid\\': _(\"This value may contain only letters, numbers and \"\\n                          \"@/./+/-/_ characters.\")})\\n     password1 = forms.CharField(label=_(\"Password\"),\\n-        widget=forms.PasswordInput)\\n+        widget=forms.PasswordInput, max_length=MAXIMUM_PASSWORD_LENGTH)\\n     password2 = forms.CharField(label=_(\"Password confirmation\"),\\n         widget=forms.PasswordInput,\\n-        help_text = _(\"Enter the same password as above, for verification.\"))\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+        help_text=_(\"Enter the same password as above, for verification.\"))\\n \\n     class Meta:\\n         model = User\\n@@ -137,7 +141,11 @@ class AuthenticationForm(forms.Form):\\n     username/password logins.\\n     \"\"\"\\n     username = forms.CharField(label=_(\"Username\"), max_length=30)\\n-    password = forms.CharField(label=_(\"Password\"), widget=forms.PasswordInput)\\n+    password = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     error_messages = {\\n         \\'invalid_login\\': _(\"Please enter a correct username and password. \"\\n@@ -250,10 +258,16 @@ class SetPasswordForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    new_password1 = forms.CharField(label=_(\"New password\"),\\n-                                    widget=forms.PasswordInput)\\n-    new_password2 = forms.CharField(label=_(\"New password confirmation\"),\\n-                                    widget=forms.PasswordInput)\\n+    new_password1 = forms.CharField(\\n+        label=_(\"New password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    new_password2 = forms.CharField(\\n+        label=_(\"New password confirmation\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user\\n@@ -284,8 +298,11 @@ class PasswordChangeForm(SetPasswordForm):\\n         \\'password_incorrect\\': _(\"Your old password was entered incorrectly. \"\\n                                 \"Please enter it again.\"),\\n     })\\n-    old_password = forms.CharField(label=_(\"Old password\"),\\n-                                   widget=forms.PasswordInput)\\n+    old_password = forms.CharField(\\n+        label=_(\"Old password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def clean_old_password(self):\\n         \"\"\"\\n@@ -307,10 +324,16 @@ class AdminPasswordChangeForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    password1 = forms.CharField(label=_(\"Password\"),\\n-                                widget=forms.PasswordInput)\\n-    password2 = forms.CharField(label=_(\"Password (again)\"),\\n-                                widget=forms.PasswordInput)\\n+    password1 = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    password2 = forms.CharField(\\n+        label=_(\"Password (again)\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user'], 'file': ['django/contrib/auth/hashers.py', 'django/contrib/auth/forms.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('76aac655-b413-4d74-aba7-75e24968e010'), UUID('1af3bfbb-b646-485a-9633-97cc02d09840')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 27:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 27:0: <line number missing in source>\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 662/1800 [06:53<05:40,  3.34it/s]ERROR:src.process_code_changes:Error processing commit 9cb61c93f9b24dd18a0a315f3df5445529c5c333\n",
      "ERROR:src.process_code_changes:{'repo': 'apple/ccs-pykerberos', 'vulnerability_id': '2015-3206', 'commit': '9cb61c93f9b24dd18a0a315f3df5445529c5c333', 'commit_source': 'github', 'cwe_id': ['CWE-287'], 'patch': ['@@ -38,12 +38,16 @@ def checkPassword(user, pswd, service, default_realm):\\n     That will likely mean ensuring that the edu.mit.Kerberos preference file has the correct\\n     realms and KDCs listed.\\n     \\n+    IMPORTANT This method is vulnerable to KDC spoofing attacks and it should only used\\n+    for testing. Do not use this in any production system - your security could be\\n+    compromised if you do.\\n+     \\n     @param user:          a string containing the Kerberos user name. A realm may be\\n         included by appending an \\'@\\' followed by the realm string to the actual user id.\\n         If no realm is supplied, then the realm set in the default_realm argument will\\n         be used.\\n     @param pswd:          a string containing the password for the user.\\n-    @param service:       a string containging the Kerberos service to check access for.\\n+    @param service:       a string containing the Kerberos service to check access for.\\n         This will be of the form \\'sss/xx.yy.zz\\', where \\'sss\\' is the service identifier\\n         (e.g., \\'http\\', \\'krbtgt\\'), and \\'xx.yy.zz\\' is the hostname of the server.\\n     @param default_realm: a string containing the default realm to use if one is not\\n@@ -61,7 +65,7 @@ def changePassword(user, oldpswd, newpswd):\\n         If no realm is supplied, then the realm set in the default_realm argument will\\n         be used.\\n     @param oldpswd:       a string containing the old (current) password for the user.\\n-    @param newpswd:       a string containging the new password for the user.\\n+    @param newpswd:       a string containing the new password for the user.\\n     @return:              True if password changing succeeds, False otherwise.\\n     \"\"\"\\n '], 'file': ['pysrc/kerberos.py'], 'language': ['Python'], 'temp_id': [UUID('85363f93-5302-4e85-af3b-02c927d04258')]}\n",
      "ERROR:root:Error in {'repo': 'apple/ccs-pykerberos', 'vulnerability_id': '2015-3206', 'commit': '9cb61c93f9b24dd18a0a315f3df5445529c5c333', 'commit_source': 'github', 'cwe_id': ['CWE-287'], 'patch': ['@@ -38,12 +38,16 @@ def checkPassword(user, pswd, service, default_realm):\\n     That will likely mean ensuring that the edu.mit.Kerberos preference file has the correct\\n     realms and KDCs listed.\\n     \\n+    IMPORTANT This method is vulnerable to KDC spoofing attacks and it should only used\\n+    for testing. Do not use this in any production system - your security could be\\n+    compromised if you do.\\n+     \\n     @param user:          a string containing the Kerberos user name. A realm may be\\n         included by appending an \\'@\\' followed by the realm string to the actual user id.\\n         If no realm is supplied, then the realm set in the default_realm argument will\\n         be used.\\n     @param pswd:          a string containing the password for the user.\\n-    @param service:       a string containging the Kerberos service to check access for.\\n+    @param service:       a string containing the Kerberos service to check access for.\\n         This will be of the form \\'sss/xx.yy.zz\\', where \\'sss\\' is the service identifier\\n         (e.g., \\'http\\', \\'krbtgt\\'), and \\'xx.yy.zz\\' is the hostname of the server.\\n     @param default_realm: a string containing the default realm to use if one is not\\n@@ -61,7 +65,7 @@ def changePassword(user, oldpswd, newpswd):\\n         If no realm is supplied, then the realm set in the default_realm argument will\\n         be used.\\n     @param oldpswd:       a string containing the old (current) password for the user.\\n-    @param newpswd:       a string containging the new password for the user.\\n+    @param newpswd:       a string containing the new password for the user.\\n     @return:              True if password changing succeeds, False otherwise.\\n     \"\"\"\\n '], 'file': ['pysrc/kerberos.py'], 'language': ['Python'], 'temp_id': [UUID('85363f93-5302-4e85-af3b-02c927d04258')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:11:     @param newpswd:       a string containing the new password for the user.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 29:11:     @param newpswd:       a string containing the new password for the user.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 671/1800 [06:54<03:54,  4.82it/s]ERROR:src.process_code_changes:Error processing commit a621b386397280cc8ee5a208dca4607cb71cdd65\n",
      "ERROR:src.process_code_changes:{'repo': 'jupyter-server/jupyter-scheduler', 'vulnerability_id': '2024-28188', 'commit': 'a621b386397280cc8ee5a208dca4607cb71cdd65', 'commit_source': 'github', 'cwe_id': ['CWE-200', 'CWE-200'], 'patch': ['@@ -299,6 +299,7 @@ async def get(self):\\n \\n \\n class RuntimeEnvironmentsHandler(ExtensionHandlerMixin, JobHandlersMixin, APIHandler):\\n+    @tornado.web.authenticated\\n     async def get(self):\\n         \"\"\"Returns names of available runtime environments and output formats mappings\"\"\"\\n         try:'], 'file': ['jupyter_scheduler/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('3bbd0b49-52bd-48eb-9c16-860e10bbf563')]}\n",
      "ERROR:root:Error in {'repo': 'jupyter-server/jupyter-scheduler', 'vulnerability_id': '2024-28188', 'commit': 'a621b386397280cc8ee5a208dca4607cb71cdd65', 'commit_source': 'github', 'cwe_id': ['CWE-200', 'CWE-200'], 'patch': ['@@ -299,6 +299,7 @@ async def get(self):\\n \\n \\n class RuntimeEnvironmentsHandler(ExtensionHandlerMixin, JobHandlersMixin, APIHandler):\\n+    @tornado.web.authenticated\\n     async def get(self):\\n         \"\"\"Returns names of available runtime environments and output formats mappings\"\"\"\\n         try:'], 'file': ['jupyter_scheduler/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('3bbd0b49-52bd-48eb-9c16-860e10bbf563')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 678/1800 [06:54<03:12,  5.82it/s]ERROR:src.process_code_changes:Error processing commit c28d6d387bba59d8bd5cb3ba15edc42edf54b368\n",
      "ERROR:src.process_code_changes:{'repo': 'tlsfuzzer/tlslite-ng', 'vulnerability_id': '2020-26263', 'commit': 'c28d6d387bba59d8bd5cb3ba15edc42edf54b368', 'commit_source': 'github', 'cwe_id': ['CWE-326'], 'patch': ['@@ -5,6 +5,7 @@\\n \\n import sys\\n import os\\n+import re\\n import platform\\n import math\\n import binascii\\n@@ -68,6 +69,10 @@ def formatExceptionTrace(e):\\n         \"\"\"Return exception information formatted as string\"\"\"\\n         return str(e)\\n \\n+    def remove_whitespace(text):\\n+        \"\"\"Removes all whitespace from passed in string\"\"\"\\n+        return re.sub(r\"\\\\s+\", \"\", text, flags=re.UNICODE)\\n+\\n else:\\n     # Python 2.6 requires strings instead of bytearrays in a couple places,\\n     # so we define this function so it does the conversion if needed.\\n@@ -76,9 +81,18 @@ def formatExceptionTrace(e):\\n     if sys.version_info < (2, 7) or sys.version_info < (2, 7, 4) \\\\\\n             or platform.system() == \\'Java\\':\\n         def compat26Str(x): return str(x)\\n+\\n+        def remove_whitespace(text):\\n+            \"\"\"Removes all whitespace from passed in string\"\"\"\\n+            return re.sub(r\"\\\\s+\", \"\", text)\\n+\\n     else:\\n         def compat26Str(x): return x\\n \\n+        def remove_whitespace(text):\\n+            \"\"\"Removes all whitespace from passed in string\"\"\"\\n+            return re.sub(r\"\\\\s+\", \"\", text, flags=re.UNICODE)\\n+\\n     def compatAscii2Bytes(val):\\n         \"\"\"Convert ASCII string to bytes.\"\"\"\\n         return val', '@@ -107,21 +107,23 @@ def parsePEMKey(s, private=False, public=False, passwordCallback=None,\\n \\n \\n def _parseKeyHelper(key, private, public):\\n-    if private:\\n-        if not key.hasPrivateKey():\\n-            raise SyntaxError(\"Not a private key!\")\\n+    if private and not key.hasPrivateKey():\\n+        raise SyntaxError(\"Not a private key!\")\\n \\n     if public:\\n         return _createPublicKey(key)\\n \\n     if private:\\n-        if hasattr(key, \"d\"):\\n-            return _createPrivateKey(key)\\n-        else:\\n+        if cryptomath.m2cryptoLoaded:\\n+            if type(key) == Python_RSAKey:\\n+                return _createPrivateKey(key)\\n+            assert type(key) in (OpenSSL_RSAKey, ), type(key)\\n             return key\\n-\\n+        elif hasattr(key, \"d\"):\\n+            return _createPrivateKey(key)\\n     return key\\n \\n+\\n def parseAsPublicKey(s):\\n     \"\"\"Parse a PEM-formatted public key.\\n ', '@@ -8,6 +8,7 @@\\n from .rsakey import *\\n from .python_rsakey import Python_RSAKey\\n from .compat import compatAscii2Bytes\\n+import sys\\n \\n #copied from M2Crypto.util.py, so when we load the local copy of m2\\n #we can still use it\\n@@ -65,12 +66,20 @@ def _rawPrivateKeyOp(self, m):\\n             c = bytesToNumber(bytearray(s))\\n             return c\\n \\n+        def _raw_private_key_op_bytes(self, message):\\n+            return bytearray(m2.rsa_private_encrypt(self.rsa, bytes(message),\\n+                                                    m2.no_padding))\\n+\\n         def _rawPublicKeyOp(self, c):\\n             b = numberToByteArray(c, numBytes(self.n))\\n             s = m2.rsa_public_decrypt(self.rsa, bytes(b), m2.no_padding)\\n             m = bytesToNumber(bytearray(s))\\n             return m\\n \\n+        def _raw_public_key_op_bytes(self, ciphertext):\\n+            return bytearray(m2.rsa_public_decrypt(self.rsa, bytes(ciphertext),\\n+                                                   m2.no_padding))\\n+\\n         def acceptsPassword(self): return True\\n \\n         def write(self, password=None):\\n@@ -146,6 +155,13 @@ def f():pass\\n                         key._hasPrivateKey = False\\n                     else:\\n                         raise SyntaxError()\\n+                    if key._hasPrivateKey:\\n+                        if sys.version_info < (3, 0):\\n+                            b64_key = str(key.write())\\n+                        else:\\n+                            b64_key = str(key.write(), \"ascii\")\\n+                        py_key = Python_RSAKey.parsePEM(b64_key)\\n+                        key.d = py_key.d\\n                     return key\\n                 finally:\\n                     m2.bio_free(bio)', '@@ -23,6 +23,7 @@ def ct_lt_u32(val_a, val_b):\\n \\n     return (val_a^((val_a^val_b)|(((val_a-val_b)&0xffffffff)^val_b)))>>31\\n \\n+\\n def ct_gt_u32(val_a, val_b):\\n     \"\"\"\\n     Return 1 if val_a > val_b, 0 otherwise. Constant time.\\n@@ -35,6 +36,7 @@ def ct_gt_u32(val_a, val_b):\\n     \"\"\"\\n     return ct_lt_u32(val_b, val_a)\\n \\n+\\n def ct_le_u32(val_a, val_b):\\n     \"\"\"\\n     Return 1 if val_a <= val_b, 0 otherwise. Constant time.\\n@@ -47,14 +49,26 @@ def ct_le_u32(val_a, val_b):\\n     \"\"\"\\n     return 1 ^ ct_gt_u32(val_a, val_b)\\n \\n+\\n def ct_lsb_prop_u8(val):\\n-    \"\"\"Propagate LSB to all 8 bits of the returned byte. Constant time.\"\"\"\\n+    \"\"\"Propagate LSB to all 8 bits of the returned int. Constant time.\"\"\"\\n+    val &= 0x01\\n+    val |= val << 1\\n+    val |= val << 2\\n+    val |= val << 4\\n+    return val\\n+\\n+\\n+def ct_lsb_prop_u16(val):\\n+    \"\"\"Propagate LSB to all 16 bits of the returned int. Constant time.\"\"\"\\n     val &= 0x01\\n     val |= val << 1\\n     val |= val << 2\\n     val |= val << 4\\n+    val |= val << 8\\n     return val\\n \\n+\\n def ct_isnonzero_u32(val):\\n     \"\"\"\\n     Returns 1 if val is != 0, 0 otherwise. Constant time.\\n@@ -66,6 +80,7 @@ def ct_isnonzero_u32(val):\\n     val &= 0xffffffff\\n     return (val|(-val&0xffffffff)) >> 31\\n \\n+\\n def ct_neq_u32(val_a, val_b):\\n     \"\"\"\\n     Return 1 if val_a != val_b, 0 otherwise. Constant time.', '@@ -0,0 +1,130 @@\\n+\\n+\\n+from .python_rsakey import Python_RSAKey\\n+from .pem import dePem, pemSniff\\n+from .asn1parser import ASN1Parser\\n+from .cryptomath import bytesToNumber\\n+\\n+\\n+class Python_Key(object):\\n+    \"\"\"\\n+    Generic methods for parsing private keys from files.\\n+\\n+    Handles both RSA and ECDSA keys, irrespective of file format.\\n+    \"\"\"\\n+\\n+    @staticmethod\\n+    def parsePEM(s, passwordCallback=None):\\n+        \"\"\"Parse a string containing a PEM-encoded <privateKey>.\"\"\"\\n+\\n+        if pemSniff(s, \"PRIVATE KEY\"):\\n+            bytes = dePem(s, \"PRIVATE KEY\")\\n+            return Python_Key._parse_pkcs8(bytes)\\n+        elif pemSniff(s, \"RSA PRIVATE KEY\"):\\n+            bytes = dePem(s, \"RSA PRIVATE KEY\")\\n+            return Python_Key._parse_ssleay(bytes)\\n+        elif pemSniff(s, \"DSA PRIVATE KEY\"):\\n+            raise SyntaxError(\"DSA private key files unsupported\")\\n+        elif pemSniff(s, \"EC PRIVATE KEY\"):\\n+            raise SyntaxError(\"ECDSA private key files unsupported\")\\n+        elif pemSniff(s, \"PUBLIC KEY\"):\\n+            bytes = dePem(s, \"PUBLIC KEY\")\\n+            return Python_Key._parse_public_key(bytes)\\n+        else:\\n+            raise SyntaxError(\"Not a PEM private key file\")\\n+\\n+    @staticmethod\\n+    def _parse_public_key(bytes):\\n+        # public keys are encoded as the subject_public_key_info objects\\n+        spk_info = ASN1Parser(bytes)\\n+\\n+        # first element of the SEQUENCE is the AlgorithmIdentifier\\n+        alg_id = spk_info.getChild(0)\\n+\\n+        # AlgId has two elements, the OID of the algorithm and parameters\\n+        # parameters generally have to be NULL, with exception of RSA-PSS\\n+\\n+        alg_oid = alg_id.getChild(0)\\n+\\n+        if list(alg_oid.value) != [42, 134, 72, 134, 247, 13, 1, 1, 1]:\\n+            raise SyntaxError(\"Only RSA Public keys supported\")\\n+\\n+        subject_public_key = ASN1Parser(\\n+            ASN1Parser(spk_info.getChildBytes(1)).value[1:])\\n+\\n+        modulus = subject_public_key.getChild(0)\\n+        exponent = subject_public_key.getChild(1)\\n+\\n+        n = bytesToNumber(modulus.value)\\n+        e = bytesToNumber(exponent.value)\\n+\\n+        return Python_RSAKey(n, e)\\n+\\n+    @staticmethod\\n+    def _parse_pkcs8(bytes):\\n+        parser = ASN1Parser(bytes)\\n+\\n+        # first element in PrivateKeyInfo is an INTEGER\\n+        version = parser.getChild(0).value\\n+        if bytesToNumber(version) != 0:\\n+            raise SyntaxError(\"Unrecognized PKCS8 version\")\\n+\\n+        # second element in PrivateKeyInfo is a SEQUENCE of type\\n+        # AlgorithmIdentifier\\n+        alg_ident = parser.getChild(1)\\n+        seq_len = alg_ident.getChildCount()\\n+        # first item of AlgorithmIdentifier is an OBJECT (OID)\\n+        oid = alg_ident.getChild(0)\\n+        if list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 1]:\\n+            key_type = \"rsa\"\\n+        elif list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 10]:\\n+            key_type = \"rsa-pss\"\\n+        else:\\n+            raise SyntaxError(\"Unrecognized AlgorithmIdentifier: {0}\"\\n+                              .format(list(oid.value)))\\n+        # second item of AlgorithmIdentifier are parameters (defined by\\n+        # above algorithm)\\n+        if key_type == \"rsa\":\\n+            if seq_len != 2:\\n+                raise SyntaxError(\"Missing parameters for RSA algorithm ID\")\\n+            parameters = alg_ident.getChild(1)\\n+            if parameters.value != bytearray(0):\\n+                raise SyntaxError(\"RSA parameters are not NULL\")\\n+        else:  # rsa-pss\\n+            pass  # ignore parameters - don\\'t apply restrictions\\n+\\n+        if seq_len > 2:\\n+            raise SyntaxError(\"Invalid encoding of AlgorithmIdentifier\")\\n+\\n+        #Get the privateKey\\n+        private_key_parser = parser.getChild(2)\\n+\\n+        #Adjust for OCTET STRING encapsulation\\n+        private_key_parser = ASN1Parser(private_key_parser.value)\\n+\\n+        return Python_Key._parse_asn1_private_key(private_key_parser)\\n+\\n+    @staticmethod\\n+    def _parse_ssleay(data):\\n+        \"\"\"\\n+        Parse binary structure of the old SSLeay file format used by OpenSSL.\\n+\\n+        For RSA keys.\\n+        \"\"\"\\n+        private_key_parser = ASN1Parser(data)\\n+        return Python_Key._parse_asn1_private_key(private_key_parser)\\n+\\n+    @staticmethod\\n+    def _parse_asn1_private_key(private_key_parser):\\n+        version = private_key_parser.getChild(0).value[0]\\n+        if version != 0:\\n+            raise SyntaxError(\"Unrecognized RSAPrivateKey version\")\\n+        n = bytesToNumber(private_key_parser.getChild(1).value)\\n+        e = bytesToNumber(private_key_parser.getChild(2).value)\\n+        d = bytesToNumber(private_key_parser.getChild(3).value)\\n+        p = bytesToNumber(private_key_parser.getChild(4).value)\\n+        q = bytesToNumber(private_key_parser.getChild(5).value)\\n+        dP = bytesToNumber(private_key_parser.getChild(6).value)\\n+        dQ = bytesToNumber(private_key_parser.getChild(7).value)\\n+        qInv = bytesToNumber(private_key_parser.getChild(8).value)\\n+        return Python_RSAKey(n, e, d, p, q, dP, dQ, qInv)', '@@ -86,79 +86,8 @@ def generate(bits):\\n         return key\\n     generate = staticmethod(generate)\\n \\n+    @staticmethod\\n     def parsePEM(s, passwordCallback=None):\\n         \"\"\"Parse a string containing a PEM-encoded <privateKey>.\"\"\"\\n-\\n-        if pemSniff(s, \"PRIVATE KEY\"):\\n-            bytes = dePem(s, \"PRIVATE KEY\")\\n-            return Python_RSAKey._parsePKCS8(bytes)\\n-        elif pemSniff(s, \"RSA PRIVATE KEY\"):\\n-            bytes = dePem(s, \"RSA PRIVATE KEY\")\\n-            return Python_RSAKey._parseSSLeay(bytes)\\n-        else:\\n-            raise SyntaxError(\"Not a PEM private key file\")\\n-    parsePEM = staticmethod(parsePEM)\\n-\\n-    def _parsePKCS8(bytes):\\n-        p = ASN1Parser(bytes)\\n-\\n-        # first element in PrivateKeyInfo is an INTEGER\\n-        version = p.getChild(0).value\\n-        if bytesToNumber(version) != 0:\\n-            raise SyntaxError(\"Unrecognized PKCS8 version\")\\n-\\n-        # second element in PrivateKeyInfo is a SEQUENCE of type\\n-        # AlgorithmIdentifier\\n-        algIdent = p.getChild(1)\\n-        seqLen = algIdent.getChildCount()\\n-        # first item of AlgorithmIdentifier is an OBJECT (OID)\\n-        oid = algIdent.getChild(0)\\n-        if list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 1]:\\n-            keyType = \"rsa\"\\n-        elif list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 10]:\\n-            keyType = \"rsa-pss\"\\n-        else:\\n-            raise SyntaxError(\"Unrecognized AlgorithmIdentifier: {0}\"\\n-                              .format(list(oid.value)))\\n-        # second item of AlgorithmIdentifier are parameters (defined by\\n-        # above algorithm)\\n-        if keyType == \"rsa\":\\n-            if seqLen != 2:\\n-                raise SyntaxError(\"Missing parameters for RSA algorithm ID\")\\n-            parameters = algIdent.getChild(1)\\n-            if parameters.value != bytearray(0):\\n-                raise SyntaxError(\"RSA parameters are not NULL\")\\n-        else:  # rsa-pss\\n-            pass  # ignore parameters - don\\'t apply restrictions\\n-\\n-        if seqLen > 2:\\n-            raise SyntaxError(\"Invalid encoding of AlgorithmIdentifier\")\\n-\\n-        #Get the privateKey\\n-        privateKeyP = p.getChild(2)\\n-\\n-        #Adjust for OCTET STRING encapsulation\\n-        privateKeyP = ASN1Parser(privateKeyP.value)\\n-\\n-        return Python_RSAKey._parseASN1PrivateKey(privateKeyP)\\n-    _parsePKCS8 = staticmethod(_parsePKCS8)\\n-\\n-    def _parseSSLeay(bytes):\\n-        privateKeyP = ASN1Parser(bytes)\\n-        return Python_RSAKey._parseASN1PrivateKey(privateKeyP)\\n-    _parseSSLeay = staticmethod(_parseSSLeay)\\n-\\n-    def _parseASN1PrivateKey(privateKeyP):\\n-        version = privateKeyP.getChild(0).value[0]\\n-        if version != 0:\\n-            raise SyntaxError(\"Unrecognized RSAPrivateKey version\")\\n-        n = bytesToNumber(privateKeyP.getChild(1).value)\\n-        e = bytesToNumber(privateKeyP.getChild(2).value)\\n-        d = bytesToNumber(privateKeyP.getChild(3).value)\\n-        p = bytesToNumber(privateKeyP.getChild(4).value)\\n-        q = bytesToNumber(privateKeyP.getChild(5).value)\\n-        dP = bytesToNumber(privateKeyP.getChild(6).value)\\n-        dQ = bytesToNumber(privateKeyP.getChild(7).value)\\n-        qInv = bytesToNumber(privateKeyP.getChild(8).value)\\n-        return Python_RSAKey(n, e, d, p, q, dP, dQ, qInv)\\n-    _parseASN1PrivateKey = staticmethod(_parseASN1PrivateKey)\\n+        from .python_key import Python_Key\\n+        return Python_Key.parsePEM(s, passwordCallback)', '@@ -7,6 +7,8 @@\\n from . import tlshashlib as hashlib\\n from ..errors import MaskTooLongError, MessageTooLongError, EncodingError, \\\\\\n     InvalidSignature, UnknownRSAType\\n+from .constanttime import ct_isnonzero_u32, ct_neq_u32, ct_lsb_prop_u8, \\\\\\n+    ct_lsb_prop_u16, ct_lt_u32\\n \\n \\n class RSAKey(object):\\n@@ -34,6 +36,7 @@ def __init__(self, n=0, e=0):\\n         :type e: int\\n         :param e: RSA public exponent.\\n         \"\"\"\\n+        self._key_hash = None\\n         raise NotImplementedError()\\n \\n     def __len__(self):\\n@@ -187,12 +190,11 @@ def RSASSA_PSS_sign(self, mHash, hAlg, sLen=0):\\n         :type sLen: int\\n         :param sLen: length of salt\"\"\"\\n         EM = self.EMSA_PSS_encode(mHash, numBits(self.n) - 1, hAlg, sLen)\\n-        m = bytesToNumber(EM)\\n-        if m >= self.n:\\n+        try:\\n+            ret = self._raw_private_key_op_bytes(EM)\\n+        except ValueError:\\n             raise MessageTooLongError(\"Encode output too long\")\\n-        s = self._rawPrivateKeyOp(m)\\n-        S = numberToByteArray(s, numBytes(self.n))\\n-        return S\\n+        return ret\\n \\n     def EMSA_PSS_verify(self, mHash, EM, emBits, hAlg, sLen=0):\\n         \"\"\"Verify signature in passed in encoded message\\n@@ -264,11 +266,10 @@ def RSASSA_PSS_verify(self, mHash, S, hAlg, sLen=0):\\n         :type sLen: int\\n         :param sLen: Length of salt\\n         \"\"\"\\n-        if len(bytearray(S)) != len(numberToByteArray(self.n)):\\n+        try:\\n+            EM = self._raw_public_key_op_bytes(S)\\n+        except ValueError:\\n             raise InvalidSignature(\"Invalid signature\")\\n-        s = bytesToNumber(S)\\n-        m = self._rawPublicKeyOp(s)\\n-        EM = numberToByteArray(m, divceil(numBits(self.n) - 1, 8))\\n         result = self.EMSA_PSS_verify(mHash, EM, numBits(self.n) - 1,\\n                                       hAlg, sLen)\\n         if result:\\n@@ -281,12 +282,7 @@ def _raw_pkcs1_sign(self, bytes):\\n         if not self.hasPrivateKey():\\n             raise AssertionError()\\n         paddedBytes = self._addPKCS1Padding(bytes, 1)\\n-        m = bytesToNumber(paddedBytes)\\n-        if m >= self.n:\\n-            raise ValueError()\\n-        c = self._rawPrivateKeyOp(m)\\n-        sigBytes = numberToByteArray(c, numBytes(self.n))\\n-        return sigBytes\\n+        return self._raw_private_key_op_bytes(paddedBytes)\\n \\n     def sign(self, bytes, padding=\\'pkcs1\\', hashAlg=None, saltLen=None):\\n         \"\"\"Sign the passed-in bytes.\\n@@ -326,14 +322,11 @@ def sign(self, bytes, padding=\\'pkcs1\\', hashAlg=None, saltLen=None):\\n \\n     def _raw_pkcs1_verify(self, sigBytes, bytes):\\n         \"\"\"Perform verification operation on raw PKCS#1 padded signature\"\"\"\\n-        if len(sigBytes) != numBytes(self.n):\\n+        try:\\n+            checkBytes = self._raw_public_key_op_bytes(sigBytes)\\n+        except ValueError:\\n             return False\\n         paddedBytes = self._addPKCS1Padding(bytes, 1)\\n-        c = bytesToNumber(sigBytes)\\n-        if c >= self.n:\\n-            return False\\n-        m = self._rawPublicKeyOp(c)\\n-        checkBytes = numberToByteArray(m, numBytes(self.n))\\n         return checkBytes == paddedBytes\\n \\n     def verify(self, sigBytes, bytes, padding=\\'pkcs1\\', hashAlg=None,\\n@@ -384,52 +377,203 @@ def encrypt(self, bytes):\\n         :returns: A PKCS1 encryption of the passed-in data.\\n         \"\"\"\\n         paddedBytes = self._addPKCS1Padding(bytes, 2)\\n-        m = bytesToNumber(paddedBytes)\\n-        if m >= self.n:\\n-            raise ValueError()\\n-        c = self._rawPublicKeyOp(m)\\n-        encBytes = numberToByteArray(c, numBytes(self.n))\\n-        return encBytes\\n+        return self._raw_public_key_op_bytes(paddedBytes)\\n+\\n+    def _dec_prf(self, key, label, out_len):\\n+        \"\"\"PRF for deterministic implicit rejection in the RSA decryption.\\n+\\n+        :param bytes key: key to use for derivation\\n+        :param bytes label: name of the keystream generated\\n+        :param int out_len: length of output, in bits\\n+        :rtype: bytes\\n+        :returns: a random bytestring\\n+        \"\"\"\\n+        out = bytearray()\\n+\\n+        if out_len % 8 != 0:\\n+            raise ValueError(\"only multiples of 8 supported as output size\")\\n+\\n+        iterator = 0\\n+        while len(out) < out_len // 8:\\n+            out += secureHMAC(\\n+                key,\\n+                numberToByteArray(iterator, 2) + label +\\n+                numberToByteArray(out_len, 2),\\n+                \"sha256\")\\n+            iterator += 1\\n+\\n+        return out[:out_len//8]\\n \\n     def decrypt(self, encBytes):\\n         \"\"\"Decrypt the passed-in bytes.\\n \\n         This requires the key to have a private component.  It performs\\n-        PKCS1 decryption of the passed-in data.\\n+        PKCS#1 v1.5 decryption operation of the passed-in data.\\n+\\n+        Note: as a workaround against Bleichenbacher-like attacks, it will\\n+        return a deterministically selected random message in case the padding\\n+        checks failed. It returns an error (None) only in case the ciphertext\\n+        is of incorrect length or encodes an integer bigger than the modulus\\n+        of the key (i.e. it\\'s publically invalid).\\n \\n         :type encBytes: bytearray\\n         :param encBytes: The value which will be decrypted.\\n \\n         :rtype: bytearray or None\\n-        :returns: A PKCS1 decryption of the passed-in data or None if\\n-            the data is not properly formatted.\\n+        :returns: A PKCS#1 v1.5 decryption of the passed-in data or None if\\n+            the provided data is not properly formatted. Note: encrypting\\n+            an empty string is correct, so it may return an empty bytearray\\n+            for some ciphertexts.\\n         \"\"\"\\n         if not self.hasPrivateKey():\\n             raise AssertionError()\\n-        if len(encBytes) != numBytes(self.n):\\n-            return None\\n-        c = bytesToNumber(encBytes)\\n-        if c >= self.n:\\n+        try:\\n+            dec_bytes = self._raw_private_key_op_bytes(encBytes)\\n+        except ValueError:\\n+            # _raw_private_key_op_bytes fails only when encBytes >= self.n,\\n+            # or when len(encBytes) != numBytes(self.n) and that\\'s public\\n+            # information, so we don\\'t have to handle it\\n+            # in sidechannel secure way\\n             return None\\n-        m = self._rawPrivateKeyOp(c)\\n-        decBytes = numberToByteArray(m, numBytes(self.n))\\n-        #Check first two bytes\\n-        if decBytes[0] != 0 or decBytes[1] != 2:\\n-            return None\\n-        #Scan through for zero separator\\n-        for x in range(1, len(decBytes)-1):\\n-            if decBytes[x]== 0:\\n-                break\\n-        else:\\n-            return None\\n-        return decBytes[x+1:] #Return everything after the separator\\n+\\n+        ###################\\n+        # here be dragons #\\n+        ###################\\n+        # While the code is written as-if it was side-channel secure, in\\n+        # practice, because of cPython implementation details IT IS NOT\\n+        # see:\\n+        # https://securitypitfalls.wordpress.com/2018/08/03/constant-time-compare-in-python/\\n+\\n+        n = self.n\\n+\\n+        # maximum length we can return is reduced by the mandatory prefix:\\n+        # (0x00 0x02), 8 bytes of padding, so this is the position of the\\n+        # null separator byte, as counted from the last position\\n+        max_sep_offset = numBytes(n) - 10\\n+\\n+        # the private exponent (d) doesn\\'t change so `_key_hash` doesn\\'t\\n+        # change, calculate it only once\\n+        if not hasattr(self, \\'_key_hash\\') or not self._key_hash:\\n+            self._key_hash = secureHash(numberToByteArray(self.d, numBytes(n)),\\n+                                        \"sha256\")\\n+\\n+        kdk = secureHMAC(self._key_hash, encBytes, \"sha256\")\\n+\\n+        # we need 128 2-byte numbers, encoded as the number of bits\\n+        length_randoms = self._dec_prf(kdk, b\"length\", 128 * 2 * 8)\\n+\\n+        message_random = self._dec_prf(kdk, b\"message\", numBytes(n) * 8)\\n+\\n+        # select the last length that\\'s not too large to return\\n+        synth_length = 0\\n+        length_rand_iter = iter(length_randoms)\\n+        length_mask = (1 << numBits(max_sep_offset)) - 1\\n+        for high, low in zip(length_rand_iter, length_rand_iter):\\n+            # interpret the two bytes from the PRF output as 16-bit big-endian\\n+            # integer\\n+            len_candidate = (high << 8) + low\\n+            len_candidate &= length_mask\\n+            # equivalent to:\\n+            # if len_candidate < max_sep_offset:\\n+            #    synth_length = len_candidate\\n+            mask = ct_lt_u32(len_candidate, max_sep_offset)\\n+            mask = ct_lsb_prop_u16(mask)\\n+            synth_length = synth_length & (0xffff ^ mask) \\\\\\n+                | len_candidate & mask\\n+\\n+        synth_msg_start = numBytes(n) - synth_length\\n+\\n+        error_detected = 0\\n+\\n+        # enumerate over all decrypted bytes\\n+        em_bytes = enumerate(dec_bytes)\\n+        # first check if first two bytes specify PKCS#1 v1.5 encryption padding\\n+        _, val = next(em_bytes)\\n+        error_detected |= ct_isnonzero_u32(val)\\n+        _, val = next(em_bytes)\\n+        error_detected |= ct_neq_u32(val, 0x02)\\n+        # then look for for the null separator byte among the padding bytes\\n+        # but inspect all decrypted bytes, even if we already find the\\n+        # separator earlier\\n+        msg_start = 0\\n+        for pos, val in em_bytes:\\n+            # padding must be at least 8 bytes long, fail if any of the first\\n+            # 8 bytes of it are zero\\n+            # equivalent to:\\n+            # if pos < 10 and not val:\\n+            #     error_detected = 0x01\\n+            error_detected |= ct_lt_u32(pos, 10) & (1 ^ ct_isnonzero_u32(val))\\n+\\n+            # update the msg_start only once; when it\\'s 0\\n+            # (pos+1) because we want to skip the null separator\\n+            # equivalent to:\\n+            # if pos >= 10 and not msg_start and not val:\\n+            #     msg_start = pos+1\\n+            mask = (1 ^ ct_lt_u32(pos, 10)) & (1 ^ ct_isnonzero_u32(val)) \\\\\\n+                & (1 ^ ct_isnonzero_u32(msg_start))\\n+            mask = ct_lsb_prop_u16(mask)\\n+            msg_start = msg_start & (0xffff ^ mask) | (pos+1) & mask\\n+\\n+        # if separator wasn\\'t found, it\\'s an error\\n+        # equivalent to:\\n+        # if not msg_start:\\n+        #     error_detected = 0x01\\n+        error_detected |= 1 ^ ct_isnonzero_u32(msg_start)\\n+\\n+        # equivalent to:\\n+        # if error_detected:\\n+        #     ret_msg_start = synth_msg_start\\n+        # else:\\n+        #     ret_msg_start = msg_start\\n+        mask = ct_lsb_prop_u16(error_detected)\\n+        ret_msg_start = msg_start & (0xffff ^ mask) | synth_msg_start & mask\\n+\\n+        # as at this point the length doesn\\'t leak the information if the\\n+        # padding was correct or not, we don\\'t have to worry about the\\n+        # length of the returned value (and thus the size of the buffer we\\n+        # pass to the caller); but we still need to read both buffers\\n+        # to ensure that the memory access patern is preserved (that both\\n+        # buffers are accessed, not just the one we return)\\n+\\n+        # equivalent to:\\n+        # if error_detected:\\n+        #     return message_random[ret_msg_start:]\\n+        # else:\\n+        #     return dec_bytes[ret_msg_start:]\\n+        mask = ct_lsb_prop_u8(error_detected)\\n+        not_mask = 0xff ^ mask\\n+        ret = bytearray(\\n+            x & not_mask | y & mask for x, y in\\n+            zip(dec_bytes[ret_msg_start:], message_random[ret_msg_start:]))\\n+\\n+        return ret\\n \\n     def _rawPrivateKeyOp(self, m):\\n         raise NotImplementedError()\\n \\n     def _rawPublicKeyOp(self, c):\\n         raise NotImplementedError()\\n \\n+    def _raw_private_key_op_bytes(self, message):\\n+        n = self.n\\n+        if len(message) != numBytes(n):\\n+            raise ValueError(\"Message has incorrect length for the key size\")\\n+        m_int = bytesToNumber(message)\\n+        if m_int >= n:\\n+            raise ValueError(\"Provided message value exceeds modulus\")\\n+        dec_int = self._rawPrivateKeyOp(m_int)\\n+        return numberToByteArray(dec_int, numBytes(n))\\n+\\n+    def _raw_public_key_op_bytes(self, ciphertext):\\n+        n = self.n\\n+        if len(ciphertext) != numBytes(n):\\n+            raise ValueError(\"Message has incorrect length for the key size\")\\n+        c_int = bytesToNumber(ciphertext)\\n+        if c_int >= n:\\n+            raise ValueError(\"Provided message value exceeds modulus\")\\n+        enc_int = self._rawPublicKeyOp(c_int)\\n+        return numberToByteArray(enc_int, numBytes(n))\\n+\\n     def acceptsPassword(self):\\n         \"\"\"Return True if the write() method accepts a password for use\\n         in encrypting the private key.'], 'file': ['tlslite/utils/compat.py', 'tlslite/utils/keyfactory.py', 'tlslite/utils/openssl_rsakey.py', 'tlslite/utils/constanttime.py', 'tlslite/utils/python_key.py', 'tlslite/utils/python_rsakey.py', 'tlslite/utils/rsakey.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('b19db13b-b3fd-413a-835b-3a981817ef16'), UUID('ccfd08ed-030d-48a3-ba2d-92176acffc86'), UUID('072d3058-b493-4ddf-967d-fb4b9ffd8952'), UUID('f9f8a051-038d-45e7-9081-9311c4cc35d8'), UUID('7d57d174-a179-41c7-b904-caf1a47b9b22'), UUID('d5521a35-32d0-49f9-a955-4d85250960e7'), UUID('4da3a7fd-f256-49ec-bbce-594cd37ab473')]}\n",
      "ERROR:root:Error in {'repo': 'tlsfuzzer/tlslite-ng', 'vulnerability_id': '2020-26263', 'commit': 'c28d6d387bba59d8bd5cb3ba15edc42edf54b368', 'commit_source': 'github', 'cwe_id': ['CWE-326'], 'patch': ['@@ -5,6 +5,7 @@\\n \\n import sys\\n import os\\n+import re\\n import platform\\n import math\\n import binascii\\n@@ -68,6 +69,10 @@ def formatExceptionTrace(e):\\n         \"\"\"Return exception information formatted as string\"\"\"\\n         return str(e)\\n \\n+    def remove_whitespace(text):\\n+        \"\"\"Removes all whitespace from passed in string\"\"\"\\n+        return re.sub(r\"\\\\s+\", \"\", text, flags=re.UNICODE)\\n+\\n else:\\n     # Python 2.6 requires strings instead of bytearrays in a couple places,\\n     # so we define this function so it does the conversion if needed.\\n@@ -76,9 +81,18 @@ def formatExceptionTrace(e):\\n     if sys.version_info < (2, 7) or sys.version_info < (2, 7, 4) \\\\\\n             or platform.system() == \\'Java\\':\\n         def compat26Str(x): return str(x)\\n+\\n+        def remove_whitespace(text):\\n+            \"\"\"Removes all whitespace from passed in string\"\"\"\\n+            return re.sub(r\"\\\\s+\", \"\", text)\\n+\\n     else:\\n         def compat26Str(x): return x\\n \\n+        def remove_whitespace(text):\\n+            \"\"\"Removes all whitespace from passed in string\"\"\"\\n+            return re.sub(r\"\\\\s+\", \"\", text, flags=re.UNICODE)\\n+\\n     def compatAscii2Bytes(val):\\n         \"\"\"Convert ASCII string to bytes.\"\"\"\\n         return val', '@@ -107,21 +107,23 @@ def parsePEMKey(s, private=False, public=False, passwordCallback=None,\\n \\n \\n def _parseKeyHelper(key, private, public):\\n-    if private:\\n-        if not key.hasPrivateKey():\\n-            raise SyntaxError(\"Not a private key!\")\\n+    if private and not key.hasPrivateKey():\\n+        raise SyntaxError(\"Not a private key!\")\\n \\n     if public:\\n         return _createPublicKey(key)\\n \\n     if private:\\n-        if hasattr(key, \"d\"):\\n-            return _createPrivateKey(key)\\n-        else:\\n+        if cryptomath.m2cryptoLoaded:\\n+            if type(key) == Python_RSAKey:\\n+                return _createPrivateKey(key)\\n+            assert type(key) in (OpenSSL_RSAKey, ), type(key)\\n             return key\\n-\\n+        elif hasattr(key, \"d\"):\\n+            return _createPrivateKey(key)\\n     return key\\n \\n+\\n def parseAsPublicKey(s):\\n     \"\"\"Parse a PEM-formatted public key.\\n ', '@@ -8,6 +8,7 @@\\n from .rsakey import *\\n from .python_rsakey import Python_RSAKey\\n from .compat import compatAscii2Bytes\\n+import sys\\n \\n #copied from M2Crypto.util.py, so when we load the local copy of m2\\n #we can still use it\\n@@ -65,12 +66,20 @@ def _rawPrivateKeyOp(self, m):\\n             c = bytesToNumber(bytearray(s))\\n             return c\\n \\n+        def _raw_private_key_op_bytes(self, message):\\n+            return bytearray(m2.rsa_private_encrypt(self.rsa, bytes(message),\\n+                                                    m2.no_padding))\\n+\\n         def _rawPublicKeyOp(self, c):\\n             b = numberToByteArray(c, numBytes(self.n))\\n             s = m2.rsa_public_decrypt(self.rsa, bytes(b), m2.no_padding)\\n             m = bytesToNumber(bytearray(s))\\n             return m\\n \\n+        def _raw_public_key_op_bytes(self, ciphertext):\\n+            return bytearray(m2.rsa_public_decrypt(self.rsa, bytes(ciphertext),\\n+                                                   m2.no_padding))\\n+\\n         def acceptsPassword(self): return True\\n \\n         def write(self, password=None):\\n@@ -146,6 +155,13 @@ def f():pass\\n                         key._hasPrivateKey = False\\n                     else:\\n                         raise SyntaxError()\\n+                    if key._hasPrivateKey:\\n+                        if sys.version_info < (3, 0):\\n+                            b64_key = str(key.write())\\n+                        else:\\n+                            b64_key = str(key.write(), \"ascii\")\\n+                        py_key = Python_RSAKey.parsePEM(b64_key)\\n+                        key.d = py_key.d\\n                     return key\\n                 finally:\\n                     m2.bio_free(bio)', '@@ -23,6 +23,7 @@ def ct_lt_u32(val_a, val_b):\\n \\n     return (val_a^((val_a^val_b)|(((val_a-val_b)&0xffffffff)^val_b)))>>31\\n \\n+\\n def ct_gt_u32(val_a, val_b):\\n     \"\"\"\\n     Return 1 if val_a > val_b, 0 otherwise. Constant time.\\n@@ -35,6 +36,7 @@ def ct_gt_u32(val_a, val_b):\\n     \"\"\"\\n     return ct_lt_u32(val_b, val_a)\\n \\n+\\n def ct_le_u32(val_a, val_b):\\n     \"\"\"\\n     Return 1 if val_a <= val_b, 0 otherwise. Constant time.\\n@@ -47,14 +49,26 @@ def ct_le_u32(val_a, val_b):\\n     \"\"\"\\n     return 1 ^ ct_gt_u32(val_a, val_b)\\n \\n+\\n def ct_lsb_prop_u8(val):\\n-    \"\"\"Propagate LSB to all 8 bits of the returned byte. Constant time.\"\"\"\\n+    \"\"\"Propagate LSB to all 8 bits of the returned int. Constant time.\"\"\"\\n+    val &= 0x01\\n+    val |= val << 1\\n+    val |= val << 2\\n+    val |= val << 4\\n+    return val\\n+\\n+\\n+def ct_lsb_prop_u16(val):\\n+    \"\"\"Propagate LSB to all 16 bits of the returned int. Constant time.\"\"\"\\n     val &= 0x01\\n     val |= val << 1\\n     val |= val << 2\\n     val |= val << 4\\n+    val |= val << 8\\n     return val\\n \\n+\\n def ct_isnonzero_u32(val):\\n     \"\"\"\\n     Returns 1 if val is != 0, 0 otherwise. Constant time.\\n@@ -66,6 +80,7 @@ def ct_isnonzero_u32(val):\\n     val &= 0xffffffff\\n     return (val|(-val&0xffffffff)) >> 31\\n \\n+\\n def ct_neq_u32(val_a, val_b):\\n     \"\"\"\\n     Return 1 if val_a != val_b, 0 otherwise. Constant time.', '@@ -0,0 +1,130 @@\\n+\\n+\\n+from .python_rsakey import Python_RSAKey\\n+from .pem import dePem, pemSniff\\n+from .asn1parser import ASN1Parser\\n+from .cryptomath import bytesToNumber\\n+\\n+\\n+class Python_Key(object):\\n+    \"\"\"\\n+    Generic methods for parsing private keys from files.\\n+\\n+    Handles both RSA and ECDSA keys, irrespective of file format.\\n+    \"\"\"\\n+\\n+    @staticmethod\\n+    def parsePEM(s, passwordCallback=None):\\n+        \"\"\"Parse a string containing a PEM-encoded <privateKey>.\"\"\"\\n+\\n+        if pemSniff(s, \"PRIVATE KEY\"):\\n+            bytes = dePem(s, \"PRIVATE KEY\")\\n+            return Python_Key._parse_pkcs8(bytes)\\n+        elif pemSniff(s, \"RSA PRIVATE KEY\"):\\n+            bytes = dePem(s, \"RSA PRIVATE KEY\")\\n+            return Python_Key._parse_ssleay(bytes)\\n+        elif pemSniff(s, \"DSA PRIVATE KEY\"):\\n+            raise SyntaxError(\"DSA private key files unsupported\")\\n+        elif pemSniff(s, \"EC PRIVATE KEY\"):\\n+            raise SyntaxError(\"ECDSA private key files unsupported\")\\n+        elif pemSniff(s, \"PUBLIC KEY\"):\\n+            bytes = dePem(s, \"PUBLIC KEY\")\\n+            return Python_Key._parse_public_key(bytes)\\n+        else:\\n+            raise SyntaxError(\"Not a PEM private key file\")\\n+\\n+    @staticmethod\\n+    def _parse_public_key(bytes):\\n+        # public keys are encoded as the subject_public_key_info objects\\n+        spk_info = ASN1Parser(bytes)\\n+\\n+        # first element of the SEQUENCE is the AlgorithmIdentifier\\n+        alg_id = spk_info.getChild(0)\\n+\\n+        # AlgId has two elements, the OID of the algorithm and parameters\\n+        # parameters generally have to be NULL, with exception of RSA-PSS\\n+\\n+        alg_oid = alg_id.getChild(0)\\n+\\n+        if list(alg_oid.value) != [42, 134, 72, 134, 247, 13, 1, 1, 1]:\\n+            raise SyntaxError(\"Only RSA Public keys supported\")\\n+\\n+        subject_public_key = ASN1Parser(\\n+            ASN1Parser(spk_info.getChildBytes(1)).value[1:])\\n+\\n+        modulus = subject_public_key.getChild(0)\\n+        exponent = subject_public_key.getChild(1)\\n+\\n+        n = bytesToNumber(modulus.value)\\n+        e = bytesToNumber(exponent.value)\\n+\\n+        return Python_RSAKey(n, e)\\n+\\n+    @staticmethod\\n+    def _parse_pkcs8(bytes):\\n+        parser = ASN1Parser(bytes)\\n+\\n+        # first element in PrivateKeyInfo is an INTEGER\\n+        version = parser.getChild(0).value\\n+        if bytesToNumber(version) != 0:\\n+            raise SyntaxError(\"Unrecognized PKCS8 version\")\\n+\\n+        # second element in PrivateKeyInfo is a SEQUENCE of type\\n+        # AlgorithmIdentifier\\n+        alg_ident = parser.getChild(1)\\n+        seq_len = alg_ident.getChildCount()\\n+        # first item of AlgorithmIdentifier is an OBJECT (OID)\\n+        oid = alg_ident.getChild(0)\\n+        if list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 1]:\\n+            key_type = \"rsa\"\\n+        elif list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 10]:\\n+            key_type = \"rsa-pss\"\\n+        else:\\n+            raise SyntaxError(\"Unrecognized AlgorithmIdentifier: {0}\"\\n+                              .format(list(oid.value)))\\n+        # second item of AlgorithmIdentifier are parameters (defined by\\n+        # above algorithm)\\n+        if key_type == \"rsa\":\\n+            if seq_len != 2:\\n+                raise SyntaxError(\"Missing parameters for RSA algorithm ID\")\\n+            parameters = alg_ident.getChild(1)\\n+            if parameters.value != bytearray(0):\\n+                raise SyntaxError(\"RSA parameters are not NULL\")\\n+        else:  # rsa-pss\\n+            pass  # ignore parameters - don\\'t apply restrictions\\n+\\n+        if seq_len > 2:\\n+            raise SyntaxError(\"Invalid encoding of AlgorithmIdentifier\")\\n+\\n+        #Get the privateKey\\n+        private_key_parser = parser.getChild(2)\\n+\\n+        #Adjust for OCTET STRING encapsulation\\n+        private_key_parser = ASN1Parser(private_key_parser.value)\\n+\\n+        return Python_Key._parse_asn1_private_key(private_key_parser)\\n+\\n+    @staticmethod\\n+    def _parse_ssleay(data):\\n+        \"\"\"\\n+        Parse binary structure of the old SSLeay file format used by OpenSSL.\\n+\\n+        For RSA keys.\\n+        \"\"\"\\n+        private_key_parser = ASN1Parser(data)\\n+        return Python_Key._parse_asn1_private_key(private_key_parser)\\n+\\n+    @staticmethod\\n+    def _parse_asn1_private_key(private_key_parser):\\n+        version = private_key_parser.getChild(0).value[0]\\n+        if version != 0:\\n+            raise SyntaxError(\"Unrecognized RSAPrivateKey version\")\\n+        n = bytesToNumber(private_key_parser.getChild(1).value)\\n+        e = bytesToNumber(private_key_parser.getChild(2).value)\\n+        d = bytesToNumber(private_key_parser.getChild(3).value)\\n+        p = bytesToNumber(private_key_parser.getChild(4).value)\\n+        q = bytesToNumber(private_key_parser.getChild(5).value)\\n+        dP = bytesToNumber(private_key_parser.getChild(6).value)\\n+        dQ = bytesToNumber(private_key_parser.getChild(7).value)\\n+        qInv = bytesToNumber(private_key_parser.getChild(8).value)\\n+        return Python_RSAKey(n, e, d, p, q, dP, dQ, qInv)', '@@ -86,79 +86,8 @@ def generate(bits):\\n         return key\\n     generate = staticmethod(generate)\\n \\n+    @staticmethod\\n     def parsePEM(s, passwordCallback=None):\\n         \"\"\"Parse a string containing a PEM-encoded <privateKey>.\"\"\"\\n-\\n-        if pemSniff(s, \"PRIVATE KEY\"):\\n-            bytes = dePem(s, \"PRIVATE KEY\")\\n-            return Python_RSAKey._parsePKCS8(bytes)\\n-        elif pemSniff(s, \"RSA PRIVATE KEY\"):\\n-            bytes = dePem(s, \"RSA PRIVATE KEY\")\\n-            return Python_RSAKey._parseSSLeay(bytes)\\n-        else:\\n-            raise SyntaxError(\"Not a PEM private key file\")\\n-    parsePEM = staticmethod(parsePEM)\\n-\\n-    def _parsePKCS8(bytes):\\n-        p = ASN1Parser(bytes)\\n-\\n-        # first element in PrivateKeyInfo is an INTEGER\\n-        version = p.getChild(0).value\\n-        if bytesToNumber(version) != 0:\\n-            raise SyntaxError(\"Unrecognized PKCS8 version\")\\n-\\n-        # second element in PrivateKeyInfo is a SEQUENCE of type\\n-        # AlgorithmIdentifier\\n-        algIdent = p.getChild(1)\\n-        seqLen = algIdent.getChildCount()\\n-        # first item of AlgorithmIdentifier is an OBJECT (OID)\\n-        oid = algIdent.getChild(0)\\n-        if list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 1]:\\n-            keyType = \"rsa\"\\n-        elif list(oid.value) == [42, 134, 72, 134, 247, 13, 1, 1, 10]:\\n-            keyType = \"rsa-pss\"\\n-        else:\\n-            raise SyntaxError(\"Unrecognized AlgorithmIdentifier: {0}\"\\n-                              .format(list(oid.value)))\\n-        # second item of AlgorithmIdentifier are parameters (defined by\\n-        # above algorithm)\\n-        if keyType == \"rsa\":\\n-            if seqLen != 2:\\n-                raise SyntaxError(\"Missing parameters for RSA algorithm ID\")\\n-            parameters = algIdent.getChild(1)\\n-            if parameters.value != bytearray(0):\\n-                raise SyntaxError(\"RSA parameters are not NULL\")\\n-        else:  # rsa-pss\\n-            pass  # ignore parameters - don\\'t apply restrictions\\n-\\n-        if seqLen > 2:\\n-            raise SyntaxError(\"Invalid encoding of AlgorithmIdentifier\")\\n-\\n-        #Get the privateKey\\n-        privateKeyP = p.getChild(2)\\n-\\n-        #Adjust for OCTET STRING encapsulation\\n-        privateKeyP = ASN1Parser(privateKeyP.value)\\n-\\n-        return Python_RSAKey._parseASN1PrivateKey(privateKeyP)\\n-    _parsePKCS8 = staticmethod(_parsePKCS8)\\n-\\n-    def _parseSSLeay(bytes):\\n-        privateKeyP = ASN1Parser(bytes)\\n-        return Python_RSAKey._parseASN1PrivateKey(privateKeyP)\\n-    _parseSSLeay = staticmethod(_parseSSLeay)\\n-\\n-    def _parseASN1PrivateKey(privateKeyP):\\n-        version = privateKeyP.getChild(0).value[0]\\n-        if version != 0:\\n-            raise SyntaxError(\"Unrecognized RSAPrivateKey version\")\\n-        n = bytesToNumber(privateKeyP.getChild(1).value)\\n-        e = bytesToNumber(privateKeyP.getChild(2).value)\\n-        d = bytesToNumber(privateKeyP.getChild(3).value)\\n-        p = bytesToNumber(privateKeyP.getChild(4).value)\\n-        q = bytesToNumber(privateKeyP.getChild(5).value)\\n-        dP = bytesToNumber(privateKeyP.getChild(6).value)\\n-        dQ = bytesToNumber(privateKeyP.getChild(7).value)\\n-        qInv = bytesToNumber(privateKeyP.getChild(8).value)\\n-        return Python_RSAKey(n, e, d, p, q, dP, dQ, qInv)\\n-    _parseASN1PrivateKey = staticmethod(_parseASN1PrivateKey)\\n+        from .python_key import Python_Key\\n+        return Python_Key.parsePEM(s, passwordCallback)', '@@ -7,6 +7,8 @@\\n from . import tlshashlib as hashlib\\n from ..errors import MaskTooLongError, MessageTooLongError, EncodingError, \\\\\\n     InvalidSignature, UnknownRSAType\\n+from .constanttime import ct_isnonzero_u32, ct_neq_u32, ct_lsb_prop_u8, \\\\\\n+    ct_lsb_prop_u16, ct_lt_u32\\n \\n \\n class RSAKey(object):\\n@@ -34,6 +36,7 @@ def __init__(self, n=0, e=0):\\n         :type e: int\\n         :param e: RSA public exponent.\\n         \"\"\"\\n+        self._key_hash = None\\n         raise NotImplementedError()\\n \\n     def __len__(self):\\n@@ -187,12 +190,11 @@ def RSASSA_PSS_sign(self, mHash, hAlg, sLen=0):\\n         :type sLen: int\\n         :param sLen: length of salt\"\"\"\\n         EM = self.EMSA_PSS_encode(mHash, numBits(self.n) - 1, hAlg, sLen)\\n-        m = bytesToNumber(EM)\\n-        if m >= self.n:\\n+        try:\\n+            ret = self._raw_private_key_op_bytes(EM)\\n+        except ValueError:\\n             raise MessageTooLongError(\"Encode output too long\")\\n-        s = self._rawPrivateKeyOp(m)\\n-        S = numberToByteArray(s, numBytes(self.n))\\n-        return S\\n+        return ret\\n \\n     def EMSA_PSS_verify(self, mHash, EM, emBits, hAlg, sLen=0):\\n         \"\"\"Verify signature in passed in encoded message\\n@@ -264,11 +266,10 @@ def RSASSA_PSS_verify(self, mHash, S, hAlg, sLen=0):\\n         :type sLen: int\\n         :param sLen: Length of salt\\n         \"\"\"\\n-        if len(bytearray(S)) != len(numberToByteArray(self.n)):\\n+        try:\\n+            EM = self._raw_public_key_op_bytes(S)\\n+        except ValueError:\\n             raise InvalidSignature(\"Invalid signature\")\\n-        s = bytesToNumber(S)\\n-        m = self._rawPublicKeyOp(s)\\n-        EM = numberToByteArray(m, divceil(numBits(self.n) - 1, 8))\\n         result = self.EMSA_PSS_verify(mHash, EM, numBits(self.n) - 1,\\n                                       hAlg, sLen)\\n         if result:\\n@@ -281,12 +282,7 @@ def _raw_pkcs1_sign(self, bytes):\\n         if not self.hasPrivateKey():\\n             raise AssertionError()\\n         paddedBytes = self._addPKCS1Padding(bytes, 1)\\n-        m = bytesToNumber(paddedBytes)\\n-        if m >= self.n:\\n-            raise ValueError()\\n-        c = self._rawPrivateKeyOp(m)\\n-        sigBytes = numberToByteArray(c, numBytes(self.n))\\n-        return sigBytes\\n+        return self._raw_private_key_op_bytes(paddedBytes)\\n \\n     def sign(self, bytes, padding=\\'pkcs1\\', hashAlg=None, saltLen=None):\\n         \"\"\"Sign the passed-in bytes.\\n@@ -326,14 +322,11 @@ def sign(self, bytes, padding=\\'pkcs1\\', hashAlg=None, saltLen=None):\\n \\n     def _raw_pkcs1_verify(self, sigBytes, bytes):\\n         \"\"\"Perform verification operation on raw PKCS#1 padded signature\"\"\"\\n-        if len(sigBytes) != numBytes(self.n):\\n+        try:\\n+            checkBytes = self._raw_public_key_op_bytes(sigBytes)\\n+        except ValueError:\\n             return False\\n         paddedBytes = self._addPKCS1Padding(bytes, 1)\\n-        c = bytesToNumber(sigBytes)\\n-        if c >= self.n:\\n-            return False\\n-        m = self._rawPublicKeyOp(c)\\n-        checkBytes = numberToByteArray(m, numBytes(self.n))\\n         return checkBytes == paddedBytes\\n \\n     def verify(self, sigBytes, bytes, padding=\\'pkcs1\\', hashAlg=None,\\n@@ -384,52 +377,203 @@ def encrypt(self, bytes):\\n         :returns: A PKCS1 encryption of the passed-in data.\\n         \"\"\"\\n         paddedBytes = self._addPKCS1Padding(bytes, 2)\\n-        m = bytesToNumber(paddedBytes)\\n-        if m >= self.n:\\n-            raise ValueError()\\n-        c = self._rawPublicKeyOp(m)\\n-        encBytes = numberToByteArray(c, numBytes(self.n))\\n-        return encBytes\\n+        return self._raw_public_key_op_bytes(paddedBytes)\\n+\\n+    def _dec_prf(self, key, label, out_len):\\n+        \"\"\"PRF for deterministic implicit rejection in the RSA decryption.\\n+\\n+        :param bytes key: key to use for derivation\\n+        :param bytes label: name of the keystream generated\\n+        :param int out_len: length of output, in bits\\n+        :rtype: bytes\\n+        :returns: a random bytestring\\n+        \"\"\"\\n+        out = bytearray()\\n+\\n+        if out_len % 8 != 0:\\n+            raise ValueError(\"only multiples of 8 supported as output size\")\\n+\\n+        iterator = 0\\n+        while len(out) < out_len // 8:\\n+            out += secureHMAC(\\n+                key,\\n+                numberToByteArray(iterator, 2) + label +\\n+                numberToByteArray(out_len, 2),\\n+                \"sha256\")\\n+            iterator += 1\\n+\\n+        return out[:out_len//8]\\n \\n     def decrypt(self, encBytes):\\n         \"\"\"Decrypt the passed-in bytes.\\n \\n         This requires the key to have a private component.  It performs\\n-        PKCS1 decryption of the passed-in data.\\n+        PKCS#1 v1.5 decryption operation of the passed-in data.\\n+\\n+        Note: as a workaround against Bleichenbacher-like attacks, it will\\n+        return a deterministically selected random message in case the padding\\n+        checks failed. It returns an error (None) only in case the ciphertext\\n+        is of incorrect length or encodes an integer bigger than the modulus\\n+        of the key (i.e. it\\'s publically invalid).\\n \\n         :type encBytes: bytearray\\n         :param encBytes: The value which will be decrypted.\\n \\n         :rtype: bytearray or None\\n-        :returns: A PKCS1 decryption of the passed-in data or None if\\n-            the data is not properly formatted.\\n+        :returns: A PKCS#1 v1.5 decryption of the passed-in data or None if\\n+            the provided data is not properly formatted. Note: encrypting\\n+            an empty string is correct, so it may return an empty bytearray\\n+            for some ciphertexts.\\n         \"\"\"\\n         if not self.hasPrivateKey():\\n             raise AssertionError()\\n-        if len(encBytes) != numBytes(self.n):\\n-            return None\\n-        c = bytesToNumber(encBytes)\\n-        if c >= self.n:\\n+        try:\\n+            dec_bytes = self._raw_private_key_op_bytes(encBytes)\\n+        except ValueError:\\n+            # _raw_private_key_op_bytes fails only when encBytes >= self.n,\\n+            # or when len(encBytes) != numBytes(self.n) and that\\'s public\\n+            # information, so we don\\'t have to handle it\\n+            # in sidechannel secure way\\n             return None\\n-        m = self._rawPrivateKeyOp(c)\\n-        decBytes = numberToByteArray(m, numBytes(self.n))\\n-        #Check first two bytes\\n-        if decBytes[0] != 0 or decBytes[1] != 2:\\n-            return None\\n-        #Scan through for zero separator\\n-        for x in range(1, len(decBytes)-1):\\n-            if decBytes[x]== 0:\\n-                break\\n-        else:\\n-            return None\\n-        return decBytes[x+1:] #Return everything after the separator\\n+\\n+        ###################\\n+        # here be dragons #\\n+        ###################\\n+        # While the code is written as-if it was side-channel secure, in\\n+        # practice, because of cPython implementation details IT IS NOT\\n+        # see:\\n+        # https://securitypitfalls.wordpress.com/2018/08/03/constant-time-compare-in-python/\\n+\\n+        n = self.n\\n+\\n+        # maximum length we can return is reduced by the mandatory prefix:\\n+        # (0x00 0x02), 8 bytes of padding, so this is the position of the\\n+        # null separator byte, as counted from the last position\\n+        max_sep_offset = numBytes(n) - 10\\n+\\n+        # the private exponent (d) doesn\\'t change so `_key_hash` doesn\\'t\\n+        # change, calculate it only once\\n+        if not hasattr(self, \\'_key_hash\\') or not self._key_hash:\\n+            self._key_hash = secureHash(numberToByteArray(self.d, numBytes(n)),\\n+                                        \"sha256\")\\n+\\n+        kdk = secureHMAC(self._key_hash, encBytes, \"sha256\")\\n+\\n+        # we need 128 2-byte numbers, encoded as the number of bits\\n+        length_randoms = self._dec_prf(kdk, b\"length\", 128 * 2 * 8)\\n+\\n+        message_random = self._dec_prf(kdk, b\"message\", numBytes(n) * 8)\\n+\\n+        # select the last length that\\'s not too large to return\\n+        synth_length = 0\\n+        length_rand_iter = iter(length_randoms)\\n+        length_mask = (1 << numBits(max_sep_offset)) - 1\\n+        for high, low in zip(length_rand_iter, length_rand_iter):\\n+            # interpret the two bytes from the PRF output as 16-bit big-endian\\n+            # integer\\n+            len_candidate = (high << 8) + low\\n+            len_candidate &= length_mask\\n+            # equivalent to:\\n+            # if len_candidate < max_sep_offset:\\n+            #    synth_length = len_candidate\\n+            mask = ct_lt_u32(len_candidate, max_sep_offset)\\n+            mask = ct_lsb_prop_u16(mask)\\n+            synth_length = synth_length & (0xffff ^ mask) \\\\\\n+                | len_candidate & mask\\n+\\n+        synth_msg_start = numBytes(n) - synth_length\\n+\\n+        error_detected = 0\\n+\\n+        # enumerate over all decrypted bytes\\n+        em_bytes = enumerate(dec_bytes)\\n+        # first check if first two bytes specify PKCS#1 v1.5 encryption padding\\n+        _, val = next(em_bytes)\\n+        error_detected |= ct_isnonzero_u32(val)\\n+        _, val = next(em_bytes)\\n+        error_detected |= ct_neq_u32(val, 0x02)\\n+        # then look for for the null separator byte among the padding bytes\\n+        # but inspect all decrypted bytes, even if we already find the\\n+        # separator earlier\\n+        msg_start = 0\\n+        for pos, val in em_bytes:\\n+            # padding must be at least 8 bytes long, fail if any of the first\\n+            # 8 bytes of it are zero\\n+            # equivalent to:\\n+            # if pos < 10 and not val:\\n+            #     error_detected = 0x01\\n+            error_detected |= ct_lt_u32(pos, 10) & (1 ^ ct_isnonzero_u32(val))\\n+\\n+            # update the msg_start only once; when it\\'s 0\\n+            # (pos+1) because we want to skip the null separator\\n+            # equivalent to:\\n+            # if pos >= 10 and not msg_start and not val:\\n+            #     msg_start = pos+1\\n+            mask = (1 ^ ct_lt_u32(pos, 10)) & (1 ^ ct_isnonzero_u32(val)) \\\\\\n+                & (1 ^ ct_isnonzero_u32(msg_start))\\n+            mask = ct_lsb_prop_u16(mask)\\n+            msg_start = msg_start & (0xffff ^ mask) | (pos+1) & mask\\n+\\n+        # if separator wasn\\'t found, it\\'s an error\\n+        # equivalent to:\\n+        # if not msg_start:\\n+        #     error_detected = 0x01\\n+        error_detected |= 1 ^ ct_isnonzero_u32(msg_start)\\n+\\n+        # equivalent to:\\n+        # if error_detected:\\n+        #     ret_msg_start = synth_msg_start\\n+        # else:\\n+        #     ret_msg_start = msg_start\\n+        mask = ct_lsb_prop_u16(error_detected)\\n+        ret_msg_start = msg_start & (0xffff ^ mask) | synth_msg_start & mask\\n+\\n+        # as at this point the length doesn\\'t leak the information if the\\n+        # padding was correct or not, we don\\'t have to worry about the\\n+        # length of the returned value (and thus the size of the buffer we\\n+        # pass to the caller); but we still need to read both buffers\\n+        # to ensure that the memory access patern is preserved (that both\\n+        # buffers are accessed, not just the one we return)\\n+\\n+        # equivalent to:\\n+        # if error_detected:\\n+        #     return message_random[ret_msg_start:]\\n+        # else:\\n+        #     return dec_bytes[ret_msg_start:]\\n+        mask = ct_lsb_prop_u8(error_detected)\\n+        not_mask = 0xff ^ mask\\n+        ret = bytearray(\\n+            x & not_mask | y & mask for x, y in\\n+            zip(dec_bytes[ret_msg_start:], message_random[ret_msg_start:]))\\n+\\n+        return ret\\n \\n     def _rawPrivateKeyOp(self, m):\\n         raise NotImplementedError()\\n \\n     def _rawPublicKeyOp(self, c):\\n         raise NotImplementedError()\\n \\n+    def _raw_private_key_op_bytes(self, message):\\n+        n = self.n\\n+        if len(message) != numBytes(n):\\n+            raise ValueError(\"Message has incorrect length for the key size\")\\n+        m_int = bytesToNumber(message)\\n+        if m_int >= n:\\n+            raise ValueError(\"Provided message value exceeds modulus\")\\n+        dec_int = self._rawPrivateKeyOp(m_int)\\n+        return numberToByteArray(dec_int, numBytes(n))\\n+\\n+    def _raw_public_key_op_bytes(self, ciphertext):\\n+        n = self.n\\n+        if len(ciphertext) != numBytes(n):\\n+            raise ValueError(\"Message has incorrect length for the key size\")\\n+        c_int = bytesToNumber(ciphertext)\\n+        if c_int >= n:\\n+            raise ValueError(\"Provided message value exceeds modulus\")\\n+        enc_int = self._rawPublicKeyOp(c_int)\\n+        return numberToByteArray(enc_int, numBytes(n))\\n+\\n     def acceptsPassword(self):\\n         \"\"\"Return True if the write() method accepts a password for use\\n         in encrypting the private key.'], 'file': ['tlslite/utils/compat.py', 'tlslite/utils/keyfactory.py', 'tlslite/utils/openssl_rsakey.py', 'tlslite/utils/constanttime.py', 'tlslite/utils/python_key.py', 'tlslite/utils/python_rsakey.py', 'tlslite/utils/rsakey.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('b19db13b-b3fd-413a-835b-3a981817ef16'), UUID('ccfd08ed-030d-48a3-ba2d-92176acffc86'), UUID('072d3058-b493-4ddf-967d-fb4b9ffd8952'), UUID('f9f8a051-038d-45e7-9081-9311c4cc35d8'), UUID('7d57d174-a179-41c7-b904-caf1a47b9b22'), UUID('d5521a35-32d0-49f9-a955-4d85250960e7'), UUID('4da3a7fd-f256-49ec-bbce-594cd37ab473')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     def remove_whitespace(text):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     def remove_whitespace(text):\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 711/1800 [06:55<01:16, 14.29it/s]ERROR:src.process_code_changes:Error processing commit 448762d440b51574f1906c0ec2f5ea6dc4f16eb2\n",
      "ERROR:src.process_code_changes:{'repo': 'opnsense/core', 'vulnerability_id': '2023-38997', 'commit': '448762d440b51574f1906c0ec2f5ea6dc4f16eb2', 'commit_source': 'github', 'cwe_id': ['CWE-22'], 'patch': [\"@@ -40,15 +40,15 @@\\n     cnf = Config()\\n     zoneid = sys.argv[1]\\n     target_directory = '/var/captiveportal/zone%s/htdocs/' % zoneid\\n-    template_data = cnf.fetch_template_data(sys.argv[1])\\n+    template_data = cnf.fetch_template_data(zoneid)\\n     if template_data is not None and len(template_data) > 20:\\n         print ('overlay user template package for zone %s' % zoneid)\\n         zip_content = base64.b64decode(template_data)\\n         input_data = BytesIO(zip_content)\\n         with zipfile.ZipFile(input_data, mode='r', compression=zipfile.ZIP_DEFLATED) as zf_in:\\n             for zf_info in zf_in.infolist():\\n-                if zf_info.filename[-1] != '/':\\n-                    target_filename = '%s%s' % (target_directory, zf_info.filename)\\n+                target_filename = '%s%s' % (target_directory, zf_info.filename)\\n+                if os.path.realpath(target_filename).startswith(target_directory):\\n                     file_target_directory = '/'.join(target_filename.split('/')[:-1])\\n                     if not os.path.isdir(file_target_directory):\\n                         os.makedirs(file_target_directory)\"], 'file': ['src/opnsense/scripts/OPNsense/CaptivePortal/overlay_template.py'], 'language': ['Python'], 'temp_id': [UUID('bc7e0b9c-3fd2-4237-8e8d-07b35a32ef34')]}\n",
      "ERROR:root:Error in {'repo': 'opnsense/core', 'vulnerability_id': '2023-38997', 'commit': '448762d440b51574f1906c0ec2f5ea6dc4f16eb2', 'commit_source': 'github', 'cwe_id': ['CWE-22'], 'patch': [\"@@ -40,15 +40,15 @@\\n     cnf = Config()\\n     zoneid = sys.argv[1]\\n     target_directory = '/var/captiveportal/zone%s/htdocs/' % zoneid\\n-    template_data = cnf.fetch_template_data(sys.argv[1])\\n+    template_data = cnf.fetch_template_data(zoneid)\\n     if template_data is not None and len(template_data) > 20:\\n         print ('overlay user template package for zone %s' % zoneid)\\n         zip_content = base64.b64decode(template_data)\\n         input_data = BytesIO(zip_content)\\n         with zipfile.ZipFile(input_data, mode='r', compression=zipfile.ZIP_DEFLATED) as zf_in:\\n             for zf_info in zf_in.infolist():\\n-                if zf_info.filename[-1] != '/':\\n-                    target_filename = '%s%s' % (target_directory, zf_info.filename)\\n+                target_filename = '%s%s' % (target_directory, zf_info.filename)\\n+                if os.path.realpath(target_filename).startswith(target_directory):\\n                     file_target_directory = '/'.join(target_filename.split('/')[:-1])\\n                     if not os.path.isdir(file_target_directory):\\n                         os.makedirs(file_target_directory)\"], 'file': ['src/opnsense/scripts/OPNsense/CaptivePortal/overlay_template.py'], 'language': ['Python'], 'temp_id': [UUID('bc7e0b9c-3fd2-4237-8e8d-07b35a32ef34')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:         with zipfile.ZipFile(input_data, mode='r', compression=zipfile.ZIP_DEFLATED) as zf_in:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:         with zipfile.ZipFile(input_data, mode='r', compression=zipfile.ZIP_DEFLATED) as zf_in:\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 749/1800 [07:05<04:05,  4.28it/s]ERROR:src.process_code_changes:Error processing commit 2db0bb7ec35636ea46b07b146328b87b2cb13ca5\n",
      "ERROR:src.process_code_changes:{'repo': 'ubernostrum/django-registration', 'vulnerability_id': '2021-21416', 'commit': '2db0bb7ec35636ea46b07b146328b87b2cb13ca5', 'commit_source': 'github', 'cwe_id': ['CWE-209', 'CWE-209'], 'patch': ['@@ -8,9 +8,9 @@\\n source_suffix = \".rst\"\\n master_doc = \"index\"\\n project = \"django-registration\"\\n-copyright = \"2007-2020, James Bennett\"\\n+copyright = \"2007-2021, James Bennett\"\\n version = \"3.1\"\\n-release = \"3.1.1\"\\n+release = \"3.1.2\"\\n exclude_trees = [\"_build\"]\\n pygments_style = \"sphinx\"\\n htmlhelp_basename = \"django-registrationdoc\"', '@@ -7,7 +7,9 @@\\n from django.core.exceptions import ImproperlyConfigured\\n from django.http import HttpResponseRedirect\\n from django.urls import reverse_lazy\\n+from django.utils.decorators import method_decorator\\n from django.utils.encoding import force_str\\n+from django.views.decorators.debug import sensitive_post_parameters\\n from django.views.generic.base import TemplateView\\n from django.views.generic.edit import FormView\\n \\n@@ -40,6 +42,7 @@ class RegistrationView(FormView):\\n     success_url = None\\n     template_name = \"django_registration/registration_form.html\"\\n \\n+    @method_decorator(sensitive_post_parameters())\\n     def dispatch(self, *args, **kwargs):\\n         \"\"\"\\n         Check that user signup is allowed before even bothering to'], 'file': ['docs/conf.py', 'src/django_registration/views.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('f5f91ce1-a932-48b9-a4e7-5e709f26188a'), UUID('c8e41e5a-409c-40c7-83c4-ae7206747a22')]}\n",
      "ERROR:root:Error in {'repo': 'ubernostrum/django-registration', 'vulnerability_id': '2021-21416', 'commit': '2db0bb7ec35636ea46b07b146328b87b2cb13ca5', 'commit_source': 'github', 'cwe_id': ['CWE-209', 'CWE-209'], 'patch': ['@@ -8,9 +8,9 @@\\n source_suffix = \".rst\"\\n master_doc = \"index\"\\n project = \"django-registration\"\\n-copyright = \"2007-2020, James Bennett\"\\n+copyright = \"2007-2021, James Bennett\"\\n version = \"3.1\"\\n-release = \"3.1.1\"\\n+release = \"3.1.2\"\\n exclude_trees = [\"_build\"]\\n pygments_style = \"sphinx\"\\n htmlhelp_basename = \"django-registrationdoc\"', '@@ -7,7 +7,9 @@\\n from django.core.exceptions import ImproperlyConfigured\\n from django.http import HttpResponseRedirect\\n from django.urls import reverse_lazy\\n+from django.utils.decorators import method_decorator\\n from django.utils.encoding import force_str\\n+from django.views.decorators.debug import sensitive_post_parameters\\n from django.views.generic.base import TemplateView\\n from django.views.generic.edit import FormView\\n \\n@@ -40,6 +42,7 @@ class RegistrationView(FormView):\\n     success_url = None\\n     template_name = \"django_registration/registration_form.html\"\\n \\n+    @method_decorator(sensitive_post_parameters())\\n     def dispatch(self, *args, **kwargs):\\n         \"\"\"\\n         Check that user signup is allowed before even bothering to'], 'file': ['docs/conf.py', 'src/django_registration/views.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('f5f91ce1-a932-48b9-a4e7-5e709f26188a'), UUID('c8e41e5a-409c-40c7-83c4-ae7206747a22')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0:     @method_decorator(sensitive_post_parameters())\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0:     @method_decorator(sensitive_post_parameters())\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 757/1800 [07:12<10:22,  1.68it/s]ERROR:src.process_code_changes:Error processing commit c658b4f3e57258acf5f6207a90c2f2169698ae22\n",
      "ERROR:src.process_code_changes:{'repo': 'LiuWoodsCode/LiuOS', 'vulnerability_id': '2022-46179', 'commit': 'c658b4f3e57258acf5f6207a90c2f2169698ae22', 'commit_source': 'github', 'cwe_id': ['CWE-639'], 'patch': ['@@ -112,7 +112,7 @@ def actualsys() :\\n         if attemps == 6:\\n         ## Brute force protection\\n            raise Exception(\"Too many password attempts. Because of the risk of a brute force attack, after 6 attempts, you will need to rerun LiuOS to try 6 more times.\")\\n-        if os.environ.get(\\'GITHUB_ACTIONS\\') != \"\":\\n+        if os.environ.get(\\'GITHUB_ACTIONS\\') == \"true\":\\n             logging.warning(\"Running on Github Actions\")\\n             actualsys()\\n         elif username == cred.loginname and pwdreshash == cred.loginpass:'], 'file': ['core.py'], 'language': ['Python'], 'temp_id': [UUID('5e264521-d96c-41f5-b10f-5d3fd1859e2d')]}\n",
      "ERROR:root:Error in {'repo': 'LiuWoodsCode/LiuOS', 'vulnerability_id': '2022-46179', 'commit': 'c658b4f3e57258acf5f6207a90c2f2169698ae22', 'commit_source': 'github', 'cwe_id': ['CWE-639'], 'patch': ['@@ -112,7 +112,7 @@ def actualsys() :\\n         if attemps == 6:\\n         ## Brute force protection\\n            raise Exception(\"Too many password attempts. Because of the risk of a brute force attack, after 6 attempts, you will need to rerun LiuOS to try 6 more times.\")\\n-        if os.environ.get(\\'GITHUB_ACTIONS\\') != \"\":\\n+        if os.environ.get(\\'GITHUB_ACTIONS\\') == \"true\":\\n             logging.warning(\"Running on Github Actions\")\\n             actualsys()\\n         elif username == cred.loginname and pwdreshash == cred.loginpass:'], 'file': ['core.py'], 'language': ['Python'], 'temp_id': [UUID('5e264521-d96c-41f5-b10f-5d3fd1859e2d')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:         if attemps == 6:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:         if attemps == 6:\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 791/1800 [07:13<02:32,  6.61it/s]ERROR:src.process_code_changes:Error processing commit ad32781e782d0f604c6da4680fce48e4cc1f4433\n",
      "ERROR:src.process_code_changes:{'repo': 'zhmcclient/python-zhmcclient', 'vulnerability_id': '2024-53865', 'commit': 'ad32781e782d0f604c6da4680fce48e4cc1f4433', 'commit_source': 'github', 'cwe_id': ['CWE-312'], 'patch': ['@@ -224,7 +224,8 @@ def list(self, full_properties=False, filter_args=None,\\n             list_uri, result_prop, full_properties, filter_args,\\n             additional_properties)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'ssc-master-pw\\', \\'zaware-master-pw\\'],\\n+                     properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create and configure an Activation Profiles on this CPC, of the profile\\n@@ -344,7 +345,8 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'ssc-master-pw\\', \\'zaware-master-pw\\'],\\n+                     properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this Activation Profile.', '@@ -151,7 +151,7 @@ def list(self, full_properties=False, filter_args=None):\\n         return self._list_with_operation(\\n             list_uri, result_prop, full_properties, filter_args, None)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'bind-password\\'], properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create a new LDAP Server Definition in this HMC.\\n@@ -257,7 +257,7 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'bind-password\\'], properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this LDAP Server Definitions.', '@@ -75,9 +75,10 @@\\n \\n import logging\\n import inspect\\n-from decorator import decorate  # requires decorator>=4.0\\n+import functools\\n+from collections.abc import Mapping, Sequence\\n \\n-from ._constants import API_LOGGER_NAME\\n+from ._constants import API_LOGGER_NAME, BLANKED_OUT_STRING\\n \\n __all__ = []\\n \\n@@ -105,7 +106,8 @@ def get_logger(name):\\n     return logger\\n \\n \\n-def logged_api_call(func):\\n+def logged_api_call(\\n+        org_func=None, *, blanked_properties=None, properties_pos=None):\\n     \"\"\"\\n     Function decorator that causes the decorated API function or method to log\\n     calls to itself to a logger.\\n@@ -115,7 +117,24 @@ def logged_api_call(func):\\n \\n     Parameters:\\n \\n-      func (function object): The original function being decorated.\\n+      org_func (function object): The original function being decorated.\\n+        Will be `None` if the decorator is specified with its optional\\n+        argument \\'blanked_properties\\'.\\n+\\n+      blanked_properties (list of str): Optional: List of properties in the\\n+        \\'properties\\' argument of the decorated API function that should be\\n+        blanked out before being logged. Can be used to hide password\\n+        properties.\\n+        This parameter is required when \\'properties_pos\\' is used.\\n+        This parameter must be specified as a keyword argument.\\n+\\n+      properties_pos (int): Optional: 0-based index of the \\'properties\\'\\n+        parameter in the argument list of the decorated API function.\\n+        For methods, the \\'self\\' or \\'cls\\' parameter is included in the position.\\n+        This parameter is needed in case the properties are passed as a\\n+        positional argument by the caller of the API function.\\n+        This parameter is required when \\'blanked_properties\\' is used.\\n+        This parameter must be specified as a keyword argument.\\n \\n     Returns:\\n \\n@@ -128,109 +147,196 @@ def logged_api_call(func):\\n         method (and not on top of the @property decorator).\\n     \"\"\"\\n \\n-    # Note that in this decorator function, we are in a module loading context,\\n-    # where the decorated functions are being defined. When this decorator\\n-    # function is called, its call stack represents the definition of the\\n-    # decorated functions. Not all global definitions in the module have been\\n-    # defined yet, and methods of classes that are decorated with this\\n-    # decorator are still functions at this point (and not yet methods).\\n+    if blanked_properties is not None and properties_pos is None:\\n+        raise TypeError(\\n+            \"If the @logged_api_call decorator is specified with \"\\n+            \"\\'blanked_properties\\', \\'properties_pos\\' must also be specified.\")\\n \\n-    module = inspect.getmodule(func)\\n-    if not inspect.isfunction(func) or not hasattr(module, \\'__name__\\'):\\n-        raise TypeError(\"The @logged_api_call decorator must be used on a \"\\n-                        \"function or method (and not on top of the @property \"\\n-                        \"decorator)\")\\n+    if properties_pos is not None and blanked_properties is None:\\n+        raise TypeError(\\n+            \"If the @logged_api_call decorator is specified with \"\\n+            \"\\'properties_pos\\', \\'blanked_properties\\' must also be specified.\")\\n \\n-    try:\\n-        # We avoid the use of inspect.getouterframes() because it is slow,\\n-        # and use the pointers up the stack frame, instead.\\n+    if blanked_properties is not None and (\\n+            not isinstance(blanked_properties, Sequence) or  # noqa: W504\\n+            isinstance(blanked_properties, str)):\\n+        raise TypeError(\\n+            \"The \\'blanked_properties\\' parameter of the @logged_api_call \"\\n+            \"decorator must be a list of strings.\")\\n \\n-        this_frame = inspect.currentframe()  # this decorator function here\\n-        apifunc_frame = this_frame.f_back  # the decorated API function\\n-\\n-        apifunc_owner = inspect.getframeinfo(apifunc_frame)[2]\\n+    def _decorate(func):\\n+        \"\"\"\\n+        The actual decorator function that always gets the original decorated\\n+        function, independent of whether the \\'logged_api_call\\' decorator was\\n+        specified with or without its optional arguments.\\n \\n-    finally:\\n-        # Recommended way to deal with frame objects to avoid ref cycles\\n-        del this_frame\\n-        del apifunc_frame\\n+        Parameters:\\n \\n-    # TODO: For inner functions, show all outer levels instead of just one.\\n+          func (function object): The original function being decorated.\\n+        \"\"\"\\n \\n-    if apifunc_owner == \\'<module>\\':\\n-        # The decorated API function is defined globally (at module level)\\n-        apifunc_str = f\\'{func.__name__}()\\'\\n-    else:\\n-        # The decorated API function is defined in a class or in a function\\n-        apifunc_str = f\\'{apifunc_owner}.{func.__name__}()\\'\\n+        # Note that in this decorator function, we are in a module loading\\n+        # context, where the decorated functions are being defined. When this\\n+        # decorator function is called, its call stack represents the\\n+        # definition of the decorated functions. Not all global definitions in\\n+        # the module have been defined yet, and methods of classes that are\\n+        # decorated with this decorator are still functions at this point (and\\n+        # not yet methods).\\n \\n-    logger = get_logger(API_LOGGER_NAME)\\n+        if not inspect.isfunction(func):\\n+            raise TypeError(\"The @logged_api_call decorator must be used on a \"\\n+                            \"function or method (and not on top of the \"\\n+                            \"@property decorator)\")\\n \\n-    def is_external_call():\\n-        \"\"\"\\n-        Return a boolean indicating whether the call to the decorated API\\n-        function is an external call (vs. b eing an internal call).\\n-        \"\"\"\\n         try:\\n             # We avoid the use of inspect.getouterframes() because it is slow,\\n             # and use the pointers up the stack frame, instead.\\n \\n-            log_it_frame = inspect.currentframe()  # this log_it() function\\n-            log_api_call_frame = log_it_frame.f_back  # the log_api_call() func\\n-            apifunc_frame = log_api_call_frame.f_back  # the decorated API func\\n-            apicaller_frame = apifunc_frame.f_back  # caller of API function\\n-            apicaller_module = inspect.getmodule(apicaller_frame)\\n-            if apicaller_module is None:\\n-                apicaller_module_name = \"<unknown>\"\\n-            else:\\n-                apicaller_module_name = apicaller_module.__name__\\n+            this_frame = inspect.currentframe()  # this function\\n+            apifunc_frame = this_frame.f_back  # the decorated API function\\n+            if org_func:\\n+                # In this case, there is one more decorator function nesting\\n+                apifunc_frame = apifunc_frame.f_back\\n+            apifunc_owner = inspect.getframeinfo(apifunc_frame)[2]\\n+\\n         finally:\\n             # Recommended way to deal with frame objects to avoid ref cycles\\n-            del log_it_frame\\n-            del log_api_call_frame\\n+            del this_frame\\n             del apifunc_frame\\n-            del apicaller_frame\\n-            del apicaller_module\\n \\n-        # Log only if the caller is not from the zhmcclient package\\n-        return apicaller_module_name.split(\\'.\\')[0] != \\'zhmcclient\\'\\n+        # TODO: For inner functions, show all outer levels instead of just one.\\n+\\n+        func_name = getattr(func, \\'__name__\\', \\'<unknown>\\')\\n+        if apifunc_owner == \\'<module>\\':\\n+            # The decorated API function is defined globally (at module level)\\n+            apifunc_str = f\\'{func_name}()\\'\\n+        else:\\n+            # The decorated API function is defined in a class or in a function\\n+            apifunc_str = f\\'{apifunc_owner}.{func_name}()\\'\\n+\\n+        logger = get_logger(API_LOGGER_NAME)\\n+\\n+        def is_external_call():\\n+            \"\"\"\\n+            Return a boolean indicating whether the call to the decorated API\\n+            function is made from outside of the zhmcclient package.\\n+            \"\"\"\\n+            try:\\n+                # We avoid the use of inspect.getouterframes() because it is\\n+                # slow, and use the pointers up the stack frame, instead.\\n+\\n+                this_frame = inspect.currentframe()  # this function\\n+                log_api_call_frame = this_frame.f_back  # log_api_call()\\n+                apifunc_frame = log_api_call_frame.f_back  # the decorated func\\n+                apicaller_frame = apifunc_frame.f_back  # caller of API func\\n+                apicaller_module = inspect.getmodule(apicaller_frame)\\n+                if apicaller_module is None:\\n+                    apicaller_module_name = \"<unknown>\"\\n+                else:\\n+                    apicaller_module_name = apicaller_module.__name__\\n+            finally:\\n+                # Recommended way to deal with frame objects to avoid ref\\n+                # cycles\\n+                del this_frame\\n+                del log_api_call_frame\\n+                del apifunc_frame\\n+                del apicaller_frame\\n+                del apicaller_module\\n+\\n+            # Log only if the caller is not from the zhmcclient package\\n+            return apicaller_module_name.split(\\'.\\')[0] != \\'zhmcclient\\'\\n+\\n+        def blanked_dict(properties):\\n+            \"\"\"\\n+            Return a copy of the properties dict, with blanked out values\\n+            according to the \\'blanked_properties\\' and \\'properties_pos\\'\\n+            arguments of the \\'logged_api_call\\' decorator.\\n+            \"\"\"\\n+            # properties may also be a DictView (subclass of Mapping)\\n+            assert isinstance(properties, Mapping)\\n+            copied_properties = dict(properties)\\n+            for pn in blanked_properties:\\n+                try:\\n+                    copied_properties[pn] = BLANKED_OUT_STRING\\n+                except KeyError:\\n+                    pass\\n+            return copied_properties\\n+\\n+        def blanked_args(args, kwargs):\\n+            \"\"\"\\n+            Return a copy of args and kwargs, whereby the \\'properties\\' argument\\n+            has items blanked out according to the \\'blanked_properties\\' and\\n+            \\'properties_pos\\' arguments of the \\'logged_api_call\\' decorator.\\n+            \"\"\"\\n+            logged_kwargs = dict(kwargs)\\n+            logged_args = list(args)\\n+            if blanked_properties is not None:\\n+                if \\'properties\\' in kwargs:\\n+                    logged_kwargs[\\'properties\\'] = \\\\\\n+                        blanked_dict(kwargs[\\'properties\\'])\\n+                else:\\n+                    logged_args[properties_pos] = \\\\\\n+                        blanked_dict(args[properties_pos])\\n+            return tuple(logged_args), logged_kwargs\\n+\\n+        def log_call(args, kwargs):\\n+            \"\"\"\\n+            Log the call to the API function.\\n+            \"\"\"\\n+            logged_args, logged_kwargs = blanked_args(args, kwargs)\\n+            logger.debug(\"Called: %s, args: %.500s, kwargs: %.500s\",\\n+                         apifunc_str,\\n+                         log_escaped(repr(logged_args)),\\n+                         log_escaped(repr(logged_kwargs)))\\n+\\n+        def log_return(result):\\n+            \"\"\"\\n+            Log the return from the API function.\\n+            \"\"\"\\n+            logger.debug(\"Return: %s, result: %.1000s\",\\n+                         apifunc_str, log_escaped(repr(result)))\\n \\n-    def log_api_call(func, *args, **kwargs):\\n-        \"\"\"\\n-        Log entry to and exit from the decorated function, at the debug level.\\n+        @functools.wraps(func)\\n+        def log_api_call(*args, **kwargs):\\n+            \"\"\"\\n+            Log entry to and exit from the decorated function, at the debug\\n+            level.\\n \\n-        Note that this wrapper function is called every time the decorated\\n-        function/method is called, but that the log message only needs to be\\n-        constructed when logging for this logger and for this log level is\\n-        turned on. Therefore, we do as much as possible in the decorator\\n-        function, plus we use %-formatting and lazy interpolation provided by\\n-        the log functions, in order to save resources in this function here.\\n+            Note that this wrapper function is called every time the decorated\\n+            function/method is called, but that the log message only needs to\\n+            be constructed when logging for this logger and for this log level\\n+            is turned on. Therefore, we do as much as possible in the decorator\\n+            function, plus we use %-formatting and lazy interpolation provided\\n+            by the log functions, in order to save resources in this function\\n+            here.\\n \\n-        Parameters:\\n+            Parameters:\\n \\n-          func (function object): The decorated function.\\n+              func (function object): The decorated function.\\n \\n-          *args: Any positional arguments for the decorated function.\\n+              *args: Any positional arguments for the decorated function.\\n \\n-          **kwargs: Any keyword arguments for the decorated function.\\n-        \"\"\"\\n+              **kwargs: Any keyword arguments for the decorated function.\\n+            \"\"\"\\n \\n-        # Note that in this function, we are in the context where the\\n-        # decorated function is actually called.\\n+            # Note that in this function, we are in the context where the\\n+            # decorated function is actually called.\\n+            _log_it = is_external_call() and logger.isEnabledFor(logging.DEBUG)\\n \\n-        _log_it = is_external_call() and logger.isEnabledFor(logging.DEBUG)\\n+            if _log_it:\\n+                log_call(args, kwargs)\\n \\n-        if _log_it:\\n-            logger.debug(\"Called: %s, args: %.500s, kwargs: %.500s\",\\n-                         apifunc_str, log_escaped(repr(args)),\\n-                         log_escaped(repr(kwargs)))\\n+            result = func(*args, **kwargs)  # The zhmcclient function\\n \\n-        result = func(*args, **kwargs)\\n+            if _log_it:\\n+                log_return(result)\\n \\n-        if _log_it:\\n-            logger.debug(\"Return: %s, result: %.1000s\",\\n-                         apifunc_str, log_escaped(repr(result)))\\n+            return result\\n \\n-        return result\\n+        return log_api_call\\n \\n-    return decorate(func, log_api_call)\\n+    # When the logged_api_call decorator is specified with its optional\\n+    # arguments, org_func is None\\n+    if org_func:\\n+        return _decorate(org_func)\\n+    return _decorate', '@@ -149,7 +149,7 @@ def list(self, full_properties=False, filter_args=None):\\n         return self._list_with_operation(\\n             list_uri, result_prop, full_properties, filter_args, None)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'password\\'], properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create a new User in this HMC.\\n@@ -256,7 +256,7 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'password\\'], properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this User.', \"@@ -50,7 +50,8 @@\\n            'HTML_REASON_WEB_SERVICES_DISABLED',\\n            'HTML_REASON_OTHER',\\n            'STOMP_MIN_CONNECTION_CHECK_TIME',\\n-           'DEFAULT_WS_TIMEOUT']\\n+           'DEFAULT_WS_TIMEOUT',\\n+           'BLANKED_OUT_STRING']\\n \\n \\n #: Default HTTP connect timeout in seconds,\\n@@ -187,3 +188,7 @@\\n #: Default WebSocket connect and receive timeout in seconds, for interacting\\n #: with the :class:`zhmcclient.OSConsole` class.\\n DEFAULT_WS_TIMEOUT = 5\\n+\\n+#: Replacement string for blanked out sensitive values in log entries, such as\\n+#: passwords or session tokens.\\n+BLANKED_OUT_STRING = '********'\", '@@ -176,7 +176,8 @@ def list(self, full_properties=False, filter_args=None,\\n             list_uri, result_prop, full_properties, filter_args,\\n             additional_properties)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'boot-ftp-password\\', \\'ssc-master-pw\\'],\\n+                     properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create and configure a Partition in this CPC.\\n@@ -591,7 +592,8 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'boot-ftp-password\\', \\'ssc-master-pw\\'],\\n+                     properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this Partition.', '@@ -39,7 +39,7 @@\\n     DEFAULT_OPERATION_TIMEOUT, DEFAULT_STATUS_TIMEOUT, \\\\\\n     DEFAULT_NAME_URI_CACHE_TIMETOLIVE, HMC_LOGGER_NAME, \\\\\\n     HTML_REASON_WEB_SERVICES_DISABLED, HTML_REASON_OTHER, \\\\\\n-    DEFAULT_HMC_PORT\\n+    DEFAULT_HMC_PORT, BLANKED_OUT_STRING\\n from ._utils import repr_obj_id\\n from ._version import __version__\\n \\n@@ -54,7 +54,14 @@\\n     \\'Accept\\': \\'*/*\\'\\n }\\n \\n-BLANKED_OUT = \\'********\\'  # Replacement for blanked out sensitive values\\n+# Properties whose values are always blanked out in the HMC log entries\\n+BLANKED_OUT_PROPERTIES = [\\n+    \\'boot-ftp-password\\',    # partition create/update\\n+    \\'bind-password\\',        # LDAP server def. create/update\\n+    \\'ssc-master-pw\\',        # image profile cr/upd, part. cr/upd, LPAR upd\\n+    \\'password\\',             # user create/update\\n+    \\'zaware-master-pw\\',     # image profile create/update, LPAR update\\n+]\\n \\n \\n def _handle_request_exc(exc, retry_timeout_config):\\n@@ -263,7 +270,7 @@ def _headers_for_logging(headers):\\n     \"\"\"\\n     if headers and \\'X-API-Session\\' in headers:\\n         headers = headers.copy()\\n-        headers[\\'X-API-Session\\'] = BLANKED_OUT\\n+        headers[\\'X-API-Session\\'] = BLANKED_OUT_STRING\\n     return headers\\n \\n \\n@@ -465,7 +472,7 @@ def __repr__(self):\\n             f\"  _actual_host={self._actual_host!r},\\\\n\"\\n             f\"  _base_url={self._base_url!r},\\\\n\"\\n             f\"  _headers={headers!r},\\\\n\"\\n-            f\"  _session_id={BLANKED_OUT!r},\\\\n\"\\n+            f\"  _session_id={BLANKED_OUT_STRING!r},\\\\n\"\\n             f\"  _session={self._session!r}\\\\n\"\\n             f\"  _object_topic={self._object_topic!r}\\\\n\"\\n             f\"  _job_topic={self._job_topic!r}\\\\n\"\\n@@ -960,8 +967,11 @@ def _log_http_request(\\n                 # structured data such as a password or session IDs.\\n                 pass\\n             else:\\n-                if \\'password\\' in content_dict:\\n-                    content_dict[\\'password\\'] = BLANKED_OUT\\n+                for prop in BLANKED_OUT_PROPERTIES:\\n+                    try:\\n+                        content_dict[prop] = BLANKED_OUT_STRING\\n+                    except KeyError:\\n+                        pass\\n                 content = dict2json(content_dict)\\n             trunc = 30000\\n             if content_len > trunc:\\n@@ -1029,11 +1039,11 @@ def _log_http_response(\\n                 if \\'request-headers\\' in content_dict:\\n                     headers_dict = content_dict[\\'request-headers\\']\\n                     if \\'x-api-session\\' in headers_dict:\\n-                        headers_dict[\\'x-api-session\\'] = BLANKED_OUT\\n+                        headers_dict[\\'x-api-session\\'] = BLANKED_OUT_STRING\\n                 if \\'api-session\\' in content_dict:\\n-                    content_dict[\\'api-session\\'] = BLANKED_OUT\\n+                    content_dict[\\'api-session\\'] = BLANKED_OUT_STRING\\n                 if \\'session-credential\\' in content_dict:\\n-                    content_dict[\\'session-credential\\'] = BLANKED_OUT\\n+                    content_dict[\\'session-credential\\'] = BLANKED_OUT_STRING\\n                 content = dict2json(content_dict)\\n             if status >= 400:\\n                 content_label = \\'content\\'', '@@ -187,7 +187,8 @@ def __init__(self, manager, uri, name=None, properties=None):\\n             f\"got {type(manager)}\")\\n         super().__init__(manager, uri, name, properties)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'ssc-master-pw\\', \\'zaware-master-pw\\'],\\n+                     properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this LPAR.'], 'file': ['zhmcclient/_activation_profile.py', 'zhmcclient/_ldap_server_definition.py', 'zhmcclient/_logging.py', 'zhmcclient/_user.py', 'zhmcclient/_constants.py', 'zhmcclient/_partition.py', 'zhmcclient/_session.py', 'zhmcclient/_lpar.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('51729f67-69e6-4001-8f95-20db5d7872a5'), UUID('ac4f7397-ead3-4386-93c9-445f69edfd03'), UUID('0944b385-9f71-4618-b175-cb9b5eb434cf'), UUID('63499b9b-ca2f-47a2-9399-2a10a9d8b2ca'), UUID('9c707937-c359-493d-84cd-46d5f8bcdfae'), UUID('413c57e4-fdbf-4111-a45b-c6c4efa894da'), UUID('33064de3-11c5-4e8f-9bca-d53efdc70ff8'), UUID('b672352d-c2d4-43c6-b675-b2ab7898a874')]}\n",
      "ERROR:root:Error in {'repo': 'zhmcclient/python-zhmcclient', 'vulnerability_id': '2024-53865', 'commit': 'ad32781e782d0f604c6da4680fce48e4cc1f4433', 'commit_source': 'github', 'cwe_id': ['CWE-312'], 'patch': ['@@ -224,7 +224,8 @@ def list(self, full_properties=False, filter_args=None,\\n             list_uri, result_prop, full_properties, filter_args,\\n             additional_properties)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'ssc-master-pw\\', \\'zaware-master-pw\\'],\\n+                     properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create and configure an Activation Profiles on this CPC, of the profile\\n@@ -344,7 +345,8 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'ssc-master-pw\\', \\'zaware-master-pw\\'],\\n+                     properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this Activation Profile.', '@@ -151,7 +151,7 @@ def list(self, full_properties=False, filter_args=None):\\n         return self._list_with_operation(\\n             list_uri, result_prop, full_properties, filter_args, None)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'bind-password\\'], properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create a new LDAP Server Definition in this HMC.\\n@@ -257,7 +257,7 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'bind-password\\'], properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this LDAP Server Definitions.', '@@ -75,9 +75,10 @@\\n \\n import logging\\n import inspect\\n-from decorator import decorate  # requires decorator>=4.0\\n+import functools\\n+from collections.abc import Mapping, Sequence\\n \\n-from ._constants import API_LOGGER_NAME\\n+from ._constants import API_LOGGER_NAME, BLANKED_OUT_STRING\\n \\n __all__ = []\\n \\n@@ -105,7 +106,8 @@ def get_logger(name):\\n     return logger\\n \\n \\n-def logged_api_call(func):\\n+def logged_api_call(\\n+        org_func=None, *, blanked_properties=None, properties_pos=None):\\n     \"\"\"\\n     Function decorator that causes the decorated API function or method to log\\n     calls to itself to a logger.\\n@@ -115,7 +117,24 @@ def logged_api_call(func):\\n \\n     Parameters:\\n \\n-      func (function object): The original function being decorated.\\n+      org_func (function object): The original function being decorated.\\n+        Will be `None` if the decorator is specified with its optional\\n+        argument \\'blanked_properties\\'.\\n+\\n+      blanked_properties (list of str): Optional: List of properties in the\\n+        \\'properties\\' argument of the decorated API function that should be\\n+        blanked out before being logged. Can be used to hide password\\n+        properties.\\n+        This parameter is required when \\'properties_pos\\' is used.\\n+        This parameter must be specified as a keyword argument.\\n+\\n+      properties_pos (int): Optional: 0-based index of the \\'properties\\'\\n+        parameter in the argument list of the decorated API function.\\n+        For methods, the \\'self\\' or \\'cls\\' parameter is included in the position.\\n+        This parameter is needed in case the properties are passed as a\\n+        positional argument by the caller of the API function.\\n+        This parameter is required when \\'blanked_properties\\' is used.\\n+        This parameter must be specified as a keyword argument.\\n \\n     Returns:\\n \\n@@ -128,109 +147,196 @@ def logged_api_call(func):\\n         method (and not on top of the @property decorator).\\n     \"\"\"\\n \\n-    # Note that in this decorator function, we are in a module loading context,\\n-    # where the decorated functions are being defined. When this decorator\\n-    # function is called, its call stack represents the definition of the\\n-    # decorated functions. Not all global definitions in the module have been\\n-    # defined yet, and methods of classes that are decorated with this\\n-    # decorator are still functions at this point (and not yet methods).\\n+    if blanked_properties is not None and properties_pos is None:\\n+        raise TypeError(\\n+            \"If the @logged_api_call decorator is specified with \"\\n+            \"\\'blanked_properties\\', \\'properties_pos\\' must also be specified.\")\\n \\n-    module = inspect.getmodule(func)\\n-    if not inspect.isfunction(func) or not hasattr(module, \\'__name__\\'):\\n-        raise TypeError(\"The @logged_api_call decorator must be used on a \"\\n-                        \"function or method (and not on top of the @property \"\\n-                        \"decorator)\")\\n+    if properties_pos is not None and blanked_properties is None:\\n+        raise TypeError(\\n+            \"If the @logged_api_call decorator is specified with \"\\n+            \"\\'properties_pos\\', \\'blanked_properties\\' must also be specified.\")\\n \\n-    try:\\n-        # We avoid the use of inspect.getouterframes() because it is slow,\\n-        # and use the pointers up the stack frame, instead.\\n+    if blanked_properties is not None and (\\n+            not isinstance(blanked_properties, Sequence) or  # noqa: W504\\n+            isinstance(blanked_properties, str)):\\n+        raise TypeError(\\n+            \"The \\'blanked_properties\\' parameter of the @logged_api_call \"\\n+            \"decorator must be a list of strings.\")\\n \\n-        this_frame = inspect.currentframe()  # this decorator function here\\n-        apifunc_frame = this_frame.f_back  # the decorated API function\\n-\\n-        apifunc_owner = inspect.getframeinfo(apifunc_frame)[2]\\n+    def _decorate(func):\\n+        \"\"\"\\n+        The actual decorator function that always gets the original decorated\\n+        function, independent of whether the \\'logged_api_call\\' decorator was\\n+        specified with or without its optional arguments.\\n \\n-    finally:\\n-        # Recommended way to deal with frame objects to avoid ref cycles\\n-        del this_frame\\n-        del apifunc_frame\\n+        Parameters:\\n \\n-    # TODO: For inner functions, show all outer levels instead of just one.\\n+          func (function object): The original function being decorated.\\n+        \"\"\"\\n \\n-    if apifunc_owner == \\'<module>\\':\\n-        # The decorated API function is defined globally (at module level)\\n-        apifunc_str = f\\'{func.__name__}()\\'\\n-    else:\\n-        # The decorated API function is defined in a class or in a function\\n-        apifunc_str = f\\'{apifunc_owner}.{func.__name__}()\\'\\n+        # Note that in this decorator function, we are in a module loading\\n+        # context, where the decorated functions are being defined. When this\\n+        # decorator function is called, its call stack represents the\\n+        # definition of the decorated functions. Not all global definitions in\\n+        # the module have been defined yet, and methods of classes that are\\n+        # decorated with this decorator are still functions at this point (and\\n+        # not yet methods).\\n \\n-    logger = get_logger(API_LOGGER_NAME)\\n+        if not inspect.isfunction(func):\\n+            raise TypeError(\"The @logged_api_call decorator must be used on a \"\\n+                            \"function or method (and not on top of the \"\\n+                            \"@property decorator)\")\\n \\n-    def is_external_call():\\n-        \"\"\"\\n-        Return a boolean indicating whether the call to the decorated API\\n-        function is an external call (vs. b eing an internal call).\\n-        \"\"\"\\n         try:\\n             # We avoid the use of inspect.getouterframes() because it is slow,\\n             # and use the pointers up the stack frame, instead.\\n \\n-            log_it_frame = inspect.currentframe()  # this log_it() function\\n-            log_api_call_frame = log_it_frame.f_back  # the log_api_call() func\\n-            apifunc_frame = log_api_call_frame.f_back  # the decorated API func\\n-            apicaller_frame = apifunc_frame.f_back  # caller of API function\\n-            apicaller_module = inspect.getmodule(apicaller_frame)\\n-            if apicaller_module is None:\\n-                apicaller_module_name = \"<unknown>\"\\n-            else:\\n-                apicaller_module_name = apicaller_module.__name__\\n+            this_frame = inspect.currentframe()  # this function\\n+            apifunc_frame = this_frame.f_back  # the decorated API function\\n+            if org_func:\\n+                # In this case, there is one more decorator function nesting\\n+                apifunc_frame = apifunc_frame.f_back\\n+            apifunc_owner = inspect.getframeinfo(apifunc_frame)[2]\\n+\\n         finally:\\n             # Recommended way to deal with frame objects to avoid ref cycles\\n-            del log_it_frame\\n-            del log_api_call_frame\\n+            del this_frame\\n             del apifunc_frame\\n-            del apicaller_frame\\n-            del apicaller_module\\n \\n-        # Log only if the caller is not from the zhmcclient package\\n-        return apicaller_module_name.split(\\'.\\')[0] != \\'zhmcclient\\'\\n+        # TODO: For inner functions, show all outer levels instead of just one.\\n+\\n+        func_name = getattr(func, \\'__name__\\', \\'<unknown>\\')\\n+        if apifunc_owner == \\'<module>\\':\\n+            # The decorated API function is defined globally (at module level)\\n+            apifunc_str = f\\'{func_name}()\\'\\n+        else:\\n+            # The decorated API function is defined in a class or in a function\\n+            apifunc_str = f\\'{apifunc_owner}.{func_name}()\\'\\n+\\n+        logger = get_logger(API_LOGGER_NAME)\\n+\\n+        def is_external_call():\\n+            \"\"\"\\n+            Return a boolean indicating whether the call to the decorated API\\n+            function is made from outside of the zhmcclient package.\\n+            \"\"\"\\n+            try:\\n+                # We avoid the use of inspect.getouterframes() because it is\\n+                # slow, and use the pointers up the stack frame, instead.\\n+\\n+                this_frame = inspect.currentframe()  # this function\\n+                log_api_call_frame = this_frame.f_back  # log_api_call()\\n+                apifunc_frame = log_api_call_frame.f_back  # the decorated func\\n+                apicaller_frame = apifunc_frame.f_back  # caller of API func\\n+                apicaller_module = inspect.getmodule(apicaller_frame)\\n+                if apicaller_module is None:\\n+                    apicaller_module_name = \"<unknown>\"\\n+                else:\\n+                    apicaller_module_name = apicaller_module.__name__\\n+            finally:\\n+                # Recommended way to deal with frame objects to avoid ref\\n+                # cycles\\n+                del this_frame\\n+                del log_api_call_frame\\n+                del apifunc_frame\\n+                del apicaller_frame\\n+                del apicaller_module\\n+\\n+            # Log only if the caller is not from the zhmcclient package\\n+            return apicaller_module_name.split(\\'.\\')[0] != \\'zhmcclient\\'\\n+\\n+        def blanked_dict(properties):\\n+            \"\"\"\\n+            Return a copy of the properties dict, with blanked out values\\n+            according to the \\'blanked_properties\\' and \\'properties_pos\\'\\n+            arguments of the \\'logged_api_call\\' decorator.\\n+            \"\"\"\\n+            # properties may also be a DictView (subclass of Mapping)\\n+            assert isinstance(properties, Mapping)\\n+            copied_properties = dict(properties)\\n+            for pn in blanked_properties:\\n+                try:\\n+                    copied_properties[pn] = BLANKED_OUT_STRING\\n+                except KeyError:\\n+                    pass\\n+            return copied_properties\\n+\\n+        def blanked_args(args, kwargs):\\n+            \"\"\"\\n+            Return a copy of args and kwargs, whereby the \\'properties\\' argument\\n+            has items blanked out according to the \\'blanked_properties\\' and\\n+            \\'properties_pos\\' arguments of the \\'logged_api_call\\' decorator.\\n+            \"\"\"\\n+            logged_kwargs = dict(kwargs)\\n+            logged_args = list(args)\\n+            if blanked_properties is not None:\\n+                if \\'properties\\' in kwargs:\\n+                    logged_kwargs[\\'properties\\'] = \\\\\\n+                        blanked_dict(kwargs[\\'properties\\'])\\n+                else:\\n+                    logged_args[properties_pos] = \\\\\\n+                        blanked_dict(args[properties_pos])\\n+            return tuple(logged_args), logged_kwargs\\n+\\n+        def log_call(args, kwargs):\\n+            \"\"\"\\n+            Log the call to the API function.\\n+            \"\"\"\\n+            logged_args, logged_kwargs = blanked_args(args, kwargs)\\n+            logger.debug(\"Called: %s, args: %.500s, kwargs: %.500s\",\\n+                         apifunc_str,\\n+                         log_escaped(repr(logged_args)),\\n+                         log_escaped(repr(logged_kwargs)))\\n+\\n+        def log_return(result):\\n+            \"\"\"\\n+            Log the return from the API function.\\n+            \"\"\"\\n+            logger.debug(\"Return: %s, result: %.1000s\",\\n+                         apifunc_str, log_escaped(repr(result)))\\n \\n-    def log_api_call(func, *args, **kwargs):\\n-        \"\"\"\\n-        Log entry to and exit from the decorated function, at the debug level.\\n+        @functools.wraps(func)\\n+        def log_api_call(*args, **kwargs):\\n+            \"\"\"\\n+            Log entry to and exit from the decorated function, at the debug\\n+            level.\\n \\n-        Note that this wrapper function is called every time the decorated\\n-        function/method is called, but that the log message only needs to be\\n-        constructed when logging for this logger and for this log level is\\n-        turned on. Therefore, we do as much as possible in the decorator\\n-        function, plus we use %-formatting and lazy interpolation provided by\\n-        the log functions, in order to save resources in this function here.\\n+            Note that this wrapper function is called every time the decorated\\n+            function/method is called, but that the log message only needs to\\n+            be constructed when logging for this logger and for this log level\\n+            is turned on. Therefore, we do as much as possible in the decorator\\n+            function, plus we use %-formatting and lazy interpolation provided\\n+            by the log functions, in order to save resources in this function\\n+            here.\\n \\n-        Parameters:\\n+            Parameters:\\n \\n-          func (function object): The decorated function.\\n+              func (function object): The decorated function.\\n \\n-          *args: Any positional arguments for the decorated function.\\n+              *args: Any positional arguments for the decorated function.\\n \\n-          **kwargs: Any keyword arguments for the decorated function.\\n-        \"\"\"\\n+              **kwargs: Any keyword arguments for the decorated function.\\n+            \"\"\"\\n \\n-        # Note that in this function, we are in the context where the\\n-        # decorated function is actually called.\\n+            # Note that in this function, we are in the context where the\\n+            # decorated function is actually called.\\n+            _log_it = is_external_call() and logger.isEnabledFor(logging.DEBUG)\\n \\n-        _log_it = is_external_call() and logger.isEnabledFor(logging.DEBUG)\\n+            if _log_it:\\n+                log_call(args, kwargs)\\n \\n-        if _log_it:\\n-            logger.debug(\"Called: %s, args: %.500s, kwargs: %.500s\",\\n-                         apifunc_str, log_escaped(repr(args)),\\n-                         log_escaped(repr(kwargs)))\\n+            result = func(*args, **kwargs)  # The zhmcclient function\\n \\n-        result = func(*args, **kwargs)\\n+            if _log_it:\\n+                log_return(result)\\n \\n-        if _log_it:\\n-            logger.debug(\"Return: %s, result: %.1000s\",\\n-                         apifunc_str, log_escaped(repr(result)))\\n+            return result\\n \\n-        return result\\n+        return log_api_call\\n \\n-    return decorate(func, log_api_call)\\n+    # When the logged_api_call decorator is specified with its optional\\n+    # arguments, org_func is None\\n+    if org_func:\\n+        return _decorate(org_func)\\n+    return _decorate', '@@ -149,7 +149,7 @@ def list(self, full_properties=False, filter_args=None):\\n         return self._list_with_operation(\\n             list_uri, result_prop, full_properties, filter_args, None)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'password\\'], properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create a new User in this HMC.\\n@@ -256,7 +256,7 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'password\\'], properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this User.', \"@@ -50,7 +50,8 @@\\n            'HTML_REASON_WEB_SERVICES_DISABLED',\\n            'HTML_REASON_OTHER',\\n            'STOMP_MIN_CONNECTION_CHECK_TIME',\\n-           'DEFAULT_WS_TIMEOUT']\\n+           'DEFAULT_WS_TIMEOUT',\\n+           'BLANKED_OUT_STRING']\\n \\n \\n #: Default HTTP connect timeout in seconds,\\n@@ -187,3 +188,7 @@\\n #: Default WebSocket connect and receive timeout in seconds, for interacting\\n #: with the :class:`zhmcclient.OSConsole` class.\\n DEFAULT_WS_TIMEOUT = 5\\n+\\n+#: Replacement string for blanked out sensitive values in log entries, such as\\n+#: passwords or session tokens.\\n+BLANKED_OUT_STRING = '********'\", '@@ -176,7 +176,8 @@ def list(self, full_properties=False, filter_args=None,\\n             list_uri, result_prop, full_properties, filter_args,\\n             additional_properties)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'boot-ftp-password\\', \\'ssc-master-pw\\'],\\n+                     properties_pos=1)\\n     def create(self, properties):\\n         \"\"\"\\n         Create and configure a Partition in this CPC.\\n@@ -591,7 +592,8 @@ def delete(self):\\n             self.get_properties_local(self.manager._name_prop, None))\\n         self.cease_existence_local()\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'boot-ftp-password\\', \\'ssc-master-pw\\'],\\n+                     properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this Partition.', '@@ -39,7 +39,7 @@\\n     DEFAULT_OPERATION_TIMEOUT, DEFAULT_STATUS_TIMEOUT, \\\\\\n     DEFAULT_NAME_URI_CACHE_TIMETOLIVE, HMC_LOGGER_NAME, \\\\\\n     HTML_REASON_WEB_SERVICES_DISABLED, HTML_REASON_OTHER, \\\\\\n-    DEFAULT_HMC_PORT\\n+    DEFAULT_HMC_PORT, BLANKED_OUT_STRING\\n from ._utils import repr_obj_id\\n from ._version import __version__\\n \\n@@ -54,7 +54,14 @@\\n     \\'Accept\\': \\'*/*\\'\\n }\\n \\n-BLANKED_OUT = \\'********\\'  # Replacement for blanked out sensitive values\\n+# Properties whose values are always blanked out in the HMC log entries\\n+BLANKED_OUT_PROPERTIES = [\\n+    \\'boot-ftp-password\\',    # partition create/update\\n+    \\'bind-password\\',        # LDAP server def. create/update\\n+    \\'ssc-master-pw\\',        # image profile cr/upd, part. cr/upd, LPAR upd\\n+    \\'password\\',             # user create/update\\n+    \\'zaware-master-pw\\',     # image profile create/update, LPAR update\\n+]\\n \\n \\n def _handle_request_exc(exc, retry_timeout_config):\\n@@ -263,7 +270,7 @@ def _headers_for_logging(headers):\\n     \"\"\"\\n     if headers and \\'X-API-Session\\' in headers:\\n         headers = headers.copy()\\n-        headers[\\'X-API-Session\\'] = BLANKED_OUT\\n+        headers[\\'X-API-Session\\'] = BLANKED_OUT_STRING\\n     return headers\\n \\n \\n@@ -465,7 +472,7 @@ def __repr__(self):\\n             f\"  _actual_host={self._actual_host!r},\\\\n\"\\n             f\"  _base_url={self._base_url!r},\\\\n\"\\n             f\"  _headers={headers!r},\\\\n\"\\n-            f\"  _session_id={BLANKED_OUT!r},\\\\n\"\\n+            f\"  _session_id={BLANKED_OUT_STRING!r},\\\\n\"\\n             f\"  _session={self._session!r}\\\\n\"\\n             f\"  _object_topic={self._object_topic!r}\\\\n\"\\n             f\"  _job_topic={self._job_topic!r}\\\\n\"\\n@@ -960,8 +967,11 @@ def _log_http_request(\\n                 # structured data such as a password or session IDs.\\n                 pass\\n             else:\\n-                if \\'password\\' in content_dict:\\n-                    content_dict[\\'password\\'] = BLANKED_OUT\\n+                for prop in BLANKED_OUT_PROPERTIES:\\n+                    try:\\n+                        content_dict[prop] = BLANKED_OUT_STRING\\n+                    except KeyError:\\n+                        pass\\n                 content = dict2json(content_dict)\\n             trunc = 30000\\n             if content_len > trunc:\\n@@ -1029,11 +1039,11 @@ def _log_http_response(\\n                 if \\'request-headers\\' in content_dict:\\n                     headers_dict = content_dict[\\'request-headers\\']\\n                     if \\'x-api-session\\' in headers_dict:\\n-                        headers_dict[\\'x-api-session\\'] = BLANKED_OUT\\n+                        headers_dict[\\'x-api-session\\'] = BLANKED_OUT_STRING\\n                 if \\'api-session\\' in content_dict:\\n-                    content_dict[\\'api-session\\'] = BLANKED_OUT\\n+                    content_dict[\\'api-session\\'] = BLANKED_OUT_STRING\\n                 if \\'session-credential\\' in content_dict:\\n-                    content_dict[\\'session-credential\\'] = BLANKED_OUT\\n+                    content_dict[\\'session-credential\\'] = BLANKED_OUT_STRING\\n                 content = dict2json(content_dict)\\n             if status >= 400:\\n                 content_label = \\'content\\'', '@@ -187,7 +187,8 @@ def __init__(self, manager, uri, name=None, properties=None):\\n             f\"got {type(manager)}\")\\n         super().__init__(manager, uri, name, properties)\\n \\n-    @logged_api_call\\n+    @logged_api_call(blanked_properties=[\\'ssc-master-pw\\', \\'zaware-master-pw\\'],\\n+                     properties_pos=1)\\n     def update_properties(self, properties):\\n         \"\"\"\\n         Update writeable properties of this LPAR.'], 'file': ['zhmcclient/_activation_profile.py', 'zhmcclient/_ldap_server_definition.py', 'zhmcclient/_logging.py', 'zhmcclient/_user.py', 'zhmcclient/_constants.py', 'zhmcclient/_partition.py', 'zhmcclient/_session.py', 'zhmcclient/_lpar.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('51729f67-69e6-4001-8f95-20db5d7872a5'), UUID('ac4f7397-ead3-4386-93c9-445f69edfd03'), UUID('0944b385-9f71-4618-b175-cb9b5eb434cf'), UUID('63499b9b-ca2f-47a2-9399-2a10a9d8b2ca'), UUID('9c707937-c359-493d-84cd-46d5f8bcdfae'), UUID('413c57e4-fdbf-4111-a45b-c6c4efa894da'), UUID('33064de3-11c5-4e8f-9bca-d53efdc70ff8'), UUID('b672352d-c2d4-43c6-b675-b2ab7898a874')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 6:0: class ActivationProfile(BaseResource):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 29:0: class ActivationProfile(BaseResource):\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 798/1800 [07:14<02:19,  7.16it/s]ERROR:src.process_code_changes:Error processing commit 38cb7c08d4d890e8a1badadbd46f459f06e3cdcd\n",
      "ERROR:src.process_code_changes:{'repo': 'Redon-Tech/Redon-Hub', 'vulnerability_id': '2024-31442', 'commit': '38cb7c08d4d890e8a1badadbd46f459f06e3cdcd', 'commit_source': 'github', 'cwe_id': ['CWE-276'], 'patch': ['@@ -2,10 +2,10 @@\\n     File: /bot/cogs/product.py\\n     Usage: Product related commands\\n \"\"\"\\n+\\n from discord import (\\n     app_commands,\\n     Interaction,\\n-    Member,\\n     Embed,\\n     utils,\\n     ui,\\n@@ -14,13 +14,13 @@\\n     ButtonStyle,\\n     Role,\\n )\\n+from discord.app_commands import MissingPermissions\\n from asyncio import TimeoutError\\n from discord.ext.commands import Cog\\n from bot.data import (\\n     get_users,\\n     get_products,\\n     get_product_by_name,\\n-    get_product,\\n     create_product,\\n     delete_product,\\n     get_tags,\\n@@ -817,7 +817,6 @@ def __init__(self, bot):\\n         name=\"admin\",\\n         description=\"Product Admin Commands\",\\n         parent=product_commands,\\n-        default_permissions=None,\\n     )\\n \\n     @app_commands.command(\\n@@ -893,6 +892,7 @@ async def get_product_info_command_autocomplete(\\n     @product_admin.command(\\n         name=\"stats\", description=\"Get statistics on a specific product\"\\n     )\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def get_product_stats_info_command(\\n         self, interaction: Interaction, product_name: str\\n     ):\\n@@ -939,10 +939,12 @@ async def get_product_stats_info_command_autocomplete(\\n         ]\\n \\n     @product_admin.command(name=\"create\", description=\"Create a new product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def create_product_command(self, interaction: Interaction):\\n         await interaction.response.send_modal(createProduct(bot=self.bot))\\n \\n     @product_admin.command(name=\"delete\", description=\"Delete a product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def delete_product_command(self, interaction: Interaction, product_name: str):\\n         try:\\n             product = await get_product_by_name(product_name)\\n@@ -1033,17 +1035,19 @@ async def update_product_command_autocomplete(\\n         ]\\n \\n     @product_admin.command(name=\"clear\", description=\"Delete all products\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def clear_products_command(self, interaction: Interaction):\\n         if interaction.user.id not in config.Bot.Owners:\\n-            await interaction.response.send_message(\\n-                embed=Embed(\\n-                    title=\"Error\",\\n-                    description=\"You are not allowed to use this command.\",\\n-                    colour=interaction.user.colour,\\n-                    timestamp=utils.utcnow(),\\n-                ).set_footer(text=f\"Redon Hub â€¢ Version {self.bot.version}\"),\\n-            )\\n-            return\\n+            # await interaction.response.send_message(\\n+            #     embed=Embed(\\n+            #         title=\"Error\",\\n+            #         description=\"You are not allowed to use this command.\",\\n+            #         colour=interaction.user.colour,\\n+            #         timestamp=utils.utcnow(),\\n+            #     ).set_footer(text=f\"Redon Hub â€¢ Version {self.bot.version}\"),\\n+            # )\\n+            # return\\n+            raise MissingPermissions(\"Bot Owner\")\\n \\n         try:\\n             products = await get_products()\\n@@ -1114,6 +1118,7 @@ async def clear_products_command(self, interaction: Interaction):\\n             )\\n \\n     @product_admin.command(name=\"update\", description=\"Update a product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def update_product_command(self, interaction: Interaction, product_name: str):\\n         await interaction.response.defer()\\n ', '@@ -2,7 +2,15 @@\\n     File: /bot/cogs/user.py\\n     Usage: User related commands\\n \"\"\"\\n-from discord import app_commands, Interaction, Member, Embed, utils, Forbidden\\n+\\n+from discord import (\\n+    app_commands,\\n+    Interaction,\\n+    Member,\\n+    Embed,\\n+    utils,\\n+    Forbidden,\\n+)\\n from discord.ext.commands import Cog\\n from bot.data import (\\n     get_user_by_discord_id,\\n@@ -27,7 +35,6 @@ def __init__(self, bot):\\n         name=\"admin\",\\n         description=\"User Admin Commands\",\\n         parent=user_commands,\\n-        default_permissions=None,\\n     )\\n \\n     @app_commands.command(name=\"profile\", description=\"View a user\\'s profile\")\\n@@ -391,6 +398,7 @@ async def user_transfer_autocomplete(\\n         ]\\n \\n     @user_admin.command(name=\"give\", description=\"Give a user a product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_give(\\n         self, interaction: Interaction, product_name: str, member: Member\\n     ):\\n@@ -483,6 +491,7 @@ async def user_admin_give(\\n             )\\n \\n     @user_admin_give.autocomplete(\"product_name\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_give_autocomplete(\\n         self, interaction: Interaction, current_product_name: str\\n     ):\\n@@ -498,6 +507,7 @@ async def user_admin_give_autocomplete(\\n         ]\\n \\n     @user_admin.command(name=\"revoke\", description=\"Revoke a product from a user\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_revoke(\\n         self, interaction: Interaction, product_name: str, member: Member\\n     ):\\n@@ -572,6 +582,7 @@ async def user_admin_revoke(\\n             )\\n \\n     @user_admin_revoke.autocomplete(\"product_name\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_revoke_autocomplete(\\n         self, interaction: Interaction, current_product_name: str\\n     ):', '@@ -2,15 +2,14 @@\\n     File: /bot/cogs/tags.py\\n     Usage: Tag related commands\\n \"\"\"\\n+\\n from discord import (\\n     app_commands,\\n     Interaction,\\n     Embed,\\n     utils,\\n     ui,\\n-    TextStyle,\\n     SelectOption,\\n-    ButtonStyle,\\n )\\n from discord.ext.commands import Cog\\n from bot.data import get_tag, get_tag_by_name, get_tags, create_tag, delete_tag, Tag\\n@@ -192,7 +191,6 @@ def __init__(self, bot):\\n         name=\"admin\",\\n         description=\"Tag Admin Commands\",\\n         parent=tag_commands,\\n-        default_permissions=None,\\n     )\\n \\n     @app_commands.command(name=\"tags\", description=\"View all the tags this server has\")\\n@@ -260,10 +258,12 @@ async def get_tag_info_command_autocomplete(\\n         ]\\n \\n     @tag_admin.command(name=\"create\", description=\"Create a new tag\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def create_tag_command(self, interaction: Interaction):\\n         await interaction.response.send_modal(createTag(bot=self.bot))\\n \\n     @tag_admin.command(name=\"delete\", description=\"Delete a tag\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def delete_tag_command(self, interaction: Interaction):\\n         await interaction.response.defer()\\n \\n@@ -280,6 +280,7 @@ async def delete_tag_command(self, interaction: Interaction):\\n         )\\n \\n     @tag_admin.command(name=\"update\", description=\"Update a tag\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def update_tag_command(self, interaction: Interaction, tag_name: str):\\n         try:\\n             tag = await get_tag_by_name(tag_name)'], 'file': ['bot/cogs/product.py', 'bot/cogs/user.py', 'bot/cogs/tag.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('d855b332-8eb0-49f8-bb63-77ac98bac621'), UUID('8140c271-8e87-49e2-b0f3-1435ae3dd5a8'), UUID('594585be-d8cd-404c-86ec-b07c840a4519')]}\n",
      "ERROR:root:Error in {'repo': 'Redon-Tech/Redon-Hub', 'vulnerability_id': '2024-31442', 'commit': '38cb7c08d4d890e8a1badadbd46f459f06e3cdcd', 'commit_source': 'github', 'cwe_id': ['CWE-276'], 'patch': ['@@ -2,10 +2,10 @@\\n     File: /bot/cogs/product.py\\n     Usage: Product related commands\\n \"\"\"\\n+\\n from discord import (\\n     app_commands,\\n     Interaction,\\n-    Member,\\n     Embed,\\n     utils,\\n     ui,\\n@@ -14,13 +14,13 @@\\n     ButtonStyle,\\n     Role,\\n )\\n+from discord.app_commands import MissingPermissions\\n from asyncio import TimeoutError\\n from discord.ext.commands import Cog\\n from bot.data import (\\n     get_users,\\n     get_products,\\n     get_product_by_name,\\n-    get_product,\\n     create_product,\\n     delete_product,\\n     get_tags,\\n@@ -817,7 +817,6 @@ def __init__(self, bot):\\n         name=\"admin\",\\n         description=\"Product Admin Commands\",\\n         parent=product_commands,\\n-        default_permissions=None,\\n     )\\n \\n     @app_commands.command(\\n@@ -893,6 +892,7 @@ async def get_product_info_command_autocomplete(\\n     @product_admin.command(\\n         name=\"stats\", description=\"Get statistics on a specific product\"\\n     )\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def get_product_stats_info_command(\\n         self, interaction: Interaction, product_name: str\\n     ):\\n@@ -939,10 +939,12 @@ async def get_product_stats_info_command_autocomplete(\\n         ]\\n \\n     @product_admin.command(name=\"create\", description=\"Create a new product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def create_product_command(self, interaction: Interaction):\\n         await interaction.response.send_modal(createProduct(bot=self.bot))\\n \\n     @product_admin.command(name=\"delete\", description=\"Delete a product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def delete_product_command(self, interaction: Interaction, product_name: str):\\n         try:\\n             product = await get_product_by_name(product_name)\\n@@ -1033,17 +1035,19 @@ async def update_product_command_autocomplete(\\n         ]\\n \\n     @product_admin.command(name=\"clear\", description=\"Delete all products\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def clear_products_command(self, interaction: Interaction):\\n         if interaction.user.id not in config.Bot.Owners:\\n-            await interaction.response.send_message(\\n-                embed=Embed(\\n-                    title=\"Error\",\\n-                    description=\"You are not allowed to use this command.\",\\n-                    colour=interaction.user.colour,\\n-                    timestamp=utils.utcnow(),\\n-                ).set_footer(text=f\"Redon Hub â€¢ Version {self.bot.version}\"),\\n-            )\\n-            return\\n+            # await interaction.response.send_message(\\n+            #     embed=Embed(\\n+            #         title=\"Error\",\\n+            #         description=\"You are not allowed to use this command.\",\\n+            #         colour=interaction.user.colour,\\n+            #         timestamp=utils.utcnow(),\\n+            #     ).set_footer(text=f\"Redon Hub â€¢ Version {self.bot.version}\"),\\n+            # )\\n+            # return\\n+            raise MissingPermissions(\"Bot Owner\")\\n \\n         try:\\n             products = await get_products()\\n@@ -1114,6 +1118,7 @@ async def clear_products_command(self, interaction: Interaction):\\n             )\\n \\n     @product_admin.command(name=\"update\", description=\"Update a product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def update_product_command(self, interaction: Interaction, product_name: str):\\n         await interaction.response.defer()\\n ', '@@ -2,7 +2,15 @@\\n     File: /bot/cogs/user.py\\n     Usage: User related commands\\n \"\"\"\\n-from discord import app_commands, Interaction, Member, Embed, utils, Forbidden\\n+\\n+from discord import (\\n+    app_commands,\\n+    Interaction,\\n+    Member,\\n+    Embed,\\n+    utils,\\n+    Forbidden,\\n+)\\n from discord.ext.commands import Cog\\n from bot.data import (\\n     get_user_by_discord_id,\\n@@ -27,7 +35,6 @@ def __init__(self, bot):\\n         name=\"admin\",\\n         description=\"User Admin Commands\",\\n         parent=user_commands,\\n-        default_permissions=None,\\n     )\\n \\n     @app_commands.command(name=\"profile\", description=\"View a user\\'s profile\")\\n@@ -391,6 +398,7 @@ async def user_transfer_autocomplete(\\n         ]\\n \\n     @user_admin.command(name=\"give\", description=\"Give a user a product\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_give(\\n         self, interaction: Interaction, product_name: str, member: Member\\n     ):\\n@@ -483,6 +491,7 @@ async def user_admin_give(\\n             )\\n \\n     @user_admin_give.autocomplete(\"product_name\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_give_autocomplete(\\n         self, interaction: Interaction, current_product_name: str\\n     ):\\n@@ -498,6 +507,7 @@ async def user_admin_give_autocomplete(\\n         ]\\n \\n     @user_admin.command(name=\"revoke\", description=\"Revoke a product from a user\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_revoke(\\n         self, interaction: Interaction, product_name: str, member: Member\\n     ):\\n@@ -572,6 +582,7 @@ async def user_admin_revoke(\\n             )\\n \\n     @user_admin_revoke.autocomplete(\"product_name\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def user_admin_revoke_autocomplete(\\n         self, interaction: Interaction, current_product_name: str\\n     ):', '@@ -2,15 +2,14 @@\\n     File: /bot/cogs/tags.py\\n     Usage: Tag related commands\\n \"\"\"\\n+\\n from discord import (\\n     app_commands,\\n     Interaction,\\n     Embed,\\n     utils,\\n     ui,\\n-    TextStyle,\\n     SelectOption,\\n-    ButtonStyle,\\n )\\n from discord.ext.commands import Cog\\n from bot.data import get_tag, get_tag_by_name, get_tags, create_tag, delete_tag, Tag\\n@@ -192,7 +191,6 @@ def __init__(self, bot):\\n         name=\"admin\",\\n         description=\"Tag Admin Commands\",\\n         parent=tag_commands,\\n-        default_permissions=None,\\n     )\\n \\n     @app_commands.command(name=\"tags\", description=\"View all the tags this server has\")\\n@@ -260,10 +258,12 @@ async def get_tag_info_command_autocomplete(\\n         ]\\n \\n     @tag_admin.command(name=\"create\", description=\"Create a new tag\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def create_tag_command(self, interaction: Interaction):\\n         await interaction.response.send_modal(createTag(bot=self.bot))\\n \\n     @tag_admin.command(name=\"delete\", description=\"Delete a tag\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def delete_tag_command(self, interaction: Interaction):\\n         await interaction.response.defer()\\n \\n@@ -280,6 +280,7 @@ async def delete_tag_command(self, interaction: Interaction):\\n         )\\n \\n     @tag_admin.command(name=\"update\", description=\"Update a tag\")\\n+    @app_commands.checks.has_permissions(administrator=True)\\n     async def update_tag_command(self, interaction: Interaction, tag_name: str):\\n         try:\\n             tag = await get_tag_by_name(tag_name)'], 'file': ['bot/cogs/product.py', 'bot/cogs/user.py', 'bot/cogs/tag.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('d855b332-8eb0-49f8-bb63-77ac98bac621'), UUID('8140c271-8e87-49e2-b0f3-1435ae3dd5a8'), UUID('594585be-d8cd-404c-86ec-b07c840a4519')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 88:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 88:0: <line number missing in source>\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 835/1800 [07:16<01:34, 10.18it/s]ERROR:src.process_code_changes:Error processing commit 795f2597a4be988e2bb19b69ff9958e981cb894e\n",
      "ERROR:src.process_code_changes:{'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': '795f2597a4be988e2bb19b69ff9958e981cb894e', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -9,6 +9,7 @@\\n import itertools\\n import os\\n import posixpath\\n+import re\\n import shutil\\n import stat\\n import struct\\n@@ -2243,7 +2244,65 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class CompleteDirs(ZipFile):\\n+class SanitizedNames:\\n+    \"\"\"\\n+    ZipFile mix-in to ensure names are sanitized.\\n+    \"\"\"\\n+\\n+    def namelist(self):\\n+        return list(map(self._sanitize, super().namelist()))\\n+\\n+    @staticmethod\\n+    def _sanitize(name):\\n+        r\"\"\"\\n+        Ensure a relative path with posix separators and no dot names.\\n+        Modeled after\\n+        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n+        but provides consistent cross-platform behavior.\\n+        >>> san = SanitizedNames._sanitize\\n+        >>> san(\\'/foo/bar\\')\\n+        \\'foo/bar\\'\\n+        >>> san(\\'//foo.txt\\')\\n+        \\'foo.txt\\'\\n+        >>> san(\\'foo/.././bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'foo../.bar.txt\\')\\n+        \\'foo../.bar.txt\\'\\n+        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n+        \\'D/foo.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n+        \\'server/share/file.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n+        \\'?/GLOBALROOT/Volume3\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n+        \\'PhysicalDrive1/root\\'\\n+        Retain any trailing slash.\\n+        >>> san(\\'abc/\\')\\n+        \\'abc/\\'\\n+        Raises a ValueError if the result is empty.\\n+        >>> san(\\'../..\\')\\n+        Traceback (most recent call last):\\n+        ...\\n+        ValueError: Empty filename\\n+        \"\"\"\\n+\\n+        def allowed(part):\\n+            return part and part not in {\\'..\\', \\'.\\'}\\n+\\n+        # Remove the drive letter.\\n+        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n+        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n+        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n+        parts = clean.split(\\'/\\')\\n+        joined = \\'/\\'.join(filter(allowed, parts))\\n+        if not joined:\\n+            raise ValueError(\"Empty filename\")\\n+        return joined + \\'/\\' * name.endswith(\\'/\\')\\n+\\n+\\n+class CompleteDirs(SanitizedNames, ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('ec1554d1-dd3b-4fa9-9c9d-9667e89106bd')]}\n",
      "ERROR:root:Error in {'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': '795f2597a4be988e2bb19b69ff9958e981cb894e', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -9,6 +9,7 @@\\n import itertools\\n import os\\n import posixpath\\n+import re\\n import shutil\\n import stat\\n import struct\\n@@ -2243,7 +2244,65 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class CompleteDirs(ZipFile):\\n+class SanitizedNames:\\n+    \"\"\"\\n+    ZipFile mix-in to ensure names are sanitized.\\n+    \"\"\"\\n+\\n+    def namelist(self):\\n+        return list(map(self._sanitize, super().namelist()))\\n+\\n+    @staticmethod\\n+    def _sanitize(name):\\n+        r\"\"\"\\n+        Ensure a relative path with posix separators and no dot names.\\n+        Modeled after\\n+        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n+        but provides consistent cross-platform behavior.\\n+        >>> san = SanitizedNames._sanitize\\n+        >>> san(\\'/foo/bar\\')\\n+        \\'foo/bar\\'\\n+        >>> san(\\'//foo.txt\\')\\n+        \\'foo.txt\\'\\n+        >>> san(\\'foo/.././bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'foo../.bar.txt\\')\\n+        \\'foo../.bar.txt\\'\\n+        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n+        \\'foo/bar.txt\\'\\n+        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n+        \\'D/foo.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n+        \\'server/share/file.txt\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n+        \\'?/GLOBALROOT/Volume3\\'\\n+        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n+        \\'PhysicalDrive1/root\\'\\n+        Retain any trailing slash.\\n+        >>> san(\\'abc/\\')\\n+        \\'abc/\\'\\n+        Raises a ValueError if the result is empty.\\n+        >>> san(\\'../..\\')\\n+        Traceback (most recent call last):\\n+        ...\\n+        ValueError: Empty filename\\n+        \"\"\"\\n+\\n+        def allowed(part):\\n+            return part and part not in {\\'..\\', \\'.\\'}\\n+\\n+        # Remove the drive letter.\\n+        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n+        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n+        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n+        parts = clean.split(\\'/\\')\\n+        joined = \\'/\\'.join(filter(allowed, parts))\\n+        if not joined:\\n+            raise ValueError(\"Empty filename\")\\n+        return joined + \\'/\\' * name.endswith(\\'/\\')\\n+\\n+\\n+class CompleteDirs(SanitizedNames, ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('ec1554d1-dd3b-4fa9-9c9d-9667e89106bd')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 27:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 64:0: <line number missing in source>\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 874/1800 [07:17<01:04, 14.32it/s]ERROR:src.process_code_changes:Error processing commit 8e14ac93669df4f35fcdebd55dc9d2f0fed3ed48\n",
      "ERROR:src.process_code_changes:{'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-0398', 'commit': '8e14ac93669df4f35fcdebd55dc9d2f0fed3ed48', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -16,6 +16,7 @@\\n from django.utils.translation import ugettext as _, ungettext\\n from django.views import generic\\n from django.views.decorators.csrf import ensure_csrf_cookie\\n+from django.views.decorators.http import require_http_methods\\n \\n from modoboa.core import signals as core_signals\\n from modoboa.lib.exceptions import PermDeniedException\\n@@ -230,6 +231,7 @@ def editdomain(request, dom_id):\\n \\n @login_required\\n @permission_required(\"admin.delete_domain\")\\n+@require_http_methods([\"POST\"])\\n def deldomain(request, dom_id):\\n     keepdir = request.POST.get(\"keepdir\", \"false\") == \"true\"\\n     try:'], 'file': ['modoboa/admin/views/domain.py'], 'language': ['Python'], 'temp_id': [UUID('0f1e258c-a610-49c6-8945-376a9ca8483d')]}\n",
      "ERROR:root:Error in {'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-0398', 'commit': '8e14ac93669df4f35fcdebd55dc9d2f0fed3ed48', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -16,6 +16,7 @@\\n from django.utils.translation import ugettext as _, ungettext\\n from django.views import generic\\n from django.views.decorators.csrf import ensure_csrf_cookie\\n+from django.views.decorators.http import require_http_methods\\n \\n from modoboa.core import signals as core_signals\\n from modoboa.lib.exceptions import PermDeniedException\\n@@ -230,6 +231,7 @@ def editdomain(request, dom_id):\\n \\n @login_required\\n @permission_required(\"admin.delete_domain\")\\n+@require_http_methods([\"POST\"])\\n def deldomain(request, dom_id):\\n     keepdir = request.POST.get(\"keepdir\", \"false\") == \"true\"\\n     try:'], 'file': ['modoboa/admin/views/domain.py'], 'language': ['Python'], 'temp_id': [UUID('0f1e258c-a610-49c6-8945-376a9ca8483d')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 892/1800 [07:23<02:27,  6.15it/s]ERROR:src.process_code_changes:Error processing commit 38d778cc71e370216e067d054ce0169ad83078c8\n",
      "ERROR:src.process_code_changes:{'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-0438', 'commit': '38d778cc71e370216e067d054ce0169ad83078c8', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -196,6 +196,7 @@ def delaccount(request, pk):\\n \\n @login_required\\n @permission_required(\"admin.add_domain\")\\n+@require_http_methods([\"DELETE\"])\\n def remove_permission(request):\\n     domid = request.GET.get(\"domid\", None)\\n     daid = request.GET.get(\"daid\", None)', '@@ -125,7 +125,7 @@ <h3 class=\"panel-title\">\\n                 {% trans \"Show key\" %}\\n               </button>\\n               <button data-toggle=\"modal\" class=\"btn btn-default btn-xs\" data-target=\"#dkim_regenerate_dialog\"><span class=\"fa fa-refresh\"></span></button>\\n-              \\n+\\n               <div class=\"modal fade\" id=\"dkim_regenerate_dialog\" tabindex=\"-1\" role=\"dialog\" aria-labelledby=\"dkim_regenerate_dialog\" aria-hidden=\"true\">\\n                 <div class=\"modal-dialog\" role=\"document\">\\n                     <div class=\"modal-content\">\\n@@ -206,6 +206,16 @@ <h5 class=\"modal-title\" id=\"dkim_regenerate_dialog\">{% trans \"Warning\" %}</h5>\\n               window.location.reload();\\n           });\\n       });\\n+      $(\\'a[name=\"removeperm\"]\\').click(function (evt) {\\n+          evt.preventDefault();\\n+          var $this = $(this);\\n+          $.ajax({\\n+              url: $this.attr(\\'href\\'),\\n+              method: \\'DELETE\\'\\n+          }).done(function () {\\n+              window.location.reload();\\n+          });\\n+      });\\n   });\\n   function copy(id) {\\n     navigator.clipboard.writeText(document.getElementById(id).textContent);'], 'file': ['modoboa/admin/views/identity.py', 'modoboa/admin/templates/admin/domain_detail.html'], 'language': ['Python', 'HTML'], 'temp_id': [UUID('875e1728-217a-45aa-b70a-2e7ba3cc321f'), UUID('b9d5ef32-8bf7-4f1e-83ee-0d8b82571762')]}\n",
      "ERROR:root:Error in {'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-0438', 'commit': '38d778cc71e370216e067d054ce0169ad83078c8', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -196,6 +196,7 @@ def delaccount(request, pk):\\n \\n @login_required\\n @permission_required(\"admin.add_domain\")\\n+@require_http_methods([\"DELETE\"])\\n def remove_permission(request):\\n     domid = request.GET.get(\"domid\", None)\\n     daid = request.GET.get(\"daid\", None)', '@@ -125,7 +125,7 @@ <h3 class=\"panel-title\">\\n                 {% trans \"Show key\" %}\\n               </button>\\n               <button data-toggle=\"modal\" class=\"btn btn-default btn-xs\" data-target=\"#dkim_regenerate_dialog\"><span class=\"fa fa-refresh\"></span></button>\\n-              \\n+\\n               <div class=\"modal fade\" id=\"dkim_regenerate_dialog\" tabindex=\"-1\" role=\"dialog\" aria-labelledby=\"dkim_regenerate_dialog\" aria-hidden=\"true\">\\n                 <div class=\"modal-dialog\" role=\"document\">\\n                     <div class=\"modal-content\">\\n@@ -206,6 +206,16 @@ <h5 class=\"modal-title\" id=\"dkim_regenerate_dialog\">{% trans \"Warning\" %}</h5>\\n               window.location.reload();\\n           });\\n       });\\n+      $(\\'a[name=\"removeperm\"]\\').click(function (evt) {\\n+          evt.preventDefault();\\n+          var $this = $(this);\\n+          $.ajax({\\n+              url: $this.attr(\\'href\\'),\\n+              method: \\'DELETE\\'\\n+          }).done(function () {\\n+              window.location.reload();\\n+          });\\n+      });\\n   });\\n   function copy(id) {\\n     navigator.clipboard.writeText(document.getElementById(id).textContent);'], 'file': ['modoboa/admin/views/identity.py', 'modoboa/admin/templates/admin/domain_detail.html'], 'language': ['Python', 'HTML'], 'temp_id': [UUID('875e1728-217a-45aa-b70a-2e7ba3cc321f'), UUID('b9d5ef32-8bf7-4f1e-83ee-0d8b82571762')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 922/1800 [07:34<05:01,  2.91it/s]ERROR:src.process_code_changes:Error processing commit d428ac871909444e175ba421bf8ab4980d6ebf9f\n",
      "ERROR:src.process_code_changes:{'repo': 'jupyter-server/jupyter-scheduler', 'vulnerability_id': '2024-28188', 'commit': 'd428ac871909444e175ba421bf8ab4980d6ebf9f', 'commit_source': 'github', 'cwe_id': ['CWE-200', 'CWE-200'], 'patch': ['@@ -355,6 +355,7 @@ async def get(self):\\n \\n \\n class RuntimeEnvironmentsHandler(ExtensionHandlerMixin, JobHandlersMixin, APIHandler):\\n+    @authenticated\\n     async def get(self):\\n         \"\"\"Returns names of available runtime environments and output formats mappings\"\"\"\\n         try:'], 'file': ['jupyter_scheduler/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('1c01b279-ebb3-4599-89bc-242b3da50d41')]}\n",
      "ERROR:root:Error in {'repo': 'jupyter-server/jupyter-scheduler', 'vulnerability_id': '2024-28188', 'commit': 'd428ac871909444e175ba421bf8ab4980d6ebf9f', 'commit_source': 'github', 'cwe_id': ['CWE-200', 'CWE-200'], 'patch': ['@@ -355,6 +355,7 @@ async def get(self):\\n \\n \\n class RuntimeEnvironmentsHandler(ExtensionHandlerMixin, JobHandlersMixin, APIHandler):\\n+    @authenticated\\n     async def get(self):\\n         \"\"\"Returns names of available runtime environments and output formats mappings\"\"\"\\n         try:'], 'file': ['jupyter_scheduler/handlers.py'], 'language': ['Python'], 'temp_id': [UUID('1c01b279-ebb3-4599-89bc-242b3da50d41')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 972/1800 [07:44<03:22,  4.10it/s]ERROR:src.process_code_changes:Error processing commit 5ce582448ececb8d9c30c8c31f58330090ced03a\n",
      "ERROR:src.process_code_changes:{'repo': 'yt-dlp/yt-dlp', 'vulnerability_id': '2024-38519', 'commit': '5ce582448ececb8d9c30c8c31f58330090ced03a', 'commit_source': 'github', 'cwe_id': ['CWE-669', 'CWE-669'], 'patch': [\"@@ -474,7 +474,7 @@ def _alias_callback(option, opt_str, value, parser, opts, nargs):\\n                 'no-attach-info-json', 'embed-thumbnail-atomicparsley', 'no-external-downloader-progress',\\n                 'embed-metadata', 'seperate-video-versions', 'no-clean-infojson', 'no-keep-subs', 'no-certifi',\\n                 'no-youtube-channel-redirect', 'no-youtube-unavailable-videos', 'no-youtube-prefer-utc-upload-date',\\n-                'prefer-legacy-http-handler', 'manifest-filesize-approx',\\n+                'prefer-legacy-http-handler', 'manifest-filesize-approx', 'allow-unsafe-ext',\\n             }, 'aliases': {\\n                 'youtube-dl': ['all', '-multistreams', '-playlist-match-filter', '-manifest-filesize-approx'],\\n                 'youtube-dlc': ['all', '-no-youtube-channel-redirect', '-no-live-chat', '-playlist-match-filter', '-manifest-filesize-approx'],\", '@@ -159,7 +159,7 @@\\n     write_json_file,\\n     write_string,\\n )\\n-from .utils._utils import _YDLLogger\\n+from .utils._utils import _UnsafeExtensionError, _YDLLogger\\n from .utils.networking import (\\n     HTTPHeaderDict,\\n     clean_headers,\\n@@ -172,6 +172,20 @@\\n     import ctypes\\n \\n \\n+def _catch_unsafe_extension_error(func):\\n+    @functools.wraps(func)\\n+    def wrapper(self, *args, **kwargs):\\n+        try:\\n+            return func(self, *args, **kwargs)\\n+        except _UnsafeExtensionError as error:\\n+            self.report_error(\\n+                f\\'The extracted extension ({error.extension!r}) is unusual \\'\\n+                \\'and will be skipped for safety reasons. \\'\\n+                f\\'If you believe this is an error{bug_reports_message(\",\")}\\')\\n+\\n+    return wrapper\\n+\\n+\\n class YoutubeDL:\\n     \"\"\"YoutubeDL class.\\n \\n@@ -454,8 +468,9 @@ class YoutubeDL:\\n                        Set the value to \\'native\\' to use the native downloader\\n     compat_opts:       Compatibility options. See \"Differences in default behavior\".\\n                        The following options do not work when used through the API:\\n-                       filename, abort-on-error, multistreams, no-live-chat, format-sort\\n-                       no-clean-infojson, no-playlist-metafiles, no-keep-subs, no-attach-info-json.\\n+                       filename, abort-on-error, multistreams, no-live-chat,\\n+                       format-sort, no-clean-infojson, no-playlist-metafiles,\\n+                       no-keep-subs, no-attach-info-json, allow-unsafe-ext.\\n                        Refer __init__.py for their implementation\\n     progress_template: Dictionary of templates for progress outputs.\\n                        Allowed keys are \\'download\\', \\'postprocess\\',\\n@@ -1400,6 +1415,7 @@ def evaluate_outtmpl(self, outtmpl, info_dict, *args, **kwargs):\\n         outtmpl, info_dict = self.prepare_outtmpl(outtmpl, info_dict, *args, **kwargs)\\n         return self.escape_outtmpl(outtmpl) % info_dict\\n \\n+    @_catch_unsafe_extension_error\\n     def _prepare_filename(self, info_dict, *, outtmpl=None, tmpl_type=None):\\n         assert None in (outtmpl, tmpl_type), \\'outtmpl and tmpl_type are mutually exclusive\\'\\n         if outtmpl is None:\\n@@ -3192,6 +3208,7 @@ def existing_file(self, filepaths, *, default_overwrite=True):\\n             os.remove(file)\\n         return None\\n \\n+    @_catch_unsafe_extension_error\\n     def process_info(self, info_dict):\\n         \"\"\"Process a single resolved IE result. (Modifies it in-place)\"\"\"\\n ', '@@ -2085,17 +2085,20 @@ def parse_duration(s):\\n         (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))\\n \\n \\n-def prepend_extension(filename, ext, expected_real_ext=None):\\n+def _change_extension(prepend, filename, ext, expected_real_ext=None):\\n     name, real_ext = os.path.splitext(filename)\\n-    return (\\n-        f\\'{name}.{ext}{real_ext}\\'\\n-        if not expected_real_ext or real_ext[1:] == expected_real_ext\\n-        else f\\'{filename}.{ext}\\')\\n \\n+    if not expected_real_ext or real_ext[1:] == expected_real_ext:\\n+        filename = name\\n+        if prepend and real_ext:\\n+            _UnsafeExtensionError.sanitize_extension(ext, prepend=True)\\n+            return f\\'{filename}.{ext}{real_ext}\\'\\n+\\n+    return f\\'{filename}.{_UnsafeExtensionError.sanitize_extension(ext)}\\'\\n \\n-def replace_extension(filename, ext, expected_real_ext=None):\\n-    name, real_ext = os.path.splitext(filename)\\n-    return f\\'{name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename}.{ext}\\'\\n+\\n+prepend_extension = functools.partial(_change_extension, True)\\n+replace_extension = functools.partial(_change_extension, False)\\n \\n \\n def check_executable(exe, args=[]):\\n@@ -5035,6 +5038,101 @@ def items_(self):\\n KNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)\\n \\n \\n+class _UnsafeExtensionError(Exception):\\n+    \"\"\"\\n+    Mitigation exception for uncommon/malicious file extensions\\n+    This should be caught in YoutubeDL.py alongside a warning\\n+\\n+    Ref: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\\n+    \"\"\"\\n+    ALLOWED_EXTENSIONS = frozenset([\\n+        # internal\\n+        \\'description\\',\\n+        \\'json\\',\\n+        \\'meta\\',\\n+        \\'orig\\',\\n+        \\'part\\',\\n+        \\'temp\\',\\n+        \\'uncut\\',\\n+        \\'unknown_video\\',\\n+        \\'ytdl\\',\\n+\\n+        # video\\n+        *MEDIA_EXTENSIONS.video,\\n+        \\'avif\\',\\n+        \\'ismv\\',\\n+        \\'m2ts\\',\\n+        \\'m4s\\',\\n+        \\'mng\\',\\n+        \\'mpeg\\',\\n+        \\'qt\\',\\n+        \\'swf\\',\\n+        \\'ts\\',\\n+        \\'vp9\\',\\n+        \\'wvm\\',\\n+\\n+        # audio\\n+        *MEDIA_EXTENSIONS.audio,\\n+        \\'isma\\',\\n+        \\'mid\\',\\n+        \\'mpga\\',\\n+        \\'ra\\',\\n+\\n+        # image\\n+        *MEDIA_EXTENSIONS.thumbnails,\\n+        \\'bmp\\',\\n+        \\'gif\\',\\n+        \\'heic\\',\\n+        \\'ico\\',\\n+        \\'jng\\',\\n+        \\'jpeg\\',\\n+        \\'jxl\\',\\n+        \\'svg\\',\\n+        \\'tif\\',\\n+        \\'wbmp\\',\\n+\\n+        # subtitle\\n+        *MEDIA_EXTENSIONS.subtitles,\\n+        \\'dfxp\\',\\n+        \\'fs\\',\\n+        \\'ismt\\',\\n+        \\'sami\\',\\n+        \\'scc\\',\\n+        \\'ssa\\',\\n+        \\'tt\\',\\n+        \\'ttml\\',\\n+\\n+        # others\\n+        *MEDIA_EXTENSIONS.manifests,\\n+        *MEDIA_EXTENSIONS.storyboards,\\n+        \\'desktop\\',\\n+        \\'ism\\',\\n+        \\'m3u\\',\\n+        \\'sbv\\',\\n+        \\'url\\',\\n+        \\'webloc\\',\\n+        \\'xml\\',\\n+    ])\\n+\\n+    def __init__(self, extension, /):\\n+        super().__init__(f\\'unsafe file extension: {extension!r}\\')\\n+        self.extension = extension\\n+\\n+    @classmethod\\n+    def sanitize_extension(cls, extension, /, *, prepend=False):\\n+        if \\'/\\' in extension or \\'\\\\\\\\\\' in extension:\\n+            raise cls(extension)\\n+\\n+        if not prepend:\\n+            _, _, last = extension.rpartition(\\'.\\')\\n+            if last == \\'bin\\':\\n+                extension = last = \\'unknown_video\\'\\n+            if last.lower() not in cls.ALLOWED_EXTENSIONS:\\n+                raise cls(extension)\\n+\\n+        return extension\\n+\\n+\\n class RetryManager:\\n     \"\"\"Usage:\\n         for retry in RetryManager(...):'], 'file': ['yt_dlp/options.py', 'yt_dlp/YoutubeDL.py', 'yt_dlp/utils/_utils.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('94468184-6356-4067-870f-761403065982'), UUID('e2b0df89-aa85-4dd6-b875-f43dcca1e3a9'), UUID('2a4e9557-492d-454d-87dc-5e655787afc8')]}\n",
      "ERROR:root:Error in {'repo': 'yt-dlp/yt-dlp', 'vulnerability_id': '2024-38519', 'commit': '5ce582448ececb8d9c30c8c31f58330090ced03a', 'commit_source': 'github', 'cwe_id': ['CWE-669', 'CWE-669'], 'patch': [\"@@ -474,7 +474,7 @@ def _alias_callback(option, opt_str, value, parser, opts, nargs):\\n                 'no-attach-info-json', 'embed-thumbnail-atomicparsley', 'no-external-downloader-progress',\\n                 'embed-metadata', 'seperate-video-versions', 'no-clean-infojson', 'no-keep-subs', 'no-certifi',\\n                 'no-youtube-channel-redirect', 'no-youtube-unavailable-videos', 'no-youtube-prefer-utc-upload-date',\\n-                'prefer-legacy-http-handler', 'manifest-filesize-approx',\\n+                'prefer-legacy-http-handler', 'manifest-filesize-approx', 'allow-unsafe-ext',\\n             }, 'aliases': {\\n                 'youtube-dl': ['all', '-multistreams', '-playlist-match-filter', '-manifest-filesize-approx'],\\n                 'youtube-dlc': ['all', '-no-youtube-channel-redirect', '-no-live-chat', '-playlist-match-filter', '-manifest-filesize-approx'],\", '@@ -159,7 +159,7 @@\\n     write_json_file,\\n     write_string,\\n )\\n-from .utils._utils import _YDLLogger\\n+from .utils._utils import _UnsafeExtensionError, _YDLLogger\\n from .utils.networking import (\\n     HTTPHeaderDict,\\n     clean_headers,\\n@@ -172,6 +172,20 @@\\n     import ctypes\\n \\n \\n+def _catch_unsafe_extension_error(func):\\n+    @functools.wraps(func)\\n+    def wrapper(self, *args, **kwargs):\\n+        try:\\n+            return func(self, *args, **kwargs)\\n+        except _UnsafeExtensionError as error:\\n+            self.report_error(\\n+                f\\'The extracted extension ({error.extension!r}) is unusual \\'\\n+                \\'and will be skipped for safety reasons. \\'\\n+                f\\'If you believe this is an error{bug_reports_message(\",\")}\\')\\n+\\n+    return wrapper\\n+\\n+\\n class YoutubeDL:\\n     \"\"\"YoutubeDL class.\\n \\n@@ -454,8 +468,9 @@ class YoutubeDL:\\n                        Set the value to \\'native\\' to use the native downloader\\n     compat_opts:       Compatibility options. See \"Differences in default behavior\".\\n                        The following options do not work when used through the API:\\n-                       filename, abort-on-error, multistreams, no-live-chat, format-sort\\n-                       no-clean-infojson, no-playlist-metafiles, no-keep-subs, no-attach-info-json.\\n+                       filename, abort-on-error, multistreams, no-live-chat,\\n+                       format-sort, no-clean-infojson, no-playlist-metafiles,\\n+                       no-keep-subs, no-attach-info-json, allow-unsafe-ext.\\n                        Refer __init__.py for their implementation\\n     progress_template: Dictionary of templates for progress outputs.\\n                        Allowed keys are \\'download\\', \\'postprocess\\',\\n@@ -1400,6 +1415,7 @@ def evaluate_outtmpl(self, outtmpl, info_dict, *args, **kwargs):\\n         outtmpl, info_dict = self.prepare_outtmpl(outtmpl, info_dict, *args, **kwargs)\\n         return self.escape_outtmpl(outtmpl) % info_dict\\n \\n+    @_catch_unsafe_extension_error\\n     def _prepare_filename(self, info_dict, *, outtmpl=None, tmpl_type=None):\\n         assert None in (outtmpl, tmpl_type), \\'outtmpl and tmpl_type are mutually exclusive\\'\\n         if outtmpl is None:\\n@@ -3192,6 +3208,7 @@ def existing_file(self, filepaths, *, default_overwrite=True):\\n             os.remove(file)\\n         return None\\n \\n+    @_catch_unsafe_extension_error\\n     def process_info(self, info_dict):\\n         \"\"\"Process a single resolved IE result. (Modifies it in-place)\"\"\"\\n ', '@@ -2085,17 +2085,20 @@ def parse_duration(s):\\n         (days, 86400), (hours, 3600), (mins, 60), (secs, 1), (ms, 1)))\\n \\n \\n-def prepend_extension(filename, ext, expected_real_ext=None):\\n+def _change_extension(prepend, filename, ext, expected_real_ext=None):\\n     name, real_ext = os.path.splitext(filename)\\n-    return (\\n-        f\\'{name}.{ext}{real_ext}\\'\\n-        if not expected_real_ext or real_ext[1:] == expected_real_ext\\n-        else f\\'{filename}.{ext}\\')\\n \\n+    if not expected_real_ext or real_ext[1:] == expected_real_ext:\\n+        filename = name\\n+        if prepend and real_ext:\\n+            _UnsafeExtensionError.sanitize_extension(ext, prepend=True)\\n+            return f\\'{filename}.{ext}{real_ext}\\'\\n+\\n+    return f\\'{filename}.{_UnsafeExtensionError.sanitize_extension(ext)}\\'\\n \\n-def replace_extension(filename, ext, expected_real_ext=None):\\n-    name, real_ext = os.path.splitext(filename)\\n-    return f\\'{name if not expected_real_ext or real_ext[1:] == expected_real_ext else filename}.{ext}\\'\\n+\\n+prepend_extension = functools.partial(_change_extension, True)\\n+replace_extension = functools.partial(_change_extension, False)\\n \\n \\n def check_executable(exe, args=[]):\\n@@ -5035,6 +5038,101 @@ def items_(self):\\n KNOWN_EXTENSIONS = (*MEDIA_EXTENSIONS.video, *MEDIA_EXTENSIONS.audio, *MEDIA_EXTENSIONS.manifests)\\n \\n \\n+class _UnsafeExtensionError(Exception):\\n+    \"\"\"\\n+    Mitigation exception for uncommon/malicious file extensions\\n+    This should be caught in YoutubeDL.py alongside a warning\\n+\\n+    Ref: https://github.com/yt-dlp/yt-dlp/security/advisories/GHSA-79w7-vh3h-8g4j\\n+    \"\"\"\\n+    ALLOWED_EXTENSIONS = frozenset([\\n+        # internal\\n+        \\'description\\',\\n+        \\'json\\',\\n+        \\'meta\\',\\n+        \\'orig\\',\\n+        \\'part\\',\\n+        \\'temp\\',\\n+        \\'uncut\\',\\n+        \\'unknown_video\\',\\n+        \\'ytdl\\',\\n+\\n+        # video\\n+        *MEDIA_EXTENSIONS.video,\\n+        \\'avif\\',\\n+        \\'ismv\\',\\n+        \\'m2ts\\',\\n+        \\'m4s\\',\\n+        \\'mng\\',\\n+        \\'mpeg\\',\\n+        \\'qt\\',\\n+        \\'swf\\',\\n+        \\'ts\\',\\n+        \\'vp9\\',\\n+        \\'wvm\\',\\n+\\n+        # audio\\n+        *MEDIA_EXTENSIONS.audio,\\n+        \\'isma\\',\\n+        \\'mid\\',\\n+        \\'mpga\\',\\n+        \\'ra\\',\\n+\\n+        # image\\n+        *MEDIA_EXTENSIONS.thumbnails,\\n+        \\'bmp\\',\\n+        \\'gif\\',\\n+        \\'heic\\',\\n+        \\'ico\\',\\n+        \\'jng\\',\\n+        \\'jpeg\\',\\n+        \\'jxl\\',\\n+        \\'svg\\',\\n+        \\'tif\\',\\n+        \\'wbmp\\',\\n+\\n+        # subtitle\\n+        *MEDIA_EXTENSIONS.subtitles,\\n+        \\'dfxp\\',\\n+        \\'fs\\',\\n+        \\'ismt\\',\\n+        \\'sami\\',\\n+        \\'scc\\',\\n+        \\'ssa\\',\\n+        \\'tt\\',\\n+        \\'ttml\\',\\n+\\n+        # others\\n+        *MEDIA_EXTENSIONS.manifests,\\n+        *MEDIA_EXTENSIONS.storyboards,\\n+        \\'desktop\\',\\n+        \\'ism\\',\\n+        \\'m3u\\',\\n+        \\'sbv\\',\\n+        \\'url\\',\\n+        \\'webloc\\',\\n+        \\'xml\\',\\n+    ])\\n+\\n+    def __init__(self, extension, /):\\n+        super().__init__(f\\'unsafe file extension: {extension!r}\\')\\n+        self.extension = extension\\n+\\n+    @classmethod\\n+    def sanitize_extension(cls, extension, /, *, prepend=False):\\n+        if \\'/\\' in extension or \\'\\\\\\\\\\' in extension:\\n+            raise cls(extension)\\n+\\n+        if not prepend:\\n+            _, _, last = extension.rpartition(\\'.\\')\\n+            if last == \\'bin\\':\\n+                extension = last = \\'unknown_video\\'\\n+            if last.lower() not in cls.ALLOWED_EXTENSIONS:\\n+                raise cls(extension)\\n+\\n+        return extension\\n+\\n+\\n class RetryManager:\\n     \"\"\"Usage:\\n         for retry in RetryManager(...):'], 'file': ['yt_dlp/options.py', 'yt_dlp/YoutubeDL.py', 'yt_dlp/utils/_utils.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('94468184-6356-4067-870f-761403065982'), UUID('e2b0df89-aa85-4dd6-b875-f43dcca1e3a9'), UUID('2a4e9557-492d-454d-87dc-5e655787afc8')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 19:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 409:0: <line number missing in source>\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 999/1800 [07:49<02:31,  5.30it/s]ERROR:src.process_code_changes:Error processing commit f7599379b26a685d772b2620a16316130f46c474\n",
      "ERROR:src.process_code_changes:{'repo': 'pywbem/pywbem', 'vulnerability_id': '2013-6418', 'commit': 'f7599379b26a685d772b2620a16316130f46c474', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': ['@@ -28,6 +28,7 @@\\n data and interpret the result.\\n \\'\\'\\'\\n \\n+from M2Crypto import SSL, Err\\n import sys, string, re, os, socket, getpass\\n from stat import S_ISSOCK\\n import cim_obj\\n@@ -74,8 +75,26 @@ def parse_url(url):\\n \\n     return host, port, ssl\\n \\n+def get_default_ca_certs():\\n+    \"\"\"\\n+    Try to find out system path with ca certificates. This path is cached and\\n+    returned. If no path is found out, None is returned.\\n+    \"\"\"\\n+    if not hasattr(get_default_ca_certs, \\'_path\\'):\\n+        for path in (\\n+                \\'/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt\\',\\n+                \\'/etc/ssl/certs\\',\\n+                \\'/etc/ssl/certificates\\'):\\n+            if os.path.exists(path):\\n+                get_default_ca_certs._path = path\\n+                break\\n+        else:\\n+            get_default_ca_certs._path = None\\n+    return get_default_ca_certs._path\\n+\\n def wbem_request(url, data, creds, headers = [], debug = 0, x509 = None,\\n-                 verify_callback = None):\\n+                 verify_callback = None, ca_certs = None,\\n+                 no_verification = False):\\n     \"\"\"Send XML data over HTTP to the specified url. Return the\\n     response in XML.  Uses Python\\'s build-in httplib.  x509 may be a\\n     dictionary containing the location of the SSL certificate and key\\n@@ -105,10 +124,49 @@ def __init__(self, host, port=None, strict=None):\\n     \\n     class HTTPSConnection(HTTPBaseConnection, httplib.HTTPSConnection):\\n         def __init__(self, host, port=None, key_file=None, cert_file=None, \\n-                     strict=None):\\n+                     strict=None, ca_certs=None, verify_callback=None):\\n             httplib.HTTPSConnection.__init__(self, host, port, key_file, \\n                                              cert_file, strict)\\n-    \\n+            self.ca_certs = ca_certs\\n+            self.verify_callback = verify_callback\\n+\\n+        def connect(self):\\n+            \"Connect to a host on a given (SSL) port.\"\\n+            self.sock = socket.create_connection((self.host, self.port),\\n+                                            self.timeout, self.source_address)\\n+            if self._tunnel_host:\\n+                self.sock = sock\\n+                self._tunnel()\\n+            ctx = SSL.Context(\\'sslv23\\')\\n+            if self.cert_file:\\n+                ctx.load_cert(self.cert_file, keyfile=self.key_file)\\n+            if self.ca_certs:\\n+                ctx.set_verify(SSL.verify_peer | SSL.verify_fail_if_no_peer_cert,\\n+                    depth=9, callback=verify_callback)\\n+                if os.path.isdir(self.ca_certs):\\n+                    ctx.load_verify_locations(capath=self.ca_certs)\\n+                else:\\n+                    ctx.load_verify_locations(cafile=self.ca_certs)\\n+            try:\\n+                self.sock = SSL.Connection(ctx, self.sock)\\n+                # Below is a body of SSL.Connection.connect() method\\n+                # except for the first line (socket connection). We want to preserve\\n+                # tunneling ability.\\n+                self.sock.addr = (self.host, self.port)\\n+                self.sock.setup_ssl()\\n+                self.sock.set_connect_state()\\n+                ret = self.sock.connect_ssl()\\n+                if self.ca_certs:\\n+                    check = getattr(self.sock, \\'postConnectionCheck\\',\\n+                             self.sock.clientPostConnectionCheck)\\n+                    if check is not None:\\n+                        if not check(self.sock.get_peer_cert(), self.host):\\n+                            raise Error(\\'SSL error: post connection check failed\\')\\n+                return ret\\n+            except ( Err.SSLError, SSL.SSLError, SSL.SSLTimeoutError\\n+                   , SSL.Checker.WrongHost), arg:\\n+                raise Error(\"SSL error: %s\" % arg)\\n+\\n     class FileHTTPConnection(HTTPBaseConnection, httplib.HTTPConnection):\\n         def __init__(self, uds_path):\\n             httplib.HTTPConnection.__init__(self, \\'localhost\\')\\n@@ -117,64 +175,36 @@ def connect(self):\\n             self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\\n             self.sock.connect(self.uds_path)\\n \\n-    host, port, ssl = parse_url(url)\\n+    host, port, use_ssl = parse_url(url)\\n \\n     key_file = None\\n     cert_file = None\\n \\n-    if ssl:\\n-\\n-        if x509 is not None:\\n-            cert_file = x509.get(\\'cert_file\\')\\n-            key_file = x509.get(\\'key_file\\')\\n-\\n-        if verify_callback is not None:\\n-            addr_ind = 0\\n-            # Temporary exception store\\n-            addr_exc = None\\n-            # Get a list of arguments for socket().\\n-            addr_list = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\\n-            for addr_ind in xrange(len(addr_list)):\\n-                family, socktype, proto, canonname, sockaddr = addr_list[addr_ind]\\n-                try:\\n-                    from OpenSSL import SSL\\n-                    ctx = SSL.Context(SSL.SSLv3_METHOD)\\n-                    ctx.set_verify(SSL.VERIFY_PEER, verify_callback)\\n-                    ctx.set_default_verify_paths()\\n-                    # Add the key and certificate to the session\\n-                    if cert_file is not None and key_file is not None:\\n-                      ctx.use_certificate_file(cert_file)\\n-                      ctx.use_privatekey_file(key_file)\\n-                    s = SSL.Connection(ctx, socket.socket(family, socktype, proto))\\n-                    s.connect((host, port))\\n-                    s.do_handshake()\\n-                    s.shutdown()\\n-                    s.close()\\n-                    addr_exc = None\\n-                    break\\n-                except (socket.gaierror, socket.error), arg:\\n-                    # Could not perform connect() call, store the exception object for\\n-                    # later use.\\n-                    addr_exc = arg\\n-                    continue\\n-                except socket.sslerror, arg:\\n-                    raise Error(\"SSL error: %s\" % (arg,))\\n-\\n-            # Did we try all the addresses from getaddrinfo() and no successful\\n-            # connection performed?\\n-            if addr_exc:\\n-                raise Error(\"Socket error: %s\" % (addr_exc),)\\n+    if use_ssl and x509 is not None:\\n+        cert_file = x509.get(\\'cert_file\\')\\n+        key_file = x509.get(\\'key_file\\')\\n \\n     numTries = 0\\n     localAuthHeader = None\\n     tryLimit = 5\\n \\n+    if isinstance(data, unicode):\\n+        data = data.encode(\\'utf-8\\')\\n     data = \\'<?xml version=\"1.0\" encoding=\"utf-8\" ?>\\\\n\\' + data\\n \\n+    if not no_verification and ca_certs is None:\\n+        ca_certs = get_default_ca_certs()\\n+    elif no_verification:\\n+        ca_certs = None\\n+\\n     local = False\\n-    if ssl:\\n-        h = HTTPSConnection(host, port = port, key_file = key_file,\\n-                                            cert_file = cert_file)\\n+    if use_ssl:\\n+        h = HTTPSConnection(host,\\n+                port = port,\\n+                key_file = key_file,\\n+                cert_file = cert_file,\\n+                ca_certs = ca_certs,\\n+                verify_callback = verify_callback)\\n     else:\\n         if url.startswith(\\'http\\'):\\n             h = HTTPConnection(host, port = port)\\n@@ -216,6 +246,8 @@ def connect(self):\\n             h.putheader(\\'PegasusAuthorization\\', \\'Local \"%s\"\\' % locallogin)\\n \\n         for hdr in headers:\\n+            if isinstance(hdr, unicode):\\n+                hdr = hdr.encode(\\'utf-8\\')\\n             s = map(lambda x: string.strip(x), string.split(hdr, \":\", 1))\\n             h.putheader(urllib.quote(s[0]), urllib.quote(s[1]))\\n ', '@@ -27,7 +27,7 @@\\n from types import StringTypes\\n from xml.dom import minidom\\n import cim_obj, cim_xml, cim_http, cim_types\\n-from cim_obj import CIMClassName, CIMInstanceName, CIMInstance, CIMClass, NocaseDict\\n+from cim_obj import CIMClassName, CIMInstanceName, CIMInstance, CIMClass\\n from datetime import datetime, timedelta\\n from tupletree import dom_to_tupletree, xml_to_tupletree\\n from tupleparse import parse_cim\\n@@ -78,27 +78,40 @@ class WBEMConnection(object):\\n     the request before it is sent, and the reply before it is\\n     unpacked.\\n \\n-    verify_callback is used to verify the server certificate.  \\n-    It is passed to OpenSSL.SSL.set_verify, and is called during the SSL\\n-    handshake.  verify_callback should take five arguments: A Connection \\n-    object, an X509 object, and three integer variables, which are in turn \\n-    potential error number, error depth and return code. verify_callback \\n-    should return True if verification passes and False otherwise.\\n+    verify_callback is used to verify the server certificate.  It is passed to\\n+    M2Crypto.SSL.Context.set_verify, and is called during the SSL handshake.\\n+    verify_callback should take five arguments: An SSL Context object, an X509\\n+    object, and three integer variables, which are in turn potential error\\n+    number, error depth and return code. verify_callback should return True if\\n+    verification passes and False otherwise.\\n \\n     The value of the x509 argument is used only when the url contains\\n     \\'https\\'. x509 must be a dictionary containing the keys \\'cert_file\\' \\n     and \\'key_file\\'. The value of \\'cert_file\\' must consist of the\\n     filename of an certificate and the value of \\'key_file\\' must consist \\n     of a filename containing the private key belonging to the public key \\n     that is part of the certificate in cert_file. \\n+\\n+    ca_certs specifies where CA certificates for verification purposes are\\n+    located. These are trusted certificates. Note that the certificates have to\\n+    be in PEM format. Either it is a directory prepared using the c_rehash tool\\n+    included with OpenSSL or an pemfile. If None, default system path will be\\n+    used.\\n+\\n+    no_verification allows to disable peer\\'s verification. This is insecure and\\n+    should be avoided. If True, peer\\'s certificate is not verified and ca_certs\\n+    argument is ignored.\\n     \"\"\"\\n     \\n     def __init__(self, url, creds = None, default_namespace = DEFAULT_NAMESPACE,\\n-                 x509 = None, verify_callback = None):\\n+                 x509 = None, verify_callback = None, ca_certs = None,\\n+                 no_verification = False):\\n         self.url = url\\n         self.creds = creds\\n         self.x509 = x509\\n         self.verify_callback = verify_callback\\n+        self.ca_certs = ca_certs\\n+        self.no_verification = no_verification\\n         self.last_request = self.last_reply = \\'\\'\\n         self.default_namespace = default_namespace\\n         self.debug = False\\n@@ -164,7 +177,9 @@ def imethodcall(self, methodname, namespace, **params):\\n             resp_xml = cim_http.wbem_request(self.url, req_xml.toxml(),\\n                                              self.creds, headers,\\n                                              x509 = self.x509,\\n-                                             verify_callback = self.verify_callback)\\n+                                             verify_callback = self.verify_callback,\\n+                                             ca_certs = self.ca_certs,\\n+                                             no_verification = self.no_verification)\\n         except cim_http.AuthError:\\n             raise\\n         except cim_http.Error, arg:\\n@@ -321,7 +336,9 @@ def is_embedded(obj):\\n             resp_xml = cim_http.wbem_request(self.url, req_xml.toxml(),\\n                                              self.creds, headers,\\n                                              x509 = self.x509,\\n-                                             verify_callback = self.verify_callback)\\n+                                             verify_callback = self.verify_callback,\\n+                                             ca_certs = self.ca_certs,\\n+                                             no_verification = self.no_verification)\\n         except cim_http.Error, arg:\\n             # Convert cim_http exceptions to CIMError exceptions\\n             raise CIMError(0, str(arg))\\n@@ -811,7 +828,7 @@ def InvokeMethod(self, MethodName, ObjectName, **params):\\n \\n         # Convert zero or more PARAMVALUE elements into dictionary\\n \\n-        output_params = NocaseDict()\\n+        output_params = {}\\n \\n         for p in result:\\n             if p[1] == \\'reference\\':'], 'file': ['trunk/cim_http.py', 'trunk/cim_operations.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('83891f9b-181c-4039-8f9c-092fbea14648'), UUID('0d4dc84a-4928-4ed6-8184-27b5d51eee82')]}\n",
      "ERROR:root:Error in {'repo': 'pywbem/pywbem', 'vulnerability_id': '2013-6418', 'commit': 'f7599379b26a685d772b2620a16316130f46c474', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': ['@@ -28,6 +28,7 @@\\n data and interpret the result.\\n \\'\\'\\'\\n \\n+from M2Crypto import SSL, Err\\n import sys, string, re, os, socket, getpass\\n from stat import S_ISSOCK\\n import cim_obj\\n@@ -74,8 +75,26 @@ def parse_url(url):\\n \\n     return host, port, ssl\\n \\n+def get_default_ca_certs():\\n+    \"\"\"\\n+    Try to find out system path with ca certificates. This path is cached and\\n+    returned. If no path is found out, None is returned.\\n+    \"\"\"\\n+    if not hasattr(get_default_ca_certs, \\'_path\\'):\\n+        for path in (\\n+                \\'/etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt\\',\\n+                \\'/etc/ssl/certs\\',\\n+                \\'/etc/ssl/certificates\\'):\\n+            if os.path.exists(path):\\n+                get_default_ca_certs._path = path\\n+                break\\n+        else:\\n+            get_default_ca_certs._path = None\\n+    return get_default_ca_certs._path\\n+\\n def wbem_request(url, data, creds, headers = [], debug = 0, x509 = None,\\n-                 verify_callback = None):\\n+                 verify_callback = None, ca_certs = None,\\n+                 no_verification = False):\\n     \"\"\"Send XML data over HTTP to the specified url. Return the\\n     response in XML.  Uses Python\\'s build-in httplib.  x509 may be a\\n     dictionary containing the location of the SSL certificate and key\\n@@ -105,10 +124,49 @@ def __init__(self, host, port=None, strict=None):\\n     \\n     class HTTPSConnection(HTTPBaseConnection, httplib.HTTPSConnection):\\n         def __init__(self, host, port=None, key_file=None, cert_file=None, \\n-                     strict=None):\\n+                     strict=None, ca_certs=None, verify_callback=None):\\n             httplib.HTTPSConnection.__init__(self, host, port, key_file, \\n                                              cert_file, strict)\\n-    \\n+            self.ca_certs = ca_certs\\n+            self.verify_callback = verify_callback\\n+\\n+        def connect(self):\\n+            \"Connect to a host on a given (SSL) port.\"\\n+            self.sock = socket.create_connection((self.host, self.port),\\n+                                            self.timeout, self.source_address)\\n+            if self._tunnel_host:\\n+                self.sock = sock\\n+                self._tunnel()\\n+            ctx = SSL.Context(\\'sslv23\\')\\n+            if self.cert_file:\\n+                ctx.load_cert(self.cert_file, keyfile=self.key_file)\\n+            if self.ca_certs:\\n+                ctx.set_verify(SSL.verify_peer | SSL.verify_fail_if_no_peer_cert,\\n+                    depth=9, callback=verify_callback)\\n+                if os.path.isdir(self.ca_certs):\\n+                    ctx.load_verify_locations(capath=self.ca_certs)\\n+                else:\\n+                    ctx.load_verify_locations(cafile=self.ca_certs)\\n+            try:\\n+                self.sock = SSL.Connection(ctx, self.sock)\\n+                # Below is a body of SSL.Connection.connect() method\\n+                # except for the first line (socket connection). We want to preserve\\n+                # tunneling ability.\\n+                self.sock.addr = (self.host, self.port)\\n+                self.sock.setup_ssl()\\n+                self.sock.set_connect_state()\\n+                ret = self.sock.connect_ssl()\\n+                if self.ca_certs:\\n+                    check = getattr(self.sock, \\'postConnectionCheck\\',\\n+                             self.sock.clientPostConnectionCheck)\\n+                    if check is not None:\\n+                        if not check(self.sock.get_peer_cert(), self.host):\\n+                            raise Error(\\'SSL error: post connection check failed\\')\\n+                return ret\\n+            except ( Err.SSLError, SSL.SSLError, SSL.SSLTimeoutError\\n+                   , SSL.Checker.WrongHost), arg:\\n+                raise Error(\"SSL error: %s\" % arg)\\n+\\n     class FileHTTPConnection(HTTPBaseConnection, httplib.HTTPConnection):\\n         def __init__(self, uds_path):\\n             httplib.HTTPConnection.__init__(self, \\'localhost\\')\\n@@ -117,64 +175,36 @@ def connect(self):\\n             self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\\n             self.sock.connect(self.uds_path)\\n \\n-    host, port, ssl = parse_url(url)\\n+    host, port, use_ssl = parse_url(url)\\n \\n     key_file = None\\n     cert_file = None\\n \\n-    if ssl:\\n-\\n-        if x509 is not None:\\n-            cert_file = x509.get(\\'cert_file\\')\\n-            key_file = x509.get(\\'key_file\\')\\n-\\n-        if verify_callback is not None:\\n-            addr_ind = 0\\n-            # Temporary exception store\\n-            addr_exc = None\\n-            # Get a list of arguments for socket().\\n-            addr_list = socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM)\\n-            for addr_ind in xrange(len(addr_list)):\\n-                family, socktype, proto, canonname, sockaddr = addr_list[addr_ind]\\n-                try:\\n-                    from OpenSSL import SSL\\n-                    ctx = SSL.Context(SSL.SSLv3_METHOD)\\n-                    ctx.set_verify(SSL.VERIFY_PEER, verify_callback)\\n-                    ctx.set_default_verify_paths()\\n-                    # Add the key and certificate to the session\\n-                    if cert_file is not None and key_file is not None:\\n-                      ctx.use_certificate_file(cert_file)\\n-                      ctx.use_privatekey_file(key_file)\\n-                    s = SSL.Connection(ctx, socket.socket(family, socktype, proto))\\n-                    s.connect((host, port))\\n-                    s.do_handshake()\\n-                    s.shutdown()\\n-                    s.close()\\n-                    addr_exc = None\\n-                    break\\n-                except (socket.gaierror, socket.error), arg:\\n-                    # Could not perform connect() call, store the exception object for\\n-                    # later use.\\n-                    addr_exc = arg\\n-                    continue\\n-                except socket.sslerror, arg:\\n-                    raise Error(\"SSL error: %s\" % (arg,))\\n-\\n-            # Did we try all the addresses from getaddrinfo() and no successful\\n-            # connection performed?\\n-            if addr_exc:\\n-                raise Error(\"Socket error: %s\" % (addr_exc),)\\n+    if use_ssl and x509 is not None:\\n+        cert_file = x509.get(\\'cert_file\\')\\n+        key_file = x509.get(\\'key_file\\')\\n \\n     numTries = 0\\n     localAuthHeader = None\\n     tryLimit = 5\\n \\n+    if isinstance(data, unicode):\\n+        data = data.encode(\\'utf-8\\')\\n     data = \\'<?xml version=\"1.0\" encoding=\"utf-8\" ?>\\\\n\\' + data\\n \\n+    if not no_verification and ca_certs is None:\\n+        ca_certs = get_default_ca_certs()\\n+    elif no_verification:\\n+        ca_certs = None\\n+\\n     local = False\\n-    if ssl:\\n-        h = HTTPSConnection(host, port = port, key_file = key_file,\\n-                                            cert_file = cert_file)\\n+    if use_ssl:\\n+        h = HTTPSConnection(host,\\n+                port = port,\\n+                key_file = key_file,\\n+                cert_file = cert_file,\\n+                ca_certs = ca_certs,\\n+                verify_callback = verify_callback)\\n     else:\\n         if url.startswith(\\'http\\'):\\n             h = HTTPConnection(host, port = port)\\n@@ -216,6 +246,8 @@ def connect(self):\\n             h.putheader(\\'PegasusAuthorization\\', \\'Local \"%s\"\\' % locallogin)\\n \\n         for hdr in headers:\\n+            if isinstance(hdr, unicode):\\n+                hdr = hdr.encode(\\'utf-8\\')\\n             s = map(lambda x: string.strip(x), string.split(hdr, \":\", 1))\\n             h.putheader(urllib.quote(s[0]), urllib.quote(s[1]))\\n ', '@@ -27,7 +27,7 @@\\n from types import StringTypes\\n from xml.dom import minidom\\n import cim_obj, cim_xml, cim_http, cim_types\\n-from cim_obj import CIMClassName, CIMInstanceName, CIMInstance, CIMClass, NocaseDict\\n+from cim_obj import CIMClassName, CIMInstanceName, CIMInstance, CIMClass\\n from datetime import datetime, timedelta\\n from tupletree import dom_to_tupletree, xml_to_tupletree\\n from tupleparse import parse_cim\\n@@ -78,27 +78,40 @@ class WBEMConnection(object):\\n     the request before it is sent, and the reply before it is\\n     unpacked.\\n \\n-    verify_callback is used to verify the server certificate.  \\n-    It is passed to OpenSSL.SSL.set_verify, and is called during the SSL\\n-    handshake.  verify_callback should take five arguments: A Connection \\n-    object, an X509 object, and three integer variables, which are in turn \\n-    potential error number, error depth and return code. verify_callback \\n-    should return True if verification passes and False otherwise.\\n+    verify_callback is used to verify the server certificate.  It is passed to\\n+    M2Crypto.SSL.Context.set_verify, and is called during the SSL handshake.\\n+    verify_callback should take five arguments: An SSL Context object, an X509\\n+    object, and three integer variables, which are in turn potential error\\n+    number, error depth and return code. verify_callback should return True if\\n+    verification passes and False otherwise.\\n \\n     The value of the x509 argument is used only when the url contains\\n     \\'https\\'. x509 must be a dictionary containing the keys \\'cert_file\\' \\n     and \\'key_file\\'. The value of \\'cert_file\\' must consist of the\\n     filename of an certificate and the value of \\'key_file\\' must consist \\n     of a filename containing the private key belonging to the public key \\n     that is part of the certificate in cert_file. \\n+\\n+    ca_certs specifies where CA certificates for verification purposes are\\n+    located. These are trusted certificates. Note that the certificates have to\\n+    be in PEM format. Either it is a directory prepared using the c_rehash tool\\n+    included with OpenSSL or an pemfile. If None, default system path will be\\n+    used.\\n+\\n+    no_verification allows to disable peer\\'s verification. This is insecure and\\n+    should be avoided. If True, peer\\'s certificate is not verified and ca_certs\\n+    argument is ignored.\\n     \"\"\"\\n     \\n     def __init__(self, url, creds = None, default_namespace = DEFAULT_NAMESPACE,\\n-                 x509 = None, verify_callback = None):\\n+                 x509 = None, verify_callback = None, ca_certs = None,\\n+                 no_verification = False):\\n         self.url = url\\n         self.creds = creds\\n         self.x509 = x509\\n         self.verify_callback = verify_callback\\n+        self.ca_certs = ca_certs\\n+        self.no_verification = no_verification\\n         self.last_request = self.last_reply = \\'\\'\\n         self.default_namespace = default_namespace\\n         self.debug = False\\n@@ -164,7 +177,9 @@ def imethodcall(self, methodname, namespace, **params):\\n             resp_xml = cim_http.wbem_request(self.url, req_xml.toxml(),\\n                                              self.creds, headers,\\n                                              x509 = self.x509,\\n-                                             verify_callback = self.verify_callback)\\n+                                             verify_callback = self.verify_callback,\\n+                                             ca_certs = self.ca_certs,\\n+                                             no_verification = self.no_verification)\\n         except cim_http.AuthError:\\n             raise\\n         except cim_http.Error, arg:\\n@@ -321,7 +336,9 @@ def is_embedded(obj):\\n             resp_xml = cim_http.wbem_request(self.url, req_xml.toxml(),\\n                                              self.creds, headers,\\n                                              x509 = self.x509,\\n-                                             verify_callback = self.verify_callback)\\n+                                             verify_callback = self.verify_callback,\\n+                                             ca_certs = self.ca_certs,\\n+                                             no_verification = self.no_verification)\\n         except cim_http.Error, arg:\\n             # Convert cim_http exceptions to CIMError exceptions\\n             raise CIMError(0, str(arg))\\n@@ -811,7 +828,7 @@ def InvokeMethod(self, MethodName, ObjectName, **params):\\n \\n         # Convert zero or more PARAMVALUE elements into dictionary\\n \\n-        output_params = NocaseDict()\\n+        output_params = {}\\n \\n         for p in result:\\n             if p[1] == \\'reference\\':'], 'file': ['trunk/cim_http.py', 'trunk/cim_operations.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('83891f9b-181c-4039-8f9c-092fbea14648'), UUID('0d4dc84a-4928-4ed6-8184-27b5d51eee82')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 32:22:                 print \"send:\", repr(str)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 41:22:                 print \"send:\", repr(str)\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1007/1800 [07:50<02:09,  6.11it/s]ERROR:src.process_code_changes:Error processing commit de015e930747165dbb8fcd360f8775fd973b7d6e\n",
      "ERROR:src.process_code_changes:{'repo': 'yt-dlp/yt-dlp', 'vulnerability_id': '2024-22423', 'commit': 'de015e930747165dbb8fcd360f8775fd973b7d6e', 'commit_source': 'github', 'cwe_id': ['CWE-78'], 'patch': [\"@@ -1,8 +1,6 @@\\n-import subprocess\\n-\\n from .common import PostProcessor\\n from ..compat import compat_shlex_quote\\n-from ..utils import PostProcessingError, encodeArgument, variadic\\n+from ..utils import Popen, PostProcessingError, variadic\\n \\n \\n class ExecPP(PostProcessor):\\n@@ -27,10 +25,10 @@ def parse_cmd(self, cmd, info):\\n     def run(self, info):\\n         for tmpl in self.exec_cmd:\\n             cmd = self.parse_cmd(tmpl, info)\\n-            self.to_screen('Executing command: %s' % cmd)\\n-            retCode = subprocess.call(encodeArgument(cmd), shell=True)\\n-            if retCode != 0:\\n-                raise PostProcessingError('Command returned error code %d' % retCode)\\n+            self.to_screen(f'Executing command: {cmd}')\\n+            _, _, return_code = Popen.run(cmd, shell=True)\\n+            if return_code != 0:\\n+                raise PostProcessingError(f'Command returned error code {return_code}')\\n         return [], info\\n \\n \", '@@ -825,7 +825,7 @@ def _fix(key):\\n         _fix(\\'LD_LIBRARY_PATH\\')  # Linux\\n         _fix(\\'DYLD_LIBRARY_PATH\\')  # macOS\\n \\n-    def __init__(self, *args, env=None, text=False, **kwargs):\\n+    def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\\n         if env is None:\\n             env = os.environ.copy()\\n         self._fix_pyinstaller_ld_path(env)\\n@@ -835,7 +835,21 @@ def __init__(self, *args, env=None, text=False, **kwargs):\\n             kwargs[\\'universal_newlines\\'] = True  # For 3.6 compatibility\\n             kwargs.setdefault(\\'encoding\\', \\'utf-8\\')\\n             kwargs.setdefault(\\'errors\\', \\'replace\\')\\n-        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\\n+\\n+        if shell and compat_os_name == \\'nt\\' and kwargs.get(\\'executable\\') is None:\\n+            if not isinstance(args, str):\\n+                args = \\' \\'.join(compat_shlex_quote(a) for a in args)\\n+            shell = False\\n+            args = f\\'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"\\'\\n+\\n+        super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)\\n+\\n+    def __comspec(self):\\n+        comspec = os.environ.get(\\'ComSpec\\') or os.path.join(\\n+            os.environ.get(\\'SystemRoot\\', \\'\\'), \\'System32\\', \\'cmd.exe\\')\\n+        if os.path.isabs(comspec):\\n+            return comspec\\n+        raise FileNotFoundError(\\'shell not found: neither %ComSpec% nor %SystemRoot% is set\\')\\n \\n     def communicate_or_kill(self, *args, **kwargs):\\n         try:'], 'file': ['yt_dlp/postprocessor/exec.py', 'yt_dlp/utils/_utils.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('1ebc539e-5112-4739-bd99-7e904f09a9db'), UUID('1c35ad66-d00e-46ec-970f-de976a42ce04')]}\n",
      "ERROR:root:Error in {'repo': 'yt-dlp/yt-dlp', 'vulnerability_id': '2024-22423', 'commit': 'de015e930747165dbb8fcd360f8775fd973b7d6e', 'commit_source': 'github', 'cwe_id': ['CWE-78'], 'patch': [\"@@ -1,8 +1,6 @@\\n-import subprocess\\n-\\n from .common import PostProcessor\\n from ..compat import compat_shlex_quote\\n-from ..utils import PostProcessingError, encodeArgument, variadic\\n+from ..utils import Popen, PostProcessingError, variadic\\n \\n \\n class ExecPP(PostProcessor):\\n@@ -27,10 +25,10 @@ def parse_cmd(self, cmd, info):\\n     def run(self, info):\\n         for tmpl in self.exec_cmd:\\n             cmd = self.parse_cmd(tmpl, info)\\n-            self.to_screen('Executing command: %s' % cmd)\\n-            retCode = subprocess.call(encodeArgument(cmd), shell=True)\\n-            if retCode != 0:\\n-                raise PostProcessingError('Command returned error code %d' % retCode)\\n+            self.to_screen(f'Executing command: {cmd}')\\n+            _, _, return_code = Popen.run(cmd, shell=True)\\n+            if return_code != 0:\\n+                raise PostProcessingError(f'Command returned error code {return_code}')\\n         return [], info\\n \\n \", '@@ -825,7 +825,7 @@ def _fix(key):\\n         _fix(\\'LD_LIBRARY_PATH\\')  # Linux\\n         _fix(\\'DYLD_LIBRARY_PATH\\')  # macOS\\n \\n-    def __init__(self, *args, env=None, text=False, **kwargs):\\n+    def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\\n         if env is None:\\n             env = os.environ.copy()\\n         self._fix_pyinstaller_ld_path(env)\\n@@ -835,7 +835,21 @@ def __init__(self, *args, env=None, text=False, **kwargs):\\n             kwargs[\\'universal_newlines\\'] = True  # For 3.6 compatibility\\n             kwargs.setdefault(\\'encoding\\', \\'utf-8\\')\\n             kwargs.setdefault(\\'errors\\', \\'replace\\')\\n-        super().__init__(*args, env=env, **kwargs, startupinfo=self._startupinfo)\\n+\\n+        if shell and compat_os_name == \\'nt\\' and kwargs.get(\\'executable\\') is None:\\n+            if not isinstance(args, str):\\n+                args = \\' \\'.join(compat_shlex_quote(a) for a in args)\\n+            shell = False\\n+            args = f\\'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"\\'\\n+\\n+        super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)\\n+\\n+    def __comspec(self):\\n+        comspec = os.environ.get(\\'ComSpec\\') or os.path.join(\\n+            os.environ.get(\\'SystemRoot\\', \\'\\'), \\'System32\\', \\'cmd.exe\\')\\n+        if os.path.isabs(comspec):\\n+            return comspec\\n+        raise FileNotFoundError(\\'shell not found: neither %ComSpec% nor %SystemRoot% is set\\')\\n \\n     def communicate_or_kill(self, *args, **kwargs):\\n         try:'], 'file': ['yt_dlp/postprocessor/exec.py', 'yt_dlp/utils/_utils.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('1ebc539e-5112-4739-bd99-7e904f09a9db'), UUID('1c35ad66-d00e-46ec-970f-de976a42ce04')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:4:     def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:4:     def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1008/1800 [07:51<02:42,  4.87it/s]ERROR:src.process_code_changes:Error processing commit a3edc09beffa2104f357fe24971ea3211ce40751\n",
      "ERROR:src.process_code_changes:{'repo': 'privacyidea/privacyidea', 'vulnerability_id': '2018-1000809', 'commit': 'a3edc09beffa2104f357fe24971ea3211ce40751', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': [\"@@ -17,7 +17,7 @@\\n # built documents.\\n #\\n # The short X.Y version.\\n-version = '2.23.1'\\n+version = '2.23.2'\\n # The full version, including alpha/beta/rc tags.\\n #release = '2.16dev5'\\n release = version\", '@@ -84,10 +84,15 @@ def __init__(self, request):\\n     def __call__(self, func):\\n         @functools.wraps(func)\\n         def check_user_or_serial_in_request_wrapper(*args, **kwds):\\n-            user = self.request.all_data.get(\"user\")\\n-            serial = self.request.all_data.get(\"serial\")\\n+            user = self.request.all_data.get(\"user\", \"\").strip()\\n+            serial = self.request.all_data.get(\"serial\", \"\").strip()\\n             if not serial and not user:\\n                 raise ParameterError(_(\"You need to specify a serial or a user.\"))\\n+            if \"*\" in serial:\\n+                raise ParameterError(_(\"Invalid serial number.\"))\\n+            if \"%\" in user:\\n+                raise ParameterError(_(\"Invalid user.\"))\\n+\\n             f_result = func(*args, **kwds)\\n             return f_result\\n ', '@@ -164,6 +164,7 @@ def after_request(response):\\n \\n \\n @validate_blueprint.route(\\'/offlinerefill\\', methods=[\\'POST\\'])\\n+@check_user_or_serial_in_request(request)\\n @event(\"validate_offlinerefill\", request, g)\\n def offlinerefill():\\n     \"\"\"'], 'file': ['doc/conf.py', 'privacyidea/lib/decorators.py', 'privacyidea/api/validate.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('6503c067-198a-4d4e-aa43-172918b3daa1'), UUID('fb369357-c175-4361-a1a3-bf41841236e5'), UUID('6b66115b-32c8-4008-bdc4-de3d55d2f96f')]}\n",
      "ERROR:root:Error in {'repo': 'privacyidea/privacyidea', 'vulnerability_id': '2018-1000809', 'commit': 'a3edc09beffa2104f357fe24971ea3211ce40751', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': [\"@@ -17,7 +17,7 @@\\n # built documents.\\n #\\n # The short X.Y version.\\n-version = '2.23.1'\\n+version = '2.23.2'\\n # The full version, including alpha/beta/rc tags.\\n #release = '2.16dev5'\\n release = version\", '@@ -84,10 +84,15 @@ def __init__(self, request):\\n     def __call__(self, func):\\n         @functools.wraps(func)\\n         def check_user_or_serial_in_request_wrapper(*args, **kwds):\\n-            user = self.request.all_data.get(\"user\")\\n-            serial = self.request.all_data.get(\"serial\")\\n+            user = self.request.all_data.get(\"user\", \"\").strip()\\n+            serial = self.request.all_data.get(\"serial\", \"\").strip()\\n             if not serial and not user:\\n                 raise ParameterError(_(\"You need to specify a serial or a user.\"))\\n+            if \"*\" in serial:\\n+                raise ParameterError(_(\"Invalid serial number.\"))\\n+            if \"%\" in user:\\n+                raise ParameterError(_(\"Invalid user.\"))\\n+\\n             f_result = func(*args, **kwds)\\n             return f_result\\n ', '@@ -164,6 +164,7 @@ def after_request(response):\\n \\n \\n @validate_blueprint.route(\\'/offlinerefill\\', methods=[\\'POST\\'])\\n+@check_user_or_serial_in_request(request)\\n @event(\"validate_offlinerefill\", request, g)\\n def offlinerefill():\\n     \"\"\"'], 'file': ['doc/conf.py', 'privacyidea/lib/decorators.py', 'privacyidea/api/validate.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('6503c067-198a-4d4e-aa43-172918b3daa1'), UUID('fb369357-c175-4361-a1a3-bf41841236e5'), UUID('6b66115b-32c8-4008-bdc4-de3d55d2f96f')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1019/1800 [07:52<01:57,  6.67it/s]ERROR:src.process_code_changes:Error processing commit 2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6\n",
      "ERROR:src.process_code_changes:{'repo': 'RasaHQ/rasa', 'vulnerability_id': '2024-49375', 'commit': '2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6', 'commit_source': 'github', 'cwe_id': ['CWE-94', 'CWE-94'], 'patch': ['@@ -1,11 +1,13 @@\\n from __future__ import annotations\\n+\\n import logging\\n import re\\n from typing import Any, Dict, List, Optional, Text, Tuple, Type\\n+\\n import numpy as np\\n import scipy.sparse\\n-from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n \\n+from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n import rasa.shared.utils.io\\n import rasa.utils.io\\n import rasa.nlu.utils.pattern_utils as pattern_utils\\n@@ -240,7 +242,7 @@ def load(\\n \\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n-                patterns_file_name = model_dir / \"patterns.pkl\"\\n+                patterns_file_name = model_dir / \"patterns.json\"\\n                 known_patterns = rasa.shared.utils.io.read_json_file(patterns_file_name)\\n         except (ValueError, FileNotFoundError):\\n             logger.warning(\\n@@ -258,7 +260,7 @@ def load(\\n \\n     def _persist(self) -> None:\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n-            regex_file = model_dir / \"patterns.pkl\"\\n+            regex_file = model_dir / \"patterns.json\"\\n             rasa.shared.utils.io.dump_obj_as_json_to_file(\\n                 regex_file, self.known_patterns\\n             )', '@@ -1,7 +1,8 @@\\n import logging\\n+from typing import List, Optional, Dict, Text, Set, Any\\n+\\n import numpy as np\\n import scipy.sparse\\n-from typing import List, Optional, Dict, Text, Set, Any\\n \\n from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\\n from rasa.nlu.extractors.extractor import EntityTagSpec\\n@@ -362,6 +363,26 @@ def encode_all_labels(\\n             for action in domain.action_names_or_texts\\n         ]\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return {\\n+            \"action_texts\": self.action_texts,\\n+            \"entity_tag_specs\": self.entity_tag_specs,\\n+            \"feature_states\": self._default_feature_states,\\n+        }\\n+\\n+    @classmethod\\n+    def create_from_dict(\\n+        cls, data: Dict[str, Any]\\n+    ) -> Optional[\"SingleStateFeaturizer\"]:\\n+        if not data:\\n+            return None\\n+\\n+        featurizer = SingleStateFeaturizer()\\n+        featurizer.action_texts = data[\"action_texts\"]\\n+        featurizer._default_feature_states = data[\"feature_states\"]\\n+        featurizer.entity_tag_specs = data[\"entity_tag_specs\"]\\n+        return featurizer\\n+\\n \\n class IntentTokenizerSingleStateFeaturizer(SingleStateFeaturizer):\\n     \"\"\"A SingleStateFeaturizer for use with policies that predict intent labels.\"\"\"', '@@ -1,25 +1,25 @@\\n from __future__ import annotations\\n+\\n import logging\\n-from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer\\n import typing\\n import warnings\\n from typing import Any, Dict, List, Optional, Text, Tuple, Type\\n \\n import numpy as np\\n \\n import rasa.shared.utils.io\\n-import rasa.utils.io as io_utils\\n from rasa.engine.graph import GraphComponent, ExecutionContext\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n-from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU\\n from rasa.nlu.classifiers import LABEL_RANKING_LENGTH\\n+from rasa.nlu.classifiers.classifier import IntentClassifier\\n+from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer\\n+from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU\\n from rasa.shared.exceptions import RasaException\\n from rasa.shared.nlu.constants import TEXT\\n-from rasa.nlu.classifiers.classifier import IntentClassifier\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.utils.tensorflow.constants import FEATURIZERS\\n \\n logger = logging.getLogger(__name__)\\n@@ -266,14 +266,20 @@ def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\\n \\n     def persist(self) -> None:\\n         \"\"\"Persist this model into the passed directory.\"\"\"\\n+        import skops.io as sio\\n+\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n             file_name = self.__class__.__name__\\n-            classifier_file_name = model_dir / f\"{file_name}_classifier.pkl\"\\n-            encoder_file_name = model_dir / f\"{file_name}_encoder.pkl\"\\n+            classifier_file_name = model_dir / f\"{file_name}_classifier.skops\"\\n+            encoder_file_name = model_dir / f\"{file_name}_encoder.json\"\\n \\n             if self.clf and self.le:\\n-                io_utils.json_pickle(encoder_file_name, self.le.classes_)\\n-                io_utils.json_pickle(classifier_file_name, self.clf.best_estimator_)\\n+                # convert self.le.classes_ (numpy array of strings) to a list in order\\n+                # to use json dump\\n+                rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                    encoder_file_name, list(self.le.classes_)\\n+                )\\n+                sio.dump(self.clf.best_estimator_, classifier_file_name)\\n \\n     @classmethod\\n     def load(\\n@@ -286,21 +292,36 @@ def load(\\n     ) -> SklearnIntentClassifier:\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n         from sklearn.preprocessing import LabelEncoder\\n+        import skops.io as sio\\n \\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n                 file_name = cls.__name__\\n-                classifier_file = model_dir / f\"{file_name}_classifier.pkl\"\\n+                classifier_file = model_dir / f\"{file_name}_classifier.skops\"\\n \\n                 if classifier_file.exists():\\n-                    classifier = io_utils.json_unpickle(classifier_file)\\n+                    unknown_types = sio.get_untrusted_types(file=classifier_file)\\n \\n-                    encoder_file = model_dir / f\"{file_name}_encoder.pkl\"\\n-                    classes = io_utils.json_unpickle(encoder_file)\\n-                    encoder = LabelEncoder()\\n-                    encoder.classes_ = classes\\n+                    if unknown_types:\\n+                        logger.error(\\n+                            f\"Untrusted types ({unknown_types}) found when \"\\n+                            f\"loading {classifier_file}!\"\\n+                        )\\n+                        raise ValueError()\\n+                    else:\\n+                        classifier = sio.load(classifier_file, trusted=unknown_types)\\n+\\n+                    encoder_file = model_dir / f\"{file_name}_encoder.json\"\\n+                    classes = rasa.shared.utils.io.read_json_file(encoder_file)\\n \\n-                    return cls(config, model_storage, resource, classifier, encoder)\\n+                    encoder = LabelEncoder()\\n+                    intent_classifier = cls(\\n+                        config, model_storage, resource, classifier, encoder\\n+                    )\\n+                    # convert list of strings (class labels) back to numpy array of\\n+                    # strings\\n+                    intent_classifier.transform_labels_str2num(classes)\\n+                    return intent_classifier\\n         except ValueError:\\n             logger.debug(\\n                 f\"Failed to load \\'{cls.__name__}\\' from model storage. Resource \"', '@@ -5,6 +5,7 @@\\n \\n import numpy as np\\n import tensorflow as tf\\n+\\n import rasa.utils.common\\n from rasa.engine.graph import ExecutionContext\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n@@ -16,6 +17,7 @@\\n from rasa.shared.core.trackers import DialogueStateTracker\\n from rasa.shared.core.constants import SLOTS, ACTIVE_LOOP, ACTION_UNLIKELY_INTENT_NAME\\n from rasa.shared.core.events import UserUttered, ActionExecuted\\n+import rasa.shared.utils.io\\n from rasa.shared.nlu.constants import (\\n     INTENT,\\n     TEXT,\\n@@ -103,8 +105,6 @@\\n )\\n from rasa.utils.tensorflow import layers\\n from rasa.utils.tensorflow.model_data import RasaModelData, FeatureArray, Data\\n-\\n-import rasa.utils.io as io_utils\\n from rasa.core.exceptions import RasaCoreException\\n from rasa.shared.utils import common\\n \\n@@ -881,9 +881,12 @@ def persist_model_utilities(self, model_path: Path) -> None:\\n             model_path: Path where model is to be persisted\\n         \"\"\"\\n         super().persist_model_utilities(model_path)\\n-        io_utils.pickle_dump(\\n-            model_path / f\"{self._metadata_filename()}.label_quantiles.pkl\",\\n-            self.label_quantiles,\\n+\\n+        from safetensors.numpy import save_file\\n+\\n+        save_file(\\n+            {str(k): np.array(v) for k, v in self.label_quantiles.items()},\\n+            model_path / f\"{self._metadata_filename()}.label_quantiles.st\",\\n         )\\n \\n     @classmethod\\n@@ -894,9 +897,14 @@ def _load_model_utilities(cls, model_path: Path) -> Dict[Text, Any]:\\n             model_path: Path where model is to be persisted.\\n         \"\"\"\\n         model_utilties = super()._load_model_utilities(model_path)\\n-        label_quantiles = io_utils.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.label_quantiles.pkl\"\\n+\\n+        from safetensors.numpy import load_file\\n+\\n+        loaded_label_quantiles = load_file(\\n+            model_path / f\"{cls._metadata_filename()}.label_quantiles.st\"\\n         )\\n+        label_quantiles = {int(k): list(v) for k, v in loaded_label_quantiles.items()}\\n+\\n         model_utilties.update({\"label_quantiles\": label_quantiles})\\n         return model_utilties\\n ', '@@ -1,29 +1,31 @@\\n from __future__ import annotations\\n+\\n import logging\\n import re\\n+from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type, Union\\n+\\n+import numpy as np\\n import scipy.sparse\\n-from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type\\n-from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n+from sklearn.feature_extraction.text import CountVectorizer\\n \\n import rasa.shared.utils.io\\n from rasa.engine.graph import GraphComponent, ExecutionContext\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n-from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n-from rasa.nlu.utils.spacy_utils import SpacyModel\\n-from rasa.shared.constants import DOCS_URL_COMPONENTS\\n-import rasa.utils.io as io_utils\\n-from sklearn.feature_extraction.text import CountVectorizer\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n-from rasa.shared.exceptions import RasaException, FileIOException\\n from rasa.nlu.constants import (\\n     TOKENS_NAMES,\\n     MESSAGE_ATTRIBUTES,\\n     DENSE_FEATURIZABLE_ATTRIBUTES,\\n )\\n+from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n+from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n+from rasa.nlu.utils.spacy_utils import SpacyModel\\n+from rasa.shared.constants import DOCS_URL_COMPONENTS\\n+from rasa.shared.exceptions import RasaException, FileIOException\\n from rasa.shared.nlu.constants import TEXT, INTENT, INTENT_RESPONSE_KEY, ACTION_NAME\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n \\n BUFFER_SLOTS_PREFIX = \"buf_\"\\n \\n@@ -686,6 +688,31 @@ def _is_any_model_trained(\\n         \"\"\"Check if any model got trained.\"\"\"\\n         return any(value is not None for value in attribute_vocabularies.values())\\n \\n+    @staticmethod\\n+    def convert_vocab(\\n+        vocab: Dict[str, Union[int, Optional[Dict[str, int]]]], to_int: bool\\n+    ) -> Dict[str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]]:\\n+        \"\"\"Converts numpy integers in the vocabulary to Python integers.\"\"\"\\n+\\n+        def convert_value(value: int) -> Union[int, np.int64]:\\n+            \"\"\"Helper function to convert a single value based on to_int flag.\"\"\"\\n+            return int(value) if to_int else np.int64(value)\\n+\\n+        result_dict: Dict[\\n+            str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]\\n+        ] = {}\\n+        for key, sub_dict in vocab.items():\\n+            if isinstance(sub_dict, int):\\n+                result_dict[key] = convert_value(sub_dict)\\n+            elif not sub_dict:\\n+                result_dict[key] = None\\n+            else:\\n+                result_dict[key] = {\\n+                    sub_key: convert_value(value) for sub_key, value in sub_dict.items()\\n+                }\\n+\\n+        return result_dict\\n+\\n     def persist(self) -> None:\\n         \"\"\"Persist this model into the passed directory.\\n \\n@@ -699,17 +726,18 @@ def persist(self) -> None:\\n             attribute_vocabularies = self._collect_vectorizer_vocabularies()\\n             if self._is_any_model_trained(attribute_vocabularies):\\n                 # Definitely need to persist some vocabularies\\n-                featurizer_file = model_dir / \"vocabularies.pkl\"\\n+                featurizer_file = model_dir / \"vocabularies.json\"\\n \\n                 # Only persist vocabulary from one attribute if `use_shared_vocab`.\\n                 # Can be loaded and distributed to all attributes.\\n-                vocab = (\\n+                loaded_vocab = (\\n                     attribute_vocabularies[TEXT]\\n                     if self.use_shared_vocab\\n                     else attribute_vocabularies\\n                 )\\n+                vocab = self.convert_vocab(loaded_vocab, to_int=True)\\n \\n-                io_utils.json_pickle(featurizer_file, vocab)\\n+                rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, vocab)\\n \\n                 # Dump OOV words separately as they might have been modified during\\n                 # training\\n@@ -784,8 +812,9 @@ def load(\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n-                featurizer_file = model_dir / \"vocabularies.pkl\"\\n-                vocabulary = io_utils.json_unpickle(featurizer_file)\\n+                featurizer_file = model_dir / \"vocabularies.json\"\\n+                vocabulary = rasa.shared.utils.io.read_json_file(featurizer_file)\\n+                vocabulary = cls.convert_vocab(vocabulary, to_int=False)\\n \\n                 share_vocabulary = config[\"use_shared_vocab\"]\\n ', '@@ -2,13 +2,13 @@\\n import filecmp\\n import logging\\n import os\\n-import pickle\\n+import re\\n import tempfile\\n import warnings\\n-import re\\n from asyncio import AbstractEventLoop\\n from pathlib import Path\\n-from typing import Text, Any, Union, List, Type, Callable, TYPE_CHECKING, Pattern\\n+from typing import Text, Any, List, Type, Callable, TYPE_CHECKING, Pattern\\n+\\n from typing_extensions import Protocol\\n \\n import rasa.shared.constants\\n@@ -81,29 +81,6 @@ def enable_async_loop_debugging(\\n     return event_loop\\n \\n \\n-def pickle_dump(filename: Union[Text, Path], obj: Any) -> None:\\n-    \"\"\"Saves object to file.\\n-\\n-    Args:\\n-        filename: the filename to save the object to\\n-        obj: the object to store\\n-    \"\"\"\\n-    with open(filename, \"wb\") as f:\\n-        pickle.dump(obj, f)\\n-\\n-\\n-def pickle_load(filename: Union[Text, Path]) -> Any:\\n-    \"\"\"Loads an object from a file.\\n-\\n-    Args:\\n-        filename: the filename to load the object from\\n-\\n-    Returns: the loaded object\\n-    \"\"\"\\n-    with open(filename, \"rb\") as f:\\n-        return pickle.load(f)\\n-\\n-\\n def create_temporary_file(data: Any, suffix: Text = \"\", mode: Text = \"w+\") -> Text:\\n     \"\"\"Creates a tempfile.NamedTemporaryFile object for data.\"\"\"\\n     encoding = None if \"b\" in mode else rasa.shared.utils.io.DEFAULT_ENCODING\\n@@ -124,7 +101,6 @@ def create_temporary_directory() -> Text:\\n \\n def create_path(file_path: Text) -> None:\\n     \"\"\"Makes sure all directories in the \\'file_path\\' exists.\"\"\"\\n-\\n     parent_dir = os.path.dirname(os.path.abspath(file_path))\\n     if not os.path.exists(parent_dir):\\n         os.makedirs(parent_dir)\\n@@ -160,8 +136,8 @@ def create_validator(\\n     function: Callable[[Text], bool], error_message: Text\\n ) -> Type[\"Validator\"]:\\n     \"\"\"Helper method to create `Validator` classes from callable functions. Should be\\n-    removed when questionary supports `Validator` objects.\"\"\"\\n-\\n+    removed when questionary supports `Validator` objects.\\n+    \"\"\"\\n     from prompt_toolkit.validation import Validator, ValidationError\\n     from prompt_toolkit.document import Document\\n \\n@@ -175,48 +151,6 @@ def validate(document: Document) -> None:\\n     return FunctionValidator\\n \\n \\n-def json_unpickle(\\n-    file_name: Union[Text, Path], encode_non_string_keys: bool = False\\n-) -> Any:\\n-    \"\"\"Unpickle an object from file using json.\\n-\\n-    Args:\\n-        file_name: the file to load the object from\\n-        encode_non_string_keys: If set to `True` then jsonpickle will encode non-string\\n-          dictionary keys instead of coercing them into strings via `repr()`.\\n-\\n-    Returns: the object\\n-    \"\"\"\\n-    import jsonpickle.ext.numpy as jsonpickle_numpy\\n-    import jsonpickle\\n-\\n-    jsonpickle_numpy.register_handlers()\\n-\\n-    file_content = rasa.shared.utils.io.read_file(file_name)\\n-    return jsonpickle.loads(file_content, keys=encode_non_string_keys)\\n-\\n-\\n-def json_pickle(\\n-    file_name: Union[Text, Path], obj: Any, encode_non_string_keys: bool = False\\n-) -> None:\\n-    \"\"\"Pickle an object to a file using json.\\n-\\n-    Args:\\n-        file_name: the file to store the object to\\n-        obj: the object to store\\n-        encode_non_string_keys: If set to `True` then jsonpickle will encode non-string\\n-          dictionary keys instead of coercing them into strings via `repr()`.\\n-    \"\"\"\\n-    import jsonpickle.ext.numpy as jsonpickle_numpy\\n-    import jsonpickle\\n-\\n-    jsonpickle_numpy.register_handlers()\\n-\\n-    rasa.shared.utils.io.write_text_file(\\n-        jsonpickle.dumps(obj, keys=encode_non_string_keys), file_name\\n-    )\\n-\\n-\\n def get_emoji_regex() -> Pattern:\\n     \"\"\"Returns regex to identify emojis.\"\"\"\\n     return re.compile(', '@@ -0,0 +1,370 @@\\n+from typing import Dict, Any, List, Tuple, Optional, Union\\n+\\n+import numpy as np\\n+import scipy.sparse\\n+from safetensors.numpy import load_file\\n+from safetensors.numpy import save_file\\n+\\n+import rasa.shared.utils.io\\n+\\n+\\n+def _recursive_serialize(\\n+    array: Any, prefix: str, data_dict: Dict[str, Any], metadata: List[Dict[str, Any]]\\n+) -> None:\\n+    \"\"\"Recursively serialize arrays and matrices for high dimensional data.\"\"\"\\n+    if isinstance(array, np.ndarray) and array.ndim <= 2:\\n+        data_key = f\"{prefix}_array\"\\n+        data_dict[data_key] = array\\n+        metadata.append({\"type\": \"dense\", \"key\": data_key, \"shape\": array.shape})\\n+\\n+    elif isinstance(array, list) and all([isinstance(v, float) for v in array]):\\n+        data_key = f\"{prefix}_list\"\\n+        data_dict[data_key] = np.array(array, dtype=np.float32)\\n+        metadata.append({\"type\": \"list\", \"key\": data_key})\\n+\\n+    elif isinstance(array, list) and all([isinstance(v, int) for v in array]):\\n+        data_key = f\"{prefix}_list\"\\n+        data_dict[data_key] = np.array(array, dtype=np.int64)\\n+        metadata.append({\"type\": \"list\", \"key\": data_key})\\n+\\n+    elif isinstance(array, scipy.sparse.spmatrix):\\n+        data_key_data = f\"{prefix}_data\"\\n+        data_key_row = f\"{prefix}_row\"\\n+        data_key_col = f\"{prefix}_col\"\\n+        array = array.tocoo()\\n+        data_dict.update(\\n+            {\\n+                data_key_data: array.data,\\n+                data_key_row: array.row,\\n+                data_key_col: array.col,\\n+            }\\n+        )\\n+        metadata.append({\"type\": \"sparse\", \"key\": prefix, \"shape\": array.shape})\\n+\\n+    elif isinstance(array, list) or isinstance(array, np.ndarray):\\n+        group_metadata = {\"type\": \"group\", \"subcomponents\": []}\\n+        for idx, item in enumerate(array):\\n+            new_prefix = f\"{prefix}_{idx}\"\\n+            _recursive_serialize(\\n+                item, new_prefix, data_dict, group_metadata[\"subcomponents\"]\\n+            )\\n+        metadata.append(group_metadata)\\n+\\n+\\n+def _serialize_nested_data(\\n+    nested_data: Dict[str, Dict[str, List[\"FeatureArray\"]]],\\n+    prefix: str,\\n+    data_dict: Dict[str, np.ndarray],\\n+    metadata: List[Dict[str, Union[str, List]]],\\n+) -> None:\\n+    \"\"\"Handle serialization across dictionary and list levels.\"\"\"\\n+    for outer_key, inner_dict in nested_data.items():\\n+        inner_metadata = {\"key\": outer_key, \"components\": []}\\n+\\n+        for inner_key, feature_arrays in inner_dict.items():\\n+            array_metadata = {\\n+                \"key\": inner_key,\\n+                \"number_of_dimensions\": feature_arrays[0].number_of_dimensions,\\n+                \"features\": [],\\n+            }\\n+\\n+            for idx, feature_array in enumerate(feature_arrays):\\n+                feature_prefix = f\"{prefix}_{outer_key}_{inner_key}_{idx}\"\\n+                _recursive_serialize(\\n+                    feature_array.tolist(),\\n+                    feature_prefix,\\n+                    data_dict,\\n+                    array_metadata[\"features\"],\\n+                )\\n+\\n+            inner_metadata[\"components\"].append(  # type:ignore[attr-defined]\\n+                array_metadata\\n+            )\\n+\\n+        metadata.append(inner_metadata)\\n+\\n+\\n+def serialize_nested_feature_arrays(\\n+    nested_feature_array: Dict[str, Dict[str, List[\"FeatureArray\"]]],\\n+    data_filename: str,\\n+    metadata_filename: str,\\n+) -> None:\\n+    data_dict: Dict[str, np.ndarray] = {}\\n+    metadata: List[Dict[str, Union[str, List]]] = []\\n+\\n+    _serialize_nested_data(nested_feature_array, \"component\", data_dict, metadata)\\n+\\n+    # Save serialized data and metadata\\n+    save_file(data_dict, data_filename)\\n+    rasa.shared.utils.io.dump_obj_as_json_to_file(metadata_filename, metadata)\\n+\\n+\\n+def _recursive_deserialize(\\n+    metadata: List[Dict[str, Any]], data: Dict[str, Any]\\n+) -> List[Any]:\\n+    \"\"\"Recursively deserialize arrays and matrices for high dimensional data.\"\"\"\\n+    result = []\\n+\\n+    for item in metadata:\\n+        if item[\"type\"] == \"dense\":\\n+            key = item[\"key\"]\\n+            array = np.asarray(data[key]).reshape(item[\"shape\"])\\n+            result.append(array)\\n+\\n+        elif item[\"type\"] == \"list\":\\n+            key = item[\"key\"]\\n+            result.append(list(data[key]))\\n+\\n+        elif item[\"type\"] == \"sparse\":\\n+            data_vals = data[f\"{item[\\'key\\']}_data\"]\\n+            row_vals = data[f\"{item[\\'key\\']}_row\"]\\n+            col_vals = data[f\"{item[\\'key\\']}_col\"]\\n+            sparse_matrix = scipy.sparse.coo_matrix(\\n+                (data_vals, (row_vals, col_vals)), shape=item[\"shape\"]\\n+            )\\n+            result.append(sparse_matrix)\\n+        elif item[\"type\"] == \"group\":\\n+            sublist = _recursive_deserialize(item[\"subcomponents\"], data)\\n+            result.append(sublist)\\n+\\n+    return result\\n+\\n+\\n+def _deserialize_nested_data(\\n+    metadata: List[Dict[str, Any]], data_dict: Dict[str, Any]\\n+) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:\\n+    \"\"\"Handle deserialization across all dictionary and list levels.\"\"\"\\n+    result: Dict[str, Dict[str, List[\"FeatureArray\"]]] = {}\\n+\\n+    for outer_item in metadata:\\n+        outer_key = outer_item[\"key\"]\\n+        result[outer_key] = {}\\n+\\n+        for inner_item in outer_item[\"components\"]:\\n+            inner_key = inner_item[\"key\"]\\n+            feature_arrays = []\\n+\\n+            # Reconstruct the list of FeatureArrays\\n+            for feature_item in inner_item[\"features\"]:\\n+                # Reconstruct the list of FeatureArrays\\n+                feature_array_data = _recursive_deserialize([feature_item], data_dict)\\n+                # Prepare the input for the FeatureArray;\\n+                # ensure it is np.ndarray compatible\\n+                input_array = np.array(feature_array_data[0], dtype=object)\\n+                feature_array = FeatureArray(\\n+                    input_array, inner_item[\"number_of_dimensions\"]\\n+                )\\n+                feature_arrays.append(feature_array)\\n+\\n+            result[outer_key][inner_key] = feature_arrays\\n+\\n+    return result\\n+\\n+\\n+def deserialize_nested_feature_arrays(\\n+    data_filename: str, metadata_filename: str\\n+) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:\\n+    metadata = rasa.shared.utils.io.read_json_file(metadata_filename)\\n+    data_dict = load_file(data_filename)\\n+\\n+    return _deserialize_nested_data(metadata, data_dict)\\n+\\n+\\n+class FeatureArray(np.ndarray):\\n+    \"\"\"Stores any kind of features ready to be used by a RasaModel.\\n+\\n+    Next to the input numpy array of features, it also received the number of\\n+    dimensions of the features.\\n+    As our features can have 1 to 4 dimensions we might have different number of numpy\\n+    arrays stacked. The number of dimensions helps us to figure out how to handle this\\n+    particular feature array. Also, it is automatically determined whether the feature\\n+    array is sparse or not and the number of units is determined as well.\\n+\\n+    Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html\\n+    \"\"\"\\n+\\n+    def __new__(\\n+        cls, input_array: np.ndarray, number_of_dimensions: int\\n+    ) -> \"FeatureArray\":\\n+        \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"\\n+        FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)\\n+\\n+        feature_array = np.asarray(input_array).view(cls)\\n+\\n+        if number_of_dimensions <= 2:\\n+            feature_array.units = input_array.shape[-1]\\n+            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n+        elif number_of_dimensions == 3:\\n+            feature_array.units = input_array[0].shape[-1]\\n+            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n+        elif number_of_dimensions == 4:\\n+            feature_array.units = input_array[0][0].shape[-1]\\n+            feature_array.is_sparse = isinstance(\\n+                input_array[0][0], scipy.sparse.spmatrix\\n+            )\\n+        else:\\n+            raise ValueError(\\n+                f\"Number of dimensions \\'{number_of_dimensions}\\' currently not \"\\n+                f\"supported.\"\\n+            )\\n+\\n+        feature_array.number_of_dimensions = number_of_dimensions\\n+\\n+        return feature_array\\n+\\n+    def __init__(\\n+        self, input_array: Any, number_of_dimensions: int, **kwargs: Any\\n+    ) -> None:\\n+        \"\"\"Initialize. FeatureArray.\\n+\\n+        Needed in order to avoid \\'Invalid keyword argument number_of_dimensions\\n+        to function FeatureArray.__init__ \\'\\n+        Args:\\n+            input_array: the array that contains features\\n+            number_of_dimensions: number of dimensions in input_array\\n+        \"\"\"\\n+        super().__init__(**kwargs)\\n+        self.number_of_dimensions = number_of_dimensions\\n+\\n+    def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:\\n+        \"\"\"This method is called when the system allocates a new array from obj.\\n+\\n+        Args:\\n+            obj: A subclass (subtype) of ndarray.\\n+        \"\"\"\\n+        if obj is None:\\n+            return\\n+\\n+        self.units = getattr(obj, \"units\", None)\\n+        self.number_of_dimensions = getattr(\\n+            obj, \"number_of_dimensions\", None\\n+        )  # type: ignore[assignment]\\n+        self.is_sparse = getattr(obj, \"is_sparse\", None)\\n+\\n+        default_attributes = {\\n+            \"units\": self.units,\\n+            \"number_of_dimensions\": self.number_of_dimensions,\\n+            \"is_spare\": self.is_sparse,\\n+        }\\n+        self.__dict__.update(default_attributes)\\n+\\n+    # pytype: disable=attribute-error\\n+    def __array_ufunc__(\\n+        self, ufunc: Any, method: str, *inputs: Any, **kwargs: Any\\n+    ) -> Any:\\n+        \"\"\"Overwrite this method as we are subclassing numpy array.\\n+\\n+        Args:\\n+            ufunc: The ufunc object that was called.\\n+            method: A string indicating which Ufunc method was called\\n+                    (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",\\n+                    \"inner\").\\n+            *inputs: A tuple of the input arguments to the ufunc.\\n+            **kwargs: Any additional arguments\\n+\\n+        Returns:\\n+            The result of the operation.\\n+        \"\"\"\\n+        f = {\\n+            \"reduce\": ufunc.reduce,\\n+            \"accumulate\": ufunc.accumulate,\\n+            \"reduceat\": ufunc.reduceat,\\n+            \"outer\": ufunc.outer,\\n+            \"at\": ufunc.at,\\n+            \"__call__\": ufunc,\\n+        }\\n+        # convert the inputs to np.ndarray to prevent recursion, call the function,\\n+        # then cast it back as FeatureArray\\n+        output = FeatureArray(\\n+            f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),\\n+            number_of_dimensions=kwargs[\"number_of_dimensions\"],\\n+        )\\n+        output.__dict__ = self.__dict__  # carry forward attributes\\n+        return output\\n+\\n+    def __reduce__(self) -> Tuple[Any, Any, Any]:\\n+        \"\"\"Needed in order to pickle this object.\\n+\\n+        Returns:\\n+            A tuple.\\n+        \"\"\"\\n+        pickled_state = super(FeatureArray, self).__reduce__()\\n+        if isinstance(pickled_state, str):\\n+            raise TypeError(\"np array __reduce__ returned string instead of tuple.\")\\n+        new_state = pickled_state[2] + (\\n+            self.number_of_dimensions,\\n+            self.is_sparse,\\n+            self.units,\\n+        )\\n+        return pickled_state[0], pickled_state[1], new_state\\n+\\n+    def __setstate__(self, state: Any, **kwargs: Any) -> None:\\n+        \"\"\"Sets the state.\\n+\\n+        Args:\\n+            state: The state argument must be a sequence that contains the following\\n+                   elements version, shape, dtype, isFortan, rawdata.\\n+            **kwargs: Any additional parameter\\n+        \"\"\"\\n+        # Needed in order to load the object\\n+        self.number_of_dimensions = state[-3]\\n+        self.is_sparse = state[-2]\\n+        self.units = state[-1]\\n+        super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)\\n+\\n+    # pytype: enable=attribute-error\\n+\\n+    @staticmethod\\n+    def _validate_number_of_dimensions(\\n+        number_of_dimensions: int, input_array: np.ndarray\\n+    ) -> None:\\n+        \"\"\"Validates if the input array has given number of dimensions.\\n+\\n+        Args:\\n+            number_of_dimensions: number of dimensions\\n+            input_array: input array\\n+\\n+        Raises: ValueError in case the dimensions do not match\\n+        \"\"\"\\n+        # when loading the feature arrays from disk, the shape represents\\n+        # the correct number of dimensions\\n+        if len(input_array.shape) == number_of_dimensions:\\n+            return\\n+\\n+        _sub_array = input_array\\n+        dim = 0\\n+        # Go number_of_dimensions into the given input_array\\n+        for i in range(1, number_of_dimensions + 1):\\n+            _sub_array = _sub_array[0]\\n+            if isinstance(_sub_array, scipy.sparse.spmatrix):\\n+                dim = i\\n+                break\\n+            if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n+                # sequence dimension is 0, we are dealing with \"fake\" features\\n+                dim = i\\n+                break\\n+\\n+        # If the resulting sub_array is sparse, the remaining number of dimensions\\n+        # should be at least 2\\n+        if isinstance(_sub_array, scipy.sparse.spmatrix):\\n+            if dim > 2:\\n+                raise ValueError(\\n+                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n+                    f\"match dimensions of given input array: {input_array}.\"\\n+                )\\n+        elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n+            # sequence dimension is 0, we are dealing with \"fake\" features,\\n+            # but they should be of dim 2\\n+            if dim > 2:\\n+                raise ValueError(\\n+                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n+                    f\"match dimensions of given input array: {input_array}.\"\\n+                )\\n+        # If the resulting sub_array is dense, the sub_array should be a single number\\n+        elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(\\n+            _sub_array, (np.float32, np.float64)\\n+        ):\\n+            raise ValueError(\\n+                f\"Given number of dimensions \\'{number_of_dimensions}\\' does not match \"\\n+                f\"dimensions of given input array: {input_array}.\"\\n+            )', '@@ -12,6 +12,7 @@\\n import warnings\\n import random\\n import string\\n+\\n import portalocker\\n \\n from ruamel import yaml as yaml', '@@ -7,4 +7,3 @@ if [[ ${GITHUB_TAG} =~ ^[0-9]+\\\\.[0-9]+\\\\.[0-9]+$ ]]; then\\n \\t --data \"{\\\\\"text\\\\\":\\\\\"ğŸ’¥ New *Rasa Open Source* version ${GITHUB_TAG} has been released! https://github.com/RasaHQ/rasa/releases/tag/${GITHUB_TAG}\\\\\"}\" \\\\\\n \\t \"https://hooks.slack.com/services/T0GHWFTS8/BMTQQL47K/${SLACK_WEBHOOK_TOKEN}\"\\n fi\\n-', '@@ -1,9 +1,7 @@\\n from __future__ import annotations\\n+\\n import logging\\n from collections import OrderedDict\\n-\\n-import scipy.sparse\\n-import numpy as np\\n from typing import (\\n     Any,\\n     Dict,\\n@@ -17,30 +15,34 @@\\n     Union,\\n )\\n \\n+import numpy as np\\n+import scipy.sparse\\n+\\n+import rasa.shared.utils.io\\n+import rasa.utils.io\\n from rasa.engine.graph import ExecutionContext, GraphComponent\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n+from rasa.nlu.constants import TOKENS_NAMES\\n+from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY, SpacyTokenizer\\n from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer\\n-from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n-from rasa.nlu.constants import TOKENS_NAMES\\n from rasa.shared.constants import DOCS_URL_COMPONENTS\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n-from rasa.shared.nlu.constants import TEXT\\n from rasa.shared.exceptions import InvalidConfigException\\n-import rasa.shared.utils.io\\n-import rasa.utils.io\\n+from rasa.shared.nlu.constants import TEXT\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n \\n logger = logging.getLogger(__name__)\\n \\n-\\n END_OF_SENTENCE = \"EOS\"\\n BEGIN_OF_SENTENCE = \"BOS\"\\n \\n FEATURES = \"features\"\\n \\n+SEPERATOR = \"###\"\\n+\\n \\n @DefaultV1Recipe.register(\\n     DefaultV1Recipe.ComponentType.MESSAGE_FEATURIZER, is_trainable=True\\n@@ -72,7 +74,7 @@ class LexicalSyntacticFeaturizer(SparseFeaturizer, GraphComponent):\\n       of the token at position `t+1`.\\n     \"\"\"\\n \\n-    FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.pkl\"\\n+    FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.json\"\\n \\n     # NOTE: \"suffix5\" of the token \"is\" will be \"is\". Hence, when combining multiple\\n     # prefixes, short words will be represented/encoded repeatedly.\\n@@ -489,6 +491,32 @@ def create(\\n         \"\"\"Creates a new untrained component (see parent class for full docstring).\"\"\"\\n         return cls(config, model_storage, resource, execution_context)\\n \\n+    @staticmethod\\n+    def _restructure_feature_to_idx_dict(\\n+        loaded_data: Dict[str, Dict[str, int]],\\n+    ) -> Dict[Tuple[int, str], Dict[str, int]]:\\n+        \"\"\"Reconstructs the feature to idx dict.\\n+\\n+        When storing the feature_to_idx_dict to disk, we need to convert the tuple (key)\\n+        into a string to be able to store it via json. When loading the data\\n+        we need to reconstruct the tuple from the stored string.\\n+\\n+        Args:\\n+            loaded_data: The loaded feature to idx dict from file.\\n+\\n+        Returns:\\n+            The reconstructed feature_to_idx_dict\\n+        \"\"\"\\n+        feature_to_idx_dict = {}\\n+        for tuple_string, feature_value in loaded_data.items():\\n+            # Example of tuple_string: \"1###low\"\\n+            index, feature_name = tuple_string.split(SEPERATOR)\\n+\\n+            feature_key = (int(index), feature_name)\\n+            feature_to_idx_dict[feature_key] = feature_value\\n+\\n+        return feature_to_idx_dict\\n+\\n     @classmethod\\n     def load(\\n         cls,\\n@@ -501,10 +529,13 @@ def load(\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n         try:\\n             with model_storage.read_from(resource) as model_path:\\n-                feature_to_idx_dict = rasa.utils.io.json_unpickle(\\n+                loaded_data = rasa.shared.utils.io.read_json_file(\\n                     model_path / cls.FILENAME_FEATURE_TO_IDX_DICT,\\n-                    encode_non_string_keys=True,\\n                 )\\n+\\n+                # convert the key back into tuple\\n+                feature_to_idx_dict = cls._restructure_feature_to_idx_dict(loaded_data)\\n+\\n                 return cls(\\n                     config=config,\\n                     model_storage=model_storage,\\n@@ -529,9 +560,13 @@ def persist(self) -> None:\\n         if not self._feature_to_idx_dict:\\n             return None\\n \\n+        # as we cannot dump tuples, convert the tuple into a string\\n+        restructured_feature_dict = {\\n+            f\"{k[0]}{SEPERATOR}{k[1]}\": v for k, v in self._feature_to_idx_dict.items()\\n+        }\\n+\\n         with self._model_storage.write_to(self._resource) as model_path:\\n-            rasa.utils.io.json_pickle(\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n                 model_path / self.FILENAME_FEATURE_TO_IDX_DICT,\\n-                self._feature_to_idx_dict,\\n-                encode_non_string_keys=True,\\n+                restructured_feature_dict,\\n             )', '@@ -8,6 +8,7 @@\\n import tempfile\\n import warnings\\n from pathlib import Path\\n+from socket import SOCK_DGRAM, SOCK_STREAM\\n from types import TracebackType\\n from typing import (\\n     Any,\\n@@ -24,8 +25,9 @@\\n     Tuple,\\n )\\n \\n-from socket import SOCK_DGRAM, SOCK_STREAM\\n import numpy as np\\n+\\n+import rasa.shared.utils.io\\n import rasa.utils.io\\n from rasa.constants import (\\n     DEFAULT_LOG_LEVEL_LIBRARIES,\\n@@ -36,7 +38,6 @@\\n )\\n from rasa.shared.constants import DEFAULT_LOG_LEVEL, ENV_LOG_LEVEL, TCP_PROTOCOL\\n from rasa.shared.exceptions import RasaException\\n-import rasa.shared.utils.io\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -153,7 +154,7 @@ def configure_logging_from_file(logging_config_file: Text) -> None:\\n     try:\\n         logging.config.dictConfig(logging_config_dict)\\n     except (ValueError, TypeError, AttributeError, ImportError) as e:\\n-        logging.debug(\\n+        logger.debug(\\n             f\"The logging config file {logging_config_file} could not \"\\n             f\"be applied because it failed validation against \"\\n             f\"the built-in Python logging schema. \"', '@@ -1,20 +1,19 @@\\n import logging\\n from typing import Any, Text, Dict, List, Type, Tuple\\n \\n-import joblib\\n from scipy.sparse import hstack, vstack, csr_matrix\\n from sklearn.linear_model import LogisticRegression\\n \\n+from rasa.engine.graph import ExecutionContext, GraphComponent\\n+from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n-from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n-from rasa.engine.graph import ExecutionContext, GraphComponent\\n from rasa.nlu.classifiers import LABEL_RANKING_LENGTH\\n-from rasa.nlu.featurizers.featurizer import Featurizer\\n from rasa.nlu.classifiers.classifier import IntentClassifier\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n+from rasa.nlu.featurizers.featurizer import Featurizer\\n from rasa.shared.nlu.constants import TEXT, INTENT\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.utils.tensorflow.constants import RANKING_LENGTH\\n \\n logger = logging.getLogger(__name__)\\n@@ -158,9 +157,11 @@ def process(self, messages: List[Message]) -> List[Message]:\\n \\n     def persist(self) -> None:\\n         \"\"\"Persist this model into the passed directory.\"\"\"\\n+        import skops.io as sio\\n+\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n-            path = model_dir / f\"{self._resource.name}.joblib\"\\n-            joblib.dump(self.clf, path)\\n+            path = model_dir / f\"{self._resource.name}.skops\"\\n+            sio.dump(self.clf, path)\\n             logger.debug(f\"Saved intent classifier to \\'{path}\\'.\")\\n \\n     @classmethod\\n@@ -173,9 +174,21 @@ def load(\\n         **kwargs: Any,\\n     ) -> \"LogisticRegressionClassifier\":\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n+        import skops.io as sio\\n+\\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n-                classifier = joblib.load(model_dir / f\"{resource.name}.joblib\")\\n+                classifier_file = model_dir / f\"{resource.name}.skops\"\\n+                unknown_types = sio.get_untrusted_types(file=classifier_file)\\n+\\n+                if unknown_types:\\n+                    logger.debug(\\n+                        f\"Untrusted types ({unknown_types}) found when \"\\n+                        f\"loading {classifier_file}!\",\\n+                    )\\n+                    raise ValueError()\\n+\\n+                classifier = sio.load(classifier_file, trusted=unknown_types)\\n                 component = cls(\\n                     config, execution_context.node_name, model_storage, resource\\n                 )', '@@ -20,6 +20,8 @@\\n import scipy.sparse\\n from sklearn.model_selection import train_test_split\\n \\n+from rasa.utils.tensorflow.feature_array import FeatureArray\\n+\\n logger = logging.getLogger(__name__)\\n \\n \\n@@ -37,199 +39,6 @@ def ragged_array_to_ndarray(ragged_array: Iterable[np.ndarray]) -> np.ndarray:\\n         return np.array(ragged_array, dtype=object)\\n \\n \\n-class FeatureArray(np.ndarray):\\n-    \"\"\"Stores any kind of features ready to be used by a RasaModel.\\n-\\n-    Next to the input numpy array of features, it also received the number of\\n-    dimensions of the features.\\n-    As our features can have 1 to 4 dimensions we might have different number of numpy\\n-    arrays stacked. The number of dimensions helps us to figure out how to handle this\\n-    particular feature array. Also, it is automatically determined whether the feature\\n-    array is sparse or not and the number of units is determined as well.\\n-\\n-    Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html\\n-    \"\"\"\\n-\\n-    def __new__(\\n-        cls, input_array: np.ndarray, number_of_dimensions: int\\n-    ) -> \"FeatureArray\":\\n-        \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"\\n-        FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)\\n-\\n-        feature_array = np.asarray(input_array).view(cls)\\n-\\n-        if number_of_dimensions <= 2:\\n-            feature_array.units = input_array.shape[-1]\\n-            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n-        elif number_of_dimensions == 3:\\n-            feature_array.units = input_array[0].shape[-1]\\n-            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n-        elif number_of_dimensions == 4:\\n-            feature_array.units = input_array[0][0].shape[-1]\\n-            feature_array.is_sparse = isinstance(\\n-                input_array[0][0], scipy.sparse.spmatrix\\n-            )\\n-        else:\\n-            raise ValueError(\\n-                f\"Number of dimensions \\'{number_of_dimensions}\\' currently not \"\\n-                f\"supported.\"\\n-            )\\n-\\n-        feature_array.number_of_dimensions = number_of_dimensions\\n-\\n-        return feature_array\\n-\\n-    def __init__(\\n-        self, input_array: Any, number_of_dimensions: int, **kwargs: Any\\n-    ) -> None:\\n-        \"\"\"Initialize. FeatureArray.\\n-\\n-        Needed in order to avoid \\'Invalid keyword argument number_of_dimensions\\n-        to function FeatureArray.__init__ \\'\\n-        Args:\\n-            input_array: the array that contains features\\n-            number_of_dimensions: number of dimensions in input_array\\n-        \"\"\"\\n-        super().__init__(**kwargs)\\n-        self.number_of_dimensions = number_of_dimensions\\n-\\n-    def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:\\n-        \"\"\"This method is called when the system allocates a new array from obj.\\n-\\n-        Args:\\n-            obj: A subclass (subtype) of ndarray.\\n-        \"\"\"\\n-        if obj is None:\\n-            return\\n-\\n-        self.units = getattr(obj, \"units\", None)\\n-        self.number_of_dimensions = getattr(obj, \"number_of_dimensions\", None)  # type: ignore[assignment] # noqa:E501\\n-        self.is_sparse = getattr(obj, \"is_sparse\", None)\\n-\\n-        default_attributes = {\\n-            \"units\": self.units,\\n-            \"number_of_dimensions\": self.number_of_dimensions,\\n-            \"is_spare\": self.is_sparse,\\n-        }\\n-        self.__dict__.update(default_attributes)\\n-\\n-    # pytype: disable=attribute-error\\n-    def __array_ufunc__(\\n-        self, ufunc: Any, method: Text, *inputs: Any, **kwargs: Any\\n-    ) -> Any:\\n-        \"\"\"Overwrite this method as we are subclassing numpy array.\\n-\\n-        Args:\\n-            ufunc: The ufunc object that was called.\\n-            method: A string indicating which Ufunc method was called\\n-                    (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",\\n-                    \"inner\").\\n-            *inputs: A tuple of the input arguments to the ufunc.\\n-            **kwargs: Any additional arguments\\n-\\n-        Returns:\\n-            The result of the operation.\\n-        \"\"\"\\n-        f = {\\n-            \"reduce\": ufunc.reduce,\\n-            \"accumulate\": ufunc.accumulate,\\n-            \"reduceat\": ufunc.reduceat,\\n-            \"outer\": ufunc.outer,\\n-            \"at\": ufunc.at,\\n-            \"__call__\": ufunc,\\n-        }\\n-        # convert the inputs to np.ndarray to prevent recursion, call the function,\\n-        # then cast it back as FeatureArray\\n-        output = FeatureArray(\\n-            f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),\\n-            number_of_dimensions=kwargs[\"number_of_dimensions\"],\\n-        )\\n-        output.__dict__ = self.__dict__  # carry forward attributes\\n-        return output\\n-\\n-    def __reduce__(self) -> Tuple[Any, Any, Any]:\\n-        \"\"\"Needed in order to pickle this object.\\n-\\n-        Returns:\\n-            A tuple.\\n-        \"\"\"\\n-        pickled_state = super(FeatureArray, self).__reduce__()\\n-        if isinstance(pickled_state, str):\\n-            raise TypeError(\"np array __reduce__ returned string instead of tuple.\")\\n-        new_state = pickled_state[2] + (\\n-            self.number_of_dimensions,\\n-            self.is_sparse,\\n-            self.units,\\n-        )\\n-        return pickled_state[0], pickled_state[1], new_state\\n-\\n-    def __setstate__(self, state: Any, **kwargs: Any) -> None:\\n-        \"\"\"Sets the state.\\n-\\n-        Args:\\n-            state: The state argument must be a sequence that contains the following\\n-                   elements version, shape, dtype, isFortan, rawdata.\\n-            **kwargs: Any additional parameter\\n-        \"\"\"\\n-        # Needed in order to load the object\\n-        self.number_of_dimensions = state[-3]\\n-        self.is_sparse = state[-2]\\n-        self.units = state[-1]\\n-        super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)\\n-\\n-    # pytype: enable=attribute-error\\n-\\n-    @staticmethod\\n-    def _validate_number_of_dimensions(\\n-        number_of_dimensions: int, input_array: np.ndarray\\n-    ) -> None:\\n-        \"\"\"Validates if the the input array has given number of dimensions.\\n-\\n-        Args:\\n-            number_of_dimensions: number of dimensions\\n-            input_array: input array\\n-\\n-        Raises: ValueError in case the dimensions do not match\\n-        \"\"\"\\n-        _sub_array = input_array\\n-        dim = 0\\n-        # Go number_of_dimensions into the given input_array\\n-        for i in range(1, number_of_dimensions + 1):\\n-            _sub_array = _sub_array[0]\\n-            if isinstance(_sub_array, scipy.sparse.spmatrix):\\n-                dim = i\\n-                break\\n-            if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n-                # sequence dimension is 0, we are dealing with \"fake\" features\\n-                dim = i\\n-                break\\n-\\n-        # If the resulting sub_array is sparse, the remaining number of dimensions\\n-        # should be at least 2\\n-        if isinstance(_sub_array, scipy.sparse.spmatrix):\\n-            if dim > 2:\\n-                raise ValueError(\\n-                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n-                    f\"match dimensions of given input array: {input_array}.\"\\n-                )\\n-        elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n-            # sequence dimension is 0, we are dealing with \"fake\" features,\\n-            # but they should be of dim 2\\n-            if dim > 2:\\n-                raise ValueError(\\n-                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n-                    f\"match dimensions of given input array: {input_array}.\"\\n-                )\\n-        # If the resulting sub_array is dense, the sub_array should be a single number\\n-        elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(\\n-            _sub_array, (np.float32, np.float64)\\n-        ):\\n-            raise ValueError(\\n-                f\"Given number of dimensions \\'{number_of_dimensions}\\' does not match \"\\n-                f\"dimensions of given input array: {input_array}.\"\\n-            )\\n-\\n-\\n class FeatureSignature(NamedTuple):\\n     \"\"\"Signature of feature arrays.\\n \\n@@ -270,8 +79,7 @@ def __init__(\\n         label_sub_key: Optional[Text] = None,\\n         data: Optional[Data] = None,\\n     ) -> None:\\n-        \"\"\"\\n-        Initializes the RasaModelData object.\\n+        \"\"\"Initializes the RasaModelData object.\\n \\n         Args:\\n             label_key: the key of a label used for balancing, etc.', '@@ -1,11 +1,9 @@\\n from __future__ import annotations\\n-from pathlib import Path\\n-from collections import defaultdict\\n-from abc import abstractmethod\\n-import jsonpickle\\n-import logging\\n \\n-from tqdm import tqdm\\n+import logging\\n+from abc import abstractmethod\\n+from collections import defaultdict\\n+from pathlib import Path\\n from typing import (\\n     Tuple,\\n     List,\\n@@ -18,25 +16,30 @@\\n     Set,\\n     DefaultDict,\\n     cast,\\n+    Type,\\n+    Callable,\\n+    ClassVar,\\n )\\n+\\n import numpy as np\\n+from tqdm import tqdm\\n \\n-from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer\\n-from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\\n-from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError\\n import rasa.shared.core.trackers\\n import rasa.shared.utils.io\\n-from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME\\n-from rasa.shared.nlu.training_data.features import Features\\n-from rasa.shared.core.trackers import DialogueStateTracker\\n-from rasa.shared.core.domain import State, Domain\\n-from rasa.shared.core.events import Event, ActionExecuted, UserUttered\\n+from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError\\n+from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\\n+from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer\\n from rasa.shared.core.constants import (\\n     USER,\\n     ACTION_UNLIKELY_INTENT_NAME,\\n     PREVIOUS_ACTION,\\n )\\n+from rasa.shared.core.domain import State, Domain\\n+from rasa.shared.core.events import Event, ActionExecuted, UserUttered\\n+from rasa.shared.core.trackers import DialogueStateTracker\\n from rasa.shared.exceptions import RasaException\\n+from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME\\n+from rasa.shared.nlu.training_data.features import Features\\n from rasa.utils.tensorflow.constants import LABEL_PAD_ID\\n from rasa.utils.tensorflow.model_data import ragged_array_to_ndarray\\n \\n@@ -64,6 +67,10 @@ def __str__(self) -> Text:\\n class TrackerFeaturizer:\\n     \"\"\"Base class for actual tracker featurizers.\"\"\"\\n \\n+    # Class registry to store all subclasses\\n+    _registry: ClassVar[Dict[str, Type[\"TrackerFeaturizer\"]]] = {}\\n+    _featurizer_type: str = \"TrackerFeaturizer\"\\n+\\n     def __init__(\\n         self, state_featurizer: Optional[SingleStateFeaturizer] = None\\n     ) -> None:\\n@@ -74,6 +81,36 @@ def __init__(\\n         \"\"\"\\n         self.state_featurizer = state_featurizer\\n \\n+    @classmethod\\n+    def register(cls, featurizer_type: str) -> Callable:\\n+        \"\"\"Decorator to register featurizer subclasses.\"\"\"\\n+\\n+        def wrapper(subclass: Type[\"TrackerFeaturizer\"]) -> Type[\"TrackerFeaturizer\"]:\\n+            cls._registry[featurizer_type] = subclass\\n+            # Store the type identifier in the class for serialization\\n+            subclass._featurizer_type = featurizer_type\\n+            return subclass\\n+\\n+        return wrapper\\n+\\n+    @classmethod\\n+    def from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":\\n+        \"\"\"Create featurizer instance from dictionary.\"\"\"\\n+        featurizer_type = data.pop(\"type\")\\n+\\n+        if featurizer_type not in cls._registry:\\n+            raise ValueError(f\"Unknown featurizer type: {featurizer_type}\")\\n+\\n+        # Get the correct subclass and instantiate it\\n+        subclass = cls._registry[featurizer_type]\\n+        return subclass.create_from_dict(data)\\n+\\n+    @classmethod\\n+    @abstractmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":\\n+        \"\"\"Each subclass must implement its own creation from dict method.\"\"\"\\n+        pass\\n+\\n     @staticmethod\\n     def _create_states(\\n         tracker: DialogueStateTracker,\\n@@ -465,9 +502,7 @@ def persist(self, path: Union[Text, Path]) -> None:\\n             self.state_featurizer.entity_tag_specs = []\\n \\n         # noinspection PyTypeChecker\\n-        rasa.shared.utils.io.write_text_file(\\n-            str(jsonpickle.encode(self)), featurizer_file\\n-        )\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, self.to_dict())\\n \\n     @staticmethod\\n     def load(path: Union[Text, Path]) -> Optional[TrackerFeaturizer]:\\n@@ -481,7 +516,17 @@ def load(path: Union[Text, Path]) -> Optional[TrackerFeaturizer]:\\n         \"\"\"\\n         featurizer_file = Path(path) / FEATURIZER_FILE\\n         if featurizer_file.is_file():\\n-            return jsonpickle.decode(rasa.shared.utils.io.read_file(featurizer_file))\\n+            data = rasa.shared.utils.io.read_json_file(featurizer_file)\\n+\\n+            if \"type\" not in data:\\n+                logger.error(\\n+                    f\"Couldn\\'t load featurizer for policy. \"\\n+                    f\"File \\'{featurizer_file}\\' does not contain all \"\\n+                    f\"necessary information. \\'type\\' is missing.\"\\n+                )\\n+                return None\\n+\\n+            return TrackerFeaturizer.from_dict(data)\\n \\n         logger.error(\\n             f\"Couldn\\'t load featurizer for policy. \"\\n@@ -508,7 +553,16 @@ def _remove_action_unlikely_intent_from_events(events: List[Event]) -> List[Even\\n             )\\n         ]\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return {\\n+            \"type\": self.__class__._featurizer_type,\\n+            \"state_featurizer\": (\\n+                self.state_featurizer.to_dict() if self.state_featurizer else None\\n+            ),\\n+        }\\n+\\n \\n+@TrackerFeaturizer.register(\"FullDialogueTrackerFeaturizer\")\\n class FullDialogueTrackerFeaturizer(TrackerFeaturizer):\\n     \"\"\"Creates full dialogue training data for time distributed architectures.\\n \\n@@ -646,7 +700,20 @@ def prediction_states(\\n \\n         return trackers_as_states\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return super().to_dict()\\n \\n+    @classmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"FullDialogueTrackerFeaturizer\":\\n+        state_featurizer = SingleStateFeaturizer.create_from_dict(\\n+            data[\"state_featurizer\"]\\n+        )\\n+        return cls(\\n+            state_featurizer,\\n+        )\\n+\\n+\\n+@TrackerFeaturizer.register(\"MaxHistoryTrackerFeaturizer\")\\n class MaxHistoryTrackerFeaturizer(TrackerFeaturizer):\\n     \"\"\"Truncates the tracker history into `max_history` long sequences.\\n \\n@@ -887,7 +954,25 @@ def prediction_states(\\n \\n         return trackers_as_states\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        data = super().to_dict()\\n+        data.update(\\n+            {\\n+                \"remove_duplicates\": self.remove_duplicates,\\n+                \"max_history\": self.max_history,\\n+            }\\n+        )\\n+        return data\\n+\\n+    @classmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"MaxHistoryTrackerFeaturizer\":\\n+        state_featurizer = SingleStateFeaturizer.create_from_dict(\\n+            data[\"state_featurizer\"]\\n+        )\\n+        return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])\\n \\n+\\n+@TrackerFeaturizer.register(\"IntentMaxHistoryTrackerFeaturizer\")\\n class IntentMaxHistoryTrackerFeaturizer(MaxHistoryTrackerFeaturizer):\\n     \"\"\"Truncates the tracker history into `max_history` long sequences.\\n \\n@@ -1166,6 +1251,18 @@ def prediction_states(\\n \\n         return trackers_as_states\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return super().to_dict()\\n+\\n+    @classmethod\\n+    def create_from_dict(\\n+        cls, data: Dict[str, Any]\\n+    ) -> \"IntentMaxHistoryTrackerFeaturizer\":\\n+        state_featurizer = SingleStateFeaturizer.create_from_dict(\\n+            data[\"state_featurizer\"]\\n+        )\\n+        return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])\\n+\\n \\n def _is_prev_action_unlikely_intent_in_state(state: State) -> bool:\\n     prev_action_name = state.get(PREVIOUS_ACTION, {}).get(ACTION_NAME)', '@@ -1,12 +1,12 @@\\n from __future__ import annotations\\n \\n-from collections import OrderedDict\\n-from enum import Enum\\n import logging\\n import typing\\n+from collections import OrderedDict\\n+from enum import Enum\\n+from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type\\n \\n import numpy as np\\n-from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type\\n \\n import rasa.nlu.utils.bilou_utils as bilou_utils\\n import rasa.shared.utils.io\\n@@ -15,13 +15,12 @@\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n+from rasa.nlu.constants import TOKENS_NAMES\\n+from rasa.nlu.extractors.extractor import EntityExtractorMixin\\n from rasa.nlu.test import determine_token_labels\\n from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY\\n-from rasa.nlu.extractors.extractor import EntityExtractorMixin\\n from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n-from rasa.nlu.constants import TOKENS_NAMES\\n+from rasa.shared.constants import DOCS_URL_COMPONENTS\\n from rasa.shared.nlu.constants import (\\n     TEXT,\\n     ENTITIES,\\n@@ -32,7 +31,8 @@\\n     SPLIT_ENTITIES_BY_COMMA,\\n     SPLIT_ENTITIES_BY_COMMA_DEFAULT_VALUE,\\n )\\n-from rasa.shared.constants import DOCS_URL_COMPONENTS\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.utils.tensorflow.constants import BILOU_FLAG, FEATURIZERS\\n \\n logger = logging.getLogger(__name__)\\n@@ -41,6 +41,9 @@\\n     from sklearn_crfsuite import CRF\\n \\n \\n+CONFIG_FEATURES = \"features\"\\n+\\n+\\n class CRFToken:\\n     def __init__(\\n         self,\\n@@ -60,6 +63,29 @@ def __init__(\\n         self.entity_role_tag = entity_role_tag\\n         self.entity_group_tag = entity_group_tag\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return {\\n+            \"text\": self.text,\\n+            \"pos_tag\": self.pos_tag,\\n+            \"pattern\": self.pattern,\\n+            \"dense_features\": [str(x) for x in list(self.dense_features)],\\n+            \"entity_tag\": self.entity_tag,\\n+            \"entity_role_tag\": self.entity_role_tag,\\n+            \"entity_group_tag\": self.entity_group_tag,\\n+        }\\n+\\n+    @classmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"CRFToken\":\\n+        return cls(\\n+            data[\"text\"],\\n+            data[\"pos_tag\"],\\n+            data[\"pattern\"],\\n+            np.array([float(x) for x in data[\"dense_features\"]]),\\n+            data[\"entity_tag\"],\\n+            data[\"entity_role_tag\"],\\n+            data[\"entity_group_tag\"],\\n+        )\\n+\\n \\n class CRFEntityExtractorOptions(str, Enum):\\n     \"\"\"Features that can be used for the \\'CRFEntityExtractor\\'.\"\"\"\\n@@ -137,7 +163,7 @@ def get_default_config() -> Dict[Text, Any]:\\n             # \"is the preceding token in title case?\"\\n             # POS features require SpacyTokenizer\\n             # pattern feature require RegexFeaturizer\\n-            CRFEntityExtractor.CONFIG_FEATURES: [\\n+            CONFIG_FEATURES: [\\n                 [\\n                     CRFEntityExtractorOptions.LOW,\\n                     CRFEntityExtractorOptions.TITLE,\\n@@ -200,7 +226,7 @@ def __init__(\\n         )\\n \\n     def _validate_configuration(self) -> None:\\n-        if len(self.component_config.get(self.CONFIG_FEATURES, [])) % 2 != 1:\\n+        if len(self.component_config.get(CONFIG_FEATURES, [])) % 2 != 1:\\n             raise ValueError(\\n                 \"Need an odd number of crf feature lists to have a center word.\"\\n             )\\n@@ -251,9 +277,11 @@ def train(self, training_data: TrainingData) -> Resource:\\n         ]\\n         dataset = [self._convert_to_crf_tokens(example) for example in entity_examples]\\n \\n-        self._train_model(dataset)\\n+        self.entity_taggers = self.train_model(\\n+            dataset, self.component_config, self.crf_order\\n+        )\\n \\n-        self.persist()\\n+        self.persist(dataset)\\n \\n         return self._resource\\n \\n@@ -299,7 +327,9 @@ def extract_entities(self, message: Message) -> List[Dict[Text, Any]]:\\n             if include_tag_features:\\n                 self._add_tag_to_crf_token(crf_tokens, predictions)\\n \\n-            features = self._crf_tokens_to_features(crf_tokens, include_tag_features)\\n+            features = self._crf_tokens_to_features(\\n+                crf_tokens, self.component_config, include_tag_features\\n+            )\\n             predictions[tag_name] = entity_tagger.predict_marginals_single(features)\\n \\n         # convert predictions into a list of tags and a list of confidences\\n@@ -389,51 +419,55 @@ def load(\\n         **kwargs: Any,\\n     ) -> CRFEntityExtractor:\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n-        import joblib\\n-\\n         try:\\n-            entity_taggers = OrderedDict()\\n             with model_storage.read_from(resource) as model_dir:\\n-                # We have to load in the same order as we persisted things as otherwise\\n-                # the predictions might be off\\n-                file_names = sorted(model_dir.glob(\"**/*.pkl\"))\\n-                if not file_names:\\n-                    logger.debug(\\n-                        \"Failed to load model for \\'CRFEntityExtractor\\'. \"\\n-                        \"Maybe you did not provide enough training data and \"\\n-                        \"no model was trained.\"\\n-                    )\\n-                    return cls(config, model_storage, resource)\\n+                dataset = rasa.shared.utils.io.read_json_file(\\n+                    model_dir / \"crf_dataset.json\"\\n+                )\\n+                crf_order = rasa.shared.utils.io.read_json_file(\\n+                    model_dir / \"crf_order.json\"\\n+                )\\n+\\n+                dataset = [\\n+                    [CRFToken.create_from_dict(token_data) for token_data in sub_list]\\n+                    for sub_list in dataset\\n+                ]\\n \\n-                for file_name in file_names:\\n-                    name = file_name.stem[1:]\\n-                    entity_taggers[name] = joblib.load(file_name)\\n+                entity_taggers = cls.train_model(dataset, config, crf_order)\\n \\n-                return cls(config, model_storage, resource, entity_taggers)\\n+                entity_extractor = cls(config, model_storage, resource, entity_taggers)\\n+                entity_extractor.crf_order = crf_order\\n+                return entity_extractor\\n         except ValueError:\\n             logger.warning(\\n                 f\"Failed to load {cls.__name__} from model storage. Resource \"\\n                 f\"\\'{resource.name}\\' doesn\\'t exist.\"\\n             )\\n             return cls(config, model_storage, resource)\\n \\n-    def persist(self) -> None:\\n+    def persist(self, dataset: List[List[CRFToken]]) -> None:\\n         \"\"\"Persist this model into the passed directory.\"\"\"\\n-        import joblib\\n-\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n-            if self.entity_taggers:\\n-                for idx, (name, entity_tagger) in enumerate(\\n-                    self.entity_taggers.items()\\n-                ):\\n-                    model_file_name = model_dir / f\"{idx}{name}.pkl\"\\n-                    joblib.dump(entity_tagger, model_file_name)\\n+            data_to_store = [\\n+                [token.to_dict() for token in sub_list] for sub_list in dataset\\n+            ]\\n \\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                model_dir / \"crf_dataset.json\", data_to_store\\n+            )\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                model_dir / \"crf_order.json\", self.crf_order\\n+            )\\n+\\n+    @classmethod\\n     def _crf_tokens_to_features(\\n-        self, crf_tokens: List[CRFToken], include_tag_features: bool = False\\n+        cls,\\n+        crf_tokens: List[CRFToken],\\n+        config: Dict[str, Any],\\n+        include_tag_features: bool = False,\\n     ) -> List[Dict[Text, Any]]:\\n         \"\"\"Convert the list of tokens into discrete features.\"\"\"\\n-        configured_features = self.component_config[self.CONFIG_FEATURES]\\n+        configured_features = config[CONFIG_FEATURES]\\n         sentence_features = []\\n \\n         for token_idx in range(len(crf_tokens)):\\n@@ -444,28 +478,31 @@ def _crf_tokens_to_features(\\n             half_window_size = window_size // 2\\n             window_range = range(-half_window_size, half_window_size + 1)\\n \\n-            token_features = self._create_features_for_token(\\n+            token_features = cls._create_features_for_token(\\n                 crf_tokens,\\n                 token_idx,\\n                 half_window_size,\\n                 window_range,\\n                 include_tag_features,\\n+                config,\\n             )\\n \\n             sentence_features.append(token_features)\\n \\n         return sentence_features\\n \\n+    @classmethod\\n     def _create_features_for_token(\\n-        self,\\n+        cls,\\n         crf_tokens: List[CRFToken],\\n         token_idx: int,\\n         half_window_size: int,\\n         window_range: range,\\n         include_tag_features: bool,\\n+        config: Dict[str, Any],\\n     ) -> Dict[Text, Any]:\\n         \"\"\"Convert a token into discrete features including words before and after.\"\"\"\\n-        configured_features = self.component_config[self.CONFIG_FEATURES]\\n+        configured_features = config[CONFIG_FEATURES]\\n         prefixes = [str(i) for i in window_range]\\n \\n         token_features = {}\\n@@ -505,13 +542,13 @@ def _create_features_for_token(\\n                         # set in the training data, \\'matched\\' is either \\'True\\' or\\n                         # \\'False\\' depending on whether the token actually matches the\\n                         # pattern or not\\n-                        regex_patterns = self.function_dict[feature](token)\\n+                        regex_patterns = cls.function_dict[feature](token)\\n                         for pattern_name, matched in regex_patterns.items():\\n                             token_features[\\n                                 f\"{prefix}:{feature}:{pattern_name}\"\\n                             ] = matched\\n                     else:\\n-                        value = self.function_dict[feature](token)\\n+                        value = cls.function_dict[feature](token)\\n                         token_features[f\"{prefix}:{feature}\"] = value\\n \\n         return token_features\\n@@ -635,38 +672,46 @@ def _get_tags(self, message: Message) -> Dict[Text, List[Text]]:\\n \\n         return tags\\n \\n-    def _train_model(self, df_train: List[List[CRFToken]]) -> None:\\n+    @classmethod\\n+    def train_model(\\n+        cls,\\n+        df_train: List[List[CRFToken]],\\n+        config: Dict[str, Any],\\n+        crf_order: List[str],\\n+    ) -> OrderedDict[str, CRF]:\\n         \"\"\"Train the crf tagger based on the training data.\"\"\"\\n         import sklearn_crfsuite\\n \\n-        self.entity_taggers = OrderedDict()\\n+        entity_taggers = OrderedDict()\\n \\n-        for tag_name in self.crf_order:\\n+        for tag_name in crf_order:\\n             logger.debug(f\"Training CRF for \\'{tag_name}\\'.\")\\n \\n             # add entity tag features for second level CRFs\\n             include_tag_features = tag_name != ENTITY_ATTRIBUTE_TYPE\\n             X_train = (\\n-                self._crf_tokens_to_features(sentence, include_tag_features)\\n+                cls._crf_tokens_to_features(sentence, config, include_tag_features)\\n                 for sentence in df_train\\n             )\\n             y_train = (\\n-                self._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train\\n+                cls._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train\\n             )\\n \\n             entity_tagger = sklearn_crfsuite.CRF(\\n                 algorithm=\"lbfgs\",\\n                 # coefficient for L1 penalty\\n-                c1=self.component_config[\"L1_c\"],\\n+                c1=config[\"L1_c\"],\\n                 # coefficient for L2 penalty\\n-                c2=self.component_config[\"L2_c\"],\\n+                c2=config[\"L2_c\"],\\n                 # stop earlier\\n-                max_iterations=self.component_config[\"max_iterations\"],\\n+                max_iterations=config[\"max_iterations\"],\\n                 # include transitions that are possible, but not observed\\n                 all_possible_transitions=True,\\n             )\\n             entity_tagger.fit(X_train, y_train)\\n \\n-            self.entity_taggers[tag_name] = entity_tagger\\n+            entity_taggers[tag_name] = entity_tagger\\n \\n             logger.debug(\"Training finished.\")\\n+\\n+        return entity_taggers', '@@ -1,15 +1,15 @@\\n from __future__ import annotations\\n-import logging\\n \\n-from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n+import logging\\n from pathlib import Path\\n from collections import defaultdict\\n import contextlib\\n+from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type\\n \\n import numpy as np\\n import tensorflow as tf\\n-from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type\\n \\n+from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.graph import ExecutionContext\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n@@ -49,18 +49,22 @@\\n from rasa.shared.core.events import EntitiesAdded, Event\\n from rasa.shared.core.domain import Domain\\n from rasa.shared.nlu.training_data.message import Message\\n-from rasa.shared.nlu.training_data.features import Features\\n+from rasa.shared.nlu.training_data.features import (\\n+    Features,\\n+    save_features,\\n+    load_features,\\n+)\\n import rasa.shared.utils.io\\n import rasa.utils.io\\n from rasa.utils import train_utils\\n-from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\\n-from rasa.utils.tensorflow import rasa_layers\\n-from rasa.utils.tensorflow.model_data import (\\n-    RasaModelData,\\n-    FeatureSignature,\\n+from rasa.utils.tensorflow.feature_array import (\\n     FeatureArray,\\n-    Data,\\n+    serialize_nested_feature_arrays,\\n+    deserialize_nested_feature_arrays,\\n )\\n+from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\\n+from rasa.utils.tensorflow import rasa_layers\\n+from rasa.utils.tensorflow.model_data import RasaModelData, FeatureSignature, Data\\n from rasa.utils.tensorflow.model_data_utils import convert_to_data_format\\n from rasa.utils.tensorflow.constants import (\\n     LABEL,\\n@@ -961,22 +965,32 @@ def persist_model_utilities(self, model_path: Path) -> None:\\n             model_path: Path where model is to be persisted\\n         \"\"\"\\n         model_filename = self._metadata_filename()\\n-        rasa.utils.io.json_pickle(\\n-            model_path / f\"{model_filename}.priority.pkl\", self.priority\\n-        )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.meta.pkl\", self.config\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+            model_path / f\"{model_filename}.priority.json\", self.priority\\n         )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.data_example.pkl\", self.data_example\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+            model_path / f\"{model_filename}.meta.json\", self.config\\n         )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.fake_features.pkl\", self.fake_features\\n+        # save data example\\n+        serialize_nested_feature_arrays(\\n+            self.data_example,\\n+            str(model_path / f\"{model_filename}.data_example.st\"),\\n+            str(model_path / f\"{model_filename}.data_example_metadata.json\"),\\n         )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.label_data.pkl\",\\n+        # save label data\\n+        serialize_nested_feature_arrays(\\n             dict(self._label_data.data) if self._label_data is not None else {},\\n+            str(model_path / f\"{model_filename}.label_data.st\"),\\n+            str(model_path / f\"{model_filename}.label_data_metadata.json\"),\\n+        )\\n+        # save fake features\\n+        metadata = save_features(\\n+            self.fake_features, str(model_path / f\"{model_filename}.fake_features.st\")\\n+        )\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+            model_path / f\"{model_filename}.fake_features_metadata.json\", metadata\\n         )\\n+\\n         entity_tag_specs = (\\n             [tag_spec._asdict() for tag_spec in self._entity_tag_specs]\\n             if self._entity_tag_specs\\n@@ -994,18 +1008,29 @@ def _load_model_utilities(cls, model_path: Path) -> Dict[Text, Any]:\\n             model_path: Path where model is to be persisted.\\n         \"\"\"\\n         tf_model_file = model_path / f\"{cls._metadata_filename()}.tf_model\"\\n-        loaded_data = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.data_example.pkl\"\\n+\\n+        # load data example\\n+        loaded_data = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{cls._metadata_filename()}.data_example.st\"),\\n+            str(model_path / f\"{cls._metadata_filename()}.data_example_metadata.json\"),\\n         )\\n-        label_data = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.label_data.pkl\"\\n+        # load label data\\n+        loaded_label_data = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{cls._metadata_filename()}.label_data.st\"),\\n+            str(model_path / f\"{cls._metadata_filename()}.label_data_metadata.json\"),\\n         )\\n-        fake_features = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.fake_features.pkl\"\\n+        label_data = RasaModelData(data=loaded_label_data)\\n+\\n+        # load fake features\\n+        metadata = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{cls._metadata_filename()}.fake_features_metadata.json\"\\n         )\\n-        label_data = RasaModelData(data=label_data)\\n-        priority = rasa.utils.io.json_unpickle(\\n-            model_path / f\"{cls._metadata_filename()}.priority.pkl\"\\n+        fake_features = load_features(\\n+            str(model_path / f\"{cls._metadata_filename()}.fake_features.st\"), metadata\\n+        )\\n+\\n+        priority = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{cls._metadata_filename()}.priority.json\"\\n         )\\n         entity_tag_specs = rasa.shared.utils.io.read_json_file(\\n             model_path / f\"{cls._metadata_filename()}.entity_tag_specs.json\"\\n@@ -1023,8 +1048,8 @@ def _load_model_utilities(cls, model_path: Path) -> Dict[Text, Any]:\\n             )\\n             for tag_spec in entity_tag_specs\\n         ]\\n-        model_config = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.meta.pkl\"\\n+        model_config = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{cls._metadata_filename()}.meta.json\"\\n         )\\n \\n         return {\\n@@ -1070,7 +1095,7 @@ def _load(\\n     ) -> TEDPolicy:\\n         featurizer = TrackerFeaturizer.load(model_path)\\n \\n-        if not (model_path / f\"{cls._metadata_filename()}.data_example.pkl\").is_file():\\n+        if not (model_path / f\"{cls._metadata_filename()}.data_example.st\").is_file():\\n             return cls(\\n                 config,\\n                 model_storage,', '@@ -1,15 +1,133 @@\\n from __future__ import annotations\\n-from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set\\n+\\n import itertools\\n+from dataclasses import dataclass\\n+from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set\\n \\n import numpy as np\\n import scipy.sparse\\n+from safetensors.numpy import save_file, load_file\\n \\n-import rasa.shared.utils.io\\n import rasa.shared.nlu.training_data.util\\n+import rasa.shared.utils.io\\n from rasa.shared.nlu.constants import FEATURE_TYPE_SEQUENCE, FEATURE_TYPE_SENTENCE\\n \\n \\n+@dataclass\\n+class FeatureMetadata:\\n+    data_type: str\\n+    attribute: str\\n+    origin: Union[str, List[str]]\\n+    is_sparse: bool\\n+    shape: tuple\\n+    safetensors_key: str\\n+\\n+\\n+def save_features(\\n+    features_dict: Dict[Text, List[Features]], file_name: str\\n+) -> Dict[str, Any]:\\n+    \"\"\"Save a dictionary of Features lists to disk using safetensors.\\n+\\n+    Args:\\n+        features_dict: Dictionary mapping strings to lists of Features objects\\n+        file_name: File to save the features to\\n+\\n+    Returns:\\n+        The metadata to reconstruct the features.\\n+    \"\"\"\\n+    # All tensors are stored in a single safetensors file\\n+    tensors_to_save = {}\\n+    # Metadata will be stored separately\\n+    metadata = {}\\n+\\n+    for key, features_list in features_dict.items():\\n+        feature_metadata_list = []\\n+\\n+        for idx, feature in enumerate(features_list):\\n+            # Create a unique key for this tensor in the safetensors file\\n+            safetensors_key = f\"{key}_{idx}\"\\n+\\n+            # Convert sparse matrices to dense if needed\\n+            if feature.is_sparse():\\n+                # For sparse matrices, use the COO format\\n+                coo = feature.features.tocoo()  # type:ignore[union-attr]\\n+                # Save data, row indices and col indices separately\\n+                tensors_to_save[f\"{safetensors_key}_data\"] = coo.data\\n+                tensors_to_save[f\"{safetensors_key}_row\"] = coo.row\\n+                tensors_to_save[f\"{safetensors_key}_col\"] = coo.col\\n+            else:\\n+                tensors_to_save[safetensors_key] = feature.features\\n+\\n+            # Store metadata\\n+            metadata_item = FeatureMetadata(\\n+                data_type=feature.type,\\n+                attribute=feature.attribute,\\n+                origin=feature.origin,\\n+                is_sparse=feature.is_sparse(),\\n+                shape=feature.features.shape,\\n+                safetensors_key=safetensors_key,\\n+            )\\n+            feature_metadata_list.append(vars(metadata_item))\\n+\\n+        metadata[key] = feature_metadata_list\\n+\\n+    # Save tensors\\n+    save_file(tensors_to_save, file_name)\\n+\\n+    return metadata\\n+\\n+\\n+def load_features(\\n+    filename: str, metadata: Dict[str, Any]\\n+) -> Dict[Text, List[Features]]:\\n+    \"\"\"Load Features dictionary from disk.\\n+\\n+    Args:\\n+        filename: File name of the safetensors file.\\n+        metadata: Metadata to reconstruct the features.\\n+\\n+    Returns:\\n+        Dictionary mapping strings to lists of Features objects\\n+    \"\"\"\\n+    # Load tensors\\n+    tensors = load_file(filename)\\n+\\n+    # Reconstruct the features dictionary\\n+    features_dict: Dict[Text, List[Features]] = {}\\n+\\n+    for key, feature_metadata_list in metadata.items():\\n+        features_list = []\\n+\\n+        for meta in feature_metadata_list:\\n+            safetensors_key = meta[\"safetensors_key\"]\\n+\\n+            if meta[\"is_sparse\"]:\\n+                # Reconstruct sparse matrix from COO format\\n+                data = tensors[f\"{safetensors_key}_data\"]\\n+                row = tensors[f\"{safetensors_key}_row\"]\\n+                col = tensors[f\"{safetensors_key}_col\"]\\n+\\n+                features_matrix = scipy.sparse.coo_matrix(\\n+                    (data, (row, col)), shape=tuple(meta[\"shape\"])\\n+                ).tocsr()  # Convert back to CSR format\\n+            else:\\n+                features_matrix = tensors[safetensors_key]\\n+\\n+            # Reconstruct Features object\\n+            features = Features(\\n+                features=features_matrix,\\n+                feature_type=meta[\"data_type\"],\\n+                attribute=meta[\"attribute\"],\\n+                origin=meta[\"origin\"],\\n+            )\\n+\\n+            features_list.append(features)\\n+\\n+        features_dict[key] = features_list\\n+\\n+    return features_dict\\n+\\n+\\n class Features:\\n     \"\"\"Stores the features produced by any featurizer.\"\"\"\\n ', '@@ -1,37 +1,39 @@\\n from __future__ import annotations\\n+\\n import copy\\n import logging\\n from collections import defaultdict\\n from pathlib import Path\\n-\\n-from rasa.exceptions import ModelNotFound\\n-from rasa.nlu.featurizers.featurizer import Featurizer\\n+from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type\\n \\n import numpy as np\\n import scipy.sparse\\n import tensorflow as tf\\n \\n-from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type\\n-\\n+from rasa.exceptions import ModelNotFound\\n+from rasa.nlu.featurizers.featurizer import Featurizer\\n from rasa.engine.graph import ExecutionContext, GraphComponent\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n from rasa.nlu.extractors.extractor import EntityExtractorMixin\\n from rasa.nlu.classifiers.classifier import IntentClassifier\\n import rasa.shared.utils.io\\n-import rasa.utils.io as io_utils\\n import rasa.nlu.utils.bilou_utils as bilou_utils\\n from rasa.shared.constants import DIAGNOSTIC_DATA\\n from rasa.nlu.extractors.extractor import EntityTagSpec\\n from rasa.nlu.classifiers import LABEL_RANKING_LENGTH\\n from rasa.utils import train_utils\\n from rasa.utils.tensorflow import rasa_layers\\n+from rasa.utils.tensorflow.feature_array import (\\n+    FeatureArray,\\n+    serialize_nested_feature_arrays,\\n+    deserialize_nested_feature_arrays,\\n+)\\n from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\\n from rasa.utils.tensorflow.model_data import (\\n     RasaModelData,\\n     FeatureSignature,\\n-    FeatureArray,\\n )\\n from rasa.nlu.constants import TOKENS_NAMES, DEFAULT_TRANSFORMER_SIZE\\n from rasa.shared.nlu.constants import (\\n@@ -118,7 +120,6 @@\\n \\n POSSIBLE_TAGS = [ENTITY_ATTRIBUTE_TYPE, ENTITY_ATTRIBUTE_ROLE, ENTITY_ATTRIBUTE_GROUP]\\n \\n-\\n DIETClassifierT = TypeVar(\"DIETClassifierT\", bound=\"DIETClassifier\")\\n \\n \\n@@ -1085,18 +1086,24 @@ def persist(self) -> None:\\n \\n             self.model.save(str(tf_model_file))\\n \\n-            io_utils.pickle_dump(\\n-                model_path / f\"{file_name}.data_example.pkl\", self._data_example\\n-            )\\n-            io_utils.pickle_dump(\\n-                model_path / f\"{file_name}.sparse_feature_sizes.pkl\",\\n-                self._sparse_feature_sizes,\\n+            # save data example\\n+            serialize_nested_feature_arrays(\\n+                self._data_example,\\n+                model_path / f\"{file_name}.data_example.st\",\\n+                model_path / f\"{file_name}.data_example_metadata.json\",\\n             )\\n-            io_utils.pickle_dump(\\n-                model_path / f\"{file_name}.label_data.pkl\",\\n+            # save label data\\n+            serialize_nested_feature_arrays(\\n                 dict(self._label_data.data) if self._label_data is not None else {},\\n+                model_path / f\"{file_name}.label_data.st\",\\n+                model_path / f\"{file_name}.label_data_metadata.json\",\\n             )\\n-            io_utils.json_pickle(\\n+\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                model_path / f\"{file_name}.sparse_feature_sizes.json\",\\n+                self._sparse_feature_sizes,\\n+            )\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n                 model_path / f\"{file_name}.index_label_id_mapping.json\",\\n                 self.index_label_id_mapping,\\n             )\\n@@ -1185,15 +1192,22 @@ def _load_from_files(\\n     ]:\\n         file_name = cls.__name__\\n \\n-        data_example = io_utils.pickle_load(\\n-            model_path / f\"{file_name}.data_example.pkl\"\\n+        # load data example\\n+        data_example = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{file_name}.data_example.st\"),\\n+            str(model_path / f\"{file_name}.data_example_metadata.json\"),\\n         )\\n-        label_data = io_utils.pickle_load(model_path / f\"{file_name}.label_data.pkl\")\\n-        label_data = RasaModelData(data=label_data)\\n-        sparse_feature_sizes = io_utils.pickle_load(\\n-            model_path / f\"{file_name}.sparse_feature_sizes.pkl\"\\n+        # load label data\\n+        loaded_label_data = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{file_name}.label_data.st\"),\\n+            str(model_path / f\"{file_name}.label_data_metadata.json\"),\\n+        )\\n+        label_data = RasaModelData(data=loaded_label_data)\\n+\\n+        sparse_feature_sizes = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{file_name}.sparse_feature_sizes.json\"\\n         )\\n-        index_label_id_mapping = io_utils.json_unpickle(\\n+        index_label_id_mapping = rasa.shared.utils.io.read_json_file(\\n             model_path / f\"{file_name}.index_label_id_mapping.json\"\\n         )\\n         entity_tag_specs = rasa.shared.utils.io.read_json_file(\\n@@ -1213,7 +1227,6 @@ def _load_from_files(\\n             for tag_spec in entity_tag_specs\\n         ]\\n \\n-        # jsonpickle converts dictionary keys to strings\\n         index_label_id_mapping = {\\n             int(key): value for key, value in index_label_id_mapping.items()\\n         }'], 'file': ['rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py', 'rasa/core/featurizers/single_state_featurizer.py', 'rasa/nlu/classifiers/sklearn_intent_classifier.py', 'rasa/core/policies/unexpected_intent_policy.py', 'rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py', 'rasa/utils/io.py', 'rasa/utils/tensorflow/feature_array.py', 'rasa/shared/utils/io.py', 'scripts/ping_slack_about_package_release.sh', 'rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py', 'rasa/utils/common.py', 'rasa/nlu/classifiers/logistic_regression_classifier.py', 'rasa/utils/tensorflow/model_data.py', 'rasa/core/featurizers/tracker_featurizers.py', 'rasa/nlu/extractors/crf_entity_extractor.py', 'rasa/core/policies/ted_policy.py', 'rasa/shared/nlu/training_data/features.py', 'rasa/nlu/classifiers/diet_classifier.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Shell', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('fb2376ec-7581-433b-8417-ce31865cc381'), UUID('42523fdb-d82a-4e9f-8a38-9397386a291c'), UUID('aee502be-564a-4b05-aadb-b63ff6bacad6'), UUID('f429d8a3-b518-49fa-b605-822e01c99675'), UUID('2722187a-4d90-4953-a606-dcb4d97066fc'), UUID('7338d2db-0e35-4cf7-b97a-dd1217f4c5e0'), UUID('d8bdc762-a4d5-411c-bbb9-dcf06b222e40'), UUID('cd8dc7e7-298a-4545-ae25-c65f8fea6c82'), UUID('43f2c1c8-980c-42cd-a8bf-13d084465ff2'), UUID('1cfbbe77-5365-4a8e-beb7-89d844633fc6'), UUID('2f1b3e8c-aea3-4150-abd7-59d7b117439c'), UUID('5f06ddc2-a672-482e-b8e8-c00ca8898e05'), UUID('0cab3d96-cbf8-4655-8e52-bdc423b0466e'), UUID('a7950cfc-3098-499e-8556-108bb3ee2b96'), UUID('7d23552d-9574-4303-b923-0d1874de2869'), UUID('7447554e-c13e-4f30-aa7c-0eb4df57d2e3'), UUID('3c2b5049-b3df-4708-a662-9a9b44925c6f'), UUID('df4ae407-6ad1-4add-9d02-71b530c85654')]}\n",
      "ERROR:root:Error in {'repo': 'RasaHQ/rasa', 'vulnerability_id': '2024-49375', 'commit': '2bb1d779d4f4acaf70b6dfa35dd1899dccbb1ae6', 'commit_source': 'github', 'cwe_id': ['CWE-94', 'CWE-94'], 'patch': ['@@ -1,11 +1,13 @@\\n from __future__ import annotations\\n+\\n import logging\\n import re\\n from typing import Any, Dict, List, Optional, Text, Tuple, Type\\n+\\n import numpy as np\\n import scipy.sparse\\n-from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n \\n+from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n import rasa.shared.utils.io\\n import rasa.utils.io\\n import rasa.nlu.utils.pattern_utils as pattern_utils\\n@@ -240,7 +242,7 @@ def load(\\n \\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n-                patterns_file_name = model_dir / \"patterns.pkl\"\\n+                patterns_file_name = model_dir / \"patterns.json\"\\n                 known_patterns = rasa.shared.utils.io.read_json_file(patterns_file_name)\\n         except (ValueError, FileNotFoundError):\\n             logger.warning(\\n@@ -258,7 +260,7 @@ def load(\\n \\n     def _persist(self) -> None:\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n-            regex_file = model_dir / \"patterns.pkl\"\\n+            regex_file = model_dir / \"patterns.json\"\\n             rasa.shared.utils.io.dump_obj_as_json_to_file(\\n                 regex_file, self.known_patterns\\n             )', '@@ -1,7 +1,8 @@\\n import logging\\n+from typing import List, Optional, Dict, Text, Set, Any\\n+\\n import numpy as np\\n import scipy.sparse\\n-from typing import List, Optional, Dict, Text, Set, Any\\n \\n from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\\n from rasa.nlu.extractors.extractor import EntityTagSpec\\n@@ -362,6 +363,26 @@ def encode_all_labels(\\n             for action in domain.action_names_or_texts\\n         ]\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return {\\n+            \"action_texts\": self.action_texts,\\n+            \"entity_tag_specs\": self.entity_tag_specs,\\n+            \"feature_states\": self._default_feature_states,\\n+        }\\n+\\n+    @classmethod\\n+    def create_from_dict(\\n+        cls, data: Dict[str, Any]\\n+    ) -> Optional[\"SingleStateFeaturizer\"]:\\n+        if not data:\\n+            return None\\n+\\n+        featurizer = SingleStateFeaturizer()\\n+        featurizer.action_texts = data[\"action_texts\"]\\n+        featurizer._default_feature_states = data[\"feature_states\"]\\n+        featurizer.entity_tag_specs = data[\"entity_tag_specs\"]\\n+        return featurizer\\n+\\n \\n class IntentTokenizerSingleStateFeaturizer(SingleStateFeaturizer):\\n     \"\"\"A SingleStateFeaturizer for use with policies that predict intent labels.\"\"\"', '@@ -1,25 +1,25 @@\\n from __future__ import annotations\\n+\\n import logging\\n-from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer\\n import typing\\n import warnings\\n from typing import Any, Dict, List, Optional, Text, Tuple, Type\\n \\n import numpy as np\\n \\n import rasa.shared.utils.io\\n-import rasa.utils.io as io_utils\\n from rasa.engine.graph import GraphComponent, ExecutionContext\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n-from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU\\n from rasa.nlu.classifiers import LABEL_RANKING_LENGTH\\n+from rasa.nlu.classifiers.classifier import IntentClassifier\\n+from rasa.nlu.featurizers.dense_featurizer.dense_featurizer import DenseFeaturizer\\n+from rasa.shared.constants import DOCS_URL_TRAINING_DATA_NLU\\n from rasa.shared.exceptions import RasaException\\n from rasa.shared.nlu.constants import TEXT\\n-from rasa.nlu.classifiers.classifier import IntentClassifier\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.utils.tensorflow.constants import FEATURIZERS\\n \\n logger = logging.getLogger(__name__)\\n@@ -266,14 +266,20 @@ def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\\n \\n     def persist(self) -> None:\\n         \"\"\"Persist this model into the passed directory.\"\"\"\\n+        import skops.io as sio\\n+\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n             file_name = self.__class__.__name__\\n-            classifier_file_name = model_dir / f\"{file_name}_classifier.pkl\"\\n-            encoder_file_name = model_dir / f\"{file_name}_encoder.pkl\"\\n+            classifier_file_name = model_dir / f\"{file_name}_classifier.skops\"\\n+            encoder_file_name = model_dir / f\"{file_name}_encoder.json\"\\n \\n             if self.clf and self.le:\\n-                io_utils.json_pickle(encoder_file_name, self.le.classes_)\\n-                io_utils.json_pickle(classifier_file_name, self.clf.best_estimator_)\\n+                # convert self.le.classes_ (numpy array of strings) to a list in order\\n+                # to use json dump\\n+                rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                    encoder_file_name, list(self.le.classes_)\\n+                )\\n+                sio.dump(self.clf.best_estimator_, classifier_file_name)\\n \\n     @classmethod\\n     def load(\\n@@ -286,21 +292,36 @@ def load(\\n     ) -> SklearnIntentClassifier:\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n         from sklearn.preprocessing import LabelEncoder\\n+        import skops.io as sio\\n \\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n                 file_name = cls.__name__\\n-                classifier_file = model_dir / f\"{file_name}_classifier.pkl\"\\n+                classifier_file = model_dir / f\"{file_name}_classifier.skops\"\\n \\n                 if classifier_file.exists():\\n-                    classifier = io_utils.json_unpickle(classifier_file)\\n+                    unknown_types = sio.get_untrusted_types(file=classifier_file)\\n \\n-                    encoder_file = model_dir / f\"{file_name}_encoder.pkl\"\\n-                    classes = io_utils.json_unpickle(encoder_file)\\n-                    encoder = LabelEncoder()\\n-                    encoder.classes_ = classes\\n+                    if unknown_types:\\n+                        logger.error(\\n+                            f\"Untrusted types ({unknown_types}) found when \"\\n+                            f\"loading {classifier_file}!\"\\n+                        )\\n+                        raise ValueError()\\n+                    else:\\n+                        classifier = sio.load(classifier_file, trusted=unknown_types)\\n+\\n+                    encoder_file = model_dir / f\"{file_name}_encoder.json\"\\n+                    classes = rasa.shared.utils.io.read_json_file(encoder_file)\\n \\n-                    return cls(config, model_storage, resource, classifier, encoder)\\n+                    encoder = LabelEncoder()\\n+                    intent_classifier = cls(\\n+                        config, model_storage, resource, classifier, encoder\\n+                    )\\n+                    # convert list of strings (class labels) back to numpy array of\\n+                    # strings\\n+                    intent_classifier.transform_labels_str2num(classes)\\n+                    return intent_classifier\\n         except ValueError:\\n             logger.debug(\\n                 f\"Failed to load \\'{cls.__name__}\\' from model storage. Resource \"', '@@ -5,6 +5,7 @@\\n \\n import numpy as np\\n import tensorflow as tf\\n+\\n import rasa.utils.common\\n from rasa.engine.graph import ExecutionContext\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n@@ -16,6 +17,7 @@\\n from rasa.shared.core.trackers import DialogueStateTracker\\n from rasa.shared.core.constants import SLOTS, ACTIVE_LOOP, ACTION_UNLIKELY_INTENT_NAME\\n from rasa.shared.core.events import UserUttered, ActionExecuted\\n+import rasa.shared.utils.io\\n from rasa.shared.nlu.constants import (\\n     INTENT,\\n     TEXT,\\n@@ -103,8 +105,6 @@\\n )\\n from rasa.utils.tensorflow import layers\\n from rasa.utils.tensorflow.model_data import RasaModelData, FeatureArray, Data\\n-\\n-import rasa.utils.io as io_utils\\n from rasa.core.exceptions import RasaCoreException\\n from rasa.shared.utils import common\\n \\n@@ -881,9 +881,12 @@ def persist_model_utilities(self, model_path: Path) -> None:\\n             model_path: Path where model is to be persisted\\n         \"\"\"\\n         super().persist_model_utilities(model_path)\\n-        io_utils.pickle_dump(\\n-            model_path / f\"{self._metadata_filename()}.label_quantiles.pkl\",\\n-            self.label_quantiles,\\n+\\n+        from safetensors.numpy import save_file\\n+\\n+        save_file(\\n+            {str(k): np.array(v) for k, v in self.label_quantiles.items()},\\n+            model_path / f\"{self._metadata_filename()}.label_quantiles.st\",\\n         )\\n \\n     @classmethod\\n@@ -894,9 +897,14 @@ def _load_model_utilities(cls, model_path: Path) -> Dict[Text, Any]:\\n             model_path: Path where model is to be persisted.\\n         \"\"\"\\n         model_utilties = super()._load_model_utilities(model_path)\\n-        label_quantiles = io_utils.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.label_quantiles.pkl\"\\n+\\n+        from safetensors.numpy import load_file\\n+\\n+        loaded_label_quantiles = load_file(\\n+            model_path / f\"{cls._metadata_filename()}.label_quantiles.st\"\\n         )\\n+        label_quantiles = {int(k): list(v) for k, v in loaded_label_quantiles.items()}\\n+\\n         model_utilties.update({\"label_quantiles\": label_quantiles})\\n         return model_utilties\\n ', '@@ -1,29 +1,31 @@\\n from __future__ import annotations\\n+\\n import logging\\n import re\\n+from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type, Union\\n+\\n+import numpy as np\\n import scipy.sparse\\n-from typing import Any, Dict, List, Optional, Text, Tuple, Set, Type\\n-from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n+from sklearn.feature_extraction.text import CountVectorizer\\n \\n import rasa.shared.utils.io\\n from rasa.engine.graph import GraphComponent, ExecutionContext\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n-from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n-from rasa.nlu.utils.spacy_utils import SpacyModel\\n-from rasa.shared.constants import DOCS_URL_COMPONENTS\\n-import rasa.utils.io as io_utils\\n-from sklearn.feature_extraction.text import CountVectorizer\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n-from rasa.shared.exceptions import RasaException, FileIOException\\n from rasa.nlu.constants import (\\n     TOKENS_NAMES,\\n     MESSAGE_ATTRIBUTES,\\n     DENSE_FEATURIZABLE_ATTRIBUTES,\\n )\\n+from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n+from rasa.nlu.tokenizers.tokenizer import Tokenizer\\n+from rasa.nlu.utils.spacy_utils import SpacyModel\\n+from rasa.shared.constants import DOCS_URL_COMPONENTS\\n+from rasa.shared.exceptions import RasaException, FileIOException\\n from rasa.shared.nlu.constants import TEXT, INTENT, INTENT_RESPONSE_KEY, ACTION_NAME\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n \\n BUFFER_SLOTS_PREFIX = \"buf_\"\\n \\n@@ -686,6 +688,31 @@ def _is_any_model_trained(\\n         \"\"\"Check if any model got trained.\"\"\"\\n         return any(value is not None for value in attribute_vocabularies.values())\\n \\n+    @staticmethod\\n+    def convert_vocab(\\n+        vocab: Dict[str, Union[int, Optional[Dict[str, int]]]], to_int: bool\\n+    ) -> Dict[str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]]:\\n+        \"\"\"Converts numpy integers in the vocabulary to Python integers.\"\"\"\\n+\\n+        def convert_value(value: int) -> Union[int, np.int64]:\\n+            \"\"\"Helper function to convert a single value based on to_int flag.\"\"\"\\n+            return int(value) if to_int else np.int64(value)\\n+\\n+        result_dict: Dict[\\n+            str, Union[None, int, np.int64, Dict[str, Union[int, np.int64]]]\\n+        ] = {}\\n+        for key, sub_dict in vocab.items():\\n+            if isinstance(sub_dict, int):\\n+                result_dict[key] = convert_value(sub_dict)\\n+            elif not sub_dict:\\n+                result_dict[key] = None\\n+            else:\\n+                result_dict[key] = {\\n+                    sub_key: convert_value(value) for sub_key, value in sub_dict.items()\\n+                }\\n+\\n+        return result_dict\\n+\\n     def persist(self) -> None:\\n         \"\"\"Persist this model into the passed directory.\\n \\n@@ -699,17 +726,18 @@ def persist(self) -> None:\\n             attribute_vocabularies = self._collect_vectorizer_vocabularies()\\n             if self._is_any_model_trained(attribute_vocabularies):\\n                 # Definitely need to persist some vocabularies\\n-                featurizer_file = model_dir / \"vocabularies.pkl\"\\n+                featurizer_file = model_dir / \"vocabularies.json\"\\n \\n                 # Only persist vocabulary from one attribute if `use_shared_vocab`.\\n                 # Can be loaded and distributed to all attributes.\\n-                vocab = (\\n+                loaded_vocab = (\\n                     attribute_vocabularies[TEXT]\\n                     if self.use_shared_vocab\\n                     else attribute_vocabularies\\n                 )\\n+                vocab = self.convert_vocab(loaded_vocab, to_int=True)\\n \\n-                io_utils.json_pickle(featurizer_file, vocab)\\n+                rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, vocab)\\n \\n                 # Dump OOV words separately as they might have been modified during\\n                 # training\\n@@ -784,8 +812,9 @@ def load(\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n-                featurizer_file = model_dir / \"vocabularies.pkl\"\\n-                vocabulary = io_utils.json_unpickle(featurizer_file)\\n+                featurizer_file = model_dir / \"vocabularies.json\"\\n+                vocabulary = rasa.shared.utils.io.read_json_file(featurizer_file)\\n+                vocabulary = cls.convert_vocab(vocabulary, to_int=False)\\n \\n                 share_vocabulary = config[\"use_shared_vocab\"]\\n ', '@@ -2,13 +2,13 @@\\n import filecmp\\n import logging\\n import os\\n-import pickle\\n+import re\\n import tempfile\\n import warnings\\n-import re\\n from asyncio import AbstractEventLoop\\n from pathlib import Path\\n-from typing import Text, Any, Union, List, Type, Callable, TYPE_CHECKING, Pattern\\n+from typing import Text, Any, List, Type, Callable, TYPE_CHECKING, Pattern\\n+\\n from typing_extensions import Protocol\\n \\n import rasa.shared.constants\\n@@ -81,29 +81,6 @@ def enable_async_loop_debugging(\\n     return event_loop\\n \\n \\n-def pickle_dump(filename: Union[Text, Path], obj: Any) -> None:\\n-    \"\"\"Saves object to file.\\n-\\n-    Args:\\n-        filename: the filename to save the object to\\n-        obj: the object to store\\n-    \"\"\"\\n-    with open(filename, \"wb\") as f:\\n-        pickle.dump(obj, f)\\n-\\n-\\n-def pickle_load(filename: Union[Text, Path]) -> Any:\\n-    \"\"\"Loads an object from a file.\\n-\\n-    Args:\\n-        filename: the filename to load the object from\\n-\\n-    Returns: the loaded object\\n-    \"\"\"\\n-    with open(filename, \"rb\") as f:\\n-        return pickle.load(f)\\n-\\n-\\n def create_temporary_file(data: Any, suffix: Text = \"\", mode: Text = \"w+\") -> Text:\\n     \"\"\"Creates a tempfile.NamedTemporaryFile object for data.\"\"\"\\n     encoding = None if \"b\" in mode else rasa.shared.utils.io.DEFAULT_ENCODING\\n@@ -124,7 +101,6 @@ def create_temporary_directory() -> Text:\\n \\n def create_path(file_path: Text) -> None:\\n     \"\"\"Makes sure all directories in the \\'file_path\\' exists.\"\"\"\\n-\\n     parent_dir = os.path.dirname(os.path.abspath(file_path))\\n     if not os.path.exists(parent_dir):\\n         os.makedirs(parent_dir)\\n@@ -160,8 +136,8 @@ def create_validator(\\n     function: Callable[[Text], bool], error_message: Text\\n ) -> Type[\"Validator\"]:\\n     \"\"\"Helper method to create `Validator` classes from callable functions. Should be\\n-    removed when questionary supports `Validator` objects.\"\"\"\\n-\\n+    removed when questionary supports `Validator` objects.\\n+    \"\"\"\\n     from prompt_toolkit.validation import Validator, ValidationError\\n     from prompt_toolkit.document import Document\\n \\n@@ -175,48 +151,6 @@ def validate(document: Document) -> None:\\n     return FunctionValidator\\n \\n \\n-def json_unpickle(\\n-    file_name: Union[Text, Path], encode_non_string_keys: bool = False\\n-) -> Any:\\n-    \"\"\"Unpickle an object from file using json.\\n-\\n-    Args:\\n-        file_name: the file to load the object from\\n-        encode_non_string_keys: If set to `True` then jsonpickle will encode non-string\\n-          dictionary keys instead of coercing them into strings via `repr()`.\\n-\\n-    Returns: the object\\n-    \"\"\"\\n-    import jsonpickle.ext.numpy as jsonpickle_numpy\\n-    import jsonpickle\\n-\\n-    jsonpickle_numpy.register_handlers()\\n-\\n-    file_content = rasa.shared.utils.io.read_file(file_name)\\n-    return jsonpickle.loads(file_content, keys=encode_non_string_keys)\\n-\\n-\\n-def json_pickle(\\n-    file_name: Union[Text, Path], obj: Any, encode_non_string_keys: bool = False\\n-) -> None:\\n-    \"\"\"Pickle an object to a file using json.\\n-\\n-    Args:\\n-        file_name: the file to store the object to\\n-        obj: the object to store\\n-        encode_non_string_keys: If set to `True` then jsonpickle will encode non-string\\n-          dictionary keys instead of coercing them into strings via `repr()`.\\n-    \"\"\"\\n-    import jsonpickle.ext.numpy as jsonpickle_numpy\\n-    import jsonpickle\\n-\\n-    jsonpickle_numpy.register_handlers()\\n-\\n-    rasa.shared.utils.io.write_text_file(\\n-        jsonpickle.dumps(obj, keys=encode_non_string_keys), file_name\\n-    )\\n-\\n-\\n def get_emoji_regex() -> Pattern:\\n     \"\"\"Returns regex to identify emojis.\"\"\"\\n     return re.compile(', '@@ -0,0 +1,370 @@\\n+from typing import Dict, Any, List, Tuple, Optional, Union\\n+\\n+import numpy as np\\n+import scipy.sparse\\n+from safetensors.numpy import load_file\\n+from safetensors.numpy import save_file\\n+\\n+import rasa.shared.utils.io\\n+\\n+\\n+def _recursive_serialize(\\n+    array: Any, prefix: str, data_dict: Dict[str, Any], metadata: List[Dict[str, Any]]\\n+) -> None:\\n+    \"\"\"Recursively serialize arrays and matrices for high dimensional data.\"\"\"\\n+    if isinstance(array, np.ndarray) and array.ndim <= 2:\\n+        data_key = f\"{prefix}_array\"\\n+        data_dict[data_key] = array\\n+        metadata.append({\"type\": \"dense\", \"key\": data_key, \"shape\": array.shape})\\n+\\n+    elif isinstance(array, list) and all([isinstance(v, float) for v in array]):\\n+        data_key = f\"{prefix}_list\"\\n+        data_dict[data_key] = np.array(array, dtype=np.float32)\\n+        metadata.append({\"type\": \"list\", \"key\": data_key})\\n+\\n+    elif isinstance(array, list) and all([isinstance(v, int) for v in array]):\\n+        data_key = f\"{prefix}_list\"\\n+        data_dict[data_key] = np.array(array, dtype=np.int64)\\n+        metadata.append({\"type\": \"list\", \"key\": data_key})\\n+\\n+    elif isinstance(array, scipy.sparse.spmatrix):\\n+        data_key_data = f\"{prefix}_data\"\\n+        data_key_row = f\"{prefix}_row\"\\n+        data_key_col = f\"{prefix}_col\"\\n+        array = array.tocoo()\\n+        data_dict.update(\\n+            {\\n+                data_key_data: array.data,\\n+                data_key_row: array.row,\\n+                data_key_col: array.col,\\n+            }\\n+        )\\n+        metadata.append({\"type\": \"sparse\", \"key\": prefix, \"shape\": array.shape})\\n+\\n+    elif isinstance(array, list) or isinstance(array, np.ndarray):\\n+        group_metadata = {\"type\": \"group\", \"subcomponents\": []}\\n+        for idx, item in enumerate(array):\\n+            new_prefix = f\"{prefix}_{idx}\"\\n+            _recursive_serialize(\\n+                item, new_prefix, data_dict, group_metadata[\"subcomponents\"]\\n+            )\\n+        metadata.append(group_metadata)\\n+\\n+\\n+def _serialize_nested_data(\\n+    nested_data: Dict[str, Dict[str, List[\"FeatureArray\"]]],\\n+    prefix: str,\\n+    data_dict: Dict[str, np.ndarray],\\n+    metadata: List[Dict[str, Union[str, List]]],\\n+) -> None:\\n+    \"\"\"Handle serialization across dictionary and list levels.\"\"\"\\n+    for outer_key, inner_dict in nested_data.items():\\n+        inner_metadata = {\"key\": outer_key, \"components\": []}\\n+\\n+        for inner_key, feature_arrays in inner_dict.items():\\n+            array_metadata = {\\n+                \"key\": inner_key,\\n+                \"number_of_dimensions\": feature_arrays[0].number_of_dimensions,\\n+                \"features\": [],\\n+            }\\n+\\n+            for idx, feature_array in enumerate(feature_arrays):\\n+                feature_prefix = f\"{prefix}_{outer_key}_{inner_key}_{idx}\"\\n+                _recursive_serialize(\\n+                    feature_array.tolist(),\\n+                    feature_prefix,\\n+                    data_dict,\\n+                    array_metadata[\"features\"],\\n+                )\\n+\\n+            inner_metadata[\"components\"].append(  # type:ignore[attr-defined]\\n+                array_metadata\\n+            )\\n+\\n+        metadata.append(inner_metadata)\\n+\\n+\\n+def serialize_nested_feature_arrays(\\n+    nested_feature_array: Dict[str, Dict[str, List[\"FeatureArray\"]]],\\n+    data_filename: str,\\n+    metadata_filename: str,\\n+) -> None:\\n+    data_dict: Dict[str, np.ndarray] = {}\\n+    metadata: List[Dict[str, Union[str, List]]] = []\\n+\\n+    _serialize_nested_data(nested_feature_array, \"component\", data_dict, metadata)\\n+\\n+    # Save serialized data and metadata\\n+    save_file(data_dict, data_filename)\\n+    rasa.shared.utils.io.dump_obj_as_json_to_file(metadata_filename, metadata)\\n+\\n+\\n+def _recursive_deserialize(\\n+    metadata: List[Dict[str, Any]], data: Dict[str, Any]\\n+) -> List[Any]:\\n+    \"\"\"Recursively deserialize arrays and matrices for high dimensional data.\"\"\"\\n+    result = []\\n+\\n+    for item in metadata:\\n+        if item[\"type\"] == \"dense\":\\n+            key = item[\"key\"]\\n+            array = np.asarray(data[key]).reshape(item[\"shape\"])\\n+            result.append(array)\\n+\\n+        elif item[\"type\"] == \"list\":\\n+            key = item[\"key\"]\\n+            result.append(list(data[key]))\\n+\\n+        elif item[\"type\"] == \"sparse\":\\n+            data_vals = data[f\"{item[\\'key\\']}_data\"]\\n+            row_vals = data[f\"{item[\\'key\\']}_row\"]\\n+            col_vals = data[f\"{item[\\'key\\']}_col\"]\\n+            sparse_matrix = scipy.sparse.coo_matrix(\\n+                (data_vals, (row_vals, col_vals)), shape=item[\"shape\"]\\n+            )\\n+            result.append(sparse_matrix)\\n+        elif item[\"type\"] == \"group\":\\n+            sublist = _recursive_deserialize(item[\"subcomponents\"], data)\\n+            result.append(sublist)\\n+\\n+    return result\\n+\\n+\\n+def _deserialize_nested_data(\\n+    metadata: List[Dict[str, Any]], data_dict: Dict[str, Any]\\n+) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:\\n+    \"\"\"Handle deserialization across all dictionary and list levels.\"\"\"\\n+    result: Dict[str, Dict[str, List[\"FeatureArray\"]]] = {}\\n+\\n+    for outer_item in metadata:\\n+        outer_key = outer_item[\"key\"]\\n+        result[outer_key] = {}\\n+\\n+        for inner_item in outer_item[\"components\"]:\\n+            inner_key = inner_item[\"key\"]\\n+            feature_arrays = []\\n+\\n+            # Reconstruct the list of FeatureArrays\\n+            for feature_item in inner_item[\"features\"]:\\n+                # Reconstruct the list of FeatureArrays\\n+                feature_array_data = _recursive_deserialize([feature_item], data_dict)\\n+                # Prepare the input for the FeatureArray;\\n+                # ensure it is np.ndarray compatible\\n+                input_array = np.array(feature_array_data[0], dtype=object)\\n+                feature_array = FeatureArray(\\n+                    input_array, inner_item[\"number_of_dimensions\"]\\n+                )\\n+                feature_arrays.append(feature_array)\\n+\\n+            result[outer_key][inner_key] = feature_arrays\\n+\\n+    return result\\n+\\n+\\n+def deserialize_nested_feature_arrays(\\n+    data_filename: str, metadata_filename: str\\n+) -> Dict[str, Dict[str, List[\"FeatureArray\"]]]:\\n+    metadata = rasa.shared.utils.io.read_json_file(metadata_filename)\\n+    data_dict = load_file(data_filename)\\n+\\n+    return _deserialize_nested_data(metadata, data_dict)\\n+\\n+\\n+class FeatureArray(np.ndarray):\\n+    \"\"\"Stores any kind of features ready to be used by a RasaModel.\\n+\\n+    Next to the input numpy array of features, it also received the number of\\n+    dimensions of the features.\\n+    As our features can have 1 to 4 dimensions we might have different number of numpy\\n+    arrays stacked. The number of dimensions helps us to figure out how to handle this\\n+    particular feature array. Also, it is automatically determined whether the feature\\n+    array is sparse or not and the number of units is determined as well.\\n+\\n+    Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html\\n+    \"\"\"\\n+\\n+    def __new__(\\n+        cls, input_array: np.ndarray, number_of_dimensions: int\\n+    ) -> \"FeatureArray\":\\n+        \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"\\n+        FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)\\n+\\n+        feature_array = np.asarray(input_array).view(cls)\\n+\\n+        if number_of_dimensions <= 2:\\n+            feature_array.units = input_array.shape[-1]\\n+            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n+        elif number_of_dimensions == 3:\\n+            feature_array.units = input_array[0].shape[-1]\\n+            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n+        elif number_of_dimensions == 4:\\n+            feature_array.units = input_array[0][0].shape[-1]\\n+            feature_array.is_sparse = isinstance(\\n+                input_array[0][0], scipy.sparse.spmatrix\\n+            )\\n+        else:\\n+            raise ValueError(\\n+                f\"Number of dimensions \\'{number_of_dimensions}\\' currently not \"\\n+                f\"supported.\"\\n+            )\\n+\\n+        feature_array.number_of_dimensions = number_of_dimensions\\n+\\n+        return feature_array\\n+\\n+    def __init__(\\n+        self, input_array: Any, number_of_dimensions: int, **kwargs: Any\\n+    ) -> None:\\n+        \"\"\"Initialize. FeatureArray.\\n+\\n+        Needed in order to avoid \\'Invalid keyword argument number_of_dimensions\\n+        to function FeatureArray.__init__ \\'\\n+        Args:\\n+            input_array: the array that contains features\\n+            number_of_dimensions: number of dimensions in input_array\\n+        \"\"\"\\n+        super().__init__(**kwargs)\\n+        self.number_of_dimensions = number_of_dimensions\\n+\\n+    def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:\\n+        \"\"\"This method is called when the system allocates a new array from obj.\\n+\\n+        Args:\\n+            obj: A subclass (subtype) of ndarray.\\n+        \"\"\"\\n+        if obj is None:\\n+            return\\n+\\n+        self.units = getattr(obj, \"units\", None)\\n+        self.number_of_dimensions = getattr(\\n+            obj, \"number_of_dimensions\", None\\n+        )  # type: ignore[assignment]\\n+        self.is_sparse = getattr(obj, \"is_sparse\", None)\\n+\\n+        default_attributes = {\\n+            \"units\": self.units,\\n+            \"number_of_dimensions\": self.number_of_dimensions,\\n+            \"is_spare\": self.is_sparse,\\n+        }\\n+        self.__dict__.update(default_attributes)\\n+\\n+    # pytype: disable=attribute-error\\n+    def __array_ufunc__(\\n+        self, ufunc: Any, method: str, *inputs: Any, **kwargs: Any\\n+    ) -> Any:\\n+        \"\"\"Overwrite this method as we are subclassing numpy array.\\n+\\n+        Args:\\n+            ufunc: The ufunc object that was called.\\n+            method: A string indicating which Ufunc method was called\\n+                    (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",\\n+                    \"inner\").\\n+            *inputs: A tuple of the input arguments to the ufunc.\\n+            **kwargs: Any additional arguments\\n+\\n+        Returns:\\n+            The result of the operation.\\n+        \"\"\"\\n+        f = {\\n+            \"reduce\": ufunc.reduce,\\n+            \"accumulate\": ufunc.accumulate,\\n+            \"reduceat\": ufunc.reduceat,\\n+            \"outer\": ufunc.outer,\\n+            \"at\": ufunc.at,\\n+            \"__call__\": ufunc,\\n+        }\\n+        # convert the inputs to np.ndarray to prevent recursion, call the function,\\n+        # then cast it back as FeatureArray\\n+        output = FeatureArray(\\n+            f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),\\n+            number_of_dimensions=kwargs[\"number_of_dimensions\"],\\n+        )\\n+        output.__dict__ = self.__dict__  # carry forward attributes\\n+        return output\\n+\\n+    def __reduce__(self) -> Tuple[Any, Any, Any]:\\n+        \"\"\"Needed in order to pickle this object.\\n+\\n+        Returns:\\n+            A tuple.\\n+        \"\"\"\\n+        pickled_state = super(FeatureArray, self).__reduce__()\\n+        if isinstance(pickled_state, str):\\n+            raise TypeError(\"np array __reduce__ returned string instead of tuple.\")\\n+        new_state = pickled_state[2] + (\\n+            self.number_of_dimensions,\\n+            self.is_sparse,\\n+            self.units,\\n+        )\\n+        return pickled_state[0], pickled_state[1], new_state\\n+\\n+    def __setstate__(self, state: Any, **kwargs: Any) -> None:\\n+        \"\"\"Sets the state.\\n+\\n+        Args:\\n+            state: The state argument must be a sequence that contains the following\\n+                   elements version, shape, dtype, isFortan, rawdata.\\n+            **kwargs: Any additional parameter\\n+        \"\"\"\\n+        # Needed in order to load the object\\n+        self.number_of_dimensions = state[-3]\\n+        self.is_sparse = state[-2]\\n+        self.units = state[-1]\\n+        super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)\\n+\\n+    # pytype: enable=attribute-error\\n+\\n+    @staticmethod\\n+    def _validate_number_of_dimensions(\\n+        number_of_dimensions: int, input_array: np.ndarray\\n+    ) -> None:\\n+        \"\"\"Validates if the input array has given number of dimensions.\\n+\\n+        Args:\\n+            number_of_dimensions: number of dimensions\\n+            input_array: input array\\n+\\n+        Raises: ValueError in case the dimensions do not match\\n+        \"\"\"\\n+        # when loading the feature arrays from disk, the shape represents\\n+        # the correct number of dimensions\\n+        if len(input_array.shape) == number_of_dimensions:\\n+            return\\n+\\n+        _sub_array = input_array\\n+        dim = 0\\n+        # Go number_of_dimensions into the given input_array\\n+        for i in range(1, number_of_dimensions + 1):\\n+            _sub_array = _sub_array[0]\\n+            if isinstance(_sub_array, scipy.sparse.spmatrix):\\n+                dim = i\\n+                break\\n+            if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n+                # sequence dimension is 0, we are dealing with \"fake\" features\\n+                dim = i\\n+                break\\n+\\n+        # If the resulting sub_array is sparse, the remaining number of dimensions\\n+        # should be at least 2\\n+        if isinstance(_sub_array, scipy.sparse.spmatrix):\\n+            if dim > 2:\\n+                raise ValueError(\\n+                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n+                    f\"match dimensions of given input array: {input_array}.\"\\n+                )\\n+        elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n+            # sequence dimension is 0, we are dealing with \"fake\" features,\\n+            # but they should be of dim 2\\n+            if dim > 2:\\n+                raise ValueError(\\n+                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n+                    f\"match dimensions of given input array: {input_array}.\"\\n+                )\\n+        # If the resulting sub_array is dense, the sub_array should be a single number\\n+        elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(\\n+            _sub_array, (np.float32, np.float64)\\n+        ):\\n+            raise ValueError(\\n+                f\"Given number of dimensions \\'{number_of_dimensions}\\' does not match \"\\n+                f\"dimensions of given input array: {input_array}.\"\\n+            )', '@@ -12,6 +12,7 @@\\n import warnings\\n import random\\n import string\\n+\\n import portalocker\\n \\n from ruamel import yaml as yaml', '@@ -7,4 +7,3 @@ if [[ ${GITHUB_TAG} =~ ^[0-9]+\\\\.[0-9]+\\\\.[0-9]+$ ]]; then\\n \\t --data \"{\\\\\"text\\\\\":\\\\\"ğŸ’¥ New *Rasa Open Source* version ${GITHUB_TAG} has been released! https://github.com/RasaHQ/rasa/releases/tag/${GITHUB_TAG}\\\\\"}\" \\\\\\n \\t \"https://hooks.slack.com/services/T0GHWFTS8/BMTQQL47K/${SLACK_WEBHOOK_TOKEN}\"\\n fi\\n-', '@@ -1,9 +1,7 @@\\n from __future__ import annotations\\n+\\n import logging\\n from collections import OrderedDict\\n-\\n-import scipy.sparse\\n-import numpy as np\\n from typing import (\\n     Any,\\n     Dict,\\n@@ -17,30 +15,34 @@\\n     Union,\\n )\\n \\n+import numpy as np\\n+import scipy.sparse\\n+\\n+import rasa.shared.utils.io\\n+import rasa.utils.io\\n from rasa.engine.graph import ExecutionContext, GraphComponent\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n+from rasa.nlu.constants import TOKENS_NAMES\\n+from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY, SpacyTokenizer\\n from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer\\n-from rasa.nlu.featurizers.sparse_featurizer.sparse_featurizer import SparseFeaturizer\\n-from rasa.nlu.constants import TOKENS_NAMES\\n from rasa.shared.constants import DOCS_URL_COMPONENTS\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n-from rasa.shared.nlu.constants import TEXT\\n from rasa.shared.exceptions import InvalidConfigException\\n-import rasa.shared.utils.io\\n-import rasa.utils.io\\n+from rasa.shared.nlu.constants import TEXT\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n \\n logger = logging.getLogger(__name__)\\n \\n-\\n END_OF_SENTENCE = \"EOS\"\\n BEGIN_OF_SENTENCE = \"BOS\"\\n \\n FEATURES = \"features\"\\n \\n+SEPERATOR = \"###\"\\n+\\n \\n @DefaultV1Recipe.register(\\n     DefaultV1Recipe.ComponentType.MESSAGE_FEATURIZER, is_trainable=True\\n@@ -72,7 +74,7 @@ class LexicalSyntacticFeaturizer(SparseFeaturizer, GraphComponent):\\n       of the token at position `t+1`.\\n     \"\"\"\\n \\n-    FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.pkl\"\\n+    FILENAME_FEATURE_TO_IDX_DICT = \"feature_to_idx_dict.json\"\\n \\n     # NOTE: \"suffix5\" of the token \"is\" will be \"is\". Hence, when combining multiple\\n     # prefixes, short words will be represented/encoded repeatedly.\\n@@ -489,6 +491,32 @@ def create(\\n         \"\"\"Creates a new untrained component (see parent class for full docstring).\"\"\"\\n         return cls(config, model_storage, resource, execution_context)\\n \\n+    @staticmethod\\n+    def _restructure_feature_to_idx_dict(\\n+        loaded_data: Dict[str, Dict[str, int]],\\n+    ) -> Dict[Tuple[int, str], Dict[str, int]]:\\n+        \"\"\"Reconstructs the feature to idx dict.\\n+\\n+        When storing the feature_to_idx_dict to disk, we need to convert the tuple (key)\\n+        into a string to be able to store it via json. When loading the data\\n+        we need to reconstruct the tuple from the stored string.\\n+\\n+        Args:\\n+            loaded_data: The loaded feature to idx dict from file.\\n+\\n+        Returns:\\n+            The reconstructed feature_to_idx_dict\\n+        \"\"\"\\n+        feature_to_idx_dict = {}\\n+        for tuple_string, feature_value in loaded_data.items():\\n+            # Example of tuple_string: \"1###low\"\\n+            index, feature_name = tuple_string.split(SEPERATOR)\\n+\\n+            feature_key = (int(index), feature_name)\\n+            feature_to_idx_dict[feature_key] = feature_value\\n+\\n+        return feature_to_idx_dict\\n+\\n     @classmethod\\n     def load(\\n         cls,\\n@@ -501,10 +529,13 @@ def load(\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n         try:\\n             with model_storage.read_from(resource) as model_path:\\n-                feature_to_idx_dict = rasa.utils.io.json_unpickle(\\n+                loaded_data = rasa.shared.utils.io.read_json_file(\\n                     model_path / cls.FILENAME_FEATURE_TO_IDX_DICT,\\n-                    encode_non_string_keys=True,\\n                 )\\n+\\n+                # convert the key back into tuple\\n+                feature_to_idx_dict = cls._restructure_feature_to_idx_dict(loaded_data)\\n+\\n                 return cls(\\n                     config=config,\\n                     model_storage=model_storage,\\n@@ -529,9 +560,13 @@ def persist(self) -> None:\\n         if not self._feature_to_idx_dict:\\n             return None\\n \\n+        # as we cannot dump tuples, convert the tuple into a string\\n+        restructured_feature_dict = {\\n+            f\"{k[0]}{SEPERATOR}{k[1]}\": v for k, v in self._feature_to_idx_dict.items()\\n+        }\\n+\\n         with self._model_storage.write_to(self._resource) as model_path:\\n-            rasa.utils.io.json_pickle(\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n                 model_path / self.FILENAME_FEATURE_TO_IDX_DICT,\\n-                self._feature_to_idx_dict,\\n-                encode_non_string_keys=True,\\n+                restructured_feature_dict,\\n             )', '@@ -8,6 +8,7 @@\\n import tempfile\\n import warnings\\n from pathlib import Path\\n+from socket import SOCK_DGRAM, SOCK_STREAM\\n from types import TracebackType\\n from typing import (\\n     Any,\\n@@ -24,8 +25,9 @@\\n     Tuple,\\n )\\n \\n-from socket import SOCK_DGRAM, SOCK_STREAM\\n import numpy as np\\n+\\n+import rasa.shared.utils.io\\n import rasa.utils.io\\n from rasa.constants import (\\n     DEFAULT_LOG_LEVEL_LIBRARIES,\\n@@ -36,7 +38,6 @@\\n )\\n from rasa.shared.constants import DEFAULT_LOG_LEVEL, ENV_LOG_LEVEL, TCP_PROTOCOL\\n from rasa.shared.exceptions import RasaException\\n-import rasa.shared.utils.io\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -153,7 +154,7 @@ def configure_logging_from_file(logging_config_file: Text) -> None:\\n     try:\\n         logging.config.dictConfig(logging_config_dict)\\n     except (ValueError, TypeError, AttributeError, ImportError) as e:\\n-        logging.debug(\\n+        logger.debug(\\n             f\"The logging config file {logging_config_file} could not \"\\n             f\"be applied because it failed validation against \"\\n             f\"the built-in Python logging schema. \"', '@@ -1,20 +1,19 @@\\n import logging\\n from typing import Any, Text, Dict, List, Type, Tuple\\n \\n-import joblib\\n from scipy.sparse import hstack, vstack, csr_matrix\\n from sklearn.linear_model import LogisticRegression\\n \\n+from rasa.engine.graph import ExecutionContext, GraphComponent\\n+from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n-from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n-from rasa.engine.graph import ExecutionContext, GraphComponent\\n from rasa.nlu.classifiers import LABEL_RANKING_LENGTH\\n-from rasa.nlu.featurizers.featurizer import Featurizer\\n from rasa.nlu.classifiers.classifier import IntentClassifier\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n+from rasa.nlu.featurizers.featurizer import Featurizer\\n from rasa.shared.nlu.constants import TEXT, INTENT\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.utils.tensorflow.constants import RANKING_LENGTH\\n \\n logger = logging.getLogger(__name__)\\n@@ -158,9 +157,11 @@ def process(self, messages: List[Message]) -> List[Message]:\\n \\n     def persist(self) -> None:\\n         \"\"\"Persist this model into the passed directory.\"\"\"\\n+        import skops.io as sio\\n+\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n-            path = model_dir / f\"{self._resource.name}.joblib\"\\n-            joblib.dump(self.clf, path)\\n+            path = model_dir / f\"{self._resource.name}.skops\"\\n+            sio.dump(self.clf, path)\\n             logger.debug(f\"Saved intent classifier to \\'{path}\\'.\")\\n \\n     @classmethod\\n@@ -173,9 +174,21 @@ def load(\\n         **kwargs: Any,\\n     ) -> \"LogisticRegressionClassifier\":\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n+        import skops.io as sio\\n+\\n         try:\\n             with model_storage.read_from(resource) as model_dir:\\n-                classifier = joblib.load(model_dir / f\"{resource.name}.joblib\")\\n+                classifier_file = model_dir / f\"{resource.name}.skops\"\\n+                unknown_types = sio.get_untrusted_types(file=classifier_file)\\n+\\n+                if unknown_types:\\n+                    logger.debug(\\n+                        f\"Untrusted types ({unknown_types}) found when \"\\n+                        f\"loading {classifier_file}!\",\\n+                    )\\n+                    raise ValueError()\\n+\\n+                classifier = sio.load(classifier_file, trusted=unknown_types)\\n                 component = cls(\\n                     config, execution_context.node_name, model_storage, resource\\n                 )', '@@ -20,6 +20,8 @@\\n import scipy.sparse\\n from sklearn.model_selection import train_test_split\\n \\n+from rasa.utils.tensorflow.feature_array import FeatureArray\\n+\\n logger = logging.getLogger(__name__)\\n \\n \\n@@ -37,199 +39,6 @@ def ragged_array_to_ndarray(ragged_array: Iterable[np.ndarray]) -> np.ndarray:\\n         return np.array(ragged_array, dtype=object)\\n \\n \\n-class FeatureArray(np.ndarray):\\n-    \"\"\"Stores any kind of features ready to be used by a RasaModel.\\n-\\n-    Next to the input numpy array of features, it also received the number of\\n-    dimensions of the features.\\n-    As our features can have 1 to 4 dimensions we might have different number of numpy\\n-    arrays stacked. The number of dimensions helps us to figure out how to handle this\\n-    particular feature array. Also, it is automatically determined whether the feature\\n-    array is sparse or not and the number of units is determined as well.\\n-\\n-    Subclassing np.array: https://numpy.org/doc/stable/user/basics.subclassing.html\\n-    \"\"\"\\n-\\n-    def __new__(\\n-        cls, input_array: np.ndarray, number_of_dimensions: int\\n-    ) -> \"FeatureArray\":\\n-        \"\"\"Create and return a new object.  See help(type) for accurate signature.\"\"\"\\n-        FeatureArray._validate_number_of_dimensions(number_of_dimensions, input_array)\\n-\\n-        feature_array = np.asarray(input_array).view(cls)\\n-\\n-        if number_of_dimensions <= 2:\\n-            feature_array.units = input_array.shape[-1]\\n-            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n-        elif number_of_dimensions == 3:\\n-            feature_array.units = input_array[0].shape[-1]\\n-            feature_array.is_sparse = isinstance(input_array[0], scipy.sparse.spmatrix)\\n-        elif number_of_dimensions == 4:\\n-            feature_array.units = input_array[0][0].shape[-1]\\n-            feature_array.is_sparse = isinstance(\\n-                input_array[0][0], scipy.sparse.spmatrix\\n-            )\\n-        else:\\n-            raise ValueError(\\n-                f\"Number of dimensions \\'{number_of_dimensions}\\' currently not \"\\n-                f\"supported.\"\\n-            )\\n-\\n-        feature_array.number_of_dimensions = number_of_dimensions\\n-\\n-        return feature_array\\n-\\n-    def __init__(\\n-        self, input_array: Any, number_of_dimensions: int, **kwargs: Any\\n-    ) -> None:\\n-        \"\"\"Initialize. FeatureArray.\\n-\\n-        Needed in order to avoid \\'Invalid keyword argument number_of_dimensions\\n-        to function FeatureArray.__init__ \\'\\n-        Args:\\n-            input_array: the array that contains features\\n-            number_of_dimensions: number of dimensions in input_array\\n-        \"\"\"\\n-        super().__init__(**kwargs)\\n-        self.number_of_dimensions = number_of_dimensions\\n-\\n-    def __array_finalize__(self, obj: Optional[np.ndarray]) -> None:\\n-        \"\"\"This method is called when the system allocates a new array from obj.\\n-\\n-        Args:\\n-            obj: A subclass (subtype) of ndarray.\\n-        \"\"\"\\n-        if obj is None:\\n-            return\\n-\\n-        self.units = getattr(obj, \"units\", None)\\n-        self.number_of_dimensions = getattr(obj, \"number_of_dimensions\", None)  # type: ignore[assignment] # noqa:E501\\n-        self.is_sparse = getattr(obj, \"is_sparse\", None)\\n-\\n-        default_attributes = {\\n-            \"units\": self.units,\\n-            \"number_of_dimensions\": self.number_of_dimensions,\\n-            \"is_spare\": self.is_sparse,\\n-        }\\n-        self.__dict__.update(default_attributes)\\n-\\n-    # pytype: disable=attribute-error\\n-    def __array_ufunc__(\\n-        self, ufunc: Any, method: Text, *inputs: Any, **kwargs: Any\\n-    ) -> Any:\\n-        \"\"\"Overwrite this method as we are subclassing numpy array.\\n-\\n-        Args:\\n-            ufunc: The ufunc object that was called.\\n-            method: A string indicating which Ufunc method was called\\n-                    (one of \"__call__\", \"reduce\", \"reduceat\", \"accumulate\", \"outer\",\\n-                    \"inner\").\\n-            *inputs: A tuple of the input arguments to the ufunc.\\n-            **kwargs: Any additional arguments\\n-\\n-        Returns:\\n-            The result of the operation.\\n-        \"\"\"\\n-        f = {\\n-            \"reduce\": ufunc.reduce,\\n-            \"accumulate\": ufunc.accumulate,\\n-            \"reduceat\": ufunc.reduceat,\\n-            \"outer\": ufunc.outer,\\n-            \"at\": ufunc.at,\\n-            \"__call__\": ufunc,\\n-        }\\n-        # convert the inputs to np.ndarray to prevent recursion, call the function,\\n-        # then cast it back as FeatureArray\\n-        output = FeatureArray(\\n-            f[method](*(i.view(np.ndarray) for i in inputs), **kwargs),\\n-            number_of_dimensions=kwargs[\"number_of_dimensions\"],\\n-        )\\n-        output.__dict__ = self.__dict__  # carry forward attributes\\n-        return output\\n-\\n-    def __reduce__(self) -> Tuple[Any, Any, Any]:\\n-        \"\"\"Needed in order to pickle this object.\\n-\\n-        Returns:\\n-            A tuple.\\n-        \"\"\"\\n-        pickled_state = super(FeatureArray, self).__reduce__()\\n-        if isinstance(pickled_state, str):\\n-            raise TypeError(\"np array __reduce__ returned string instead of tuple.\")\\n-        new_state = pickled_state[2] + (\\n-            self.number_of_dimensions,\\n-            self.is_sparse,\\n-            self.units,\\n-        )\\n-        return pickled_state[0], pickled_state[1], new_state\\n-\\n-    def __setstate__(self, state: Any, **kwargs: Any) -> None:\\n-        \"\"\"Sets the state.\\n-\\n-        Args:\\n-            state: The state argument must be a sequence that contains the following\\n-                   elements version, shape, dtype, isFortan, rawdata.\\n-            **kwargs: Any additional parameter\\n-        \"\"\"\\n-        # Needed in order to load the object\\n-        self.number_of_dimensions = state[-3]\\n-        self.is_sparse = state[-2]\\n-        self.units = state[-1]\\n-        super(FeatureArray, self).__setstate__(state[0:-3], **kwargs)\\n-\\n-    # pytype: enable=attribute-error\\n-\\n-    @staticmethod\\n-    def _validate_number_of_dimensions(\\n-        number_of_dimensions: int, input_array: np.ndarray\\n-    ) -> None:\\n-        \"\"\"Validates if the the input array has given number of dimensions.\\n-\\n-        Args:\\n-            number_of_dimensions: number of dimensions\\n-            input_array: input array\\n-\\n-        Raises: ValueError in case the dimensions do not match\\n-        \"\"\"\\n-        _sub_array = input_array\\n-        dim = 0\\n-        # Go number_of_dimensions into the given input_array\\n-        for i in range(1, number_of_dimensions + 1):\\n-            _sub_array = _sub_array[0]\\n-            if isinstance(_sub_array, scipy.sparse.spmatrix):\\n-                dim = i\\n-                break\\n-            if isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n-                # sequence dimension is 0, we are dealing with \"fake\" features\\n-                dim = i\\n-                break\\n-\\n-        # If the resulting sub_array is sparse, the remaining number of dimensions\\n-        # should be at least 2\\n-        if isinstance(_sub_array, scipy.sparse.spmatrix):\\n-            if dim > 2:\\n-                raise ValueError(\\n-                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n-                    f\"match dimensions of given input array: {input_array}.\"\\n-                )\\n-        elif isinstance(_sub_array, np.ndarray) and _sub_array.shape[0] == 0:\\n-            # sequence dimension is 0, we are dealing with \"fake\" features,\\n-            # but they should be of dim 2\\n-            if dim > 2:\\n-                raise ValueError(\\n-                    f\"Given number of dimensions \\'{number_of_dimensions}\\' does not \"\\n-                    f\"match dimensions of given input array: {input_array}.\"\\n-                )\\n-        # If the resulting sub_array is dense, the sub_array should be a single number\\n-        elif not np.issubdtype(type(_sub_array), np.integer) and not isinstance(\\n-            _sub_array, (np.float32, np.float64)\\n-        ):\\n-            raise ValueError(\\n-                f\"Given number of dimensions \\'{number_of_dimensions}\\' does not match \"\\n-                f\"dimensions of given input array: {input_array}.\"\\n-            )\\n-\\n-\\n class FeatureSignature(NamedTuple):\\n     \"\"\"Signature of feature arrays.\\n \\n@@ -270,8 +79,7 @@ def __init__(\\n         label_sub_key: Optional[Text] = None,\\n         data: Optional[Data] = None,\\n     ) -> None:\\n-        \"\"\"\\n-        Initializes the RasaModelData object.\\n+        \"\"\"Initializes the RasaModelData object.\\n \\n         Args:\\n             label_key: the key of a label used for balancing, etc.', '@@ -1,11 +1,9 @@\\n from __future__ import annotations\\n-from pathlib import Path\\n-from collections import defaultdict\\n-from abc import abstractmethod\\n-import jsonpickle\\n-import logging\\n \\n-from tqdm import tqdm\\n+import logging\\n+from abc import abstractmethod\\n+from collections import defaultdict\\n+from pathlib import Path\\n from typing import (\\n     Tuple,\\n     List,\\n@@ -18,25 +16,30 @@\\n     Set,\\n     DefaultDict,\\n     cast,\\n+    Type,\\n+    Callable,\\n+    ClassVar,\\n )\\n+\\n import numpy as np\\n+from tqdm import tqdm\\n \\n-from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer\\n-from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\\n-from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError\\n import rasa.shared.core.trackers\\n import rasa.shared.utils.io\\n-from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME\\n-from rasa.shared.nlu.training_data.features import Features\\n-from rasa.shared.core.trackers import DialogueStateTracker\\n-from rasa.shared.core.domain import State, Domain\\n-from rasa.shared.core.events import Event, ActionExecuted, UserUttered\\n+from rasa.core.exceptions import InvalidTrackerFeaturizerUsageError\\n+from rasa.core.featurizers.precomputation import MessageContainerForCoreFeaturization\\n+from rasa.core.featurizers.single_state_featurizer import SingleStateFeaturizer\\n from rasa.shared.core.constants import (\\n     USER,\\n     ACTION_UNLIKELY_INTENT_NAME,\\n     PREVIOUS_ACTION,\\n )\\n+from rasa.shared.core.domain import State, Domain\\n+from rasa.shared.core.events import Event, ActionExecuted, UserUttered\\n+from rasa.shared.core.trackers import DialogueStateTracker\\n from rasa.shared.exceptions import RasaException\\n+from rasa.shared.nlu.constants import TEXT, INTENT, ENTITIES, ACTION_NAME\\n+from rasa.shared.nlu.training_data.features import Features\\n from rasa.utils.tensorflow.constants import LABEL_PAD_ID\\n from rasa.utils.tensorflow.model_data import ragged_array_to_ndarray\\n \\n@@ -64,6 +67,10 @@ def __str__(self) -> Text:\\n class TrackerFeaturizer:\\n     \"\"\"Base class for actual tracker featurizers.\"\"\"\\n \\n+    # Class registry to store all subclasses\\n+    _registry: ClassVar[Dict[str, Type[\"TrackerFeaturizer\"]]] = {}\\n+    _featurizer_type: str = \"TrackerFeaturizer\"\\n+\\n     def __init__(\\n         self, state_featurizer: Optional[SingleStateFeaturizer] = None\\n     ) -> None:\\n@@ -74,6 +81,36 @@ def __init__(\\n         \"\"\"\\n         self.state_featurizer = state_featurizer\\n \\n+    @classmethod\\n+    def register(cls, featurizer_type: str) -> Callable:\\n+        \"\"\"Decorator to register featurizer subclasses.\"\"\"\\n+\\n+        def wrapper(subclass: Type[\"TrackerFeaturizer\"]) -> Type[\"TrackerFeaturizer\"]:\\n+            cls._registry[featurizer_type] = subclass\\n+            # Store the type identifier in the class for serialization\\n+            subclass._featurizer_type = featurizer_type\\n+            return subclass\\n+\\n+        return wrapper\\n+\\n+    @classmethod\\n+    def from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":\\n+        \"\"\"Create featurizer instance from dictionary.\"\"\"\\n+        featurizer_type = data.pop(\"type\")\\n+\\n+        if featurizer_type not in cls._registry:\\n+            raise ValueError(f\"Unknown featurizer type: {featurizer_type}\")\\n+\\n+        # Get the correct subclass and instantiate it\\n+        subclass = cls._registry[featurizer_type]\\n+        return subclass.create_from_dict(data)\\n+\\n+    @classmethod\\n+    @abstractmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"TrackerFeaturizer\":\\n+        \"\"\"Each subclass must implement its own creation from dict method.\"\"\"\\n+        pass\\n+\\n     @staticmethod\\n     def _create_states(\\n         tracker: DialogueStateTracker,\\n@@ -465,9 +502,7 @@ def persist(self, path: Union[Text, Path]) -> None:\\n             self.state_featurizer.entity_tag_specs = []\\n \\n         # noinspection PyTypeChecker\\n-        rasa.shared.utils.io.write_text_file(\\n-            str(jsonpickle.encode(self)), featurizer_file\\n-        )\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(featurizer_file, self.to_dict())\\n \\n     @staticmethod\\n     def load(path: Union[Text, Path]) -> Optional[TrackerFeaturizer]:\\n@@ -481,7 +516,17 @@ def load(path: Union[Text, Path]) -> Optional[TrackerFeaturizer]:\\n         \"\"\"\\n         featurizer_file = Path(path) / FEATURIZER_FILE\\n         if featurizer_file.is_file():\\n-            return jsonpickle.decode(rasa.shared.utils.io.read_file(featurizer_file))\\n+            data = rasa.shared.utils.io.read_json_file(featurizer_file)\\n+\\n+            if \"type\" not in data:\\n+                logger.error(\\n+                    f\"Couldn\\'t load featurizer for policy. \"\\n+                    f\"File \\'{featurizer_file}\\' does not contain all \"\\n+                    f\"necessary information. \\'type\\' is missing.\"\\n+                )\\n+                return None\\n+\\n+            return TrackerFeaturizer.from_dict(data)\\n \\n         logger.error(\\n             f\"Couldn\\'t load featurizer for policy. \"\\n@@ -508,7 +553,16 @@ def _remove_action_unlikely_intent_from_events(events: List[Event]) -> List[Even\\n             )\\n         ]\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return {\\n+            \"type\": self.__class__._featurizer_type,\\n+            \"state_featurizer\": (\\n+                self.state_featurizer.to_dict() if self.state_featurizer else None\\n+            ),\\n+        }\\n+\\n \\n+@TrackerFeaturizer.register(\"FullDialogueTrackerFeaturizer\")\\n class FullDialogueTrackerFeaturizer(TrackerFeaturizer):\\n     \"\"\"Creates full dialogue training data for time distributed architectures.\\n \\n@@ -646,7 +700,20 @@ def prediction_states(\\n \\n         return trackers_as_states\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return super().to_dict()\\n \\n+    @classmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"FullDialogueTrackerFeaturizer\":\\n+        state_featurizer = SingleStateFeaturizer.create_from_dict(\\n+            data[\"state_featurizer\"]\\n+        )\\n+        return cls(\\n+            state_featurizer,\\n+        )\\n+\\n+\\n+@TrackerFeaturizer.register(\"MaxHistoryTrackerFeaturizer\")\\n class MaxHistoryTrackerFeaturizer(TrackerFeaturizer):\\n     \"\"\"Truncates the tracker history into `max_history` long sequences.\\n \\n@@ -887,7 +954,25 @@ def prediction_states(\\n \\n         return trackers_as_states\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        data = super().to_dict()\\n+        data.update(\\n+            {\\n+                \"remove_duplicates\": self.remove_duplicates,\\n+                \"max_history\": self.max_history,\\n+            }\\n+        )\\n+        return data\\n+\\n+    @classmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"MaxHistoryTrackerFeaturizer\":\\n+        state_featurizer = SingleStateFeaturizer.create_from_dict(\\n+            data[\"state_featurizer\"]\\n+        )\\n+        return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])\\n \\n+\\n+@TrackerFeaturizer.register(\"IntentMaxHistoryTrackerFeaturizer\")\\n class IntentMaxHistoryTrackerFeaturizer(MaxHistoryTrackerFeaturizer):\\n     \"\"\"Truncates the tracker history into `max_history` long sequences.\\n \\n@@ -1166,6 +1251,18 @@ def prediction_states(\\n \\n         return trackers_as_states\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return super().to_dict()\\n+\\n+    @classmethod\\n+    def create_from_dict(\\n+        cls, data: Dict[str, Any]\\n+    ) -> \"IntentMaxHistoryTrackerFeaturizer\":\\n+        state_featurizer = SingleStateFeaturizer.create_from_dict(\\n+            data[\"state_featurizer\"]\\n+        )\\n+        return cls(state_featurizer, data[\"max_history\"], data[\"remove_duplicates\"])\\n+\\n \\n def _is_prev_action_unlikely_intent_in_state(state: State) -> bool:\\n     prev_action_name = state.get(PREVIOUS_ACTION, {}).get(ACTION_NAME)', '@@ -1,12 +1,12 @@\\n from __future__ import annotations\\n \\n-from collections import OrderedDict\\n-from enum import Enum\\n import logging\\n import typing\\n+from collections import OrderedDict\\n+from enum import Enum\\n+from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type\\n \\n import numpy as np\\n-from typing import Any, Dict, List, Optional, Text, Tuple, Callable, Type\\n \\n import rasa.nlu.utils.bilou_utils as bilou_utils\\n import rasa.shared.utils.io\\n@@ -15,13 +15,12 @@\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n+from rasa.nlu.constants import TOKENS_NAMES\\n+from rasa.nlu.extractors.extractor import EntityExtractorMixin\\n from rasa.nlu.test import determine_token_labels\\n from rasa.nlu.tokenizers.spacy_tokenizer import POS_TAG_KEY\\n-from rasa.nlu.extractors.extractor import EntityExtractorMixin\\n from rasa.nlu.tokenizers.tokenizer import Token, Tokenizer\\n-from rasa.shared.nlu.training_data.training_data import TrainingData\\n-from rasa.shared.nlu.training_data.message import Message\\n-from rasa.nlu.constants import TOKENS_NAMES\\n+from rasa.shared.constants import DOCS_URL_COMPONENTS\\n from rasa.shared.nlu.constants import (\\n     TEXT,\\n     ENTITIES,\\n@@ -32,7 +31,8 @@\\n     SPLIT_ENTITIES_BY_COMMA,\\n     SPLIT_ENTITIES_BY_COMMA_DEFAULT_VALUE,\\n )\\n-from rasa.shared.constants import DOCS_URL_COMPONENTS\\n+from rasa.shared.nlu.training_data.message import Message\\n+from rasa.shared.nlu.training_data.training_data import TrainingData\\n from rasa.utils.tensorflow.constants import BILOU_FLAG, FEATURIZERS\\n \\n logger = logging.getLogger(__name__)\\n@@ -41,6 +41,9 @@\\n     from sklearn_crfsuite import CRF\\n \\n \\n+CONFIG_FEATURES = \"features\"\\n+\\n+\\n class CRFToken:\\n     def __init__(\\n         self,\\n@@ -60,6 +63,29 @@ def __init__(\\n         self.entity_role_tag = entity_role_tag\\n         self.entity_group_tag = entity_group_tag\\n \\n+    def to_dict(self) -> Dict[str, Any]:\\n+        return {\\n+            \"text\": self.text,\\n+            \"pos_tag\": self.pos_tag,\\n+            \"pattern\": self.pattern,\\n+            \"dense_features\": [str(x) for x in list(self.dense_features)],\\n+            \"entity_tag\": self.entity_tag,\\n+            \"entity_role_tag\": self.entity_role_tag,\\n+            \"entity_group_tag\": self.entity_group_tag,\\n+        }\\n+\\n+    @classmethod\\n+    def create_from_dict(cls, data: Dict[str, Any]) -> \"CRFToken\":\\n+        return cls(\\n+            data[\"text\"],\\n+            data[\"pos_tag\"],\\n+            data[\"pattern\"],\\n+            np.array([float(x) for x in data[\"dense_features\"]]),\\n+            data[\"entity_tag\"],\\n+            data[\"entity_role_tag\"],\\n+            data[\"entity_group_tag\"],\\n+        )\\n+\\n \\n class CRFEntityExtractorOptions(str, Enum):\\n     \"\"\"Features that can be used for the \\'CRFEntityExtractor\\'.\"\"\"\\n@@ -137,7 +163,7 @@ def get_default_config() -> Dict[Text, Any]:\\n             # \"is the preceding token in title case?\"\\n             # POS features require SpacyTokenizer\\n             # pattern feature require RegexFeaturizer\\n-            CRFEntityExtractor.CONFIG_FEATURES: [\\n+            CONFIG_FEATURES: [\\n                 [\\n                     CRFEntityExtractorOptions.LOW,\\n                     CRFEntityExtractorOptions.TITLE,\\n@@ -200,7 +226,7 @@ def __init__(\\n         )\\n \\n     def _validate_configuration(self) -> None:\\n-        if len(self.component_config.get(self.CONFIG_FEATURES, [])) % 2 != 1:\\n+        if len(self.component_config.get(CONFIG_FEATURES, [])) % 2 != 1:\\n             raise ValueError(\\n                 \"Need an odd number of crf feature lists to have a center word.\"\\n             )\\n@@ -251,9 +277,11 @@ def train(self, training_data: TrainingData) -> Resource:\\n         ]\\n         dataset = [self._convert_to_crf_tokens(example) for example in entity_examples]\\n \\n-        self._train_model(dataset)\\n+        self.entity_taggers = self.train_model(\\n+            dataset, self.component_config, self.crf_order\\n+        )\\n \\n-        self.persist()\\n+        self.persist(dataset)\\n \\n         return self._resource\\n \\n@@ -299,7 +327,9 @@ def extract_entities(self, message: Message) -> List[Dict[Text, Any]]:\\n             if include_tag_features:\\n                 self._add_tag_to_crf_token(crf_tokens, predictions)\\n \\n-            features = self._crf_tokens_to_features(crf_tokens, include_tag_features)\\n+            features = self._crf_tokens_to_features(\\n+                crf_tokens, self.component_config, include_tag_features\\n+            )\\n             predictions[tag_name] = entity_tagger.predict_marginals_single(features)\\n \\n         # convert predictions into a list of tags and a list of confidences\\n@@ -389,51 +419,55 @@ def load(\\n         **kwargs: Any,\\n     ) -> CRFEntityExtractor:\\n         \"\"\"Loads trained component (see parent class for full docstring).\"\"\"\\n-        import joblib\\n-\\n         try:\\n-            entity_taggers = OrderedDict()\\n             with model_storage.read_from(resource) as model_dir:\\n-                # We have to load in the same order as we persisted things as otherwise\\n-                # the predictions might be off\\n-                file_names = sorted(model_dir.glob(\"**/*.pkl\"))\\n-                if not file_names:\\n-                    logger.debug(\\n-                        \"Failed to load model for \\'CRFEntityExtractor\\'. \"\\n-                        \"Maybe you did not provide enough training data and \"\\n-                        \"no model was trained.\"\\n-                    )\\n-                    return cls(config, model_storage, resource)\\n+                dataset = rasa.shared.utils.io.read_json_file(\\n+                    model_dir / \"crf_dataset.json\"\\n+                )\\n+                crf_order = rasa.shared.utils.io.read_json_file(\\n+                    model_dir / \"crf_order.json\"\\n+                )\\n+\\n+                dataset = [\\n+                    [CRFToken.create_from_dict(token_data) for token_data in sub_list]\\n+                    for sub_list in dataset\\n+                ]\\n \\n-                for file_name in file_names:\\n-                    name = file_name.stem[1:]\\n-                    entity_taggers[name] = joblib.load(file_name)\\n+                entity_taggers = cls.train_model(dataset, config, crf_order)\\n \\n-                return cls(config, model_storage, resource, entity_taggers)\\n+                entity_extractor = cls(config, model_storage, resource, entity_taggers)\\n+                entity_extractor.crf_order = crf_order\\n+                return entity_extractor\\n         except ValueError:\\n             logger.warning(\\n                 f\"Failed to load {cls.__name__} from model storage. Resource \"\\n                 f\"\\'{resource.name}\\' doesn\\'t exist.\"\\n             )\\n             return cls(config, model_storage, resource)\\n \\n-    def persist(self) -> None:\\n+    def persist(self, dataset: List[List[CRFToken]]) -> None:\\n         \"\"\"Persist this model into the passed directory.\"\"\"\\n-        import joblib\\n-\\n         with self._model_storage.write_to(self._resource) as model_dir:\\n-            if self.entity_taggers:\\n-                for idx, (name, entity_tagger) in enumerate(\\n-                    self.entity_taggers.items()\\n-                ):\\n-                    model_file_name = model_dir / f\"{idx}{name}.pkl\"\\n-                    joblib.dump(entity_tagger, model_file_name)\\n+            data_to_store = [\\n+                [token.to_dict() for token in sub_list] for sub_list in dataset\\n+            ]\\n \\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                model_dir / \"crf_dataset.json\", data_to_store\\n+            )\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                model_dir / \"crf_order.json\", self.crf_order\\n+            )\\n+\\n+    @classmethod\\n     def _crf_tokens_to_features(\\n-        self, crf_tokens: List[CRFToken], include_tag_features: bool = False\\n+        cls,\\n+        crf_tokens: List[CRFToken],\\n+        config: Dict[str, Any],\\n+        include_tag_features: bool = False,\\n     ) -> List[Dict[Text, Any]]:\\n         \"\"\"Convert the list of tokens into discrete features.\"\"\"\\n-        configured_features = self.component_config[self.CONFIG_FEATURES]\\n+        configured_features = config[CONFIG_FEATURES]\\n         sentence_features = []\\n \\n         for token_idx in range(len(crf_tokens)):\\n@@ -444,28 +478,31 @@ def _crf_tokens_to_features(\\n             half_window_size = window_size // 2\\n             window_range = range(-half_window_size, half_window_size + 1)\\n \\n-            token_features = self._create_features_for_token(\\n+            token_features = cls._create_features_for_token(\\n                 crf_tokens,\\n                 token_idx,\\n                 half_window_size,\\n                 window_range,\\n                 include_tag_features,\\n+                config,\\n             )\\n \\n             sentence_features.append(token_features)\\n \\n         return sentence_features\\n \\n+    @classmethod\\n     def _create_features_for_token(\\n-        self,\\n+        cls,\\n         crf_tokens: List[CRFToken],\\n         token_idx: int,\\n         half_window_size: int,\\n         window_range: range,\\n         include_tag_features: bool,\\n+        config: Dict[str, Any],\\n     ) -> Dict[Text, Any]:\\n         \"\"\"Convert a token into discrete features including words before and after.\"\"\"\\n-        configured_features = self.component_config[self.CONFIG_FEATURES]\\n+        configured_features = config[CONFIG_FEATURES]\\n         prefixes = [str(i) for i in window_range]\\n \\n         token_features = {}\\n@@ -505,13 +542,13 @@ def _create_features_for_token(\\n                         # set in the training data, \\'matched\\' is either \\'True\\' or\\n                         # \\'False\\' depending on whether the token actually matches the\\n                         # pattern or not\\n-                        regex_patterns = self.function_dict[feature](token)\\n+                        regex_patterns = cls.function_dict[feature](token)\\n                         for pattern_name, matched in regex_patterns.items():\\n                             token_features[\\n                                 f\"{prefix}:{feature}:{pattern_name}\"\\n                             ] = matched\\n                     else:\\n-                        value = self.function_dict[feature](token)\\n+                        value = cls.function_dict[feature](token)\\n                         token_features[f\"{prefix}:{feature}\"] = value\\n \\n         return token_features\\n@@ -635,38 +672,46 @@ def _get_tags(self, message: Message) -> Dict[Text, List[Text]]:\\n \\n         return tags\\n \\n-    def _train_model(self, df_train: List[List[CRFToken]]) -> None:\\n+    @classmethod\\n+    def train_model(\\n+        cls,\\n+        df_train: List[List[CRFToken]],\\n+        config: Dict[str, Any],\\n+        crf_order: List[str],\\n+    ) -> OrderedDict[str, CRF]:\\n         \"\"\"Train the crf tagger based on the training data.\"\"\"\\n         import sklearn_crfsuite\\n \\n-        self.entity_taggers = OrderedDict()\\n+        entity_taggers = OrderedDict()\\n \\n-        for tag_name in self.crf_order:\\n+        for tag_name in crf_order:\\n             logger.debug(f\"Training CRF for \\'{tag_name}\\'.\")\\n \\n             # add entity tag features for second level CRFs\\n             include_tag_features = tag_name != ENTITY_ATTRIBUTE_TYPE\\n             X_train = (\\n-                self._crf_tokens_to_features(sentence, include_tag_features)\\n+                cls._crf_tokens_to_features(sentence, config, include_tag_features)\\n                 for sentence in df_train\\n             )\\n             y_train = (\\n-                self._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train\\n+                cls._crf_tokens_to_tags(sentence, tag_name) for sentence in df_train\\n             )\\n \\n             entity_tagger = sklearn_crfsuite.CRF(\\n                 algorithm=\"lbfgs\",\\n                 # coefficient for L1 penalty\\n-                c1=self.component_config[\"L1_c\"],\\n+                c1=config[\"L1_c\"],\\n                 # coefficient for L2 penalty\\n-                c2=self.component_config[\"L2_c\"],\\n+                c2=config[\"L2_c\"],\\n                 # stop earlier\\n-                max_iterations=self.component_config[\"max_iterations\"],\\n+                max_iterations=config[\"max_iterations\"],\\n                 # include transitions that are possible, but not observed\\n                 all_possible_transitions=True,\\n             )\\n             entity_tagger.fit(X_train, y_train)\\n \\n-            self.entity_taggers[tag_name] = entity_tagger\\n+            entity_taggers[tag_name] = entity_tagger\\n \\n             logger.debug(\"Training finished.\")\\n+\\n+        return entity_taggers', '@@ -1,15 +1,15 @@\\n from __future__ import annotations\\n-import logging\\n \\n-from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n+import logging\\n from pathlib import Path\\n from collections import defaultdict\\n import contextlib\\n+from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type\\n \\n import numpy as np\\n import tensorflow as tf\\n-from typing import Any, List, Optional, Text, Dict, Tuple, Union, Type\\n \\n+from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.graph import ExecutionContext\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n@@ -49,18 +49,22 @@\\n from rasa.shared.core.events import EntitiesAdded, Event\\n from rasa.shared.core.domain import Domain\\n from rasa.shared.nlu.training_data.message import Message\\n-from rasa.shared.nlu.training_data.features import Features\\n+from rasa.shared.nlu.training_data.features import (\\n+    Features,\\n+    save_features,\\n+    load_features,\\n+)\\n import rasa.shared.utils.io\\n import rasa.utils.io\\n from rasa.utils import train_utils\\n-from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\\n-from rasa.utils.tensorflow import rasa_layers\\n-from rasa.utils.tensorflow.model_data import (\\n-    RasaModelData,\\n-    FeatureSignature,\\n+from rasa.utils.tensorflow.feature_array import (\\n     FeatureArray,\\n-    Data,\\n+    serialize_nested_feature_arrays,\\n+    deserialize_nested_feature_arrays,\\n )\\n+from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\\n+from rasa.utils.tensorflow import rasa_layers\\n+from rasa.utils.tensorflow.model_data import RasaModelData, FeatureSignature, Data\\n from rasa.utils.tensorflow.model_data_utils import convert_to_data_format\\n from rasa.utils.tensorflow.constants import (\\n     LABEL,\\n@@ -961,22 +965,32 @@ def persist_model_utilities(self, model_path: Path) -> None:\\n             model_path: Path where model is to be persisted\\n         \"\"\"\\n         model_filename = self._metadata_filename()\\n-        rasa.utils.io.json_pickle(\\n-            model_path / f\"{model_filename}.priority.pkl\", self.priority\\n-        )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.meta.pkl\", self.config\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+            model_path / f\"{model_filename}.priority.json\", self.priority\\n         )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.data_example.pkl\", self.data_example\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+            model_path / f\"{model_filename}.meta.json\", self.config\\n         )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.fake_features.pkl\", self.fake_features\\n+        # save data example\\n+        serialize_nested_feature_arrays(\\n+            self.data_example,\\n+            str(model_path / f\"{model_filename}.data_example.st\"),\\n+            str(model_path / f\"{model_filename}.data_example_metadata.json\"),\\n         )\\n-        rasa.utils.io.pickle_dump(\\n-            model_path / f\"{model_filename}.label_data.pkl\",\\n+        # save label data\\n+        serialize_nested_feature_arrays(\\n             dict(self._label_data.data) if self._label_data is not None else {},\\n+            str(model_path / f\"{model_filename}.label_data.st\"),\\n+            str(model_path / f\"{model_filename}.label_data_metadata.json\"),\\n+        )\\n+        # save fake features\\n+        metadata = save_features(\\n+            self.fake_features, str(model_path / f\"{model_filename}.fake_features.st\")\\n+        )\\n+        rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+            model_path / f\"{model_filename}.fake_features_metadata.json\", metadata\\n         )\\n+\\n         entity_tag_specs = (\\n             [tag_spec._asdict() for tag_spec in self._entity_tag_specs]\\n             if self._entity_tag_specs\\n@@ -994,18 +1008,29 @@ def _load_model_utilities(cls, model_path: Path) -> Dict[Text, Any]:\\n             model_path: Path where model is to be persisted.\\n         \"\"\"\\n         tf_model_file = model_path / f\"{cls._metadata_filename()}.tf_model\"\\n-        loaded_data = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.data_example.pkl\"\\n+\\n+        # load data example\\n+        loaded_data = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{cls._metadata_filename()}.data_example.st\"),\\n+            str(model_path / f\"{cls._metadata_filename()}.data_example_metadata.json\"),\\n         )\\n-        label_data = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.label_data.pkl\"\\n+        # load label data\\n+        loaded_label_data = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{cls._metadata_filename()}.label_data.st\"),\\n+            str(model_path / f\"{cls._metadata_filename()}.label_data_metadata.json\"),\\n         )\\n-        fake_features = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.fake_features.pkl\"\\n+        label_data = RasaModelData(data=loaded_label_data)\\n+\\n+        # load fake features\\n+        metadata = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{cls._metadata_filename()}.fake_features_metadata.json\"\\n         )\\n-        label_data = RasaModelData(data=label_data)\\n-        priority = rasa.utils.io.json_unpickle(\\n-            model_path / f\"{cls._metadata_filename()}.priority.pkl\"\\n+        fake_features = load_features(\\n+            str(model_path / f\"{cls._metadata_filename()}.fake_features.st\"), metadata\\n+        )\\n+\\n+        priority = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{cls._metadata_filename()}.priority.json\"\\n         )\\n         entity_tag_specs = rasa.shared.utils.io.read_json_file(\\n             model_path / f\"{cls._metadata_filename()}.entity_tag_specs.json\"\\n@@ -1023,8 +1048,8 @@ def _load_model_utilities(cls, model_path: Path) -> Dict[Text, Any]:\\n             )\\n             for tag_spec in entity_tag_specs\\n         ]\\n-        model_config = rasa.utils.io.pickle_load(\\n-            model_path / f\"{cls._metadata_filename()}.meta.pkl\"\\n+        model_config = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{cls._metadata_filename()}.meta.json\"\\n         )\\n \\n         return {\\n@@ -1070,7 +1095,7 @@ def _load(\\n     ) -> TEDPolicy:\\n         featurizer = TrackerFeaturizer.load(model_path)\\n \\n-        if not (model_path / f\"{cls._metadata_filename()}.data_example.pkl\").is_file():\\n+        if not (model_path / f\"{cls._metadata_filename()}.data_example.st\").is_file():\\n             return cls(\\n                 config,\\n                 model_storage,', '@@ -1,15 +1,133 @@\\n from __future__ import annotations\\n-from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set\\n+\\n import itertools\\n+from dataclasses import dataclass\\n+from typing import Iterable, Union, Text, Optional, List, Any, Tuple, Dict, Set\\n \\n import numpy as np\\n import scipy.sparse\\n+from safetensors.numpy import save_file, load_file\\n \\n-import rasa.shared.utils.io\\n import rasa.shared.nlu.training_data.util\\n+import rasa.shared.utils.io\\n from rasa.shared.nlu.constants import FEATURE_TYPE_SEQUENCE, FEATURE_TYPE_SENTENCE\\n \\n \\n+@dataclass\\n+class FeatureMetadata:\\n+    data_type: str\\n+    attribute: str\\n+    origin: Union[str, List[str]]\\n+    is_sparse: bool\\n+    shape: tuple\\n+    safetensors_key: str\\n+\\n+\\n+def save_features(\\n+    features_dict: Dict[Text, List[Features]], file_name: str\\n+) -> Dict[str, Any]:\\n+    \"\"\"Save a dictionary of Features lists to disk using safetensors.\\n+\\n+    Args:\\n+        features_dict: Dictionary mapping strings to lists of Features objects\\n+        file_name: File to save the features to\\n+\\n+    Returns:\\n+        The metadata to reconstruct the features.\\n+    \"\"\"\\n+    # All tensors are stored in a single safetensors file\\n+    tensors_to_save = {}\\n+    # Metadata will be stored separately\\n+    metadata = {}\\n+\\n+    for key, features_list in features_dict.items():\\n+        feature_metadata_list = []\\n+\\n+        for idx, feature in enumerate(features_list):\\n+            # Create a unique key for this tensor in the safetensors file\\n+            safetensors_key = f\"{key}_{idx}\"\\n+\\n+            # Convert sparse matrices to dense if needed\\n+            if feature.is_sparse():\\n+                # For sparse matrices, use the COO format\\n+                coo = feature.features.tocoo()  # type:ignore[union-attr]\\n+                # Save data, row indices and col indices separately\\n+                tensors_to_save[f\"{safetensors_key}_data\"] = coo.data\\n+                tensors_to_save[f\"{safetensors_key}_row\"] = coo.row\\n+                tensors_to_save[f\"{safetensors_key}_col\"] = coo.col\\n+            else:\\n+                tensors_to_save[safetensors_key] = feature.features\\n+\\n+            # Store metadata\\n+            metadata_item = FeatureMetadata(\\n+                data_type=feature.type,\\n+                attribute=feature.attribute,\\n+                origin=feature.origin,\\n+                is_sparse=feature.is_sparse(),\\n+                shape=feature.features.shape,\\n+                safetensors_key=safetensors_key,\\n+            )\\n+            feature_metadata_list.append(vars(metadata_item))\\n+\\n+        metadata[key] = feature_metadata_list\\n+\\n+    # Save tensors\\n+    save_file(tensors_to_save, file_name)\\n+\\n+    return metadata\\n+\\n+\\n+def load_features(\\n+    filename: str, metadata: Dict[str, Any]\\n+) -> Dict[Text, List[Features]]:\\n+    \"\"\"Load Features dictionary from disk.\\n+\\n+    Args:\\n+        filename: File name of the safetensors file.\\n+        metadata: Metadata to reconstruct the features.\\n+\\n+    Returns:\\n+        Dictionary mapping strings to lists of Features objects\\n+    \"\"\"\\n+    # Load tensors\\n+    tensors = load_file(filename)\\n+\\n+    # Reconstruct the features dictionary\\n+    features_dict: Dict[Text, List[Features]] = {}\\n+\\n+    for key, feature_metadata_list in metadata.items():\\n+        features_list = []\\n+\\n+        for meta in feature_metadata_list:\\n+            safetensors_key = meta[\"safetensors_key\"]\\n+\\n+            if meta[\"is_sparse\"]:\\n+                # Reconstruct sparse matrix from COO format\\n+                data = tensors[f\"{safetensors_key}_data\"]\\n+                row = tensors[f\"{safetensors_key}_row\"]\\n+                col = tensors[f\"{safetensors_key}_col\"]\\n+\\n+                features_matrix = scipy.sparse.coo_matrix(\\n+                    (data, (row, col)), shape=tuple(meta[\"shape\"])\\n+                ).tocsr()  # Convert back to CSR format\\n+            else:\\n+                features_matrix = tensors[safetensors_key]\\n+\\n+            # Reconstruct Features object\\n+            features = Features(\\n+                features=features_matrix,\\n+                feature_type=meta[\"data_type\"],\\n+                attribute=meta[\"attribute\"],\\n+                origin=meta[\"origin\"],\\n+            )\\n+\\n+            features_list.append(features)\\n+\\n+        features_dict[key] = features_list\\n+\\n+    return features_dict\\n+\\n+\\n class Features:\\n     \"\"\"Stores the features produced by any featurizer.\"\"\"\\n ', '@@ -1,37 +1,39 @@\\n from __future__ import annotations\\n+\\n import copy\\n import logging\\n from collections import defaultdict\\n from pathlib import Path\\n-\\n-from rasa.exceptions import ModelNotFound\\n-from rasa.nlu.featurizers.featurizer import Featurizer\\n+from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type\\n \\n import numpy as np\\n import scipy.sparse\\n import tensorflow as tf\\n \\n-from typing import Any, Dict, List, Optional, Text, Tuple, Union, TypeVar, Type\\n-\\n+from rasa.exceptions import ModelNotFound\\n+from rasa.nlu.featurizers.featurizer import Featurizer\\n from rasa.engine.graph import ExecutionContext, GraphComponent\\n from rasa.engine.recipes.default_recipe import DefaultV1Recipe\\n from rasa.engine.storage.resource import Resource\\n from rasa.engine.storage.storage import ModelStorage\\n from rasa.nlu.extractors.extractor import EntityExtractorMixin\\n from rasa.nlu.classifiers.classifier import IntentClassifier\\n import rasa.shared.utils.io\\n-import rasa.utils.io as io_utils\\n import rasa.nlu.utils.bilou_utils as bilou_utils\\n from rasa.shared.constants import DIAGNOSTIC_DATA\\n from rasa.nlu.extractors.extractor import EntityTagSpec\\n from rasa.nlu.classifiers import LABEL_RANKING_LENGTH\\n from rasa.utils import train_utils\\n from rasa.utils.tensorflow import rasa_layers\\n+from rasa.utils.tensorflow.feature_array import (\\n+    FeatureArray,\\n+    serialize_nested_feature_arrays,\\n+    deserialize_nested_feature_arrays,\\n+)\\n from rasa.utils.tensorflow.models import RasaModel, TransformerRasaModel\\n from rasa.utils.tensorflow.model_data import (\\n     RasaModelData,\\n     FeatureSignature,\\n-    FeatureArray,\\n )\\n from rasa.nlu.constants import TOKENS_NAMES, DEFAULT_TRANSFORMER_SIZE\\n from rasa.shared.nlu.constants import (\\n@@ -118,7 +120,6 @@\\n \\n POSSIBLE_TAGS = [ENTITY_ATTRIBUTE_TYPE, ENTITY_ATTRIBUTE_ROLE, ENTITY_ATTRIBUTE_GROUP]\\n \\n-\\n DIETClassifierT = TypeVar(\"DIETClassifierT\", bound=\"DIETClassifier\")\\n \\n \\n@@ -1085,18 +1086,24 @@ def persist(self) -> None:\\n \\n             self.model.save(str(tf_model_file))\\n \\n-            io_utils.pickle_dump(\\n-                model_path / f\"{file_name}.data_example.pkl\", self._data_example\\n-            )\\n-            io_utils.pickle_dump(\\n-                model_path / f\"{file_name}.sparse_feature_sizes.pkl\",\\n-                self._sparse_feature_sizes,\\n+            # save data example\\n+            serialize_nested_feature_arrays(\\n+                self._data_example,\\n+                model_path / f\"{file_name}.data_example.st\",\\n+                model_path / f\"{file_name}.data_example_metadata.json\",\\n             )\\n-            io_utils.pickle_dump(\\n-                model_path / f\"{file_name}.label_data.pkl\",\\n+            # save label data\\n+            serialize_nested_feature_arrays(\\n                 dict(self._label_data.data) if self._label_data is not None else {},\\n+                model_path / f\"{file_name}.label_data.st\",\\n+                model_path / f\"{file_name}.label_data_metadata.json\",\\n             )\\n-            io_utils.json_pickle(\\n+\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n+                model_path / f\"{file_name}.sparse_feature_sizes.json\",\\n+                self._sparse_feature_sizes,\\n+            )\\n+            rasa.shared.utils.io.dump_obj_as_json_to_file(\\n                 model_path / f\"{file_name}.index_label_id_mapping.json\",\\n                 self.index_label_id_mapping,\\n             )\\n@@ -1185,15 +1192,22 @@ def _load_from_files(\\n     ]:\\n         file_name = cls.__name__\\n \\n-        data_example = io_utils.pickle_load(\\n-            model_path / f\"{file_name}.data_example.pkl\"\\n+        # load data example\\n+        data_example = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{file_name}.data_example.st\"),\\n+            str(model_path / f\"{file_name}.data_example_metadata.json\"),\\n         )\\n-        label_data = io_utils.pickle_load(model_path / f\"{file_name}.label_data.pkl\")\\n-        label_data = RasaModelData(data=label_data)\\n-        sparse_feature_sizes = io_utils.pickle_load(\\n-            model_path / f\"{file_name}.sparse_feature_sizes.pkl\"\\n+        # load label data\\n+        loaded_label_data = deserialize_nested_feature_arrays(\\n+            str(model_path / f\"{file_name}.label_data.st\"),\\n+            str(model_path / f\"{file_name}.label_data_metadata.json\"),\\n+        )\\n+        label_data = RasaModelData(data=loaded_label_data)\\n+\\n+        sparse_feature_sizes = rasa.shared.utils.io.read_json_file(\\n+            model_path / f\"{file_name}.sparse_feature_sizes.json\"\\n         )\\n-        index_label_id_mapping = io_utils.json_unpickle(\\n+        index_label_id_mapping = rasa.shared.utils.io.read_json_file(\\n             model_path / f\"{file_name}.index_label_id_mapping.json\"\\n         )\\n         entity_tag_specs = rasa.shared.utils.io.read_json_file(\\n@@ -1213,7 +1227,6 @@ def _load_from_files(\\n             for tag_spec in entity_tag_specs\\n         ]\\n \\n-        # jsonpickle converts dictionary keys to strings\\n         index_label_id_mapping = {\\n             int(key): value for key, value in index_label_id_mapping.items()\\n         }'], 'file': ['rasa/nlu/featurizers/sparse_featurizer/regex_featurizer.py', 'rasa/core/featurizers/single_state_featurizer.py', 'rasa/nlu/classifiers/sklearn_intent_classifier.py', 'rasa/core/policies/unexpected_intent_policy.py', 'rasa/nlu/featurizers/sparse_featurizer/count_vectors_featurizer.py', 'rasa/utils/io.py', 'rasa/utils/tensorflow/feature_array.py', 'rasa/shared/utils/io.py', 'scripts/ping_slack_about_package_release.sh', 'rasa/nlu/featurizers/sparse_featurizer/lexical_syntactic_featurizer.py', 'rasa/utils/common.py', 'rasa/nlu/classifiers/logistic_regression_classifier.py', 'rasa/utils/tensorflow/model_data.py', 'rasa/core/featurizers/tracker_featurizers.py', 'rasa/nlu/extractors/crf_entity_extractor.py', 'rasa/core/policies/ted_policy.py', 'rasa/shared/nlu/training_data/features.py', 'rasa/nlu/classifiers/diet_classifier.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Shell', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('fb2376ec-7581-433b-8417-ce31865cc381'), UUID('42523fdb-d82a-4e9f-8a38-9397386a291c'), UUID('aee502be-564a-4b05-aadb-b63ff6bacad6'), UUID('f429d8a3-b518-49fa-b605-822e01c99675'), UUID('2722187a-4d90-4953-a606-dcb4d97066fc'), UUID('7338d2db-0e35-4cf7-b97a-dd1217f4c5e0'), UUID('d8bdc762-a4d5-411c-bbb9-dcf06b222e40'), UUID('cd8dc7e7-298a-4545-ae25-c65f8fea6c82'), UUID('43f2c1c8-980c-42cd-a8bf-13d084465ff2'), UUID('1cfbbe77-5365-4a8e-beb7-89d844633fc6'), UUID('2f1b3e8c-aea3-4150-abd7-59d7b117439c'), UUID('5f06ddc2-a672-482e-b8e8-c00ca8898e05'), UUID('0cab3d96-cbf8-4655-8e52-bdc423b0466e'), UUID('a7950cfc-3098-499e-8556-108bb3ee2b96'), UUID('7d23552d-9574-4303-b923-0d1874de2869'), UUID('7447554e-c13e-4f30-aa7c-0eb4df57d2e3'), UUID('3c2b5049-b3df-4708-a662-9a9b44925c6f'), UUID('df4ae407-6ad1-4add-9d02-71b530c85654')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 193, in get_changes\n",
      "    _get_changes_lines_units(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/get_changes_lines_units.py\", line 250, in _get_changes_lines_units\n",
      "    <= name.get_definition_end_position()[0]\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/classes.py\", line 260, in get_definition_end_position\n",
      "    if self.type in (\"function\", \"class\"):\n",
      "       ^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/classes.py\", line 190, in type\n",
      "    for value in self._name.infer():\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 281, in infer\n",
      "    return tree_name_to_values(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/__init__.py\", line 21, in wrapper\n",
      "    return built_functions[public_name](*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/stdlib.py\", line 878, in wrapper\n",
      "    return func(inference_state, context, tree_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/django.py\", line 177, in wrapper\n",
      "    result = func(inference_state, context, tree_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 771, in tree_name_to_values\n",
      "    types = imports.infer_import(context, tree_name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/cache.py\", line 44, in wrapper\n",
      "    rv = function(obj, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/imports.py\", line 57, in infer_import\n",
      "    values = values.py__getattribute__(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in py__getattribute__\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in <genexpr>\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 83, in py__getattribute__\n",
      "    names = self.goto(name_or_str, name_context, analysis_errors)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 72, in goto\n",
      "    names = finder.filter_name(filters, name_or_str)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/finder.py\", line 35, in filter_name\n",
      "    for filter in filters:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 62, in _get_value_filters\n",
      "    yield from self.get_filters(origin_scope=origin_scope)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/module.py\", line 71, in get_filters\n",
      "    yield from self.iter_star_filters()\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/module.py\", line 100, in iter_star_filters\n",
      "    f = next(star_module.get_filters(), None)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/module.py\", line 63, in get_filters\n",
      "    ParserTreeFilter(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 138, in __init__\n",
      "    super().__init__(parent_context, node_context)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 100, in __init__\n",
      "    self._parso_cache_node = get_parso_cache_node(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/parser_utils.py\", line 287, in get_parso_cache_node\n",
      "    return parser_cache[grammar._hashed][path]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: PosixPath('/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/scipy/sparse/_base.py')\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1052/1800 [08:03<03:10,  3.93it/s]ERROR:src.process_code_changes:Error processing commit ff07792676f404ffff6ee61b5638c9dc1a33a37a\n",
      "ERROR:src.process_code_changes:{'repo': 'yt-dlp/yt-dlp', 'vulnerability_id': '2024-22423', 'commit': 'ff07792676f404ffff6ee61b5638c9dc1a33a37a', 'commit_source': 'github', 'cwe_id': ['CWE-78'], 'patch': ['@@ -25,7 +25,7 @@\\n \\n from .cache import Cache\\n from .compat import functools, urllib  # isort: split\\n-from .compat import compat_os_name, compat_shlex_quote, urllib_req_to_req\\n+from .compat import compat_os_name, urllib_req_to_req\\n from .cookies import LenientSimpleCookie, load_cookies\\n from .downloader import FFmpegFD, get_suitable_downloader, shorten_protocol_name\\n from .downloader.rtmp import rtmpdump_version\\n@@ -102,7 +102,6 @@\\n     UserNotLive,\\n     YoutubeDLError,\\n     age_restricted,\\n-    args_to_str,\\n     bug_reports_message,\\n     date_from_str,\\n     deprecation_warning,\\n@@ -141,6 +140,7 @@\\n     sanitize_filename,\\n     sanitize_path,\\n     sanitize_url,\\n+    shell_quote,\\n     str_or_none,\\n     strftime_or_none,\\n     subtitles_filename,\\n@@ -823,7 +823,7 @@ def warn_if_short_id(self, argv):\\n             self.report_warning(\\n                 \\'Long argument string detected. \\'\\n                 \\'Use -- to separate parameters and URLs, like this:\\\\n%s\\' %\\n-                args_to_str(correct_argv))\\n+                shell_quote(correct_argv))\\n \\n     def add_info_extractor(self, ie):\\n         \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"\\n@@ -1355,7 +1355,7 @@ def create_key(outer_mobj):\\n                 value, fmt = escapeHTML(str(value)), str_fmt\\n             elif fmt[-1] == \\'q\\':  # quoted\\n                 value = map(str, variadic(value) if \\'#\\' in flags else [value])\\n-                value, fmt = \\' \\'.join(map(compat_shlex_quote, value)), str_fmt\\n+                value, fmt = shell_quote(value, shell=True), str_fmt\\n             elif fmt[-1] == \\'B\\':  # bytes\\n                 value = f\\'%{str_fmt}\\'.encode() % str(value).encode()\\n                 value, fmt = value.decode(\\'utf-8\\', \\'ignore\\'), \\'s\\'', '@@ -50,7 +50,6 @@\\n     compat_expanduser,\\n     compat_HTMLParseError,\\n     compat_os_name,\\n-    compat_shlex_quote,\\n )\\n from ..dependencies import xattr\\n \\n@@ -836,9 +835,11 @@ def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs\\n \\n         if shell and compat_os_name == \\'nt\\' and kwargs.get(\\'executable\\') is None:\\n             if not isinstance(args, str):\\n-                args = \\' \\'.join(compat_shlex_quote(a) for a in args)\\n+                args = shell_quote(args, shell=True)\\n             shell = False\\n-            args = f\\'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"\\'\\n+            # Set variable for `cmd.exe` newline escaping (see `utils.shell_quote`)\\n+            env[\\'=\\'] = \\'\"^\\\\n\\\\n\"\\'\\n+            args = f\\'{self.__comspec()} /Q /S /D /V:OFF /E:ON /C \"{args}\"\\'\\n \\n         super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)\\n \\n@@ -1637,15 +1638,38 @@ def get_filesystem_encoding():\\n     return encoding if encoding is not None else \\'utf-8\\'\\n \\n \\n-def shell_quote(args):\\n-    quoted_args = []\\n-    encoding = get_filesystem_encoding()\\n-    for a in args:\\n-        if isinstance(a, bytes):\\n-            # We may get a filename encoded with \\'encodeFilename\\'\\n-            a = a.decode(encoding)\\n-        quoted_args.append(compat_shlex_quote(a))\\n-    return \\' \\'.join(quoted_args)\\n+_WINDOWS_QUOTE_TRANS = str.maketrans({\\'\"\\': \\'\\\\\\\\\"\\', \\'\\\\\\\\\\': \\'\\\\\\\\\\\\\\\\\\'})\\n+_CMD_QUOTE_TRANS = str.maketrans({\\n+    # Keep quotes balanced by replacing them with `\"\"` instead of `\\\\\\\\\"`\\n+    \\'\"\\': \\'\"\"\\',\\n+    # Requires a variable `=` containing `\"^\\\\n\\\\n\"` (set in `utils.Popen`)\\n+    # `=` should be unique since variables containing `=` cannot be set using cmd\\n+    \\'\\\\n\\': \\'%=%\\',\\n+    # While we are only required to escape backslashes immediately before quotes,\\n+    # we instead escape all of \\'em anyways to be consistent\\n+    \\'\\\\\\\\\\': \\'\\\\\\\\\\\\\\\\\\',\\n+    # Use zero length variable replacement so `%` doesn\\'t get expanded\\n+    # `cd` is always set as long as extensions are enabled (`/E:ON` in `utils.Popen`)\\n+    \\'%\\': \\'%%cd:~,%\\',\\n+})\\n+\\n+\\n+def shell_quote(args, *, shell=False):\\n+    args = list(variadic(args))\\n+    if any(isinstance(item, bytes) for item in args):\\n+        deprecation_warning(\\'Passing bytes to utils.shell_quote is deprecated\\')\\n+        encoding = get_filesystem_encoding()\\n+        for index, item in enumerate(args):\\n+            if isinstance(item, bytes):\\n+                args[index] = item.decode(encoding)\\n+\\n+    if compat_os_name != \\'nt\\':\\n+        return shlex.join(args)\\n+\\n+    trans = _CMD_QUOTE_TRANS if shell else _WINDOWS_QUOTE_TRANS\\n+    return \\' \\'.join(\\n+        s if re.fullmatch(r\\'[\\\\w#$*\\\\-+./:?@\\\\\\\\]+\\', s, re.ASCII) else s.translate(trans).join(\\'\"\"\\')\\n+        for s in args)\\n \\n \\n def smuggle_url(url, data):\\n@@ -2849,7 +2873,7 @@ def ytdl_is_updateable():\\n \\n def args_to_str(args):\\n     # Get a short string representation for a subprocess command\\n-    return \\' \\'.join(compat_shlex_quote(a) for a in args)\\n+    return shell_quote(args)\\n \\n \\n def error_to_str(err):'], 'file': ['yt_dlp/YoutubeDL.py', 'yt_dlp/utils/_utils.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('81ea1b01-dd51-4915-9a51-17e527eb0198'), UUID('87ec61ae-b8f1-47eb-ba68-a279a9ed740f')]}\n",
      "ERROR:root:Error in {'repo': 'yt-dlp/yt-dlp', 'vulnerability_id': '2024-22423', 'commit': 'ff07792676f404ffff6ee61b5638c9dc1a33a37a', 'commit_source': 'github', 'cwe_id': ['CWE-78'], 'patch': ['@@ -25,7 +25,7 @@\\n \\n from .cache import Cache\\n from .compat import functools, urllib  # isort: split\\n-from .compat import compat_os_name, compat_shlex_quote, urllib_req_to_req\\n+from .compat import compat_os_name, urllib_req_to_req\\n from .cookies import LenientSimpleCookie, load_cookies\\n from .downloader import FFmpegFD, get_suitable_downloader, shorten_protocol_name\\n from .downloader.rtmp import rtmpdump_version\\n@@ -102,7 +102,6 @@\\n     UserNotLive,\\n     YoutubeDLError,\\n     age_restricted,\\n-    args_to_str,\\n     bug_reports_message,\\n     date_from_str,\\n     deprecation_warning,\\n@@ -141,6 +140,7 @@\\n     sanitize_filename,\\n     sanitize_path,\\n     sanitize_url,\\n+    shell_quote,\\n     str_or_none,\\n     strftime_or_none,\\n     subtitles_filename,\\n@@ -823,7 +823,7 @@ def warn_if_short_id(self, argv):\\n             self.report_warning(\\n                 \\'Long argument string detected. \\'\\n                 \\'Use -- to separate parameters and URLs, like this:\\\\n%s\\' %\\n-                args_to_str(correct_argv))\\n+                shell_quote(correct_argv))\\n \\n     def add_info_extractor(self, ie):\\n         \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"\\n@@ -1355,7 +1355,7 @@ def create_key(outer_mobj):\\n                 value, fmt = escapeHTML(str(value)), str_fmt\\n             elif fmt[-1] == \\'q\\':  # quoted\\n                 value = map(str, variadic(value) if \\'#\\' in flags else [value])\\n-                value, fmt = \\' \\'.join(map(compat_shlex_quote, value)), str_fmt\\n+                value, fmt = shell_quote(value, shell=True), str_fmt\\n             elif fmt[-1] == \\'B\\':  # bytes\\n                 value = f\\'%{str_fmt}\\'.encode() % str(value).encode()\\n                 value, fmt = value.decode(\\'utf-8\\', \\'ignore\\'), \\'s\\'', '@@ -50,7 +50,6 @@\\n     compat_expanduser,\\n     compat_HTMLParseError,\\n     compat_os_name,\\n-    compat_shlex_quote,\\n )\\n from ..dependencies import xattr\\n \\n@@ -836,9 +835,11 @@ def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs\\n \\n         if shell and compat_os_name == \\'nt\\' and kwargs.get(\\'executable\\') is None:\\n             if not isinstance(args, str):\\n-                args = \\' \\'.join(compat_shlex_quote(a) for a in args)\\n+                args = shell_quote(args, shell=True)\\n             shell = False\\n-            args = f\\'{self.__comspec()} /Q /S /D /V:OFF /C \"{args}\"\\'\\n+            # Set variable for `cmd.exe` newline escaping (see `utils.shell_quote`)\\n+            env[\\'=\\'] = \\'\"^\\\\n\\\\n\"\\'\\n+            args = f\\'{self.__comspec()} /Q /S /D /V:OFF /E:ON /C \"{args}\"\\'\\n \\n         super().__init__(args, *remaining, env=env, shell=shell, **kwargs, startupinfo=self._startupinfo)\\n \\n@@ -1637,15 +1638,38 @@ def get_filesystem_encoding():\\n     return encoding if encoding is not None else \\'utf-8\\'\\n \\n \\n-def shell_quote(args):\\n-    quoted_args = []\\n-    encoding = get_filesystem_encoding()\\n-    for a in args:\\n-        if isinstance(a, bytes):\\n-            # We may get a filename encoded with \\'encodeFilename\\'\\n-            a = a.decode(encoding)\\n-        quoted_args.append(compat_shlex_quote(a))\\n-    return \\' \\'.join(quoted_args)\\n+_WINDOWS_QUOTE_TRANS = str.maketrans({\\'\"\\': \\'\\\\\\\\\"\\', \\'\\\\\\\\\\': \\'\\\\\\\\\\\\\\\\\\'})\\n+_CMD_QUOTE_TRANS = str.maketrans({\\n+    # Keep quotes balanced by replacing them with `\"\"` instead of `\\\\\\\\\"`\\n+    \\'\"\\': \\'\"\"\\',\\n+    # Requires a variable `=` containing `\"^\\\\n\\\\n\"` (set in `utils.Popen`)\\n+    # `=` should be unique since variables containing `=` cannot be set using cmd\\n+    \\'\\\\n\\': \\'%=%\\',\\n+    # While we are only required to escape backslashes immediately before quotes,\\n+    # we instead escape all of \\'em anyways to be consistent\\n+    \\'\\\\\\\\\\': \\'\\\\\\\\\\\\\\\\\\',\\n+    # Use zero length variable replacement so `%` doesn\\'t get expanded\\n+    # `cd` is always set as long as extensions are enabled (`/E:ON` in `utils.Popen`)\\n+    \\'%\\': \\'%%cd:~,%\\',\\n+})\\n+\\n+\\n+def shell_quote(args, *, shell=False):\\n+    args = list(variadic(args))\\n+    if any(isinstance(item, bytes) for item in args):\\n+        deprecation_warning(\\'Passing bytes to utils.shell_quote is deprecated\\')\\n+        encoding = get_filesystem_encoding()\\n+        for index, item in enumerate(args):\\n+            if isinstance(item, bytes):\\n+                args[index] = item.decode(encoding)\\n+\\n+    if compat_os_name != \\'nt\\':\\n+        return shlex.join(args)\\n+\\n+    trans = _CMD_QUOTE_TRANS if shell else _WINDOWS_QUOTE_TRANS\\n+    return \\' \\'.join(\\n+        s if re.fullmatch(r\\'[\\\\w#$*\\\\-+./:?@\\\\\\\\]+\\', s, re.ASCII) else s.translate(trans).join(\\'\"\"\\')\\n+        for s in args)\\n \\n \\n def smuggle_url(url, data):\\n@@ -2849,7 +2873,7 @@ def ytdl_is_updateable():\\n \\n def args_to_str(args):\\n     # Get a short string representation for a subprocess command\\n-    return \\' \\'.join(compat_shlex_quote(a) for a in args)\\n+    return shell_quote(args)\\n \\n \\n def error_to_str(err):'], 'file': ['yt_dlp/YoutubeDL.py', 'yt_dlp/utils/_utils.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('81ea1b01-dd51-4915-9a51-17e527eb0198'), UUID('87ec61ae-b8f1-47eb-ba68-a279a9ed740f')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:4:     def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:4:     def __init__(self, args, *remaining, env=None, text=False, shell=False, **kwargs):\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1056/1800 [08:05<03:34,  3.47it/s]ERROR:src.process_code_changes:Error processing commit fa6b7782fbb14aa08d767bc799c531f5e1fb3bb8\n",
      "ERROR:src.process_code_changes:{'repo': 'tensorflow/tensorflow', 'vulnerability_id': '2021-41212', 'commit': 'fa6b7782fbb14aa08d767bc799c531f5e1fb3bb8', 'commit_source': 'github', 'cwe_id': ['CWE-125'], 'patch': ['@@ -99,6 +99,13 @@ REGISTER_OP(\"RaggedCross\")\\n       int dense_start = num_ragged * 2 + num_sparse * 3;\\n       for (int i = 0; i < dense_types.size(); ++i) {\\n         ShapeHandle dense_input = c->input(i + dense_start);\\n+        int32 rank = c->Rank(dense_input);\\n+        if (rank == InferenceContext::kUnknownRank) {\\n+          continue;\\n+        } else if (rank != 2) {\\n+          return errors::InvalidArgument(\\n+              \"tf.ragged.cross only supports inputs with rank=2\");\\n+        }\\n         int64_t batch_size = c->Value(c->Dim(dense_input, 0));\\n         if (batch_size != InferenceContext::kUnknownDim) {\\n           ShapeHandle row_splits = c->Vector(batch_size + 1);', \"@@ -18,10 +18,12 @@\\n \\n import numpy as np\\n \\n+from tensorflow.python.eager import def_function\\n from tensorflow.python.framework import dtypes\\n from tensorflow.python.framework import errors\\n from tensorflow.python.framework import ops\\n from tensorflow.python.framework import sparse_tensor\\n+from tensorflow.python.framework import tensor_spec\\n from tensorflow.python.framework import test_util\\n from tensorflow.python.ops import sparse_ops\\n from tensorflow.python.ops.ragged import ragged_array_ops\\n@@ -358,6 +360,16 @@ def testRaggedCrossLargeBatch(self):\\n                   dense_const([[2], [3]])],\\n           exception=(ValueError, errors.InvalidArgumentError),\\n           message='inputs must all have the same batch dimension size'),\\n+      dict(\\n+          testcase_name='3DDenseTensor',\\n+          inputs=[dense_const([[[1]]])],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n+      dict(\\n+          testcase_name='0DDenseTensor',\\n+          inputs=[dense_const(1)],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n   ])\\n   def testStaticError(self, inputs, exception=ValueError, message=None):\\n     with self.assertRaisesRegex(exception, message):\\n@@ -368,17 +380,36 @@ def testStaticError(self, inputs, exception=ValueError, message=None):\\n           testcase_name='3DRaggedTensor',\\n           inputs=[ragged_const([[[1]]], ragged_rank=1)],\\n           message='tf.ragged.cross only supports inputs with rank=2'),\\n+      dict(\\n+          testcase_name='0DDenseTensor',\\n+          inputs=[dense_const(1)],\\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n+      dict(\\n+          testcase_name='1DDenseTensor',\\n+          inputs=[dense_const([1])],\\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n       dict(\\n           testcase_name='3DDenseTensor',\\n           inputs=[dense_const([[[1]]])],\\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n           message='tf.ragged.cross only supports inputs with rank=2'),\\n   ])\\n   def testRuntimeError(self,\\n                        inputs,\\n                        exception=errors.InvalidArgumentError,\\n-                       message=None):\\n+                       message=None,\\n+                       signature=None):\\n+    @def_function.function(input_signature=signature)\\n+    def fn(x):\\n+      return ragged_array_ops.cross(x)\\n+\\n     with self.assertRaisesRegex(exception, message):\\n-      self.evaluate(ragged_array_ops.cross(inputs))\\n+      self.evaluate(fn(inputs))\\n \\n   def _ragged_to_sparse(self, t):\\n     if ragged_tensor.is_ragged(t):\"], 'file': ['tensorflow/core/ops/ragged_array_ops.cc', 'tensorflow/python/ops/ragged/ragged_cross_op_test.py'], 'language': ['C/C++', 'Python'], 'temp_id': [UUID('c819f514-7303-49e4-9a6c-8622621dc850'), UUID('cba086fe-c201-4016-8ebd-0ecf0f9c27cf')]}\n",
      "ERROR:root:Error in {'repo': 'tensorflow/tensorflow', 'vulnerability_id': '2021-41212', 'commit': 'fa6b7782fbb14aa08d767bc799c531f5e1fb3bb8', 'commit_source': 'github', 'cwe_id': ['CWE-125'], 'patch': ['@@ -99,6 +99,13 @@ REGISTER_OP(\"RaggedCross\")\\n       int dense_start = num_ragged * 2 + num_sparse * 3;\\n       for (int i = 0; i < dense_types.size(); ++i) {\\n         ShapeHandle dense_input = c->input(i + dense_start);\\n+        int32 rank = c->Rank(dense_input);\\n+        if (rank == InferenceContext::kUnknownRank) {\\n+          continue;\\n+        } else if (rank != 2) {\\n+          return errors::InvalidArgument(\\n+              \"tf.ragged.cross only supports inputs with rank=2\");\\n+        }\\n         int64_t batch_size = c->Value(c->Dim(dense_input, 0));\\n         if (batch_size != InferenceContext::kUnknownDim) {\\n           ShapeHandle row_splits = c->Vector(batch_size + 1);', \"@@ -18,10 +18,12 @@\\n \\n import numpy as np\\n \\n+from tensorflow.python.eager import def_function\\n from tensorflow.python.framework import dtypes\\n from tensorflow.python.framework import errors\\n from tensorflow.python.framework import ops\\n from tensorflow.python.framework import sparse_tensor\\n+from tensorflow.python.framework import tensor_spec\\n from tensorflow.python.framework import test_util\\n from tensorflow.python.ops import sparse_ops\\n from tensorflow.python.ops.ragged import ragged_array_ops\\n@@ -358,6 +360,16 @@ def testRaggedCrossLargeBatch(self):\\n                   dense_const([[2], [3]])],\\n           exception=(ValueError, errors.InvalidArgumentError),\\n           message='inputs must all have the same batch dimension size'),\\n+      dict(\\n+          testcase_name='3DDenseTensor',\\n+          inputs=[dense_const([[[1]]])],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n+      dict(\\n+          testcase_name='0DDenseTensor',\\n+          inputs=[dense_const(1)],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n   ])\\n   def testStaticError(self, inputs, exception=ValueError, message=None):\\n     with self.assertRaisesRegex(exception, message):\\n@@ -368,17 +380,36 @@ def testStaticError(self, inputs, exception=ValueError, message=None):\\n           testcase_name='3DRaggedTensor',\\n           inputs=[ragged_const([[[1]]], ragged_rank=1)],\\n           message='tf.ragged.cross only supports inputs with rank=2'),\\n+      dict(\\n+          testcase_name='0DDenseTensor',\\n+          inputs=[dense_const(1)],\\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n+      dict(\\n+          testcase_name='1DDenseTensor',\\n+          inputs=[dense_const([1])],\\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n+          message='tf.ragged.cross only supports inputs with rank=2'),\\n       dict(\\n           testcase_name='3DDenseTensor',\\n           inputs=[dense_const([[[1]]])],\\n+          signature=[[tensor_spec.TensorSpec(None, dtypes.int32)]],\\n+          exception=(ValueError, errors.InvalidArgumentError),\\n           message='tf.ragged.cross only supports inputs with rank=2'),\\n   ])\\n   def testRuntimeError(self,\\n                        inputs,\\n                        exception=errors.InvalidArgumentError,\\n-                       message=None):\\n+                       message=None,\\n+                       signature=None):\\n+    @def_function.function(input_signature=signature)\\n+    def fn(x):\\n+      return ragged_array_ops.cross(x)\\n+\\n     with self.assertRaisesRegex(exception, message):\\n-      self.evaluate(ragged_array_ops.cross(inputs))\\n+      self.evaluate(fn(inputs))\\n \\n   def _ragged_to_sparse(self, t):\\n     if ragged_tensor.is_ragged(t):\"], 'file': ['tensorflow/core/ops/ragged_array_ops.cc', 'tensorflow/python/ops/ragged/ragged_cross_op_test.py'], 'language': ['C/C++', 'Python'], 'temp_id': [UUID('c819f514-7303-49e4-9a6c-8622621dc850'), UUID('cba086fe-c201-4016-8ebd-0ecf0f9c27cf')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 244:0:       dict(\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 244:0:       dict(\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1063/1800 [08:11<05:00,  2.45it/s]ERROR:src.process_code_changes:Error processing commit 3030e881d2e44f4021764e18e489fe940a9b3636\n",
      "ERROR:src.process_code_changes:{'repo': 'dpgaspar/Flask-AppBuilder', 'vulnerability_id': '2024-45314', 'commit': '3030e881d2e44f4021764e18e489fe940a9b3636', 'commit_source': 'github', 'cwe_id': ['CWE-525'], 'patch': ['@@ -9,7 +9,7 @@\\n from flask_appbuilder.baseviews import BaseView\\n from flask_appbuilder.charts.views import DirectByChartView\\n from flask_appbuilder.fieldwidgets import BS3PasswordFieldWidget\\n-from flask_appbuilder.security.decorators import has_access\\n+from flask_appbuilder.security.decorators import has_access, no_cache\\n from flask_appbuilder.security.forms import (\\n     DynamicForm,\\n     LoginForm_db,\\n@@ -520,6 +520,7 @@ class AuthDBView(AuthView):\\n     login_template = \"appbuilder/general/security/login_db.html\"\\n \\n     @expose(\"/login/\", methods=[\"GET\", \"POST\"])\\n+    @no_cache\\n     def login(self):\\n         if g.user is not None and g.user.is_authenticated:\\n             return redirect(self.appbuilder.get_url_for_index)\\n@@ -543,6 +544,7 @@ class AuthLDAPView(AuthView):\\n     login_template = \"appbuilder/general/security/login_ldap.html\"\\n \\n     @expose(\"/login/\", methods=[\"GET\", \"POST\"])\\n+    @no_cache\\n     def login(self):\\n         if g.user is not None and g.user.is_authenticated:\\n             return redirect(self.appbuilder.get_url_for_index)\\n@@ -568,6 +570,7 @@ class AuthOIDView(AuthView):\\n     oid_ask_for_optional: List[str] = []\\n \\n     @expose(\"/login/\", methods=[\"GET\", \"POST\"])\\n+    @no_cache\\n     def login(self, flag=True) -> WerkzeugResponse:\\n         @self.appbuilder.sm.oid.loginhandler\\n         def login_handler(self):', '@@ -30,6 +30,20 @@\\n P = ParamSpec(\"P\")\\n \\n \\n+def no_cache(view: Callable[..., Response]) -> Callable[..., Response]:\\n+    @functools.wraps(view)\\n+    def wrapped_view(*args, **kwargs) -> Response:\\n+        response = make_response(view(*args, **kwargs))\\n+        response.headers[\\n+            \"Cache-Control\"\\n+        ] = \"no-store, no-cache, must-revalidate, max-age=0\"\\n+        response.headers[\"Pragma\"] = \"no-cache\"\\n+        response.headers[\"Expires\"] = \"0\"\\n+        return response\\n+\\n+    return wrapped_view\\n+\\n+\\n def response_unauthorized_mvc(status_code: int) -> Response:\\n     response = make_response(\\n         jsonify({\"message\": str(FLAMSG_ERR_SEC_ACCESS_DENIED), \"severity\": \"danger\"}),'], 'file': ['flask_appbuilder/security/views.py', 'flask_appbuilder/security/decorators.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('f716811c-b5a9-4c0e-96ea-ac1383c7abb1'), UUID('c0e70689-9e27-454c-ac01-91e9b66ff8c7')]}\n",
      "ERROR:root:Error in {'repo': 'dpgaspar/Flask-AppBuilder', 'vulnerability_id': '2024-45314', 'commit': '3030e881d2e44f4021764e18e489fe940a9b3636', 'commit_source': 'github', 'cwe_id': ['CWE-525'], 'patch': ['@@ -9,7 +9,7 @@\\n from flask_appbuilder.baseviews import BaseView\\n from flask_appbuilder.charts.views import DirectByChartView\\n from flask_appbuilder.fieldwidgets import BS3PasswordFieldWidget\\n-from flask_appbuilder.security.decorators import has_access\\n+from flask_appbuilder.security.decorators import has_access, no_cache\\n from flask_appbuilder.security.forms import (\\n     DynamicForm,\\n     LoginForm_db,\\n@@ -520,6 +520,7 @@ class AuthDBView(AuthView):\\n     login_template = \"appbuilder/general/security/login_db.html\"\\n \\n     @expose(\"/login/\", methods=[\"GET\", \"POST\"])\\n+    @no_cache\\n     def login(self):\\n         if g.user is not None and g.user.is_authenticated:\\n             return redirect(self.appbuilder.get_url_for_index)\\n@@ -543,6 +544,7 @@ class AuthLDAPView(AuthView):\\n     login_template = \"appbuilder/general/security/login_ldap.html\"\\n \\n     @expose(\"/login/\", methods=[\"GET\", \"POST\"])\\n+    @no_cache\\n     def login(self):\\n         if g.user is not None and g.user.is_authenticated:\\n             return redirect(self.appbuilder.get_url_for_index)\\n@@ -568,6 +570,7 @@ class AuthOIDView(AuthView):\\n     oid_ask_for_optional: List[str] = []\\n \\n     @expose(\"/login/\", methods=[\"GET\", \"POST\"])\\n+    @no_cache\\n     def login(self, flag=True) -> WerkzeugResponse:\\n         @self.appbuilder.sm.oid.loginhandler\\n         def login_handler(self):', '@@ -30,6 +30,20 @@\\n P = ParamSpec(\"P\")\\n \\n \\n+def no_cache(view: Callable[..., Response]) -> Callable[..., Response]:\\n+    @functools.wraps(view)\\n+    def wrapped_view(*args, **kwargs) -> Response:\\n+        response = make_response(view(*args, **kwargs))\\n+        response.headers[\\n+            \"Cache-Control\"\\n+        ] = \"no-store, no-cache, must-revalidate, max-age=0\"\\n+        response.headers[\"Pragma\"] = \"no-cache\"\\n+        response.headers[\"Expires\"] = \"0\"\\n+        return response\\n+\\n+    return wrapped_view\\n+\\n+\\n def response_unauthorized_mvc(status_code: int) -> Response:\\n     response = make_response(\\n         jsonify({\"message\": str(FLAMSG_ERR_SEC_ACCESS_DENIED), \"severity\": \"danger\"}),'], 'file': ['flask_appbuilder/security/views.py', 'flask_appbuilder/security/decorators.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('f716811c-b5a9-4c0e-96ea-ac1383c7abb1'), UUID('c0e70689-9e27-454c-ac01-91e9b66ff8c7')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @no_cache\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @no_cache\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1065/1800 [08:11<04:56,  2.48it/s]ERROR:src.process_code_changes:Error processing commit 809bfac4890f75fc73607318a04d2ccba71b3d9f\n",
      "ERROR:src.process_code_changes:{'repo': 'scrapy/scrapy', 'vulnerability_id': 'GHSA-rmqv-7v3j-mr7p', 'commit': '809bfac4890f75fc73607318a04d2ccba71b3d9f', 'commit_source': 'github', 'cwe_id': ['CWE-409'], 'patch': ['@@ -1,31 +1,41 @@\\n import struct\\n from gzip import GzipFile\\n from io import BytesIO\\n-from typing import List\\n \\n from scrapy.http import Response\\n \\n+from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\\n \\n-def gunzip(data: bytes) -> bytes:\\n+\\n+def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\\n     \"\"\"Gunzip the given data and return as much data as possible.\\n \\n     This is resilient to CRC checksum errors.\\n     \"\"\"\\n     f = GzipFile(fileobj=BytesIO(data))\\n-    output_list: List[bytes] = []\\n+    output_stream = BytesIO()\\n     chunk = b\".\"\\n+    decompressed_size = 0\\n     while chunk:\\n         try:\\n-            chunk = f.read1(8196)\\n-            output_list.append(chunk)\\n+            chunk = f.read1(_CHUNK_SIZE)\\n         except (OSError, EOFError, struct.error):\\n             # complete only if there is some data, otherwise re-raise\\n             # see issue 87 about catching struct.error\\n-            # some pages are quite small so output_list is empty\\n-            if output_list:\\n+            # some pages are quite small so output_stream is empty\\n+            if output_stream.getbuffer().nbytes > 0:\\n                 break\\n             raise\\n-    return b\"\".join(output_list)\\n+        decompressed_size += len(chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n \\n \\n def gzip_magic_number(response: Response) -> bool:', '@@ -1,94 +0,0 @@\\n-\"\"\" This module implements the DecompressionMiddleware which tries to recognise\\n-and extract the potentially compressed responses that may arrive.\\n-\"\"\"\\n-\\n-import bz2\\n-import gzip\\n-import logging\\n-import tarfile\\n-import zipfile\\n-from io import BytesIO\\n-from tempfile import mktemp\\n-from warnings import warn\\n-\\n-from scrapy.exceptions import ScrapyDeprecationWarning\\n-from scrapy.responsetypes import responsetypes\\n-\\n-warn(\\n-    \"scrapy.downloadermiddlewares.decompression is deprecated\",\\n-    ScrapyDeprecationWarning,\\n-    stacklevel=2,\\n-)\\n-\\n-\\n-logger = logging.getLogger(__name__)\\n-\\n-\\n-class DecompressionMiddleware:\\n-    \"\"\"This middleware tries to recognise and extract the possibly compressed\\n-    responses that may arrive.\"\"\"\\n-\\n-    def __init__(self):\\n-        self._formats = {\\n-            \"tar\": self._is_tar,\\n-            \"zip\": self._is_zip,\\n-            \"gz\": self._is_gzip,\\n-            \"bz2\": self._is_bzip2,\\n-        }\\n-\\n-    def _is_tar(self, response):\\n-        archive = BytesIO(response.body)\\n-        try:\\n-            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\\n-        except tarfile.ReadError:\\n-            return\\n-\\n-        body = tar_file.extractfile(tar_file.members[0]).read()\\n-        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def _is_zip(self, response):\\n-        archive = BytesIO(response.body)\\n-        try:\\n-            zip_file = zipfile.ZipFile(archive)\\n-        except zipfile.BadZipFile:\\n-            return\\n-\\n-        namelist = zip_file.namelist()\\n-        body = zip_file.read(namelist[0])\\n-        respcls = responsetypes.from_args(filename=namelist[0], body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def _is_gzip(self, response):\\n-        archive = BytesIO(response.body)\\n-        try:\\n-            body = gzip.GzipFile(fileobj=archive).read()\\n-        except OSError:\\n-            return\\n-\\n-        respcls = responsetypes.from_args(body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def _is_bzip2(self, response):\\n-        try:\\n-            body = bz2.decompress(response.body)\\n-        except OSError:\\n-            return\\n-\\n-        respcls = responsetypes.from_args(body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def process_response(self, request, response, spider):\\n-        if not response.body:\\n-            return response\\n-\\n-        for fmt, func in self._formats.items():\\n-            new_response = func(response)\\n-            if new_response:\\n-                logger.debug(\\n-                    \"Decompressed response with format: %(responsefmt)s\",\\n-                    {\"responsefmt\": fmt},\\n-                    extra={\"spider\": spider},\\n-                )\\n-                return new_response\\n-        return response', '@@ -1,11 +1,19 @@\\n import logging\\n import re\\n+from typing import TYPE_CHECKING, Any\\n \\n from scrapy.http import Request, XmlResponse\\n from scrapy.spiders import Spider\\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\\n from scrapy.utils.gz import gunzip, gzip_magic_number\\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\\n \\n+if TYPE_CHECKING:\\n+    # typing.Self requires Python 3.11\\n+    from typing_extensions import Self\\n+\\n+    from scrapy.crawler import Crawler\\n+\\n logger = logging.getLogger(__name__)\\n \\n \\n@@ -14,6 +22,19 @@ class SitemapSpider(Spider):\\n     sitemap_rules = [(\"\", \"parse\")]\\n     sitemap_follow = [\"\"]\\n     sitemap_alternate_links = False\\n+    _max_size: int\\n+    _warn_size: int\\n+\\n+    @classmethod\\n+    def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":\\n+        spider = super().from_crawler(crawler, *args, **kwargs)\\n+        spider._max_size = getattr(\\n+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        )\\n+        spider._warn_size = getattr(\\n+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        )\\n+        return spider\\n \\n     def __init__(self, *a, **kw):\\n         super().__init__(*a, **kw)\\n@@ -71,7 +92,19 @@ def _get_sitemap_body(self, response):\\n         if isinstance(response, XmlResponse):\\n             return response.body\\n         if gzip_magic_number(response):\\n-            return gunzip(response.body)\\n+            uncompressed_size = len(response.body)\\n+            max_size = response.meta.get(\"download_maxsize\", self._max_size)\\n+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\\n+            try:\\n+                body = gunzip(response.body, max_size=max_size)\\n+            except _DecompressionMaxSizeExceeded:\\n+                return None\\n+            if uncompressed_size < warn_size <= len(body):\\n+                logger.warning(\\n+                    f\"{response} body size after decompression ({len(body)} B) \"\\n+                    f\"is larger than the download warning size ({warn_size} B).\"\\n+                )\\n+            return body\\n         # actual gzipped sitemap files are decompressed above ;\\n         # if we are here (response body is not gzipped)\\n         # and have a response for .xml.gz,', '@@ -0,0 +1,94 @@\\n+import zlib\\n+from io import BytesIO\\n+\\n+try:\\n+    import brotli\\n+except ImportError:\\n+    pass\\n+\\n+try:\\n+    import zstandard\\n+except ImportError:\\n+    pass\\n+\\n+\\n+_CHUNK_SIZE = 65536  # 64 KiB\\n+\\n+\\n+class _DecompressionMaxSizeExceeded(ValueError):\\n+    pass\\n+\\n+\\n+def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\\n+    decompressor = zlib.decompressobj()\\n+    raw_decompressor = zlib.decompressobj(wbits=-15)\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        try:\\n+            output_chunk = decompressor.decompress(input_chunk)\\n+        except zlib.error:\\n+            if decompressor != raw_decompressor:\\n+                # ugly hack to work with raw deflate content that may\\n+                # be sent by microsoft servers. For more information, see:\\n+                # http://carsten.codimi.de/gzip.yaws/\\n+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n+                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n+                decompressor = raw_decompressor\\n+                output_chunk = decompressor.decompress(input_chunk)\\n+            else:\\n+                raise\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\\n+    decompressor = brotli.Decompressor()\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        output_chunk = decompressor.process(input_chunk)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\\n+    decompressor = zstandard.ZstdDecompressor()\\n+    stream_reader = decompressor.stream_reader(BytesIO(data))\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        output_chunk = stream_reader.read(_CHUNK_SIZE)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()', '@@ -1,53 +1,78 @@\\n-import io\\n import warnings\\n-import zlib\\n+from logging import getLogger\\n \\n-from scrapy.exceptions import NotConfigured\\n+from scrapy import signals\\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\\n from scrapy.http import Response, TextResponse\\n from scrapy.responsetypes import responsetypes\\n+from scrapy.utils._compression import (\\n+    _DecompressionMaxSizeExceeded,\\n+    _inflate,\\n+    _unbrotli,\\n+    _unzstd,\\n+)\\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\\n from scrapy.utils.gz import gunzip\\n \\n+logger = getLogger(__name__)\\n+\\n ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\\n \\n try:\\n-    import brotli\\n-\\n-    ACCEPTED_ENCODINGS.append(b\"br\")\\n+    import brotli  # noqa: F401\\n except ImportError:\\n     pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"br\")\\n \\n try:\\n-    import zstandard\\n-\\n-    ACCEPTED_ENCODINGS.append(b\"zstd\")\\n+    import zstandard  # noqa: F401\\n except ImportError:\\n     pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\\n \\n \\n class HttpCompressionMiddleware:\\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\\n     sent/received from web sites\"\"\"\\n \\n-    def __init__(self, stats=None):\\n-        self.stats = stats\\n+    def __init__(self, stats=None, *, crawler=None):\\n+        if not crawler:\\n+            self.stats = stats\\n+            self._max_size = 1073741824\\n+            self._warn_size = 33554432\\n+            return\\n+        self.stats = crawler.stats\\n+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        crawler.signals.connect(self.open_spider, signals.spider_opened)\\n \\n     @classmethod\\n     def from_crawler(cls, crawler):\\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\\n             raise NotConfigured\\n         try:\\n-            return cls(stats=crawler.stats)\\n+            return cls(crawler=crawler)\\n         except TypeError:\\n             warnings.warn(\\n                 \"HttpCompressionMiddleware subclasses must either modify \"\\n-                \"their \\'__init__\\' method to support a \\'stats\\' parameter or \"\\n-                \"reimplement the \\'from_crawler\\' method.\",\\n+                \"their \\'__init__\\' method to support a \\'crawler\\' parameter or \"\\n+                \"reimplement their \\'from_crawler\\' method.\",\\n                 ScrapyDeprecationWarning,\\n             )\\n-            result = cls()\\n-            result.stats = crawler.stats\\n-            return result\\n+            mw = cls()\\n+            mw.stats = crawler.stats\\n+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+            crawler.signals.connect(mw.open_spider, signals.spider_opened)\\n+            return mw\\n+\\n+    def open_spider(self, spider):\\n+        if hasattr(spider, \"download_maxsize\"):\\n+            self._max_size = spider.download_maxsize\\n+        if hasattr(spider, \"download_warnsize\"):\\n+            self._warn_size = spider.download_warnsize\\n \\n     def process_request(self, request, spider):\\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\\n@@ -59,7 +84,24 @@ def process_response(self, request, response, spider):\\n             content_encoding = response.headers.getlist(\"Content-Encoding\")\\n             if content_encoding:\\n                 encoding = content_encoding.pop()\\n-                decoded_body = self._decode(response.body, encoding.lower())\\n+                max_size = request.meta.get(\"download_maxsize\", self._max_size)\\n+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\\n+                try:\\n+                    decoded_body = self._decode(\\n+                        response.body, encoding.lower(), max_size\\n+                    )\\n+                except _DecompressionMaxSizeExceeded:\\n+                    raise IgnoreRequest(\\n+                        f\"Ignored response {response} because its body \"\\n+                        f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\\n+                        f\"({max_size} B) during decompression.\"\\n+                    )\\n+                if len(response.body) < warn_size <= len(decoded_body):\\n+                    logger.warning(\\n+                        f\"{response} body size after decompression \"\\n+                        f\"({len(decoded_body)} B) is larger than the \"\\n+                        f\"download warning size ({warn_size} B).\"\\n+                    )\\n                 if self.stats:\\n                     self.stats.inc_value(\\n                         \"httpcompression/response_bytes\",\\n@@ -83,25 +125,13 @@ def process_response(self, request, response, spider):\\n \\n         return response\\n \\n-    def _decode(self, body, encoding):\\n+    def _decode(self, body, encoding, max_size):\\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\\n-            body = gunzip(body)\\n-\\n+            return gunzip(body, max_size=max_size)\\n         if encoding == b\"deflate\":\\n-            try:\\n-                body = zlib.decompress(body)\\n-            except zlib.error:\\n-                # ugly hack to work with raw deflate content that may\\n-                # be sent by microsoft servers. For more information, see:\\n-                # http://carsten.codimi.de/gzip.yaws/\\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n-                body = zlib.decompress(body, -15)\\n+            return _inflate(body, max_size=max_size)\\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\\n-            body = brotli.decompress(body)\\n+            return _unbrotli(body, max_size=max_size)\\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\\n-            # Using its streaming API since its simple API could handle only cases\\n-            # where there is content size data embedded in the frame\\n-            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\\n-            body = reader.read()\\n+            return _unzstd(body, max_size=max_size)\\n         return body'], 'file': ['scrapy/utils/gz.py', 'scrapy/downloadermiddlewares/decompression.py', 'scrapy/spiders/sitemap.py', 'scrapy/utils/_compression.py', 'scrapy/downloadermiddlewares/httpcompression.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('cc5ca44a-dac9-42e0-a544-ede94612086b'), UUID('1644b397-0e5e-4b34-9b24-0f51fa5bce68'), UUID('e6f9e5c8-6ae4-4924-80ca-09ab7fc0dcbe'), UUID('9bfc40d4-71ff-403b-aead-f11c63440a0d'), UUID('6bf6cc95-164e-4baf-bf28-84fa3f276d84')]}\n",
      "ERROR:root:Error in {'repo': 'scrapy/scrapy', 'vulnerability_id': 'GHSA-rmqv-7v3j-mr7p', 'commit': '809bfac4890f75fc73607318a04d2ccba71b3d9f', 'commit_source': 'github', 'cwe_id': ['CWE-409'], 'patch': ['@@ -1,31 +1,41 @@\\n import struct\\n from gzip import GzipFile\\n from io import BytesIO\\n-from typing import List\\n \\n from scrapy.http import Response\\n \\n+from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\\n \\n-def gunzip(data: bytes) -> bytes:\\n+\\n+def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\\n     \"\"\"Gunzip the given data and return as much data as possible.\\n \\n     This is resilient to CRC checksum errors.\\n     \"\"\"\\n     f = GzipFile(fileobj=BytesIO(data))\\n-    output_list: List[bytes] = []\\n+    output_stream = BytesIO()\\n     chunk = b\".\"\\n+    decompressed_size = 0\\n     while chunk:\\n         try:\\n-            chunk = f.read1(8196)\\n-            output_list.append(chunk)\\n+            chunk = f.read1(_CHUNK_SIZE)\\n         except (OSError, EOFError, struct.error):\\n             # complete only if there is some data, otherwise re-raise\\n             # see issue 87 about catching struct.error\\n-            # some pages are quite small so output_list is empty\\n-            if output_list:\\n+            # some pages are quite small so output_stream is empty\\n+            if output_stream.getbuffer().nbytes > 0:\\n                 break\\n             raise\\n-    return b\"\".join(output_list)\\n+        decompressed_size += len(chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n \\n \\n def gzip_magic_number(response: Response) -> bool:', '@@ -1,94 +0,0 @@\\n-\"\"\" This module implements the DecompressionMiddleware which tries to recognise\\n-and extract the potentially compressed responses that may arrive.\\n-\"\"\"\\n-\\n-import bz2\\n-import gzip\\n-import logging\\n-import tarfile\\n-import zipfile\\n-from io import BytesIO\\n-from tempfile import mktemp\\n-from warnings import warn\\n-\\n-from scrapy.exceptions import ScrapyDeprecationWarning\\n-from scrapy.responsetypes import responsetypes\\n-\\n-warn(\\n-    \"scrapy.downloadermiddlewares.decompression is deprecated\",\\n-    ScrapyDeprecationWarning,\\n-    stacklevel=2,\\n-)\\n-\\n-\\n-logger = logging.getLogger(__name__)\\n-\\n-\\n-class DecompressionMiddleware:\\n-    \"\"\"This middleware tries to recognise and extract the possibly compressed\\n-    responses that may arrive.\"\"\"\\n-\\n-    def __init__(self):\\n-        self._formats = {\\n-            \"tar\": self._is_tar,\\n-            \"zip\": self._is_zip,\\n-            \"gz\": self._is_gzip,\\n-            \"bz2\": self._is_bzip2,\\n-        }\\n-\\n-    def _is_tar(self, response):\\n-        archive = BytesIO(response.body)\\n-        try:\\n-            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\\n-        except tarfile.ReadError:\\n-            return\\n-\\n-        body = tar_file.extractfile(tar_file.members[0]).read()\\n-        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def _is_zip(self, response):\\n-        archive = BytesIO(response.body)\\n-        try:\\n-            zip_file = zipfile.ZipFile(archive)\\n-        except zipfile.BadZipFile:\\n-            return\\n-\\n-        namelist = zip_file.namelist()\\n-        body = zip_file.read(namelist[0])\\n-        respcls = responsetypes.from_args(filename=namelist[0], body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def _is_gzip(self, response):\\n-        archive = BytesIO(response.body)\\n-        try:\\n-            body = gzip.GzipFile(fileobj=archive).read()\\n-        except OSError:\\n-            return\\n-\\n-        respcls = responsetypes.from_args(body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def _is_bzip2(self, response):\\n-        try:\\n-            body = bz2.decompress(response.body)\\n-        except OSError:\\n-            return\\n-\\n-        respcls = responsetypes.from_args(body=body)\\n-        return response.replace(body=body, cls=respcls)\\n-\\n-    def process_response(self, request, response, spider):\\n-        if not response.body:\\n-            return response\\n-\\n-        for fmt, func in self._formats.items():\\n-            new_response = func(response)\\n-            if new_response:\\n-                logger.debug(\\n-                    \"Decompressed response with format: %(responsefmt)s\",\\n-                    {\"responsefmt\": fmt},\\n-                    extra={\"spider\": spider},\\n-                )\\n-                return new_response\\n-        return response', '@@ -1,11 +1,19 @@\\n import logging\\n import re\\n+from typing import TYPE_CHECKING, Any\\n \\n from scrapy.http import Request, XmlResponse\\n from scrapy.spiders import Spider\\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\\n from scrapy.utils.gz import gunzip, gzip_magic_number\\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\\n \\n+if TYPE_CHECKING:\\n+    # typing.Self requires Python 3.11\\n+    from typing_extensions import Self\\n+\\n+    from scrapy.crawler import Crawler\\n+\\n logger = logging.getLogger(__name__)\\n \\n \\n@@ -14,6 +22,19 @@ class SitemapSpider(Spider):\\n     sitemap_rules = [(\"\", \"parse\")]\\n     sitemap_follow = [\"\"]\\n     sitemap_alternate_links = False\\n+    _max_size: int\\n+    _warn_size: int\\n+\\n+    @classmethod\\n+    def from_crawler(cls, crawler: \"Crawler\", *args: Any, **kwargs: Any) -> \"Self\":\\n+        spider = super().from_crawler(crawler, *args, **kwargs)\\n+        spider._max_size = getattr(\\n+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        )\\n+        spider._warn_size = getattr(\\n+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        )\\n+        return spider\\n \\n     def __init__(self, *a, **kw):\\n         super().__init__(*a, **kw)\\n@@ -71,7 +92,19 @@ def _get_sitemap_body(self, response):\\n         if isinstance(response, XmlResponse):\\n             return response.body\\n         if gzip_magic_number(response):\\n-            return gunzip(response.body)\\n+            uncompressed_size = len(response.body)\\n+            max_size = response.meta.get(\"download_maxsize\", self._max_size)\\n+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\\n+            try:\\n+                body = gunzip(response.body, max_size=max_size)\\n+            except _DecompressionMaxSizeExceeded:\\n+                return None\\n+            if uncompressed_size < warn_size <= len(body):\\n+                logger.warning(\\n+                    f\"{response} body size after decompression ({len(body)} B) \"\\n+                    f\"is larger than the download warning size ({warn_size} B).\"\\n+                )\\n+            return body\\n         # actual gzipped sitemap files are decompressed above ;\\n         # if we are here (response body is not gzipped)\\n         # and have a response for .xml.gz,', '@@ -0,0 +1,94 @@\\n+import zlib\\n+from io import BytesIO\\n+\\n+try:\\n+    import brotli\\n+except ImportError:\\n+    pass\\n+\\n+try:\\n+    import zstandard\\n+except ImportError:\\n+    pass\\n+\\n+\\n+_CHUNK_SIZE = 65536  # 64 KiB\\n+\\n+\\n+class _DecompressionMaxSizeExceeded(ValueError):\\n+    pass\\n+\\n+\\n+def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\\n+    decompressor = zlib.decompressobj()\\n+    raw_decompressor = zlib.decompressobj(wbits=-15)\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        try:\\n+            output_chunk = decompressor.decompress(input_chunk)\\n+        except zlib.error:\\n+            if decompressor != raw_decompressor:\\n+                # ugly hack to work with raw deflate content that may\\n+                # be sent by microsoft servers. For more information, see:\\n+                # http://carsten.codimi.de/gzip.yaws/\\n+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n+                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n+                decompressor = raw_decompressor\\n+                output_chunk = decompressor.decompress(input_chunk)\\n+            else:\\n+                raise\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\\n+    decompressor = brotli.Decompressor()\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        output_chunk = decompressor.process(input_chunk)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\\n+    decompressor = zstandard.ZstdDecompressor()\\n+    stream_reader = decompressor.stream_reader(BytesIO(data))\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        output_chunk = stream_reader.read(_CHUNK_SIZE)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                f\"The number of bytes decompressed so far \"\\n+                f\"({decompressed_size} B) exceed the specified maximum \"\\n+                f\"({max_size} B).\"\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()', '@@ -1,53 +1,78 @@\\n-import io\\n import warnings\\n-import zlib\\n+from logging import getLogger\\n \\n-from scrapy.exceptions import NotConfigured\\n+from scrapy import signals\\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\\n from scrapy.http import Response, TextResponse\\n from scrapy.responsetypes import responsetypes\\n+from scrapy.utils._compression import (\\n+    _DecompressionMaxSizeExceeded,\\n+    _inflate,\\n+    _unbrotli,\\n+    _unzstd,\\n+)\\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\\n from scrapy.utils.gz import gunzip\\n \\n+logger = getLogger(__name__)\\n+\\n ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\\n \\n try:\\n-    import brotli\\n-\\n-    ACCEPTED_ENCODINGS.append(b\"br\")\\n+    import brotli  # noqa: F401\\n except ImportError:\\n     pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"br\")\\n \\n try:\\n-    import zstandard\\n-\\n-    ACCEPTED_ENCODINGS.append(b\"zstd\")\\n+    import zstandard  # noqa: F401\\n except ImportError:\\n     pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\\n \\n \\n class HttpCompressionMiddleware:\\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\\n     sent/received from web sites\"\"\"\\n \\n-    def __init__(self, stats=None):\\n-        self.stats = stats\\n+    def __init__(self, stats=None, *, crawler=None):\\n+        if not crawler:\\n+            self.stats = stats\\n+            self._max_size = 1073741824\\n+            self._warn_size = 33554432\\n+            return\\n+        self.stats = crawler.stats\\n+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        crawler.signals.connect(self.open_spider, signals.spider_opened)\\n \\n     @classmethod\\n     def from_crawler(cls, crawler):\\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\\n             raise NotConfigured\\n         try:\\n-            return cls(stats=crawler.stats)\\n+            return cls(crawler=crawler)\\n         except TypeError:\\n             warnings.warn(\\n                 \"HttpCompressionMiddleware subclasses must either modify \"\\n-                \"their \\'__init__\\' method to support a \\'stats\\' parameter or \"\\n-                \"reimplement the \\'from_crawler\\' method.\",\\n+                \"their \\'__init__\\' method to support a \\'crawler\\' parameter or \"\\n+                \"reimplement their \\'from_crawler\\' method.\",\\n                 ScrapyDeprecationWarning,\\n             )\\n-            result = cls()\\n-            result.stats = crawler.stats\\n-            return result\\n+            mw = cls()\\n+            mw.stats = crawler.stats\\n+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+            crawler.signals.connect(mw.open_spider, signals.spider_opened)\\n+            return mw\\n+\\n+    def open_spider(self, spider):\\n+        if hasattr(spider, \"download_maxsize\"):\\n+            self._max_size = spider.download_maxsize\\n+        if hasattr(spider, \"download_warnsize\"):\\n+            self._warn_size = spider.download_warnsize\\n \\n     def process_request(self, request, spider):\\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\\n@@ -59,7 +84,24 @@ def process_response(self, request, response, spider):\\n             content_encoding = response.headers.getlist(\"Content-Encoding\")\\n             if content_encoding:\\n                 encoding = content_encoding.pop()\\n-                decoded_body = self._decode(response.body, encoding.lower())\\n+                max_size = request.meta.get(\"download_maxsize\", self._max_size)\\n+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\\n+                try:\\n+                    decoded_body = self._decode(\\n+                        response.body, encoding.lower(), max_size\\n+                    )\\n+                except _DecompressionMaxSizeExceeded:\\n+                    raise IgnoreRequest(\\n+                        f\"Ignored response {response} because its body \"\\n+                        f\"({len(response.body)} B) exceeded DOWNLOAD_MAXSIZE \"\\n+                        f\"({max_size} B) during decompression.\"\\n+                    )\\n+                if len(response.body) < warn_size <= len(decoded_body):\\n+                    logger.warning(\\n+                        f\"{response} body size after decompression \"\\n+                        f\"({len(decoded_body)} B) is larger than the \"\\n+                        f\"download warning size ({warn_size} B).\"\\n+                    )\\n                 if self.stats:\\n                     self.stats.inc_value(\\n                         \"httpcompression/response_bytes\",\\n@@ -83,25 +125,13 @@ def process_response(self, request, response, spider):\\n \\n         return response\\n \\n-    def _decode(self, body, encoding):\\n+    def _decode(self, body, encoding, max_size):\\n         if encoding == b\"gzip\" or encoding == b\"x-gzip\":\\n-            body = gunzip(body)\\n-\\n+            return gunzip(body, max_size=max_size)\\n         if encoding == b\"deflate\":\\n-            try:\\n-                body = zlib.decompress(body)\\n-            except zlib.error:\\n-                # ugly hack to work with raw deflate content that may\\n-                # be sent by microsoft servers. For more information, see:\\n-                # http://carsten.codimi.de/gzip.yaws/\\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n-                body = zlib.decompress(body, -15)\\n+            return _inflate(body, max_size=max_size)\\n         if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\\n-            body = brotli.decompress(body)\\n+            return _unbrotli(body, max_size=max_size)\\n         if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\\n-            # Using its streaming API since its simple API could handle only cases\\n-            # where there is content size data embedded in the frame\\n-            reader = zstandard.ZstdDecompressor().stream_reader(io.BytesIO(body))\\n-            body = reader.read()\\n+            return _unzstd(body, max_size=max_size)\\n         return body'], 'file': ['scrapy/utils/gz.py', 'scrapy/downloadermiddlewares/decompression.py', 'scrapy/spiders/sitemap.py', 'scrapy/utils/_compression.py', 'scrapy/downloadermiddlewares/httpcompression.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('cc5ca44a-dac9-42e0-a544-ede94612086b'), UUID('1644b397-0e5e-4b34-9b24-0f51fa5bce68'), UUID('e6f9e5c8-6ae4-4924-80ca-09ab7fc0dcbe'), UUID('9bfc40d4-71ff-403b-aead-f11c63440a0d'), UUID('6bf6cc95-164e-4baf-bf28-84fa3f276d84')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:0:     import brotli                \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:0:     import brotli  # noqa: F401\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1081/1800 [08:18<04:15,  2.81it/s]ERROR:src.process_code_changes:Error processing commit 5d886f3d06373d2c3292911bac0772bcd5102343\n",
      "ERROR:src.process_code_changes:{'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-2228', 'commit': '5d886f3d06373d2c3292911bac0772bcd5102343', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -167,6 +167,7 @@ def newaccount(request):\\n \\n @login_required\\n @permission_required(\"core.change_user\")\\n+@require_http_methods([\"POST\"])\\n @reversion.create_revision()\\n def editaccount(request, pk):\\n     account = User.objects.get(pk=pk)', '@@ -214,6 +214,7 @@ def newdomain(request):\\n \\n @login_required\\n @permission_required(\"admin.view_domain\")\\n+@require_http_methods([\"POST\"])\\n @reversion.create_revision()\\n def editdomain(request, dom_id):\\n     \"\"\"Edit domain view.\"\"\"'], 'file': ['modoboa/admin/views/identity.py', 'modoboa/admin/views/domain.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('3897fff8-1589-4fa5-ad6d-9e2acb470337'), UUID('c27a763c-5d74-4fb4-8c11-97f764641379')]}\n",
      "ERROR:root:Error in {'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-2228', 'commit': '5d886f3d06373d2c3292911bac0772bcd5102343', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -167,6 +167,7 @@ def newaccount(request):\\n \\n @login_required\\n @permission_required(\"core.change_user\")\\n+@require_http_methods([\"POST\"])\\n @reversion.create_revision()\\n def editaccount(request, pk):\\n     account = User.objects.get(pk=pk)', '@@ -214,6 +214,7 @@ def newdomain(request):\\n \\n @login_required\\n @permission_required(\"admin.view_domain\")\\n+@require_http_methods([\"POST\"])\\n @reversion.create_revision()\\n def editdomain(request, dom_id):\\n     \"\"\"Edit domain view.\"\"\"'], 'file': ['modoboa/admin/views/identity.py', 'modoboa/admin/views/domain.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('3897fff8-1589-4fa5-ad6d-9e2acb470337'), UUID('c27a763c-5d74-4fb4-8c11-97f764641379')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1088/1800 [08:19<03:11,  3.72it/s]ERROR:src.process_code_changes:Error processing commit 7d8f51791c4949404d78f1083f465b7b4c8e954b\n",
      "ERROR:src.process_code_changes:{'repo': 'apache/qpid-python', 'vulnerability_id': '2013-1909', 'commit': '7d8f51791c4949404d78f1083f465b7b4c8e954b', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': ['@@ -122,6 +122,10 @@ def __init__(self, url=None, **options):\\n     @param ssl_certfile: file with client\\'s public (eventually priv+pub) key (PEM format)\\n     @type ssl_trustfile: str\\n     @param ssl_trustfile: file trusted certificates to validate the server\\n+    @type ssl_skip_hostname_check: bool\\n+    @param ssl_skip_hostname_check: disable verification of hostname in\\n+    certificate. Use with caution - disabling hostname checking leaves you\\n+    vulnerable to Man-in-the-Middle attacks.\\n \\n     @rtype: Connection\\n     @return: a disconnected Connection\\n@@ -170,6 +174,7 @@ def __init__(self, url=None, **options):\\n     self.ssl_keyfile = options.get(\"ssl_keyfile\", None)\\n     self.ssl_certfile = options.get(\"ssl_certfile\", None)\\n     self.ssl_trustfile = options.get(\"ssl_trustfile\", None)\\n+    self.ssl_skip_hostname_check = options.get(\"ssl_skip_hostname_check\", False)\\n     self.client_properties = options.get(\"client_properties\", {})\\n \\n     self.options = options', '@@ -53,7 +53,7 @@ def close(self):\\n \\n try:\\n   from ssl import wrap_socket, SSLError, SSL_ERROR_WANT_READ, \\\\\\n-      SSL_ERROR_WANT_WRITE\\n+      SSL_ERROR_WANT_WRITE, CERT_REQUIRED, CERT_NONE\\n except ImportError:\\n \\n   ## try the older python SSL api:\\n@@ -69,6 +69,15 @@ def __init__(self, conn, host, port):\\n       ssl_certfile = conn.ssl_certfile\\n       if ssl_certfile and not ssl_keyfile:\\n         ssl_keyfile = ssl_certfile\\n+\\n+      # this version of SSL does NOT perform certificate validation.  If the\\n+      # connection has been configured with CA certs (via ssl_trustfile), then\\n+      # the application expects the certificate to be validated against the\\n+      # supplied CA certs. Since this version cannot validate, the peer cannot\\n+      # be trusted.\\n+      if conn.ssl_trustfile:\\n+        raise SSLError(\"This version of Python does not support verification of the peer\\'s certificate.\")\\n+\\n       self.ssl = ssl(self.socket, keyfile=ssl_keyfile, certfile=ssl_certfile)\\n       self.socket.setblocking(1)\\n \\n@@ -95,7 +104,39 @@ class tls(SocketTransport):\\n \\n     def __init__(self, conn, host, port):\\n       SocketTransport.__init__(self, conn, host, port)\\n-      self.tls = wrap_socket(self.socket, keyfile=conn.ssl_keyfile, certfile=conn.ssl_certfile, ca_certs=conn.ssl_trustfile)\\n+      if conn.ssl_trustfile:\\n+        validate = CERT_REQUIRED\\n+      else:\\n+        validate = CERT_NONE\\n+\\n+      self.tls = wrap_socket(self.socket, keyfile=conn.ssl_keyfile,\\n+                             certfile=conn.ssl_certfile,\\n+                             ca_certs=conn.ssl_trustfile,\\n+                             cert_reqs=validate)\\n+\\n+      if validate == CERT_REQUIRED and not conn.ssl_skip_hostname_check:\\n+        match_found = False\\n+        peer_cert = self.tls.getpeercert()\\n+        if peer_cert:\\n+          peer_names = []\\n+          if \\'subjectAltName\\' in peer_cert:\\n+            for san in peer_cert[\\'subjectAltName\\']:\\n+              if san[0] == \\'DNS\\':\\n+                peer_names.append(san[1].lower())\\n+          if \\'subject\\' in peer_cert:\\n+            for sub in peer_cert[\\'subject\\']:\\n+              while isinstance(sub, tuple) and isinstance(sub[0],tuple):\\n+                sub = sub[0]   # why the extra level of indirection???\\n+              if sub[0] == \\'commonName\\':\\n+                peer_names.append(sub[1].lower())\\n+          for pattern in peer_names:\\n+            if _match_dns_pattern( host.lower(), pattern ):\\n+              #print \"Match found %s\" % pattern\\n+              match_found = True\\n+              break\\n+        if not match_found:\\n+          raise SSLError(\"Connection hostname \\'%s\\' does not match names from peer certificate: %s\" % (host, peer_names))\\n+\\n       self.socket.setblocking(0)\\n       self.state = None\\n \\n@@ -146,5 +187,31 @@ def close(self):\\n       # this closes the underlying socket\\n       self.tls.close()\\n \\n+  def _match_dns_pattern( hostname, pattern ):\\n+    \"\"\" For checking the hostnames provided by the peer\\'s certificate\\n+    \"\"\"\\n+    if pattern.find(\"*\") == -1:\\n+      return hostname == pattern\\n+\\n+    # DNS wildcarded pattern - see RFC2818\\n+    h_labels = hostname.split(\".\")\\n+    p_labels = pattern.split(\".\")\\n+\\n+    while h_labels and p_labels:\\n+      if p_labels[0].find(\"*\") == -1:\\n+        if p_labels[0] != h_labels[0]:\\n+          return False\\n+      else:\\n+        p = p_labels[0].split(\"*\")\\n+        if not h_labels[0].startswith(p[0]):\\n+          return False\\n+        if not h_labels[0].endswith(p[1]):\\n+          return False\\n+      h_labels.pop(0)\\n+      p_labels.pop(0)\\n+\\n+    return not h_labels and not p_labels\\n+\\n+\\n   TRANSPORTS[\"ssl\"] = tls\\n   TRANSPORTS[\"tcp+tls\"] = tls'], 'file': ['qpid/messaging/endpoints.py', 'qpid/messaging/transports.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('76570d73-f660-4e29-824c-7146765c23f6'), UUID('b31bc0b5-972e-4f91-8791-d9d03cf249b7')]}\n",
      "ERROR:root:Error in {'repo': 'apache/qpid-python', 'vulnerability_id': '2013-1909', 'commit': '7d8f51791c4949404d78f1083f465b7b4c8e954b', 'commit_source': 'github', 'cwe_id': ['CWE-20'], 'patch': ['@@ -122,6 +122,10 @@ def __init__(self, url=None, **options):\\n     @param ssl_certfile: file with client\\'s public (eventually priv+pub) key (PEM format)\\n     @type ssl_trustfile: str\\n     @param ssl_trustfile: file trusted certificates to validate the server\\n+    @type ssl_skip_hostname_check: bool\\n+    @param ssl_skip_hostname_check: disable verification of hostname in\\n+    certificate. Use with caution - disabling hostname checking leaves you\\n+    vulnerable to Man-in-the-Middle attacks.\\n \\n     @rtype: Connection\\n     @return: a disconnected Connection\\n@@ -170,6 +174,7 @@ def __init__(self, url=None, **options):\\n     self.ssl_keyfile = options.get(\"ssl_keyfile\", None)\\n     self.ssl_certfile = options.get(\"ssl_certfile\", None)\\n     self.ssl_trustfile = options.get(\"ssl_trustfile\", None)\\n+    self.ssl_skip_hostname_check = options.get(\"ssl_skip_hostname_check\", False)\\n     self.client_properties = options.get(\"client_properties\", {})\\n \\n     self.options = options', '@@ -53,7 +53,7 @@ def close(self):\\n \\n try:\\n   from ssl import wrap_socket, SSLError, SSL_ERROR_WANT_READ, \\\\\\n-      SSL_ERROR_WANT_WRITE\\n+      SSL_ERROR_WANT_WRITE, CERT_REQUIRED, CERT_NONE\\n except ImportError:\\n \\n   ## try the older python SSL api:\\n@@ -69,6 +69,15 @@ def __init__(self, conn, host, port):\\n       ssl_certfile = conn.ssl_certfile\\n       if ssl_certfile and not ssl_keyfile:\\n         ssl_keyfile = ssl_certfile\\n+\\n+      # this version of SSL does NOT perform certificate validation.  If the\\n+      # connection has been configured with CA certs (via ssl_trustfile), then\\n+      # the application expects the certificate to be validated against the\\n+      # supplied CA certs. Since this version cannot validate, the peer cannot\\n+      # be trusted.\\n+      if conn.ssl_trustfile:\\n+        raise SSLError(\"This version of Python does not support verification of the peer\\'s certificate.\")\\n+\\n       self.ssl = ssl(self.socket, keyfile=ssl_keyfile, certfile=ssl_certfile)\\n       self.socket.setblocking(1)\\n \\n@@ -95,7 +104,39 @@ class tls(SocketTransport):\\n \\n     def __init__(self, conn, host, port):\\n       SocketTransport.__init__(self, conn, host, port)\\n-      self.tls = wrap_socket(self.socket, keyfile=conn.ssl_keyfile, certfile=conn.ssl_certfile, ca_certs=conn.ssl_trustfile)\\n+      if conn.ssl_trustfile:\\n+        validate = CERT_REQUIRED\\n+      else:\\n+        validate = CERT_NONE\\n+\\n+      self.tls = wrap_socket(self.socket, keyfile=conn.ssl_keyfile,\\n+                             certfile=conn.ssl_certfile,\\n+                             ca_certs=conn.ssl_trustfile,\\n+                             cert_reqs=validate)\\n+\\n+      if validate == CERT_REQUIRED and not conn.ssl_skip_hostname_check:\\n+        match_found = False\\n+        peer_cert = self.tls.getpeercert()\\n+        if peer_cert:\\n+          peer_names = []\\n+          if \\'subjectAltName\\' in peer_cert:\\n+            for san in peer_cert[\\'subjectAltName\\']:\\n+              if san[0] == \\'DNS\\':\\n+                peer_names.append(san[1].lower())\\n+          if \\'subject\\' in peer_cert:\\n+            for sub in peer_cert[\\'subject\\']:\\n+              while isinstance(sub, tuple) and isinstance(sub[0],tuple):\\n+                sub = sub[0]   # why the extra level of indirection???\\n+              if sub[0] == \\'commonName\\':\\n+                peer_names.append(sub[1].lower())\\n+          for pattern in peer_names:\\n+            if _match_dns_pattern( host.lower(), pattern ):\\n+              #print \"Match found %s\" % pattern\\n+              match_found = True\\n+              break\\n+        if not match_found:\\n+          raise SSLError(\"Connection hostname \\'%s\\' does not match names from peer certificate: %s\" % (host, peer_names))\\n+\\n       self.socket.setblocking(0)\\n       self.state = None\\n \\n@@ -146,5 +187,31 @@ def close(self):\\n       # this closes the underlying socket\\n       self.tls.close()\\n \\n+  def _match_dns_pattern( hostname, pattern ):\\n+    \"\"\" For checking the hostnames provided by the peer\\'s certificate\\n+    \"\"\"\\n+    if pattern.find(\"*\") == -1:\\n+      return hostname == pattern\\n+\\n+    # DNS wildcarded pattern - see RFC2818\\n+    h_labels = hostname.split(\".\")\\n+    p_labels = pattern.split(\".\")\\n+\\n+    while h_labels and p_labels:\\n+      if p_labels[0].find(\"*\") == -1:\\n+        if p_labels[0] != h_labels[0]:\\n+          return False\\n+      else:\\n+        p = p_labels[0].split(\"*\")\\n+        if not h_labels[0].startswith(p[0]):\\n+          return False\\n+        if not h_labels[0].endswith(p[1]):\\n+          return False\\n+      h_labels.pop(0)\\n+      p_labels.pop(0)\\n+\\n+    return not h_labels and not p_labels\\n+\\n+\\n   TRANSPORTS[\"ssl\"] = tls\\n   TRANSPORTS[\"tcp+tls\"] = tls'], 'file': ['qpid/messaging/endpoints.py', 'qpid/messaging/transports.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('76570d73-f660-4e29-824c-7146765c23f6'), UUID('b31bc0b5-972e-4f91-8791-d9d03cf249b7')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 193, in get_changes\n",
      "    _get_changes_lines_units(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/get_changes_lines_units.py\", line 229, in _get_changes_lines_units\n",
      "    line_context = script.get_context(fix_line)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/helpers.py\", line 487, in wrapper\n",
      "    return func(self, line, column, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/__init__.py\", line 494, in get_context\n",
      "    context = module_context.create_context(leaf)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 288, in create_context\n",
      "    return from_scope_node(scope_node, is_nested=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 258, in from_scope_node\n",
      "    return self.create_value(scope_node).as_context()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 236, in create_value\n",
      "    func = value.FunctionValue.from_context(parent_context, node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/function.py\", line 145, in from_context\n",
      "    overloaded_funcs = list(_find_overload_functions(context, tree_node))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/function.py\", line 443, in _find_overload_functions\n",
      "    filter = ParserTreeFilter(\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 138, in __init__\n",
      "    super().__init__(parent_context, node_context)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 100, in __init__\n",
      "    self._parso_cache_node = get_parso_cache_node(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/parser_utils.py\", line 287, in get_parso_cache_node\n",
      "    return parser_cache[grammar._hashed][path]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: PosixPath('/Users/somen/repos/apache/qpid-python/qpid/messaging/transports.py')\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1090/1800 [08:20<03:17,  3.59it/s]ERROR:src.process_code_changes:Error processing commit 32e27712f0f71fdec646add20cd78b4ce75acfce\n",
      "ERROR:src.process_code_changes:{'repo': 'janeczku/calibre-web', 'vulnerability_id': '2021-25964', 'commit': '32e27712f0f71fdec646add20cd78b4ce75acfce', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -120,9 +120,8 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n         </p>\\n         </div>\\n       {% endif %}\\n-\\n       {% if entry.series|length > 0 %}\\n-        <p>{{_(\"Book %(index)s of %(range)s\", index=entry.series_index|formatfloat(2), range=(\"<a href=\\'\" + url_for(\\'web.books_list\\', data=\\'series\\', sort_param=\\'stored\\', book_id=entry.series[0].id) + \"\\'>\" + entry.series[0].name + \"</a>\")|safe) }}</p>\\n+          <p>{{_(\"Book %(index)s of %(range)s\", index=entry.series_index | formatfloat(2), range=(url_for(\\'web.books_list\\', data=\\'series\\', sort_param=\\'stored\\', book_id=entry.series[0].id)|escapedlink(entry.series[0].name))|safe)}}</p>\\n \\n       {% endif %}\\n ', '@@ -26,15 +26,14 @@\\n import json\\n from shutil import copyfile\\n from uuid import uuid4\\n+from lxml.html.clean import clean_html\\n \\n # Improve this to check if scholarly is available in a global way, like other pythonic libraries\\n-have_scholar = True\\n try:\\n     from scholarly import scholarly\\n+    have_scholar = True\\n except ImportError:\\n     have_scholar = False\\n-    pass\\n-\\n \\n from babel import Locale as LC\\n from babel.core import UnknownLocaleError\\n@@ -57,6 +56,8 @@\\n     pass  # We\\'re not using Python 3\\n \\n \\n+\\n+\\n editbook = Blueprint(\\'editbook\\', __name__)\\n log = logger.create()\\n \\n@@ -459,9 +460,11 @@ def edit_book_series_index(series_index, book):\\n # Handle book comments/description\\n def edit_book_comments(comments, book):\\n     modif_date = False\\n+    if comments:\\n+        comments = clean_html(comments)\\n     if len(book.comments):\\n         if book.comments[0].text != comments:\\n-            book.comments[0].text = comments\\n+            book.comments[0].text = clean_html(comments)\\n             modif_date = True\\n     else:\\n         if comments:\\n@@ -515,6 +518,8 @@ def edit_cc_data_value(book_id, book, c, to_save, cc_db_value, cc_string):\\n         to_save[cc_string] = 1 if to_save[cc_string] == \\'True\\' else 0\\n     elif c.datatype == \\'comments\\':\\n         to_save[cc_string] = Markup(to_save[cc_string]).unescape()\\n+        if to_save[cc_string]:\\n+            to_save[cc_string] = clean_html(to_save[cc_string])\\n     elif c.datatype == \\'datetime\\':\\n         try:\\n             to_save[cc_string] = datetime.strptime(to_save[cc_string], \"%Y-%m-%d\")', '@@ -63,7 +63,7 @@ <h2 class=\"random-books\">{{_(\\'Discover (Random Books)\\')}}</h2>\\n </div>\\n {% endif %}\\n <div class=\"discover load-more\">\\n-  <h2 class=\"{{title}}\">{{_(title)}}</h2>\\n+  <h2 class=\"{{title}}\">{{title}}</h2>\\n     <div class=\"filterheader hidden-xs hidden-sm\">\\n       <a data-toggle=\"tooltip\" title=\"{{_(\\'Sort according to book date, newest first\\')}}\" id=\"new\" class=\"btn btn-primary\" href=\"{{url_for(\\'web.books_list\\', data=page, book_id=id, sort_param=\\'new\\')}}\"><span class=\"glyphicon glyphicon-book\"></span> <span class=\"glyphicon glyphicon-calendar\"></span><span class=\"glyphicon glyphicon-sort-by-order\"></span></a>\\n       <a data-toggle=\"tooltip\" title=\"{{_(\\'Sort according to book date, oldest first\\')}}\" id=\"old\" class=\"btn btn-primary\" href=\"{{url_for(\\'web.books_list\\', data=page, book_id=id, sort_param=\\'old\\')}}\"><span class=\"glyphicon glyphicon-book\"></span> <span class=\"glyphicon glyphicon-calendar\"></span><span class=\"glyphicon glyphicon-sort-by-order-alt\"></span></a>', '@@ -28,10 +28,11 @@\\n from uuid import uuid4\\n \\n from babel.dates import format_date\\n+from flask_babel import gettext as _\\n from flask import Blueprint, request, url_for\\n from flask_babel import get_locale\\n from flask_login import current_user\\n-\\n+from markupsafe import escape\\n from . import logger\\n \\n \\n@@ -129,6 +130,10 @@ def formatseriesindex_filter(series_index):\\n             return series_index\\n     return 0\\n \\n+@jinjia.app_template_filter(\\'escapedlink\\')\\n+def escapedlink_filter(url, text):\\n+    return \"<a href=\\'{}\\'>{}</a>\".format(url, escape(text))\\n+\\n @jinjia.app_template_filter(\\'uuidfilter\\')\\n def uuidfilter(var):\\n     return uuid4()', '@@ -73,9 +73,9 @@ def store_user_session():\\n                 user_session = User_Sessions(flask_session.get(\\'_user_id\\', \"\"), flask_session.get(\\'_id\\', \"\"))\\n                 session.add(user_session)\\n                 session.commit()\\n-                log.info(\"Login and store session : \" + flask_session.get(\\'_id\\', \"\"))\\n+                log.debug(\"Login and store session : \" + flask_session.get(\\'_id\\', \"\"))\\n             else:\\n-                log.info(\"Found stored session : \" + flask_session.get(\\'_id\\', \"\"))\\n+                log.debug(\"Found stored session: \" + flask_session.get(\\'_id\\', \"\"))\\n         except (exc.OperationalError, exc.InvalidRequestError) as e:\\n             session.rollback()\\n             log.exception(e)\\n@@ -84,7 +84,7 @@ def store_user_session():\\n \\n def delete_user_session(user_id, session_key):\\n     try:\\n-        log.info(\"Deleted session_key : \" + session_key)\\n+        log.debug(\"Deleted session_key: \" + session_key)\\n         session.query(User_Sessions).filter(User_Sessions.user_id==user_id,\\n                                             User_Sessions.session_key==session_key).delete()\\n         session.commit()'], 'file': ['cps/templates/detail.html', 'cps/editbooks.py', 'cps/templates/index.html', 'cps/jinjia.py', 'cps/ub.py'], 'language': ['HTML', 'Python', 'HTML', 'Python', 'Python'], 'temp_id': [UUID('e2edbe40-2d4a-41af-a4ba-df9a14d06da0'), UUID('cf60aa91-211e-48f9-afb6-aa7a011c3fb1'), UUID('1e458261-d30d-4eb5-91ad-97c1cacc1a33'), UUID('402375b7-23af-4355-b614-4743d429272c'), UUID('757719cd-9ddc-456f-93ae-5e04004d57d1')]}\n",
      "ERROR:root:Error in {'repo': 'janeczku/calibre-web', 'vulnerability_id': '2021-25964', 'commit': '32e27712f0f71fdec646add20cd78b4ce75acfce', 'commit_source': 'github', 'cwe_id': ['CWE-79'], 'patch': ['@@ -120,9 +120,8 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n         </p>\\n         </div>\\n       {% endif %}\\n-\\n       {% if entry.series|length > 0 %}\\n-        <p>{{_(\"Book %(index)s of %(range)s\", index=entry.series_index|formatfloat(2), range=(\"<a href=\\'\" + url_for(\\'web.books_list\\', data=\\'series\\', sort_param=\\'stored\\', book_id=entry.series[0].id) + \"\\'>\" + entry.series[0].name + \"</a>\")|safe) }}</p>\\n+          <p>{{_(\"Book %(index)s of %(range)s\", index=entry.series_index | formatfloat(2), range=(url_for(\\'web.books_list\\', data=\\'series\\', sort_param=\\'stored\\', book_id=entry.series[0].id)|escapedlink(entry.series[0].name))|safe)}}</p>\\n \\n       {% endif %}\\n ', '@@ -26,15 +26,14 @@\\n import json\\n from shutil import copyfile\\n from uuid import uuid4\\n+from lxml.html.clean import clean_html\\n \\n # Improve this to check if scholarly is available in a global way, like other pythonic libraries\\n-have_scholar = True\\n try:\\n     from scholarly import scholarly\\n+    have_scholar = True\\n except ImportError:\\n     have_scholar = False\\n-    pass\\n-\\n \\n from babel import Locale as LC\\n from babel.core import UnknownLocaleError\\n@@ -57,6 +56,8 @@\\n     pass  # We\\'re not using Python 3\\n \\n \\n+\\n+\\n editbook = Blueprint(\\'editbook\\', __name__)\\n log = logger.create()\\n \\n@@ -459,9 +460,11 @@ def edit_book_series_index(series_index, book):\\n # Handle book comments/description\\n def edit_book_comments(comments, book):\\n     modif_date = False\\n+    if comments:\\n+        comments = clean_html(comments)\\n     if len(book.comments):\\n         if book.comments[0].text != comments:\\n-            book.comments[0].text = comments\\n+            book.comments[0].text = clean_html(comments)\\n             modif_date = True\\n     else:\\n         if comments:\\n@@ -515,6 +518,8 @@ def edit_cc_data_value(book_id, book, c, to_save, cc_db_value, cc_string):\\n         to_save[cc_string] = 1 if to_save[cc_string] == \\'True\\' else 0\\n     elif c.datatype == \\'comments\\':\\n         to_save[cc_string] = Markup(to_save[cc_string]).unescape()\\n+        if to_save[cc_string]:\\n+            to_save[cc_string] = clean_html(to_save[cc_string])\\n     elif c.datatype == \\'datetime\\':\\n         try:\\n             to_save[cc_string] = datetime.strptime(to_save[cc_string], \"%Y-%m-%d\")', '@@ -63,7 +63,7 @@ <h2 class=\"random-books\">{{_(\\'Discover (Random Books)\\')}}</h2>\\n </div>\\n {% endif %}\\n <div class=\"discover load-more\">\\n-  <h2 class=\"{{title}}\">{{_(title)}}</h2>\\n+  <h2 class=\"{{title}}\">{{title}}</h2>\\n     <div class=\"filterheader hidden-xs hidden-sm\">\\n       <a data-toggle=\"tooltip\" title=\"{{_(\\'Sort according to book date, newest first\\')}}\" id=\"new\" class=\"btn btn-primary\" href=\"{{url_for(\\'web.books_list\\', data=page, book_id=id, sort_param=\\'new\\')}}\"><span class=\"glyphicon glyphicon-book\"></span> <span class=\"glyphicon glyphicon-calendar\"></span><span class=\"glyphicon glyphicon-sort-by-order\"></span></a>\\n       <a data-toggle=\"tooltip\" title=\"{{_(\\'Sort according to book date, oldest first\\')}}\" id=\"old\" class=\"btn btn-primary\" href=\"{{url_for(\\'web.books_list\\', data=page, book_id=id, sort_param=\\'old\\')}}\"><span class=\"glyphicon glyphicon-book\"></span> <span class=\"glyphicon glyphicon-calendar\"></span><span class=\"glyphicon glyphicon-sort-by-order-alt\"></span></a>', '@@ -28,10 +28,11 @@\\n from uuid import uuid4\\n \\n from babel.dates import format_date\\n+from flask_babel import gettext as _\\n from flask import Blueprint, request, url_for\\n from flask_babel import get_locale\\n from flask_login import current_user\\n-\\n+from markupsafe import escape\\n from . import logger\\n \\n \\n@@ -129,6 +130,10 @@ def formatseriesindex_filter(series_index):\\n             return series_index\\n     return 0\\n \\n+@jinjia.app_template_filter(\\'escapedlink\\')\\n+def escapedlink_filter(url, text):\\n+    return \"<a href=\\'{}\\'>{}</a>\".format(url, escape(text))\\n+\\n @jinjia.app_template_filter(\\'uuidfilter\\')\\n def uuidfilter(var):\\n     return uuid4()', '@@ -73,9 +73,9 @@ def store_user_session():\\n                 user_session = User_Sessions(flask_session.get(\\'_user_id\\', \"\"), flask_session.get(\\'_id\\', \"\"))\\n                 session.add(user_session)\\n                 session.commit()\\n-                log.info(\"Login and store session : \" + flask_session.get(\\'_id\\', \"\"))\\n+                log.debug(\"Login and store session : \" + flask_session.get(\\'_id\\', \"\"))\\n             else:\\n-                log.info(\"Found stored session : \" + flask_session.get(\\'_id\\', \"\"))\\n+                log.debug(\"Found stored session: \" + flask_session.get(\\'_id\\', \"\"))\\n         except (exc.OperationalError, exc.InvalidRequestError) as e:\\n             session.rollback()\\n             log.exception(e)\\n@@ -84,7 +84,7 @@ def store_user_session():\\n \\n def delete_user_session(user_id, session_key):\\n     try:\\n-        log.info(\"Deleted session_key : \" + session_key)\\n+        log.debug(\"Deleted session_key: \" + session_key)\\n         session.query(User_Sessions).filter(User_Sessions.user_id==user_id,\\n                                             User_Sessions.session_key==session_key).delete()\\n         session.commit()'], 'file': ['cps/templates/detail.html', 'cps/editbooks.py', 'cps/templates/index.html', 'cps/jinjia.py', 'cps/ub.py'], 'language': ['HTML', 'Python', 'HTML', 'Python', 'Python'], 'temp_id': [UUID('e2edbe40-2d4a-41af-a4ba-df9a14d06da0'), UUID('cf60aa91-211e-48f9-afb6-aa7a011c3fb1'), UUID('1e458261-d30d-4eb5-91ad-97c1cacc1a33'), UUID('402375b7-23af-4355-b614-4743d429272c'), UUID('757719cd-9ddc-456f-93ae-5e04004d57d1')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     have_scholar = True\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     have_scholar = True\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1110/1800 [08:20<01:27,  7.90it/s]ERROR:src.process_code_changes:Error processing commit 6e9ee210548f6d3210704cac302cfc7cdb239765\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-4723', 'commit': '6e9ee210548f6d3210704cac302cfc7cdb239765', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': [\"@@ -150,7 +150,10 @@ def check_ratelimit(\\n         cherrypy.request.app._ratelimit_datastore = datastore\\n \\n     # If user is authenticated, use the username else use the ip address\\n-    token = (request.login or request.remote.ip) + '.' + (scope or request.path_info)\\n+    identifier = request.remote.ip\\n+    if hasattr(cherrypy.serving, 'session') and cherrypy.serving.session.get('_cp_username', None):\\n+        identifier = cherrypy.serving.session.get('_cp_username', None)\\n+    token = identifier + '.' + (scope or request.path_info)\\n \\n     # Get hits count using datastore.\\n     hits = datastore.get_and_increment(token, delay, hit)\", \"@@ -107,6 +107,7 @@ def validate(self, extra_validators=None):\\n \\n class PagePrefMfa(Controller):\\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def default(self, action=None, **kwargs):\\n         form = MfaToggleForm(obj=self.app.currentuser)\\n         if form.is_submitted():\", \"@@ -471,7 +471,7 @@ def get_parser():\\n         metavar='LIMIT',\\n         type=int,\\n         default=20,\\n-        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\\n+        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API. default: 20 requests / hour',\\n     )\\n \\n     parser.add(\"], 'file': ['rdiffweb/tools/ratelimit.py', 'rdiffweb/controller/page_pref_mfa.py', 'rdiffweb/core/config.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('a63902c4-bec2-4801-a4fa-8b89d50ebbc3'), UUID('081d8b1f-7803-4c2b-8784-b8c64a9c6334'), UUID('52c161e8-7ce6-4a4f-b270-8dfa1b99ad23')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2022-4723', 'commit': '6e9ee210548f6d3210704cac302cfc7cdb239765', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': [\"@@ -150,7 +150,10 @@ def check_ratelimit(\\n         cherrypy.request.app._ratelimit_datastore = datastore\\n \\n     # If user is authenticated, use the username else use the ip address\\n-    token = (request.login or request.remote.ip) + '.' + (scope or request.path_info)\\n+    identifier = request.remote.ip\\n+    if hasattr(cherrypy.serving, 'session') and cherrypy.serving.session.get('_cp_username', None):\\n+        identifier = cherrypy.serving.session.get('_cp_username', None)\\n+    token = identifier + '.' + (scope or request.path_info)\\n \\n     # Get hits count using datastore.\\n     hits = datastore.get_and_increment(token, delay, hit)\", \"@@ -107,6 +107,7 @@ def validate(self, extra_validators=None):\\n \\n class PagePrefMfa(Controller):\\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def default(self, action=None, **kwargs):\\n         form = MfaToggleForm(obj=self.app.currentuser)\\n         if form.is_submitted():\", \"@@ -471,7 +471,7 @@ def get_parser():\\n         metavar='LIMIT',\\n         type=int,\\n         default=20,\\n-        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API.',\\n+        help='maximum number of requests per hour that can be made on sensitive endpoints. When this limit is reached, an HTTP 429 message is returned to the user or the user is logged out. This security measure is used to limit brute force attacks on the login page and the RESTful API. default: 20 requests / hour',\\n     )\\n \\n     parser.add(\"], 'file': ['rdiffweb/tools/ratelimit.py', 'rdiffweb/controller/page_pref_mfa.py', 'rdiffweb/core/config.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('a63902c4-bec2-4801-a4fa-8b89d50ebbc3'), UUID('081d8b1f-7803-4c2b-8784-b8c64a9c6334'), UUID('52c161e8-7ce6-4a4f-b270-8dfa1b99ad23')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1117/1800 [08:21<01:21,  8.35it/s]ERROR:src.process_code_changes:Error processing commit 82666df1e60c45dd6aa533b01a392f015d32f755\n",
      "ERROR:src.process_code_changes:{'repo': 'hap-wi/roxy-wi', 'vulnerability_id': '2022-31137', 'commit': '82666df1e60c45dd6aa533b01a392f015d32f755', 'commit_source': 'github', 'cwe_id': ['CWE-78'], 'patch': ['@@ -61,10 +61,7 @@\\n         print(e)\\n \\n if form.getvalue(\\'getcert\\') is not None and serv is not None:\\n-    cert_id = form.getvalue(\\'getcert\\')\\n-    if funct.checkAjaxInput(cert_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    cert_id = funct.checkAjaxInput(form.getvalue(\\'getcert\\'))\\n \\n     cert_path = sql.get_setting(\\'cert_path\\')\\n     commands = [\"openssl x509 -in \" + cert_path + \"/\" + cert_id + \" -text\"]\\n@@ -74,10 +71,8 @@\\n         print(\\'error: Cannot connect to the server \\' + e.args[0])\\n \\n if form.getvalue(\\'delcert\\') is not None and serv is not None:\\n-    if funct.checkAjaxInput(cert_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    cert_id = form.getvalue(\\'delcert\\')\\n+    cert_id = funct.checkAjaxInput(cert_id)\\n     cert_path = sql.get_setting(\\'cert_path\\')\\n     commands = [\"sudo rm -f \" + cert_path + \"/\" + cert_id]\\n     try:\\n@@ -96,10 +91,7 @@\\n     if form.getvalue(\\'ssl_name\\') is None:\\n         print(\\'error: Please enter a desired name\\')\\n     else:\\n-        name = form.getvalue(\\'ssl_name\\')\\n-        if funct.checkAjaxInput(name):\\n-            print(\\'error: Nice try\\')\\n-            sys.exit()\\n+        name = funct.checkAjaxInput(form.getvalue(\\'ssl_name\\'))\\n \\n     try:\\n         with open(name, \"w\") as ssl_cert:\\n@@ -132,10 +124,7 @@\\n \\n if form.getvalue(\\'ipbackend\\') is not None and form.getvalue(\\'backend_server\\') is None:\\n     haproxy_sock_port = int(sql.get_setting(\\'haproxy_sock_port\\'))\\n-    backend = form.getvalue(\\'ipbackend\\')\\n-    if funct.checkAjaxInput(backend):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    backend = funct.checkAjaxInput(form.getvalue(\\'ipbackend\\'))\\n     cmd = \\'echo \"show servers state\"|nc %s %s |grep \"%s\" |awk \\\\\\'{print $4}\\\\\\'\\' % (serv, haproxy_sock_port, backend)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     for i in output:\\n@@ -146,23 +135,18 @@\\n \\n if form.getvalue(\\'ipbackend\\') is not None and form.getvalue(\\'backend_server\\') is not None:\\n     haproxy_sock_port = int(sql.get_setting(\\'haproxy_sock_port\\'))\\n-    backend = form.getvalue(\\'ipbackend\\')\\n-    backend_server = form.getvalue(\\'backend_server\\')\\n-    if funct.checkAjaxInput(backend) or funct.checkAjaxInput(backend_server):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    backend = funct.checkAjaxInput(form.getvalue(\\'ipbackend\\'))\\n+    backend_server = funct.checkAjaxInput(form.getvalue(\\'backend_server\\'))\\n     cmd = \\'echo \"show servers state\"|nc %s %s |grep \"%s\" |grep \"%s\" |awk \\\\\\'{print $5\":\"$19}\\\\\\' |head -1\\' % (serv, haproxy_sock_port, backend, backend_server)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     print(output[0])\\n \\n if form.getvalue(\\'backend_ip\\') is not None:\\n-    backend_backend = form.getvalue(\\'backend_backend\\')\\n-    backend_server = form.getvalue(\\'backend_server\\')\\n-    backend_ip = form.getvalue(\\'backend_ip\\')\\n-    backend_port = form.getvalue(\\'backend_port\\')\\n-    if any((funct.checkAjaxInput(backend_backend), funct.checkAjaxInput(backend_server), funct.checkAjaxInput(backend_ip), funct.checkAjaxInput(backend_port))):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    backend_backend = funct.checkAjaxInput(form.getvalue(\\'backend_backend\\'))\\n+    backend_server = funct.checkAjaxInput(form.getvalue(\\'backend_server\\'))\\n+    backend_ip = funct.checkAjaxInput(form.getvalue(\\'backend_ip\\'))\\n+    backend_port = funct.checkAjaxInput(form.getvalue(\\'backend_port\\'))\\n+\\n     if form.getvalue(\\'backend_ip\\') is None:\\n         print(\\'error: Backend IP must be IP and not 0\\')\\n         sys.exit()\\n@@ -211,19 +195,13 @@\\n         stderr = funct.master_slave_upload_and_restart(serv, cfg, just_save=\\'save\\')\\n \\n if form.getvalue(\\'maxconn_select\\') is not None:\\n-    serv = form.getvalue(\\'maxconn_select\\')\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'maxconn_select\\'))\\n     funct.get_backends_from_config(serv, backends=\\'frontend\\')\\n \\n if form.getvalue(\\'maxconn_frontend\\') is not None:\\n-    frontend = form.getvalue(\\'maxconn_frontend\\')\\n-    maxconn = form.getvalue(\\'maxconn_int\\')\\n+    frontend = funct.checkAjaxInput(form.getvalue(\\'maxconn_frontend\\'))\\n+    maxconn = funct.checkAjaxInput(form.getvalue(\\'maxconn_int\\'))\\n \\n-    if funct.checkAjaxInput(frontend) or funct.checkAjaxInput(maxconn):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n     if form.getvalue(\\'maxconn_int\\') is None:\\n         print(\\'error: Maxconn must be integer and not 0\\')\\n         sys.exit()\\n@@ -297,12 +275,8 @@\\n \\n if form.getvalue(\\'ip_for_delete\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-    ip = form.getvalue(\\'ip_for_delete\\')\\n-    table = form.getvalue(\\'table_for_delete\\')\\n-\\n-    if funct.checkAjaxInput(ip) or funct.checkAjaxInput(table):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    ip = funct.checkAjaxInput(form.getvalue(\\'ip_for_delete\\'))\\n+    table = funct.checkAjaxInput(form.getvalue(\\'table_for_delete\\'))\\n \\n     cmd = \\'echo \"clear table %s key %s\" |nc %s %s\\' % (table, ip, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n@@ -311,11 +285,7 @@\\n \\n if form.getvalue(\\'table_for_clear\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-    table = form.getvalue(\\'table_for_clear\\')\\n-\\n-    if funct.checkAjaxInput(table):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    table = funct.checkAjaxInput(form.getvalue(\\'table_for_clear\\'))\\n \\n     cmd = \\'echo \"clear table %s \" |nc %s %s\\' % (table, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n@@ -334,12 +304,8 @@\\n     env = Environment(loader=FileSystemLoader(\\'templates/\\'), autoescape=True,\\n                       extensions=[\\'jinja2.ext.loopcontrols\\', \\'jinja2.ext.do\\'], trim_blocks=True, lstrip_blocks=True)\\n     template = env.get_template(\\'ajax/list.html\\')\\n-    list_id = form.getvalue(\\'list_select_id\\')\\n-    list_name = form.getvalue(\\'list_select_name\\')\\n-\\n-    if funct.checkAjaxInput(list_id) or funct.checkAjaxInput(list_name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    list_id = funct.checkAjaxInput(form.getvalue(\\'list_select_id\\'))\\n+    list_name = funct.checkAjaxInput(form.getvalue(\\'list_select_name\\'))\\n \\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n     cmd = \\'echo \"show acl #%s\"|nc %s %s\\' % (list_id, serv, haproxy_sock_port)\\n@@ -351,17 +317,12 @@\\n if form.getvalue(\\'list_id_for_delete\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n     lists_path = sql.get_setting(\\'lists_path\\')\\n-    lib_path = funct.get_config_var(\\'main\\', \\'lib_path\\')\\n-    ip_id = form.getvalue(\\'list_ip_id_for_delete\\')\\n-    ip = form.getvalue(\\'list_ip_for_delete\\')\\n-    list_id = form.getvalue(\\'list_id_for_delete\\')\\n-    list_name = form.getvalue(\\'list_name\\')\\n-    user_group = funct.get_user_group(id=1)\\n-\\n-    if any((funct.checkAjaxInput(ip_id), funct.checkAjaxInput(ip), funct.checkAjaxInput(list_id), funct.checkAjaxInput(list_name))):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    lib_path = funct.checkAjaxInput(funct.get_config_var(\\'main\\', \\'lib_path\\'))\\n+    ip_id = funct.checkAjaxInput(form.getvalue(\\'list_ip_id_for_delete\\'))\\n+    ip = funct.checkAjaxInput(form.getvalue(\\'list_ip_for_delete\\'))\\n+    list_id = funct.checkAjaxInput(form.getvalue(\\'list_id_for_delete\\'))\\n+    list_name = funct.checkAjaxInput(form.getvalue(\\'list_name\\'))\\n+    user_group = funct.checkAjaxInput(funct.get_user_group(id=1))\\n     cmd = \"sed -i \\'s!%s$!!\\' %s/%s/%s/%s\" % (ip, lib_path, lists_path, user_group, list_name)\\n     cmd1 = \"sed -i \\'/^$/d\\' %s/%s/%s/%s\" % (lib_path, lists_path, user_group, list_name)\\n     output, stderr = funct.subprocess_execute(cmd)\\n@@ -392,14 +353,9 @@\\n     ip = form.getvalue(\\'list_ip_for_add\\')\\n     ip = ip.strip()\\n     ip = funct.is_ip_or_dns(ip)\\n-    list_id = form.getvalue(\\'list_id_for_add\\')\\n-    list_name = form.getvalue(\\'list_name\\')\\n-    user_group = funct.get_user_group(id=1)\\n-\\n-    if any((funct.checkAjaxInput(lists_path), funct.checkAjaxInput(list_id), funct.checkAjaxInput(list_name))):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    list_id = funct.checkAjaxInput(form.getvalue(\\'list_id_for_add\\'))\\n+    list_name = funct.checkAjaxInput(form.getvalue(\\'list_name\\'))\\n+    user_group = funct.checkAjaxInput(funct.get_user_group(id=1))\\n     cmd = \\'echo \"add acl #%s %s\" |nc %s %s\\' % (list_id, ip, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     if output[0]:\\n@@ -423,15 +379,7 @@\\n \\n     env = Environment(loader=FileSystemLoader(\\'templates\\'), autoescape=True,\\n                       extensions=[\\'jinja2.ext.loopcontrols\\', \\'jinja2.ext.do\\'], trim_blocks=True, lstrip_blocks=True)\\n-    serv = form.getvalue(\\'sessions_select\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'sessions_select\\'))\\n \\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n \\n@@ -444,16 +392,11 @@\\n     print(template)\\n \\n if form.getvalue(\\'sessions_select_show\\') is not None:\\n-    serv = form.getvalue(\\'sessions_select_show\\')\\n-    sess_id = form.getvalue(\\'sessions_select_id\\')\\n-\\n-    if funct.checkAjaxInput(serv) or funct.checkAjaxInput(sess_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'sessions_select_show\\'))\\n+    sess_id = funct.checkAjaxInput(form.getvalue(\\'sessions_select_id\\'))\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-\\n     cmd = \\'echo \"show sess %s\" |nc %s %s\\' % (sess_id, serv, haproxy_sock_port)\\n+\\n     output, stderr = funct.subprocess_execute(cmd)\\n \\n     if stderr:\\n@@ -464,16 +407,7 @@\\n \\n if form.getvalue(\\'session_delete_id\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-    sess_id = form.getvalue(\\'session_delete_id\\')\\n-\\n-    if funct.checkAjaxInput(sess_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(sess_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    sess_id = funct.checkAjaxInput(form.getvalue(\\'session_delete_id\\'))\\n     cmd = \\'echo \"shutdown session %s\" |nc %s %s\\' % (sess_id, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     if output[0] != \\'\\':\\n@@ -597,15 +531,7 @@\\n     print(\"success: Apache has been %s\" % action)\\n \\n if form.getvalue(\\'action_service\\') is not None:\\n-    action = form.getvalue(\\'action_service\\')\\n-\\n-    if funct.checkAjaxInput(action):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(action):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    action = funct.checkAjaxInput(form.getvalue(\\'action_service\\'))\\n \\n     if action not in (\\'start\\', \\'stop\\', \\'restart\\'):\\n         print(\\'error: wrong action\\')\\n@@ -1233,12 +1159,8 @@ async def get_runner_overviewServers(**kwargs):\\n if form.getvalue(\\'servaction\\') is not None:\\n     server_state_file = sql.get_setting(\\'server_state_file\\')\\n     haproxy_sock = sql.get_setting(\\'haproxy_sock\\')\\n-    enable = form.getvalue(\\'servaction\\')\\n-    backend = form.getvalue(\\'servbackend\\')\\n-\\n-    if funct.checkAjaxInput(enable) or funct.checkAjaxInput(backend):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    enable = funct.checkAjaxInput(form.getvalue(\\'servaction\\'))\\n+    backend = funct.checkAjaxInput(form.getvalue(\\'servbackend\\'))\\n \\n     cmd = \\'echo \"{} {}\" |sudo socat stdio {}\\'.format(enable, backend, haproxy_sock)\\n \\n@@ -1281,12 +1203,8 @@ async def get_runner_overviewServers(**kwargs):\\n if serv is not None and form.getvalue(\\'right\\') is not None:\\n     from jinja2 import Environment, FileSystemLoader\\n \\n-    left = form.getvalue(\\'left\\')\\n-    right = form.getvalue(\\'right\\')\\n-\\n-    if funct.checkAjaxInput(left) or funct.checkAjaxInput(right):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    left = funct.checkAjaxInput(form.getvalue(\\'left\\'))\\n+    right = funct.checkAjaxInput(form.getvalue(\\'right\\'))\\n \\n     if form.getvalue(\\'service\\') == \\'nginx\\':\\n         configs_dir = funct.get_config_var(\\'configs\\', \\'nginx_save_configs_dir\\')\\n@@ -2554,15 +2472,7 @@ async def get_runner_overviewServers(**kwargs):\\n \\n if form.getvalue(\\'sshdel\\') is not None:\\n     lib_path = funct.get_config_var(\\'main\\', \\'lib_path\\')\\n-    sshdel = form.getvalue(\\'sshdel\\')\\n-\\n-    if funct.checkAjaxInput(sshdel):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(sshdel):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    sshdel = funct.checkAjaxInput(form.getvalue(\\'sshdel\\'))\\n \\n     for sshs in sql.select_ssh(id=sshdel):\\n         ssh_enable = sshs.enable\\n@@ -2612,11 +2522,7 @@ async def get_runner_overviewServers(**kwargs):\\n     import paramiko\\n \\n     user_group = funct.get_user_group()\\n-    name = form.getvalue(\\'name\\')\\n-\\n-    if funct.checkAjaxInput(name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    name = funct.checkAjaxInput(form.getvalue(\\'name\\'))\\n \\n     try:\\n         key = paramiko.pkey.load_private_key(form.getvalue(\\'ssh_cert\\'))\\n@@ -2913,11 +2819,7 @@ async def get_runner_overviewServers(**kwargs):\\n         funct.logging(\\'SMON\\', \\' Has been update the server \\' + ip + \\' to SMON \\', haproxywi=1, login=1)\\n \\n if form.getvalue(\\'showBytes\\') is not None:\\n-    serv = form.getvalue(\\'showBytes\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'showBytes\\'))\\n \\n     port = sql.get_setting(\\'haproxy_sock_port\\')\\n     bin_bout = []\\n@@ -2970,12 +2872,8 @@ async def get_runner_overviewServers(**kwargs):\\n         print(\\'error: cannot connect to Nginx stat page\\')\\n \\n if form.getvalue(\\'waf_rule_id\\'):\\n-    enable = form.getvalue(\\'waf_en\\')\\n-    rule_id = form.getvalue(\\'waf_rule_id\\')\\n-\\n-    if funct.checkAjaxInput(enable) or funct.checkAjaxInput(rule_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    enable = funct.checkAjaxInput(form.getvalue(\\'waf_en\\'))\\n+    rule_id = funct.checkAjaxInput(form.getvalue(\\'waf_rule_id\\'))\\n \\n     haproxy_path = sql.get_setting(\\'haproxy_dir\\')\\n     rule_file = sql.select_waf_rule_by_id(rule_id)\\n@@ -3051,15 +2949,7 @@ async def get_runner_overviewServers(**kwargs):\\n     os.system(\"rm -f %s\" % script)\\n \\n if form.getvalue(\\'uploadovpn\\'):\\n-    name = form.getvalue(\\'ovpnname\\')\\n-\\n-    if funct.checkAjaxInput(name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    name = funct.checkAjaxInput(form.getvalue(\\'ovpnname\\'))\\n \\n     ovpn_file = os.path.dirname(\\'/tmp/\\') + \"/\" + name + \\'.ovpn\\'\\n \\n@@ -3087,11 +2977,7 @@ async def get_runner_overviewServers(**kwargs):\\n     funct.logging(\"localhost\", \" has been uploaded a new ovpn file %s\" % ovpn_file, haproxywi=1, login=1)\\n \\n if form.getvalue(\\'openvpndel\\') is not None:\\n-    openvpndel = form.getvalue(\\'openvpndel\\')\\n-\\n-    if funct.checkAjaxInput(openvpndel):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    openvpndel = funct.checkAjaxInput(form.getvalue(\\'openvpndel\\'))\\n \\n     cmd = \\'sudo openvpn3 config-remove --config /tmp/%s.ovpn --force\\' % openvpndel\\n     try:\\n@@ -3103,12 +2989,8 @@ async def get_runner_overviewServers(**kwargs):\\n         funct.logging(\\'localhost\\', e.args[0], haproxywi=1)\\n \\n if form.getvalue(\\'actionvpn\\') is not None:\\n-    openvpn = form.getvalue(\\'openvpnprofile\\')\\n-    action = form.getvalue(\\'actionvpn\\')\\n-\\n-    if funct.checkAjaxInput(openvpn) or funct.checkAjaxInput(action):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    openvpn = funct.checkAjaxInput(form.getvalue(\\'openvpnprofile\\'))\\n+    action = funct.checkAjaxInput(form.getvalue(\\'actionvpn\\'))\\n \\n     if action == \\'start\\':\\n         cmd = \\'sudo openvpn3 session-start --config /tmp/%s.ovpn\\' % openvpn\\n@@ -3125,12 +3007,7 @@ async def get_runner_overviewServers(**kwargs):\\n         funct.logging(\\'localhost\\', e.args[0], haproxywi=1)\\n \\n if form.getvalue(\\'scan_ports\\') is not None:\\n-    serv_id = form.getvalue(\\'scan_ports\\')\\n-\\n-    if funct.checkAjaxInput(serv_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    serv_id = funct.checkAjaxInput(form.getvalue(\\'scan_ports\\'))\\n     server = sql.select_servers(id=serv_id)\\n     ip = \\'\\'\\n \\n@@ -3154,11 +3031,7 @@ async def get_runner_overviewServers(**kwargs):\\n         print(template)\\n \\n if form.getvalue(\\'viewFirewallRules\\') is not None:\\n-    serv = form.getvalue(\\'viewFirewallRules\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'viewFirewallRules\\'))\\n \\n     cmd = [\"sudo iptables -L INPUT -n --line-numbers|sed \\'s/  */ /g\\'|grep -v -E \\'Chain|target\\'\"]\\n     cmd1 = [\"sudo iptables -L IN_public_allow -n --line-numbers|sed \\'s/  */ /g\\'|grep -v -E \\'Chain|target\\'\"]\\n@@ -3186,11 +3059,6 @@ async def get_runner_overviewServers(**kwargs):\\n \\n if form.getvalue(\\'geoipserv\\') is not None:\\n     serv = form.getvalue(\\'geoipserv\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n     haproxy_dir = sql.get_setting(\\'haproxy_dir\\')\\n \\n     cmd = [\"ls \" + haproxy_dir + \"/geoip/\"]\\n@@ -4531,12 +4399,8 @@ async def get_runner_overviewServers(**kwargs):\\n     user_uuid = cookie.get(\\'uuid\\')\\n     user_id = sql.get_user_id_by_uuid(user_uuid.value)\\n     user_services = sql.select_user_services(user_id)\\n-    server_id = form.getvalue(\\'server_id\\')\\n-    service = form.getvalue(\\'service\\')\\n-\\n-    if funct.checkAjaxInput(server_id) or funct.checkAjaxInput(service):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    server_id = funct.checkAjaxInput(form.getvalue(\\'server_id\\'))\\n+    service = funct.checkAjaxInput(form.getvalue(\\'service\\'))\\n \\n     if \\'1\\' in user_services:\\n         if service == \\'haproxy\\':'], 'file': ['app/options.py'], 'language': ['Python'], 'temp_id': [UUID('bdd72afe-c943-4b27-8d7d-486118872970')]}\n",
      "ERROR:root:Error in {'repo': 'hap-wi/roxy-wi', 'vulnerability_id': '2022-31137', 'commit': '82666df1e60c45dd6aa533b01a392f015d32f755', 'commit_source': 'github', 'cwe_id': ['CWE-78'], 'patch': ['@@ -61,10 +61,7 @@\\n         print(e)\\n \\n if form.getvalue(\\'getcert\\') is not None and serv is not None:\\n-    cert_id = form.getvalue(\\'getcert\\')\\n-    if funct.checkAjaxInput(cert_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    cert_id = funct.checkAjaxInput(form.getvalue(\\'getcert\\'))\\n \\n     cert_path = sql.get_setting(\\'cert_path\\')\\n     commands = [\"openssl x509 -in \" + cert_path + \"/\" + cert_id + \" -text\"]\\n@@ -74,10 +71,8 @@\\n         print(\\'error: Cannot connect to the server \\' + e.args[0])\\n \\n if form.getvalue(\\'delcert\\') is not None and serv is not None:\\n-    if funct.checkAjaxInput(cert_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    cert_id = form.getvalue(\\'delcert\\')\\n+    cert_id = funct.checkAjaxInput(cert_id)\\n     cert_path = sql.get_setting(\\'cert_path\\')\\n     commands = [\"sudo rm -f \" + cert_path + \"/\" + cert_id]\\n     try:\\n@@ -96,10 +91,7 @@\\n     if form.getvalue(\\'ssl_name\\') is None:\\n         print(\\'error: Please enter a desired name\\')\\n     else:\\n-        name = form.getvalue(\\'ssl_name\\')\\n-        if funct.checkAjaxInput(name):\\n-            print(\\'error: Nice try\\')\\n-            sys.exit()\\n+        name = funct.checkAjaxInput(form.getvalue(\\'ssl_name\\'))\\n \\n     try:\\n         with open(name, \"w\") as ssl_cert:\\n@@ -132,10 +124,7 @@\\n \\n if form.getvalue(\\'ipbackend\\') is not None and form.getvalue(\\'backend_server\\') is None:\\n     haproxy_sock_port = int(sql.get_setting(\\'haproxy_sock_port\\'))\\n-    backend = form.getvalue(\\'ipbackend\\')\\n-    if funct.checkAjaxInput(backend):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    backend = funct.checkAjaxInput(form.getvalue(\\'ipbackend\\'))\\n     cmd = \\'echo \"show servers state\"|nc %s %s |grep \"%s\" |awk \\\\\\'{print $4}\\\\\\'\\' % (serv, haproxy_sock_port, backend)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     for i in output:\\n@@ -146,23 +135,18 @@\\n \\n if form.getvalue(\\'ipbackend\\') is not None and form.getvalue(\\'backend_server\\') is not None:\\n     haproxy_sock_port = int(sql.get_setting(\\'haproxy_sock_port\\'))\\n-    backend = form.getvalue(\\'ipbackend\\')\\n-    backend_server = form.getvalue(\\'backend_server\\')\\n-    if funct.checkAjaxInput(backend) or funct.checkAjaxInput(backend_server):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    backend = funct.checkAjaxInput(form.getvalue(\\'ipbackend\\'))\\n+    backend_server = funct.checkAjaxInput(form.getvalue(\\'backend_server\\'))\\n     cmd = \\'echo \"show servers state\"|nc %s %s |grep \"%s\" |grep \"%s\" |awk \\\\\\'{print $5\":\"$19}\\\\\\' |head -1\\' % (serv, haproxy_sock_port, backend, backend_server)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     print(output[0])\\n \\n if form.getvalue(\\'backend_ip\\') is not None:\\n-    backend_backend = form.getvalue(\\'backend_backend\\')\\n-    backend_server = form.getvalue(\\'backend_server\\')\\n-    backend_ip = form.getvalue(\\'backend_ip\\')\\n-    backend_port = form.getvalue(\\'backend_port\\')\\n-    if any((funct.checkAjaxInput(backend_backend), funct.checkAjaxInput(backend_server), funct.checkAjaxInput(backend_ip), funct.checkAjaxInput(backend_port))):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    backend_backend = funct.checkAjaxInput(form.getvalue(\\'backend_backend\\'))\\n+    backend_server = funct.checkAjaxInput(form.getvalue(\\'backend_server\\'))\\n+    backend_ip = funct.checkAjaxInput(form.getvalue(\\'backend_ip\\'))\\n+    backend_port = funct.checkAjaxInput(form.getvalue(\\'backend_port\\'))\\n+\\n     if form.getvalue(\\'backend_ip\\') is None:\\n         print(\\'error: Backend IP must be IP and not 0\\')\\n         sys.exit()\\n@@ -211,19 +195,13 @@\\n         stderr = funct.master_slave_upload_and_restart(serv, cfg, just_save=\\'save\\')\\n \\n if form.getvalue(\\'maxconn_select\\') is not None:\\n-    serv = form.getvalue(\\'maxconn_select\\')\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'maxconn_select\\'))\\n     funct.get_backends_from_config(serv, backends=\\'frontend\\')\\n \\n if form.getvalue(\\'maxconn_frontend\\') is not None:\\n-    frontend = form.getvalue(\\'maxconn_frontend\\')\\n-    maxconn = form.getvalue(\\'maxconn_int\\')\\n+    frontend = funct.checkAjaxInput(form.getvalue(\\'maxconn_frontend\\'))\\n+    maxconn = funct.checkAjaxInput(form.getvalue(\\'maxconn_int\\'))\\n \\n-    if funct.checkAjaxInput(frontend) or funct.checkAjaxInput(maxconn):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n     if form.getvalue(\\'maxconn_int\\') is None:\\n         print(\\'error: Maxconn must be integer and not 0\\')\\n         sys.exit()\\n@@ -297,12 +275,8 @@\\n \\n if form.getvalue(\\'ip_for_delete\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-    ip = form.getvalue(\\'ip_for_delete\\')\\n-    table = form.getvalue(\\'table_for_delete\\')\\n-\\n-    if funct.checkAjaxInput(ip) or funct.checkAjaxInput(table):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    ip = funct.checkAjaxInput(form.getvalue(\\'ip_for_delete\\'))\\n+    table = funct.checkAjaxInput(form.getvalue(\\'table_for_delete\\'))\\n \\n     cmd = \\'echo \"clear table %s key %s\" |nc %s %s\\' % (table, ip, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n@@ -311,11 +285,7 @@\\n \\n if form.getvalue(\\'table_for_clear\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-    table = form.getvalue(\\'table_for_clear\\')\\n-\\n-    if funct.checkAjaxInput(table):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    table = funct.checkAjaxInput(form.getvalue(\\'table_for_clear\\'))\\n \\n     cmd = \\'echo \"clear table %s \" |nc %s %s\\' % (table, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n@@ -334,12 +304,8 @@\\n     env = Environment(loader=FileSystemLoader(\\'templates/\\'), autoescape=True,\\n                       extensions=[\\'jinja2.ext.loopcontrols\\', \\'jinja2.ext.do\\'], trim_blocks=True, lstrip_blocks=True)\\n     template = env.get_template(\\'ajax/list.html\\')\\n-    list_id = form.getvalue(\\'list_select_id\\')\\n-    list_name = form.getvalue(\\'list_select_name\\')\\n-\\n-    if funct.checkAjaxInput(list_id) or funct.checkAjaxInput(list_name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    list_id = funct.checkAjaxInput(form.getvalue(\\'list_select_id\\'))\\n+    list_name = funct.checkAjaxInput(form.getvalue(\\'list_select_name\\'))\\n \\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n     cmd = \\'echo \"show acl #%s\"|nc %s %s\\' % (list_id, serv, haproxy_sock_port)\\n@@ -351,17 +317,12 @@\\n if form.getvalue(\\'list_id_for_delete\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n     lists_path = sql.get_setting(\\'lists_path\\')\\n-    lib_path = funct.get_config_var(\\'main\\', \\'lib_path\\')\\n-    ip_id = form.getvalue(\\'list_ip_id_for_delete\\')\\n-    ip = form.getvalue(\\'list_ip_for_delete\\')\\n-    list_id = form.getvalue(\\'list_id_for_delete\\')\\n-    list_name = form.getvalue(\\'list_name\\')\\n-    user_group = funct.get_user_group(id=1)\\n-\\n-    if any((funct.checkAjaxInput(ip_id), funct.checkAjaxInput(ip), funct.checkAjaxInput(list_id), funct.checkAjaxInput(list_name))):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    lib_path = funct.checkAjaxInput(funct.get_config_var(\\'main\\', \\'lib_path\\'))\\n+    ip_id = funct.checkAjaxInput(form.getvalue(\\'list_ip_id_for_delete\\'))\\n+    ip = funct.checkAjaxInput(form.getvalue(\\'list_ip_for_delete\\'))\\n+    list_id = funct.checkAjaxInput(form.getvalue(\\'list_id_for_delete\\'))\\n+    list_name = funct.checkAjaxInput(form.getvalue(\\'list_name\\'))\\n+    user_group = funct.checkAjaxInput(funct.get_user_group(id=1))\\n     cmd = \"sed -i \\'s!%s$!!\\' %s/%s/%s/%s\" % (ip, lib_path, lists_path, user_group, list_name)\\n     cmd1 = \"sed -i \\'/^$/d\\' %s/%s/%s/%s\" % (lib_path, lists_path, user_group, list_name)\\n     output, stderr = funct.subprocess_execute(cmd)\\n@@ -392,14 +353,9 @@\\n     ip = form.getvalue(\\'list_ip_for_add\\')\\n     ip = ip.strip()\\n     ip = funct.is_ip_or_dns(ip)\\n-    list_id = form.getvalue(\\'list_id_for_add\\')\\n-    list_name = form.getvalue(\\'list_name\\')\\n-    user_group = funct.get_user_group(id=1)\\n-\\n-    if any((funct.checkAjaxInput(lists_path), funct.checkAjaxInput(list_id), funct.checkAjaxInput(list_name))):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    list_id = funct.checkAjaxInput(form.getvalue(\\'list_id_for_add\\'))\\n+    list_name = funct.checkAjaxInput(form.getvalue(\\'list_name\\'))\\n+    user_group = funct.checkAjaxInput(funct.get_user_group(id=1))\\n     cmd = \\'echo \"add acl #%s %s\" |nc %s %s\\' % (list_id, ip, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     if output[0]:\\n@@ -423,15 +379,7 @@\\n \\n     env = Environment(loader=FileSystemLoader(\\'templates\\'), autoescape=True,\\n                       extensions=[\\'jinja2.ext.loopcontrols\\', \\'jinja2.ext.do\\'], trim_blocks=True, lstrip_blocks=True)\\n-    serv = form.getvalue(\\'sessions_select\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'sessions_select\\'))\\n \\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n \\n@@ -444,16 +392,11 @@\\n     print(template)\\n \\n if form.getvalue(\\'sessions_select_show\\') is not None:\\n-    serv = form.getvalue(\\'sessions_select_show\\')\\n-    sess_id = form.getvalue(\\'sessions_select_id\\')\\n-\\n-    if funct.checkAjaxInput(serv) or funct.checkAjaxInput(sess_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'sessions_select_show\\'))\\n+    sess_id = funct.checkAjaxInput(form.getvalue(\\'sessions_select_id\\'))\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-\\n     cmd = \\'echo \"show sess %s\" |nc %s %s\\' % (sess_id, serv, haproxy_sock_port)\\n+\\n     output, stderr = funct.subprocess_execute(cmd)\\n \\n     if stderr:\\n@@ -464,16 +407,7 @@\\n \\n if form.getvalue(\\'session_delete_id\\') is not None:\\n     haproxy_sock_port = sql.get_setting(\\'haproxy_sock_port\\')\\n-    sess_id = form.getvalue(\\'session_delete_id\\')\\n-\\n-    if funct.checkAjaxInput(sess_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(sess_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    sess_id = funct.checkAjaxInput(form.getvalue(\\'session_delete_id\\'))\\n     cmd = \\'echo \"shutdown session %s\" |nc %s %s\\' % (sess_id, serv, haproxy_sock_port)\\n     output, stderr = funct.subprocess_execute(cmd)\\n     if output[0] != \\'\\':\\n@@ -597,15 +531,7 @@\\n     print(\"success: Apache has been %s\" % action)\\n \\n if form.getvalue(\\'action_service\\') is not None:\\n-    action = form.getvalue(\\'action_service\\')\\n-\\n-    if funct.checkAjaxInput(action):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(action):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    action = funct.checkAjaxInput(form.getvalue(\\'action_service\\'))\\n \\n     if action not in (\\'start\\', \\'stop\\', \\'restart\\'):\\n         print(\\'error: wrong action\\')\\n@@ -1233,12 +1159,8 @@ async def get_runner_overviewServers(**kwargs):\\n if form.getvalue(\\'servaction\\') is not None:\\n     server_state_file = sql.get_setting(\\'server_state_file\\')\\n     haproxy_sock = sql.get_setting(\\'haproxy_sock\\')\\n-    enable = form.getvalue(\\'servaction\\')\\n-    backend = form.getvalue(\\'servbackend\\')\\n-\\n-    if funct.checkAjaxInput(enable) or funct.checkAjaxInput(backend):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    enable = funct.checkAjaxInput(form.getvalue(\\'servaction\\'))\\n+    backend = funct.checkAjaxInput(form.getvalue(\\'servbackend\\'))\\n \\n     cmd = \\'echo \"{} {}\" |sudo socat stdio {}\\'.format(enable, backend, haproxy_sock)\\n \\n@@ -1281,12 +1203,8 @@ async def get_runner_overviewServers(**kwargs):\\n if serv is not None and form.getvalue(\\'right\\') is not None:\\n     from jinja2 import Environment, FileSystemLoader\\n \\n-    left = form.getvalue(\\'left\\')\\n-    right = form.getvalue(\\'right\\')\\n-\\n-    if funct.checkAjaxInput(left) or funct.checkAjaxInput(right):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    left = funct.checkAjaxInput(form.getvalue(\\'left\\'))\\n+    right = funct.checkAjaxInput(form.getvalue(\\'right\\'))\\n \\n     if form.getvalue(\\'service\\') == \\'nginx\\':\\n         configs_dir = funct.get_config_var(\\'configs\\', \\'nginx_save_configs_dir\\')\\n@@ -2554,15 +2472,7 @@ async def get_runner_overviewServers(**kwargs):\\n \\n if form.getvalue(\\'sshdel\\') is not None:\\n     lib_path = funct.get_config_var(\\'main\\', \\'lib_path\\')\\n-    sshdel = form.getvalue(\\'sshdel\\')\\n-\\n-    if funct.checkAjaxInput(sshdel):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(sshdel):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    sshdel = funct.checkAjaxInput(form.getvalue(\\'sshdel\\'))\\n \\n     for sshs in sql.select_ssh(id=sshdel):\\n         ssh_enable = sshs.enable\\n@@ -2612,11 +2522,7 @@ async def get_runner_overviewServers(**kwargs):\\n     import paramiko\\n \\n     user_group = funct.get_user_group()\\n-    name = form.getvalue(\\'name\\')\\n-\\n-    if funct.checkAjaxInput(name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    name = funct.checkAjaxInput(form.getvalue(\\'name\\'))\\n \\n     try:\\n         key = paramiko.pkey.load_private_key(form.getvalue(\\'ssh_cert\\'))\\n@@ -2913,11 +2819,7 @@ async def get_runner_overviewServers(**kwargs):\\n         funct.logging(\\'SMON\\', \\' Has been update the server \\' + ip + \\' to SMON \\', haproxywi=1, login=1)\\n \\n if form.getvalue(\\'showBytes\\') is not None:\\n-    serv = form.getvalue(\\'showBytes\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'showBytes\\'))\\n \\n     port = sql.get_setting(\\'haproxy_sock_port\\')\\n     bin_bout = []\\n@@ -2970,12 +2872,8 @@ async def get_runner_overviewServers(**kwargs):\\n         print(\\'error: cannot connect to Nginx stat page\\')\\n \\n if form.getvalue(\\'waf_rule_id\\'):\\n-    enable = form.getvalue(\\'waf_en\\')\\n-    rule_id = form.getvalue(\\'waf_rule_id\\')\\n-\\n-    if funct.checkAjaxInput(enable) or funct.checkAjaxInput(rule_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    enable = funct.checkAjaxInput(form.getvalue(\\'waf_en\\'))\\n+    rule_id = funct.checkAjaxInput(form.getvalue(\\'waf_rule_id\\'))\\n \\n     haproxy_path = sql.get_setting(\\'haproxy_dir\\')\\n     rule_file = sql.select_waf_rule_by_id(rule_id)\\n@@ -3051,15 +2949,7 @@ async def get_runner_overviewServers(**kwargs):\\n     os.system(\"rm -f %s\" % script)\\n \\n if form.getvalue(\\'uploadovpn\\'):\\n-    name = form.getvalue(\\'ovpnname\\')\\n-\\n-    if funct.checkAjaxInput(name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n-    if funct.checkAjaxInput(name):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    name = funct.checkAjaxInput(form.getvalue(\\'ovpnname\\'))\\n \\n     ovpn_file = os.path.dirname(\\'/tmp/\\') + \"/\" + name + \\'.ovpn\\'\\n \\n@@ -3087,11 +2977,7 @@ async def get_runner_overviewServers(**kwargs):\\n     funct.logging(\"localhost\", \" has been uploaded a new ovpn file %s\" % ovpn_file, haproxywi=1, login=1)\\n \\n if form.getvalue(\\'openvpndel\\') is not None:\\n-    openvpndel = form.getvalue(\\'openvpndel\\')\\n-\\n-    if funct.checkAjaxInput(openvpndel):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    openvpndel = funct.checkAjaxInput(form.getvalue(\\'openvpndel\\'))\\n \\n     cmd = \\'sudo openvpn3 config-remove --config /tmp/%s.ovpn --force\\' % openvpndel\\n     try:\\n@@ -3103,12 +2989,8 @@ async def get_runner_overviewServers(**kwargs):\\n         funct.logging(\\'localhost\\', e.args[0], haproxywi=1)\\n \\n if form.getvalue(\\'actionvpn\\') is not None:\\n-    openvpn = form.getvalue(\\'openvpnprofile\\')\\n-    action = form.getvalue(\\'actionvpn\\')\\n-\\n-    if funct.checkAjaxInput(openvpn) or funct.checkAjaxInput(action):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    openvpn = funct.checkAjaxInput(form.getvalue(\\'openvpnprofile\\'))\\n+    action = funct.checkAjaxInput(form.getvalue(\\'actionvpn\\'))\\n \\n     if action == \\'start\\':\\n         cmd = \\'sudo openvpn3 session-start --config /tmp/%s.ovpn\\' % openvpn\\n@@ -3125,12 +3007,7 @@ async def get_runner_overviewServers(**kwargs):\\n         funct.logging(\\'localhost\\', e.args[0], haproxywi=1)\\n \\n if form.getvalue(\\'scan_ports\\') is not None:\\n-    serv_id = form.getvalue(\\'scan_ports\\')\\n-\\n-    if funct.checkAjaxInput(serv_id):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n+    serv_id = funct.checkAjaxInput(form.getvalue(\\'scan_ports\\'))\\n     server = sql.select_servers(id=serv_id)\\n     ip = \\'\\'\\n \\n@@ -3154,11 +3031,7 @@ async def get_runner_overviewServers(**kwargs):\\n         print(template)\\n \\n if form.getvalue(\\'viewFirewallRules\\') is not None:\\n-    serv = form.getvalue(\\'viewFirewallRules\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    serv = funct.checkAjaxInput(form.getvalue(\\'viewFirewallRules\\'))\\n \\n     cmd = [\"sudo iptables -L INPUT -n --line-numbers|sed \\'s/  */ /g\\'|grep -v -E \\'Chain|target\\'\"]\\n     cmd1 = [\"sudo iptables -L IN_public_allow -n --line-numbers|sed \\'s/  */ /g\\'|grep -v -E \\'Chain|target\\'\"]\\n@@ -3186,11 +3059,6 @@ async def get_runner_overviewServers(**kwargs):\\n \\n if form.getvalue(\\'geoipserv\\') is not None:\\n     serv = form.getvalue(\\'geoipserv\\')\\n-\\n-    if funct.checkAjaxInput(serv):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n-\\n     haproxy_dir = sql.get_setting(\\'haproxy_dir\\')\\n \\n     cmd = [\"ls \" + haproxy_dir + \"/geoip/\"]\\n@@ -4531,12 +4399,8 @@ async def get_runner_overviewServers(**kwargs):\\n     user_uuid = cookie.get(\\'uuid\\')\\n     user_id = sql.get_user_id_by_uuid(user_uuid.value)\\n     user_services = sql.select_user_services(user_id)\\n-    server_id = form.getvalue(\\'server_id\\')\\n-    service = form.getvalue(\\'service\\')\\n-\\n-    if funct.checkAjaxInput(server_id) or funct.checkAjaxInput(service):\\n-        print(\\'error: Nice try\\')\\n-        sys.exit()\\n+    server_id = funct.checkAjaxInput(form.getvalue(\\'server_id\\'))\\n+    service = funct.checkAjaxInput(form.getvalue(\\'service\\'))\\n \\n     if \\'1\\' in user_services:\\n         if service == \\'haproxy\\':'], 'file': ['app/options.py'], 'language': ['Python'], 'temp_id': [UUID('bdd72afe-c943-4b27-8d7d-486118872970')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     cert_id = form.getvalue('delcert')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     cert_id = form.getvalue('delcert')\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1121/1800 [08:25<03:22,  3.35it/s]ERROR:src.process_code_changes:Error processing commit 58a22260eca40b1a0377daf61ccd8c4dc1440e03\n",
      "ERROR:src.process_code_changes:{'repo': 'Redon-Tech/Roblox-Purchasing-Hub', 'vulnerability_id': '2021-41191', 'commit': '58a22260eca40b1a0377daf61ccd8c4dc1440e03', 'commit_source': 'github', 'cwe_id': ['CWE-116'], 'patch': ['@@ -59,6 +59,7 @@ async def status():\\n \\n \\n @app.route(\"/v1/products\", methods=[\"GET\"])\\n+@require_apikey\\n async def products():\\n     dbresponse = getproducts()\\n     results = {}'], 'file': ['BOT/lib/cogs/website.py'], 'language': ['Python'], 'temp_id': [UUID('447d59cd-001c-4100-a216-4d008eb585e3')]}\n",
      "ERROR:root:Error in {'repo': 'Redon-Tech/Roblox-Purchasing-Hub', 'vulnerability_id': '2021-41191', 'commit': '58a22260eca40b1a0377daf61ccd8c4dc1440e03', 'commit_source': 'github', 'cwe_id': ['CWE-116'], 'patch': ['@@ -59,6 +59,7 @@ async def status():\\n \\n \\n @app.route(\"/v1/products\", methods=[\"GET\"])\\n+@require_apikey\\n async def products():\\n     dbresponse = getproducts()\\n     results = {}'], 'file': ['BOT/lib/cogs/website.py'], 'language': ['Python'], 'temp_id': [UUID('447d59cd-001c-4100-a216-4d008eb585e3')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1123/1800 [08:26<03:21,  3.36it/s]ERROR:src.process_code_changes:Error processing commit 4134aedcff17c977da7717693ed89ce56d54c120\n",
      "ERROR:src.process_code_changes:{'repo': 'zopefoundation/RestrictedPython', 'vulnerability_id': '2023-41039', 'commit': '4134aedcff17c977da7717693ed89ce56d54c120', 'commit_source': 'github', 'cwe_id': ['CWE-74'], 'patch': ['@@ -246,9 +246,11 @@ def safer_getattr(object, name, default=None, getattr=getattr):\\n     http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\\n \\n     \"\"\"\\n-    if isinstance(object, str) and name == \\'format\\':\\n+    if name in (\\'format\\', \\'format_map\\') and (\\n+            isinstance(object, str) or\\n+            (isinstance(object, type) and issubclass(object, str))):\\n         raise NotImplementedError(\\n-            \\'Using format() on a %s is not safe.\\' % object.__class__.__name__)\\n+            \\'Using the format*() methods of `str` is not safe\\')\\n     if name.startswith(\\'_\\'):\\n         raise AttributeError(\\n             \\'\"{name}\" is an invalid attribute name because it \\'', '@@ -18,7 +18,21 @@\\n \\n utility_builtins = {}\\n \\n-utility_builtins[\\'string\\'] = string\\n+\\n+class _AttributeDelegator:\\n+    def __init__(self, mod, *excludes):\\n+        \"\"\"delegate attribute lookups outside *excludes* to module *mod*.\"\"\"\\n+        self.__mod = mod\\n+        self.__excludes = excludes\\n+\\n+    def __getattr__(self, attr):\\n+        if attr in self.__excludes:\\n+            raise NotImplementedError(\\n+                f\"{self.__mod.__name__}.{attr} is not safe\")\\n+        return getattr(self.__mod, attr)\\n+\\n+\\n+utility_builtins[\\'string\\'] = _AttributeDelegator(string, \"Formatter\")\\n utility_builtins[\\'math\\'] = math\\n utility_builtins[\\'random\\'] = random\\n utility_builtins[\\'whrandom\\'] = random'], 'file': ['src/RestrictedPython/Guards.py', 'src/RestrictedPython/Utilities.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('35490ea7-4638-40f0-8f8d-cac304b02f5d'), UUID('37b79b75-9c7f-4e17-9548-d6fce421ca5b')]}\n",
      "ERROR:root:Error in {'repo': 'zopefoundation/RestrictedPython', 'vulnerability_id': '2023-41039', 'commit': '4134aedcff17c977da7717693ed89ce56d54c120', 'commit_source': 'github', 'cwe_id': ['CWE-74'], 'patch': ['@@ -246,9 +246,11 @@ def safer_getattr(object, name, default=None, getattr=getattr):\\n     http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\\n \\n     \"\"\"\\n-    if isinstance(object, str) and name == \\'format\\':\\n+    if name in (\\'format\\', \\'format_map\\') and (\\n+            isinstance(object, str) or\\n+            (isinstance(object, type) and issubclass(object, str))):\\n         raise NotImplementedError(\\n-            \\'Using format() on a %s is not safe.\\' % object.__class__.__name__)\\n+            \\'Using the format*() methods of `str` is not safe\\')\\n     if name.startswith(\\'_\\'):\\n         raise AttributeError(\\n             \\'\"{name}\" is an invalid attribute name because it \\'', '@@ -18,7 +18,21 @@\\n \\n utility_builtins = {}\\n \\n-utility_builtins[\\'string\\'] = string\\n+\\n+class _AttributeDelegator:\\n+    def __init__(self, mod, *excludes):\\n+        \"\"\"delegate attribute lookups outside *excludes* to module *mod*.\"\"\"\\n+        self.__mod = mod\\n+        self.__excludes = excludes\\n+\\n+    def __getattr__(self, attr):\\n+        if attr in self.__excludes:\\n+            raise NotImplementedError(\\n+                f\"{self.__mod.__name__}.{attr} is not safe\")\\n+        return getattr(self.__mod, attr)\\n+\\n+\\n+utility_builtins[\\'string\\'] = _AttributeDelegator(string, \"Formatter\")\\n utility_builtins[\\'math\\'] = math\\n utility_builtins[\\'random\\'] = random\\n utility_builtins[\\'whrandom\\'] = random'], 'file': ['src/RestrictedPython/Guards.py', 'src/RestrictedPython/Utilities.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('35490ea7-4638-40f0-8f8d-cac304b02f5d'), UUID('37b79b75-9c7f-4e17-9548-d6fce421ca5b')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 21:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 21:0: <line number missing in source>\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1138/1800 [08:27<01:35,  6.93it/s]ERROR:src.process_code_changes:Error processing commit 2a3463b7466bc297aede50046da9550d919ec56f\n",
      "ERROR:src.process_code_changes:{'repo': 'mealie-recipes/mealie', 'vulnerability_id': '2024-31994', 'commit': '2a3463b7466bc297aede50046da9550d919ec56f', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -43,7 +43,7 @@ async def create_from_url(url: str, translator: Translator) -> tuple[Recipe, Scr\\n     recipe_data_service = RecipeDataService(new_recipe.id)\\n \\n     try:\\n-        await recipe_data_service.scrape_image(new_recipe.image)\\n+        await recipe_data_service.scrape_image(new_recipe.image)  # type: ignore\\n \\n         if new_recipe.name is None:\\n             new_recipe.name = \"Untitled\"', '@@ -0,0 +1,78 @@\\n+import ipaddress\\n+import logging\\n+import socket\\n+\\n+import httpx\\n+\\n+\\n+class ForcedTimeoutException(Exception):\\n+    \"\"\"\\n+    Raised when a request takes longer than the timeout value.\\n+    \"\"\"\\n+\\n+    ...\\n+\\n+\\n+class InvalidDomainError(Exception):\\n+    \"\"\"\\n+    Raised when a request is made to a local IP address.\\n+    \"\"\"\\n+\\n+    ...\\n+\\n+\\n+class AsyncSafeTransport(httpx.AsyncBaseTransport):\\n+    \"\"\"\\n+    A wrapper around the httpx transport class that enforces a timeout value\\n+    and that the request is not made to a local IP address.\\n+    \"\"\"\\n+\\n+    timeout: int = 15\\n+\\n+    def __init__(self, log: logging.Logger | None = None, **kwargs):\\n+        self.timeout = kwargs.pop(\"timeout\", self.timeout)\\n+        self._wrapper = httpx.AsyncHTTPTransport(**kwargs)\\n+        self._log = log\\n+\\n+    async def handle_async_request(self, request):\\n+        # override timeout value for _all_ requests\\n+        request.extensions[\"timeout\"] = httpx.Timeout(self.timeout, pool=self.timeout).as_dict()\\n+\\n+        # validate the request is not attempting to connect to a local IP\\n+        # This is a security measure to prevent SSRF attacks\\n+\\n+        ip: ipaddress.IPv4Address | ipaddress.IPv6Address | None = None\\n+\\n+        netloc = request.url.netloc.decode()\\n+        if \":\" in netloc:  # Either an IP, or a hostname:port combo\\n+            netloc_parts = netloc.split(\":\")\\n+\\n+            netloc = netloc_parts[0]\\n+\\n+            try:\\n+                ip = ipaddress.ip_address(netloc)\\n+            except ValueError:\\n+                if self._log:\\n+                    self._log.debug(f\"failed to parse ip for {netloc=} falling back to domain resolution\")\\n+                pass\\n+\\n+        # Request is a domain or a hostname.\\n+        if not ip:\\n+            if self._log:\\n+                self._log.debug(f\"resolving IP for domain: {netloc}\")\\n+\\n+            ip_str = socket.gethostbyname(netloc)\\n+            ip = ipaddress.ip_address(ip_str)\\n+\\n+            if self._log:\\n+                self._log.debug(f\"resolved IP for domain: {netloc} -> {ip}\")\\n+\\n+        if ip.is_private:\\n+            if self._log:\\n+                self._log.warning(f\"invalid request on local resource: {request.url} -> {ip}\")\\n+            raise InvalidDomainError(f\"invalid request on local resource: {request.url} -> {ip}\")\\n+\\n+        return await self._wrapper.handle_async_request(request)\\n+\\n+    async def aclose(self):\\n+        await self._wrapper.aclose()', '@@ -47,7 +47,7 @@ class AppSettings(BaseSettings):\\n \\n     GIT_COMMIT_HASH: str = \"unknown\"\\n \\n-    ALLOW_SIGNUP: bool = True\\n+    ALLOW_SIGNUP: bool = False\\n \\n     # ===============================================\\n     # Security Configuration', '@@ -12,6 +12,7 @@\\n \\n from mealie.core.root_logger import get_logger\\n from mealie.lang.providers import Translator\\n+from mealie.pkgs import safehttp\\n from mealie.schema.recipe.recipe import Recipe, RecipeStep\\n from mealie.services.scraper.scraped_extras import ScrapedExtras\\n \\n@@ -31,7 +32,7 @@ async def safe_scrape_html(url: str) -> str:\\n     if the request takes longer than 15 seconds. This is used to mitigate\\n     DDOS attacks from users providing a url with arbitrary large content.\\n     \"\"\"\\n-    async with AsyncClient() as client:\\n+    async with AsyncClient(transport=safehttp.AsyncSafeTransport()) as client:\\n         html_bytes = b\"\"\\n         async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:\\n             start_time = time.time()', '@@ -5,7 +5,8 @@\\n from httpx import AsyncClient, Response\\n from pydantic import UUID4\\n \\n-from mealie.pkgs import img\\n+from mealie.pkgs import img, safehttp\\n+from mealie.pkgs.safehttp.transport import AsyncSafeTransport\\n from mealie.schema.recipe.recipe import Recipe\\n from mealie.services._base_service import BaseService\\n \\n@@ -29,12 +30,14 @@ async def largest_content_len(urls: list[str]) -> tuple[str, int]:\\n     largest_url = \"\"\\n     largest_len = 0\\n \\n+    max_concurrency = 10\\n+\\n     async def do(client: AsyncClient, url: str) -> Response:\\n         return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})\\n \\n-    async with AsyncClient() as client:\\n+    async with AsyncClient(transport=safehttp.AsyncSafeTransport()) as client:\\n         tasks = [do(client, url) for url in urls]\\n-        responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)\\n+        responses: list[Response] = await gather_with_concurrency(max_concurrency, *tasks, ignore_exceptions=True)\\n         for response in responses:\\n             len_int = int(response.headers.get(\"Content-Length\", 0))\\n             if len_int > largest_len:\\n@@ -101,52 +104,39 @@ def write_image(self, file_data: bytes | Path, extension: str, image_dir: Path |\\n \\n         return image_path\\n \\n-    @staticmethod\\n-    def _validate_image_url(url: str) -> bool:\\n-        # sourcery skip: invert-any-all, use-any\\n-        \"\"\"\\n-        Validates that the URL is of an allowed source and restricts certain sources to prevent\\n-        malicious images from being downloaded.\\n-        \"\"\"\\n-        invalid_domains = {\"127.0.0.1\", \"localhost\"}\\n-        for domain in invalid_domains:\\n-            if domain in url:\\n-                return False\\n-\\n-        return True\\n-\\n-    async def scrape_image(self, image_url) -> None:\\n+    async def scrape_image(self, image_url: str | dict[str, str] | list[str]) -> None:\\n         self.logger.info(f\"Image URL: {image_url}\")\\n \\n-        if not self._validate_image_url(image_url):\\n-            self.logger.error(f\"Invalid image URL: {image_url}\")\\n-            raise InvalidDomainError(f\"Invalid domain: {image_url}\")\\n+        image_url_str = \"\"\\n \\n         if isinstance(image_url, str):  # Handles String Types\\n-            pass\\n+            image_url_str = image_url\\n \\n         elif isinstance(image_url, list):  # Handles List Types\\n             # Multiple images have been defined in the schema - usually different resolutions\\n             # Typically would be in smallest->biggest order, but can\\'t be certain so test each.\\n             # \\'Google will pick the best image to display in Search results based on the aspect ratio and resolution.\\'\\n-            image_url, _ = await largest_content_len(image_url)\\n+            image_url_str, _ = await largest_content_len(image_url)\\n \\n         elif isinstance(image_url, dict):  # Handles Dictionary Types\\n             for key in image_url:\\n                 if key == \"url\":\\n-                    image_url = image_url.get(\"url\")\\n+                    image_url_str = image_url.get(\"url\", \"\")\\n+\\n+        if not image_url_str:\\n+            raise ValueError(f\"image url could not be parsed from input: {image_url}\")\\n \\n-        ext = image_url.split(\".\")[-1]\\n+        ext = image_url_str.split(\".\")[-1]\\n \\n         if ext not in img.IMAGE_EXTENSIONS:\\n             ext = \"jpg\"  # Guess the extension\\n \\n         file_name = f\"{str(self.recipe_id)}.{ext}\"\\n         file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)\\n \\n-        async with AsyncClient() as client:\\n+        async with AsyncClient(transport=AsyncSafeTransport()) as client:\\n             try:\\n-                r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})\\n+                r = await client.get(image_url_str, headers={\"User-Agent\": _FIREFOX_UA})\\n             except Exception:\\n                 self.logger.exception(\"Fatal Image Request Exception\")\\n                 return None'], 'file': ['mealie/services/scraper/scraper.py', 'mealie/pkgs/safehttp/transport.py', 'mealie/core/settings/settings.py', 'mealie/services/scraper/scraper_strategies.py', 'mealie/services/recipe/recipe_data_service.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('701e84dc-94f5-4fd9-98d0-53a63d291705'), UUID('09d0405b-58a6-4dca-915b-91df1cfbf2b2'), UUID('25a50fa5-4dbd-48f0-998d-afa4488f20d8'), UUID('aeea29db-11f6-444d-8968-7700f2075a36'), UUID('b936a844-d773-426d-8f20-4a4889afe368')]}\n",
      "ERROR:root:Error in {'repo': 'mealie-recipes/mealie', 'vulnerability_id': '2024-31994', 'commit': '2a3463b7466bc297aede50046da9550d919ec56f', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -43,7 +43,7 @@ async def create_from_url(url: str, translator: Translator) -> tuple[Recipe, Scr\\n     recipe_data_service = RecipeDataService(new_recipe.id)\\n \\n     try:\\n-        await recipe_data_service.scrape_image(new_recipe.image)\\n+        await recipe_data_service.scrape_image(new_recipe.image)  # type: ignore\\n \\n         if new_recipe.name is None:\\n             new_recipe.name = \"Untitled\"', '@@ -0,0 +1,78 @@\\n+import ipaddress\\n+import logging\\n+import socket\\n+\\n+import httpx\\n+\\n+\\n+class ForcedTimeoutException(Exception):\\n+    \"\"\"\\n+    Raised when a request takes longer than the timeout value.\\n+    \"\"\"\\n+\\n+    ...\\n+\\n+\\n+class InvalidDomainError(Exception):\\n+    \"\"\"\\n+    Raised when a request is made to a local IP address.\\n+    \"\"\"\\n+\\n+    ...\\n+\\n+\\n+class AsyncSafeTransport(httpx.AsyncBaseTransport):\\n+    \"\"\"\\n+    A wrapper around the httpx transport class that enforces a timeout value\\n+    and that the request is not made to a local IP address.\\n+    \"\"\"\\n+\\n+    timeout: int = 15\\n+\\n+    def __init__(self, log: logging.Logger | None = None, **kwargs):\\n+        self.timeout = kwargs.pop(\"timeout\", self.timeout)\\n+        self._wrapper = httpx.AsyncHTTPTransport(**kwargs)\\n+        self._log = log\\n+\\n+    async def handle_async_request(self, request):\\n+        # override timeout value for _all_ requests\\n+        request.extensions[\"timeout\"] = httpx.Timeout(self.timeout, pool=self.timeout).as_dict()\\n+\\n+        # validate the request is not attempting to connect to a local IP\\n+        # This is a security measure to prevent SSRF attacks\\n+\\n+        ip: ipaddress.IPv4Address | ipaddress.IPv6Address | None = None\\n+\\n+        netloc = request.url.netloc.decode()\\n+        if \":\" in netloc:  # Either an IP, or a hostname:port combo\\n+            netloc_parts = netloc.split(\":\")\\n+\\n+            netloc = netloc_parts[0]\\n+\\n+            try:\\n+                ip = ipaddress.ip_address(netloc)\\n+            except ValueError:\\n+                if self._log:\\n+                    self._log.debug(f\"failed to parse ip for {netloc=} falling back to domain resolution\")\\n+                pass\\n+\\n+        # Request is a domain or a hostname.\\n+        if not ip:\\n+            if self._log:\\n+                self._log.debug(f\"resolving IP for domain: {netloc}\")\\n+\\n+            ip_str = socket.gethostbyname(netloc)\\n+            ip = ipaddress.ip_address(ip_str)\\n+\\n+            if self._log:\\n+                self._log.debug(f\"resolved IP for domain: {netloc} -> {ip}\")\\n+\\n+        if ip.is_private:\\n+            if self._log:\\n+                self._log.warning(f\"invalid request on local resource: {request.url} -> {ip}\")\\n+            raise InvalidDomainError(f\"invalid request on local resource: {request.url} -> {ip}\")\\n+\\n+        return await self._wrapper.handle_async_request(request)\\n+\\n+    async def aclose(self):\\n+        await self._wrapper.aclose()', '@@ -47,7 +47,7 @@ class AppSettings(BaseSettings):\\n \\n     GIT_COMMIT_HASH: str = \"unknown\"\\n \\n-    ALLOW_SIGNUP: bool = True\\n+    ALLOW_SIGNUP: bool = False\\n \\n     # ===============================================\\n     # Security Configuration', '@@ -12,6 +12,7 @@\\n \\n from mealie.core.root_logger import get_logger\\n from mealie.lang.providers import Translator\\n+from mealie.pkgs import safehttp\\n from mealie.schema.recipe.recipe import Recipe, RecipeStep\\n from mealie.services.scraper.scraped_extras import ScrapedExtras\\n \\n@@ -31,7 +32,7 @@ async def safe_scrape_html(url: str) -> str:\\n     if the request takes longer than 15 seconds. This is used to mitigate\\n     DDOS attacks from users providing a url with arbitrary large content.\\n     \"\"\"\\n-    async with AsyncClient() as client:\\n+    async with AsyncClient(transport=safehttp.AsyncSafeTransport()) as client:\\n         html_bytes = b\"\"\\n         async with client.stream(\"GET\", url, timeout=SCRAPER_TIMEOUT, headers={\"User-Agent\": _FIREFOX_UA}) as resp:\\n             start_time = time.time()', '@@ -5,7 +5,8 @@\\n from httpx import AsyncClient, Response\\n from pydantic import UUID4\\n \\n-from mealie.pkgs import img\\n+from mealie.pkgs import img, safehttp\\n+from mealie.pkgs.safehttp.transport import AsyncSafeTransport\\n from mealie.schema.recipe.recipe import Recipe\\n from mealie.services._base_service import BaseService\\n \\n@@ -29,12 +30,14 @@ async def largest_content_len(urls: list[str]) -> tuple[str, int]:\\n     largest_url = \"\"\\n     largest_len = 0\\n \\n+    max_concurrency = 10\\n+\\n     async def do(client: AsyncClient, url: str) -> Response:\\n         return await client.head(url, headers={\"User-Agent\": _FIREFOX_UA})\\n \\n-    async with AsyncClient() as client:\\n+    async with AsyncClient(transport=safehttp.AsyncSafeTransport()) as client:\\n         tasks = [do(client, url) for url in urls]\\n-        responses: list[Response] = await gather_with_concurrency(10, *tasks, ignore_exceptions=True)\\n+        responses: list[Response] = await gather_with_concurrency(max_concurrency, *tasks, ignore_exceptions=True)\\n         for response in responses:\\n             len_int = int(response.headers.get(\"Content-Length\", 0))\\n             if len_int > largest_len:\\n@@ -101,52 +104,39 @@ def write_image(self, file_data: bytes | Path, extension: str, image_dir: Path |\\n \\n         return image_path\\n \\n-    @staticmethod\\n-    def _validate_image_url(url: str) -> bool:\\n-        # sourcery skip: invert-any-all, use-any\\n-        \"\"\"\\n-        Validates that the URL is of an allowed source and restricts certain sources to prevent\\n-        malicious images from being downloaded.\\n-        \"\"\"\\n-        invalid_domains = {\"127.0.0.1\", \"localhost\"}\\n-        for domain in invalid_domains:\\n-            if domain in url:\\n-                return False\\n-\\n-        return True\\n-\\n-    async def scrape_image(self, image_url) -> None:\\n+    async def scrape_image(self, image_url: str | dict[str, str] | list[str]) -> None:\\n         self.logger.info(f\"Image URL: {image_url}\")\\n \\n-        if not self._validate_image_url(image_url):\\n-            self.logger.error(f\"Invalid image URL: {image_url}\")\\n-            raise InvalidDomainError(f\"Invalid domain: {image_url}\")\\n+        image_url_str = \"\"\\n \\n         if isinstance(image_url, str):  # Handles String Types\\n-            pass\\n+            image_url_str = image_url\\n \\n         elif isinstance(image_url, list):  # Handles List Types\\n             # Multiple images have been defined in the schema - usually different resolutions\\n             # Typically would be in smallest->biggest order, but can\\'t be certain so test each.\\n             # \\'Google will pick the best image to display in Search results based on the aspect ratio and resolution.\\'\\n-            image_url, _ = await largest_content_len(image_url)\\n+            image_url_str, _ = await largest_content_len(image_url)\\n \\n         elif isinstance(image_url, dict):  # Handles Dictionary Types\\n             for key in image_url:\\n                 if key == \"url\":\\n-                    image_url = image_url.get(\"url\")\\n+                    image_url_str = image_url.get(\"url\", \"\")\\n+\\n+        if not image_url_str:\\n+            raise ValueError(f\"image url could not be parsed from input: {image_url}\")\\n \\n-        ext = image_url.split(\".\")[-1]\\n+        ext = image_url_str.split(\".\")[-1]\\n \\n         if ext not in img.IMAGE_EXTENSIONS:\\n             ext = \"jpg\"  # Guess the extension\\n \\n         file_name = f\"{str(self.recipe_id)}.{ext}\"\\n         file_path = Recipe.directory_from_id(self.recipe_id).joinpath(\"images\", file_name)\\n \\n-        async with AsyncClient() as client:\\n+        async with AsyncClient(transport=AsyncSafeTransport()) as client:\\n             try:\\n-                r = await client.get(image_url, headers={\"User-Agent\": _FIREFOX_UA})\\n+                r = await client.get(image_url_str, headers={\"User-Agent\": _FIREFOX_UA})\\n             except Exception:\\n                 self.logger.exception(\"Fatal Image Request Exception\")\\n                 return None'], 'file': ['mealie/services/scraper/scraper.py', 'mealie/pkgs/safehttp/transport.py', 'mealie/core/settings/settings.py', 'mealie/services/scraper/scraper_strategies.py', 'mealie/services/recipe/recipe_data_service.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('701e84dc-94f5-4fd9-98d0-53a63d291705'), UUID('09d0405b-58a6-4dca-915b-91df1cfbf2b2'), UUID('25a50fa5-4dbd-48f0-998d-afa4488f20d8'), UUID('aeea29db-11f6-444d-8968-7700f2075a36'), UUID('b936a844-d773-426d-8f20-4a4889afe368')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 193, in get_changes\n",
      "    _get_changes_lines_units(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/get_changes_lines_units.py\", line 229, in _get_changes_lines_units\n",
      "    line_context = script.get_context(fix_line)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/helpers.py\", line 487, in wrapper\n",
      "    return func(self, line, column, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/__init__.py\", line 494, in get_context\n",
      "    context = module_context.create_context(leaf)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 288, in create_context\n",
      "    return from_scope_node(scope_node, is_nested=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 258, in from_scope_node\n",
      "    return self.create_value(scope_node).as_context()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 236, in create_value\n",
      "    func = value.FunctionValue.from_context(parent_context, node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/function.py\", line 145, in from_context\n",
      "    overloaded_funcs = list(_find_overload_functions(context, tree_node))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/function.py\", line 443, in _find_overload_functions\n",
      "    filter = ParserTreeFilter(\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 138, in __init__\n",
      "    super().__init__(parent_context, node_context)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 100, in __init__\n",
      "    self._parso_cache_node = get_parso_cache_node(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/parser_utils.py\", line 287, in get_parso_cache_node\n",
      "    return parser_cache[grammar._hashed][path]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: PosixPath('/Users/somen/repos/mealie-recipes/mealie/mealie/services/scraper/scraper_strategies.py')\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1154/1800 [08:30<01:48,  5.98it/s]ERROR:src.process_code_changes:Error processing commit 9e573348d81df8191bbe8c266c01999c9d57cd5f\n",
      "ERROR:src.process_code_changes:{'repo': 'matrix-org/sydent', 'vulnerability_id': '2021-29431', 'commit': '9e573348d81df8191bbe8c266c01999c9d57cd5f', 'commit_source': 'github', 'cwe_id': ['CWE-20', 'CWE-20'], 'patch': ['@@ -17,14 +17,54 @@\\n # https://matrix.org/docs/spec/client_server/r0.6.0#post-matrix-client-r0-register-email-requesttoken\\n client_secret_regex = re.compile(r\"^[0-9a-zA-Z\\\\.\\\\=\\\\_\\\\-]+$\")\\n \\n+# hostname/domain name + optional port\\n+# https://regex101.com/r/OyN1lg/2\\n+hostname_regex = re.compile(\\n+    r\"^(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?)(?:\\\\.[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?)*$\",\\n+    flags=re.IGNORECASE)\\n+\\n \\n def is_valid_client_secret(client_secret):\\n     \"\"\"Validate that a given string matches the client_secret regex defined by the spec\\n \\n     :param client_secret: The client_secret to validate\\n-    :type client_secret: unicode\\n+    :type client_secret: str\\n \\n     :return: Whether the client_secret is valid\\n     :rtype: bool\\n     \"\"\"\\n     return client_secret_regex.match(client_secret) is not None\\n+\\n+\\n+def is_valid_hostname(string: str) -> bool:\\n+    \"\"\"Validate that a given string is a valid hostname or domain name, with an\\n+    optional port number.\\n+\\n+    For domain names, this only validates that the form is right (for\\n+    instance, it doesn\\'t check that the TLD is valid). If a port is\\n+    specified, it has to be a valid port number.\\n+\\n+    :param string: The string to validate\\n+    :type string: str\\n+\\n+    :return: Whether the input is a valid hostname\\n+    :rtype: bool\\n+    \"\"\"\\n+\\n+    host_parts = string.split(\":\", 1)\\n+\\n+    if len(host_parts) == 1:\\n+        return hostname_regex.match(string) is not None\\n+    else:\\n+        host, port = host_parts\\n+        valid_hostname = hostname_regex.match(host) is not None\\n+\\n+        try:\\n+            port_num = int(port)\\n+            valid_port = (\\n+                port == str(port_num)  # exclude things like \\'08090\\' or \\' 8090\\'\\n+                and 1 <= port_num < 65536\\n+        except ValueError:\\n+            valid_port = False\\n+\\n+        return valid_hostname and valid_port', '@@ -25,7 +25,7 @@\\n from sydent.http.servlets import get_args, jsonwrap, deferjsonwrap, send_cors\\n from sydent.http.httpclient import FederationHttpClient\\n from sydent.users.tokens import issueToken\\n-\\n+from sydent.util.stringutils import is_valid_hostname\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -47,9 +47,20 @@ def render_POST(self, request):\\n \\n         args = get_args(request, (\\'matrix_server_name\\', \\'access_token\\'))\\n \\n+        hostname = args[\\'matrix_server_name\\'].lower()\\n+\\n+        if not is_valid_hostname(hostname):\\n+            request.setResponseCode(400)\\n+            return {\\n+                \\'errcode\\': \\'M_INVALID_PARAM\\',\\n+                \\'error\\': \\'matrix_server_name must be a valid hostname\\'\\n+            }\\n+\\n         result = yield self.client.get_json(\\n-            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\" % (\\n-                args[\\'matrix_server_name\\'], urllib.parse.quote(args[\\'access_token\\']),\\n+            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\"\\n+            % (\\n+                hostname,\\n+                urllib.parse.quote(args[\\'access_token\\']),\\n             ),\\n             1024 * 5,\\n         )'], 'file': ['sydent/util/stringutils.py', 'sydent/http/servlets/registerservlet.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b3ac1950-a449-400f-952b-e0c413721bef'), UUID('d8f7b0a5-8802-4650-87ff-3bf253cf2291')]}\n",
      "ERROR:root:Error in {'repo': 'matrix-org/sydent', 'vulnerability_id': '2021-29431', 'commit': '9e573348d81df8191bbe8c266c01999c9d57cd5f', 'commit_source': 'github', 'cwe_id': ['CWE-20', 'CWE-20'], 'patch': ['@@ -17,14 +17,54 @@\\n # https://matrix.org/docs/spec/client_server/r0.6.0#post-matrix-client-r0-register-email-requesttoken\\n client_secret_regex = re.compile(r\"^[0-9a-zA-Z\\\\.\\\\=\\\\_\\\\-]+$\")\\n \\n+# hostname/domain name + optional port\\n+# https://regex101.com/r/OyN1lg/2\\n+hostname_regex = re.compile(\\n+    r\"^(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?)(?:\\\\.[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?)*$\",\\n+    flags=re.IGNORECASE)\\n+\\n \\n def is_valid_client_secret(client_secret):\\n     \"\"\"Validate that a given string matches the client_secret regex defined by the spec\\n \\n     :param client_secret: The client_secret to validate\\n-    :type client_secret: unicode\\n+    :type client_secret: str\\n \\n     :return: Whether the client_secret is valid\\n     :rtype: bool\\n     \"\"\"\\n     return client_secret_regex.match(client_secret) is not None\\n+\\n+\\n+def is_valid_hostname(string: str) -> bool:\\n+    \"\"\"Validate that a given string is a valid hostname or domain name, with an\\n+    optional port number.\\n+\\n+    For domain names, this only validates that the form is right (for\\n+    instance, it doesn\\'t check that the TLD is valid). If a port is\\n+    specified, it has to be a valid port number.\\n+\\n+    :param string: The string to validate\\n+    :type string: str\\n+\\n+    :return: Whether the input is a valid hostname\\n+    :rtype: bool\\n+    \"\"\"\\n+\\n+    host_parts = string.split(\":\", 1)\\n+\\n+    if len(host_parts) == 1:\\n+        return hostname_regex.match(string) is not None\\n+    else:\\n+        host, port = host_parts\\n+        valid_hostname = hostname_regex.match(host) is not None\\n+\\n+        try:\\n+            port_num = int(port)\\n+            valid_port = (\\n+                port == str(port_num)  # exclude things like \\'08090\\' or \\' 8090\\'\\n+                and 1 <= port_num < 65536\\n+        except ValueError:\\n+            valid_port = False\\n+\\n+        return valid_hostname and valid_port', '@@ -25,7 +25,7 @@\\n from sydent.http.servlets import get_args, jsonwrap, deferjsonwrap, send_cors\\n from sydent.http.httpclient import FederationHttpClient\\n from sydent.users.tokens import issueToken\\n-\\n+from sydent.util.stringutils import is_valid_hostname\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -47,9 +47,20 @@ def render_POST(self, request):\\n \\n         args = get_args(request, (\\'matrix_server_name\\', \\'access_token\\'))\\n \\n+        hostname = args[\\'matrix_server_name\\'].lower()\\n+\\n+        if not is_valid_hostname(hostname):\\n+            request.setResponseCode(400)\\n+            return {\\n+                \\'errcode\\': \\'M_INVALID_PARAM\\',\\n+                \\'error\\': \\'matrix_server_name must be a valid hostname\\'\\n+            }\\n+\\n         result = yield self.client.get_json(\\n-            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\" % (\\n-                args[\\'matrix_server_name\\'], urllib.parse.quote(args[\\'access_token\\']),\\n+            \"matrix://%s/_matrix/federation/v1/openid/userinfo?access_token=%s\"\\n+            % (\\n+                hostname,\\n+                urllib.parse.quote(args[\\'access_token\\']),\\n             ),\\n             1024 * 5,\\n         )'], 'file': ['sydent/util/stringutils.py', 'sydent/http/servlets/registerservlet.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('b3ac1950-a449-400f-952b-e0c413721bef'), UUID('d8f7b0a5-8802-4650-87ff-3bf253cf2291')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 101, in clear_file_content\n",
      "    new_content = remove_comments(content)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 71, in remove_comments\n",
      "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 543, in _generate_tokens_from_c_tokenizer\n",
      "    raise TokenError(msg, (e.lineno, e.offset)) from None\n",
      "tokenize.TokenError: ('unexpected EOF in multi-line statement', (45, 0))\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1163/1800 [10:24<1:00:27,  5.69s/it]ERROR:src.process_code_changes:Error processing commit 9d6051be4a42f692392049fdbfc85d5dfa458b32\n",
      "ERROR:src.process_code_changes:{'repo': 'nicolargo/glances', 'vulnerability_id': '2021-23418', 'commit': '9d6051be4a42f692392049fdbfc85d5dfa458b32', 'commit_source': 'github', 'cwe_id': ['CWE-611'], 'patch': ['@@ -44,6 +44,10 @@\\n     from urllib.error import HTTPError, URLError\\n     from urllib.parse import urlparse\\n \\n+    # Correct issue #1025 by monkey path the xmlrpc lib\\n+    from defusedxml.xmlrpc import monkey_patch\\n+    monkey_patch()\\n+\\n     input = input\\n     range = range\\n     map = map\\n@@ -132,6 +136,10 @@ def system_exec(command):\\n     from urllib2 import urlopen, HTTPError, URLError\\n     from urlparse import urlparse\\n \\n+    # Correct issue #1025 by monkey path the xmlrpc lib\\n+    from defusedxml.xmlrpc import monkey_patch\\n+    monkey_patch()\\n+\\n     input = raw_input\\n     range = xrange\\n     ConfigParser.read_file = ConfigParser.readfp'], 'file': ['glances/compat.py'], 'language': ['Python'], 'temp_id': [UUID('7a5e6754-9e58-4c27-91e8-d315e138824b')]}\n",
      "ERROR:root:Error in {'repo': 'nicolargo/glances', 'vulnerability_id': '2021-23418', 'commit': '9d6051be4a42f692392049fdbfc85d5dfa458b32', 'commit_source': 'github', 'cwe_id': ['CWE-611'], 'patch': ['@@ -44,6 +44,10 @@\\n     from urllib.error import HTTPError, URLError\\n     from urllib.parse import urlparse\\n \\n+    # Correct issue #1025 by monkey path the xmlrpc lib\\n+    from defusedxml.xmlrpc import monkey_patch\\n+    monkey_patch()\\n+\\n     input = input\\n     range = range\\n     map = map\\n@@ -132,6 +136,10 @@ def system_exec(command):\\n     from urllib2 import urlopen, HTTPError, URLError\\n     from urlparse import urlparse\\n \\n+    # Correct issue #1025 by monkey path the xmlrpc lib\\n+    from defusedxml.xmlrpc import monkey_patch\\n+    monkey_patch()\\n+\\n     input = raw_input\\n     range = xrange\\n     ConfigParser.read_file = ConfigParser.readfp'], 'file': ['glances/compat.py'], 'language': ['Python'], 'temp_id': [UUID('7a5e6754-9e58-4c27-91e8-d315e138824b')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     monkey_patch()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     monkey_patch()\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1196/1800 [10:33<16:30,  1.64s/it]  ERROR:src.process_code_changes:Error processing commit 5e112b68c6faad1d4699d02c1ebbb7daf48ef8fb\n",
      "ERROR:src.process_code_changes:{'repo': 'tiredtyrant/flairbot', 'vulnerability_id': '2015-10026', 'commit': '5e112b68c6faad1d4699d02c1ebbb7daf48ef8fb', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -9,8 +9,8 @@\\n def dbLookup(msg):\\n     if len(msg.split(\\',\\')) != 2:\\n         #procura na lista de paises\\n-        query = \\'SELECT id FROM paises WHERE nome == \"%s\"\\' % (msg)\\n-        cursor.execute(query)\\n+        query = \\'SELECT id FROM paises WHERE nome == ?;\\'\\n+        cursor.execute(query,(msg,))\\n         if cursor.fetchone():\\n             return True\\n         else:\\n@@ -19,8 +19,8 @@ def dbLookup(msg):\\n         cidade = msg.split(\\',\\')[0].strip()\\n         estado = msg.split(\\',\\')[1].strip()\\n         #check cidade pertence ao estado\\n-        query = \\'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == \"%s\" AND estados.uf == \"%s\";\\' % (cidade, estado)\\n-        cursor.execute(query)\\n+        query = \\'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == ? AND estados.uf == ?;\\'\\n+        cursor.execute(query,(cidade,estado))\\n         if not cursor.fetchone():\\n             return False\\n             \\n@@ -37,28 +37,31 @@ def main():\\n         return\\n     while True:\\n         time.sleep(0.5)\\n-        for msg in r.get_unread(limit=None):\\n-            try:\\n-                print \\'AUTHOR: %s - SUBJECT: %s - BODY: %s\\' % (msg.author, msg.subject, msg.body)\\n-            except UnicodeEncodeError:\\n-                print \\'AUTHOR: %s - unprintable chars\\' % (msg.author)\\n-            sub = r.get_subreddit(\\'brasil\\')\\n-            if msg.subject == \\'flair\\':\\n-                if dbLookup(msg.body):\\n-                    estado = \\'world\\' if len(msg.body.split(\\',\\')) < 2 else msg.body.split(\\',\\')[1].strip()\\n-                    sub.set_flair(msg.author,msg.body,estado)\\n-                    r.send_message(msg.author,\\'flair\\',\\'Flair configurado.\\')\\n-                    print(\\'flair ok\\')\\n-                else:\\n-                    r.send_message(msg.author,\\'flair\\',\\'ConfiguraÃ§Ã£o de flair falhou.\\')\\n-                    print(\\'flair fail\\')\\n-            if msg.subject == \\'remover flair\\':\\n-                sub.set_flair(msg.author,\\'\\',\\'\\')\\n-                r.send_message(msg.author,\\'flair\\',\\'Flair removido.\\')\\n-                print(\\'remove flair ok\\')\\n+        try:\\n+            for msg in r.get_unread(limit=None):\\n+                try:\\n+                    print \\'AUTHOR: %s - SUBJECT: %s - BODY: %s\\' % (msg.author, msg.subject, msg.body)\\n+                except UnicodeEncodeError:\\n+                    print \\'AUTHOR: %s - unprintable chars\\' % (msg.author)\\n+                sub = r.get_subreddit(\\'brasil\\')\\n+                if msg.subject == \\'flair\\':\\n+                    if dbLookup(msg.body):\\n+                        estado = \\'world\\' if len(msg.body.split(\\',\\')) < 2 else msg.body.split(\\',\\')[1].strip()\\n+                        sub.set_flair(msg.author,msg.body,estado)\\n+                        r.send_message(msg.author,\\'flair\\',\\'Flair configurado.\\')\\n+                        print(\\'flair ok\\')\\n+                    else:\\n+                        r.send_message(msg.author,\\'flair\\',\\'ConfiguraÃ§Ã£o de flair falhou.\\')\\n+                        print(\\'flair fail\\')\\n+                if msg.subject == \\'remover flair\\':\\n+                    sub.set_flair(msg.author,\\'\\',\\'\\')\\n+                    r.send_message(msg.author,\\'flair\\',\\'Flair removido.\\')\\n+                    print(\\'remove flair ok\\')\\n                 \\n-            msg.mark_as_read()\\n+                msg.mark_as_read()\\n+        except:\\n+            pass\\n \\n if __name__ == \\'__main__\\':\\n     main()\\n-    \\n\\\\ No newline at end of file\\n+    '], 'file': ['flair.py'], 'language': ['Python'], 'temp_id': [UUID('9604b329-779f-40f7-8b59-ba9f15dd15c3')]}\n",
      "ERROR:root:Error in {'repo': 'tiredtyrant/flairbot', 'vulnerability_id': '2015-10026', 'commit': '5e112b68c6faad1d4699d02c1ebbb7daf48ef8fb', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -9,8 +9,8 @@\\n def dbLookup(msg):\\n     if len(msg.split(\\',\\')) != 2:\\n         #procura na lista de paises\\n-        query = \\'SELECT id FROM paises WHERE nome == \"%s\"\\' % (msg)\\n-        cursor.execute(query)\\n+        query = \\'SELECT id FROM paises WHERE nome == ?;\\'\\n+        cursor.execute(query,(msg,))\\n         if cursor.fetchone():\\n             return True\\n         else:\\n@@ -19,8 +19,8 @@ def dbLookup(msg):\\n         cidade = msg.split(\\',\\')[0].strip()\\n         estado = msg.split(\\',\\')[1].strip()\\n         #check cidade pertence ao estado\\n-        query = \\'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == \"%s\" AND estados.uf == \"%s\";\\' % (cidade, estado)\\n-        cursor.execute(query)\\n+        query = \\'SELECT estados.id FROM municipios JOIN estados ON municipios.estados_id == estados.id WHERE municipios.nome == ? AND estados.uf == ?;\\'\\n+        cursor.execute(query,(cidade,estado))\\n         if not cursor.fetchone():\\n             return False\\n             \\n@@ -37,28 +37,31 @@ def main():\\n         return\\n     while True:\\n         time.sleep(0.5)\\n-        for msg in r.get_unread(limit=None):\\n-            try:\\n-                print \\'AUTHOR: %s - SUBJECT: %s - BODY: %s\\' % (msg.author, msg.subject, msg.body)\\n-            except UnicodeEncodeError:\\n-                print \\'AUTHOR: %s - unprintable chars\\' % (msg.author)\\n-            sub = r.get_subreddit(\\'brasil\\')\\n-            if msg.subject == \\'flair\\':\\n-                if dbLookup(msg.body):\\n-                    estado = \\'world\\' if len(msg.body.split(\\',\\')) < 2 else msg.body.split(\\',\\')[1].strip()\\n-                    sub.set_flair(msg.author,msg.body,estado)\\n-                    r.send_message(msg.author,\\'flair\\',\\'Flair configurado.\\')\\n-                    print(\\'flair ok\\')\\n-                else:\\n-                    r.send_message(msg.author,\\'flair\\',\\'ConfiguraÃ§Ã£o de flair falhou.\\')\\n-                    print(\\'flair fail\\')\\n-            if msg.subject == \\'remover flair\\':\\n-                sub.set_flair(msg.author,\\'\\',\\'\\')\\n-                r.send_message(msg.author,\\'flair\\',\\'Flair removido.\\')\\n-                print(\\'remove flair ok\\')\\n+        try:\\n+            for msg in r.get_unread(limit=None):\\n+                try:\\n+                    print \\'AUTHOR: %s - SUBJECT: %s - BODY: %s\\' % (msg.author, msg.subject, msg.body)\\n+                except UnicodeEncodeError:\\n+                    print \\'AUTHOR: %s - unprintable chars\\' % (msg.author)\\n+                sub = r.get_subreddit(\\'brasil\\')\\n+                if msg.subject == \\'flair\\':\\n+                    if dbLookup(msg.body):\\n+                        estado = \\'world\\' if len(msg.body.split(\\',\\')) < 2 else msg.body.split(\\',\\')[1].strip()\\n+                        sub.set_flair(msg.author,msg.body,estado)\\n+                        r.send_message(msg.author,\\'flair\\',\\'Flair configurado.\\')\\n+                        print(\\'flair ok\\')\\n+                    else:\\n+                        r.send_message(msg.author,\\'flair\\',\\'ConfiguraÃ§Ã£o de flair falhou.\\')\\n+                        print(\\'flair fail\\')\\n+                if msg.subject == \\'remover flair\\':\\n+                    sub.set_flair(msg.author,\\'\\',\\'\\')\\n+                    r.send_message(msg.author,\\'flair\\',\\'Flair removido.\\')\\n+                    print(\\'remove flair ok\\')\\n                 \\n-            msg.mark_as_read()\\n+                msg.mark_as_read()\\n+        except:\\n+            pass\\n \\n if __name__ == \\'__main__\\':\\n     main()\\n-    \\n\\\\ No newline at end of file\\n+    '], 'file': ['flair.py'], 'language': ['Python'], 'temp_id': [UUID('9604b329-779f-40f7-8b59-ba9f15dd15c3')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 24:14:         print 'logged in'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 24:14:         print 'logged in'\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1198/1800 [10:34<15:13,  1.52s/it]ERROR:src.process_code_changes:Error processing commit 76e8677699ed098387d502c57980f58da642aeba\n",
      "ERROR:src.process_code_changes:{'repo': 'paulc/dnslib', 'vulnerability_id': '2022-22846', 'commit': '76e8677699ed098387d502c57980f58da642aeba', 'commit_source': 'github', 'cwe_id': ['CWE-345'], 'patch': [\"@@ -76,6 +76,9 @@\\n         a_pkt = q.send(address,port,tcp=args.tcp)\\n         a = DNSRecord.parse(a_pkt)\\n \\n+        if q.header.id != a.header.id:\\n+            raise DNSError('Response transaction id does not match query transaction id')\\n+\\n         if a.header.tc and args.noretry == False:\\n             # Truncated - retry in TCP mode\\n             a_pkt = q.send(address,port,tcp=True)\"], 'file': ['dnslib/client.py'], 'language': ['Python'], 'temp_id': [UUID('9336f63d-0f94-420a-8934-c65afe4671bb')]}\n",
      "ERROR:root:Error in {'repo': 'paulc/dnslib', 'vulnerability_id': '2022-22846', 'commit': '76e8677699ed098387d502c57980f58da642aeba', 'commit_source': 'github', 'cwe_id': ['CWE-345'], 'patch': [\"@@ -76,6 +76,9 @@\\n         a_pkt = q.send(address,port,tcp=args.tcp)\\n         a = DNSRecord.parse(a_pkt)\\n \\n+        if q.header.id != a.header.id:\\n+            raise DNSError('Response transaction id does not match query transaction id')\\n+\\n         if a.header.tc and args.noretry == False:\\n             # Truncated - retry in TCP mode\\n             a_pkt = q.send(address,port,tcp=True)\"], 'file': ['dnslib/client.py'], 'language': ['Python'], 'temp_id': [UUID('9336f63d-0f94-420a-8934-c65afe4671bb')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 73, in lib2to3_parse\n",
      "  File \"src/blib2to3/pgen2/driver.py\", line 204, in parse_string\n",
      "  File \"src/blib2to3/pgen2/driver.py\", line 129, in parse_tokens\n",
      "  File \"src/blib2to3/pgen2/driver.py\", line 93, in __next__\n",
      "  File \"src/blib2to3/pgen2/tokenize.py\", line 748, in generate_tokens\n",
      "  File \"<tokenize>\", line 4\n",
      "    if a.header.tc and args.noretry == False:\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 73, in lib2to3_parse\n",
      "  File \"src/blib2to3/pgen2/driver.py\", line 204, in parse_string\n",
      "  File \"src/blib2to3/pgen2/driver.py\", line 129, in parse_tokens\n",
      "  File \"src/blib2to3/pgen2/driver.py\", line 93, in __next__\n",
      "  File \"src/blib2to3/pgen2/tokenize.py\", line 748, in generate_tokens\n",
      "  File \"<tokenize>\", line 4\n",
      "    if a.header.tc and args.noretry == False:\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1220/1800 [10:34<06:09,  1.57it/s]ERROR:src.process_code_changes:Error processing commit 4545f4a20d9ff90b99bbd4e3e34b6de4441d6367\n",
      "ERROR:src.process_code_changes:{'repo': 'janeczku/calibre-web', 'vulnerability_id': '2022-0990', 'commit': '4545f4a20d9ff90b99bbd4e3e34b6de4441d6367', 'commit_source': 'github', 'cwe_id': ['CWE-918'], 'patch': ['@@ -53,11 +53,11 @@ def get_epub_info(tmp_file_path, original_file_name, original_file_extension):\\n \\n     txt = epub_zip.read(\\'META-INF/container.xml\\')\\n     tree = etree.fromstring(txt)\\n-    cfname = tree.xpath(\\'n:rootfiles/n:rootfile/@full-path\\', namespaces=ns)[0]\\n-    cf = epub_zip.read(cfname)\\n+    cf_name = tree.xpath(\\'n:rootfiles/n:rootfile/@full-path\\', namespaces=ns)[0]\\n+    cf = epub_zip.read(cf_name)\\n     tree = etree.fromstring(cf)\\n \\n-    coverpath = os.path.dirname(cfname)\\n+    cover_path = os.path.dirname(cf_name)\\n \\n     p = tree.xpath(\\'/pkg:package/pkg:metadata\\', namespaces=ns)[0]\\n \\n@@ -90,7 +90,7 @@ def get_epub_info(tmp_file_path, original_file_name, original_file_extension):\\n \\n     epub_metadata = parse_epub_series(ns, tree, epub_metadata)\\n \\n-    cover_file = parse_epub_cover(ns, tree, epub_zip, coverpath, tmp_file_path)\\n+    cover_file = parse_epub_cover(ns, tree, epub_zip, cover_path, tmp_file_path)\\n \\n     if not epub_metadata[\\'title\\']:\\n         title = original_file_name\\n@@ -114,9 +114,12 @@ def get_epub_info(tmp_file_path, original_file_name, original_file_extension):\\n def parse_epub_cover(ns, tree, epub_zip, cover_path, tmp_file_path):\\n     cover_section = tree.xpath(\"/pkg:package/pkg:manifest/pkg:item[@id=\\'cover-image\\']/@href\", namespaces=ns)\\n     cover_file = None\\n-    if len(cover_section) > 0:\\n-        cover_file = _extract_cover(epub_zip, cover_section[0], cover_path, tmp_file_path)\\n-    else:\\n+    # if len(cover_section) > 0:\\n+    for cs in cover_section:\\n+        cover_file = _extract_cover(epub_zip, cs, cover_path, tmp_file_path)\\n+        if cover_file:\\n+            break\\n+    if not cover_file:\\n         meta_cover = tree.xpath(\"/pkg:package/pkg:metadata/pkg:meta[@name=\\'cover\\']/@content\", namespaces=ns)\\n         if len(meta_cover) > 0:\\n             cover_section = tree.xpath(\\n@@ -143,8 +146,7 @@ def parse_epub_cover(ns, tree, epub_zip, cover_path, tmp_file_path):\\n                     cover_file = _extract_cover(epub_zip, filename, \"\", tmp_file_path)\\n             else:\\n                 cover_file = _extract_cover(epub_zip, cs, cover_path, tmp_file_path)\\n-            if cover_file:\\n-                break\\n+            if cover_file: break\\n     return cover_file\\n \\n ', '@@ -29,7 +29,7 @@\\n from functools import wraps\\n \\n from babel.dates import format_date\\n-from babel import Locale as LC\\n+from babel import Locale\\n from flask import Blueprint, jsonify\\n from flask import request, redirect, send_from_directory, make_response, flash, abort, url_for\\n from flask import session as flask_session\\n@@ -60,7 +60,6 @@\\n from .render_template import render_title_template\\n from .kobo_sync_status import change_archived_books\\n \\n-\\n feature_support = {\\n     \\'ldap\\': bool(services.ldap),\\n     \\'goodreads\\': bool(services.goodreads_support),\\n@@ -69,10 +68,12 @@\\n \\n try:\\n     from .oauth_bb import oauth_check, register_user_with_oauth, logout_oauth_user, get_oauth_status\\n+\\n     feature_support[\\'oauth\\'] = True\\n except ImportError:\\n     feature_support[\\'oauth\\'] = False\\n     oauth_check = {}\\n+    register_user_with_oauth = logout_oauth_user = get_oauth_status = None\\n \\n try:\\n     from natsort import natsorted as sort\\n@@ -82,8 +83,11 @@\\n \\n @app.after_request\\n def add_security_headers(resp):\\n-    resp.headers[\\'Content-Security-Policy\\'] = \"default-src \\'self\\'\" + \\'\\'.join([\\' \\'+host for host in config.config_trustedhosts.strip().split(\\',\\')]) + \" \\'unsafe-inline\\' \\'unsafe-eval\\'; font-src \\'self\\' data:; img-src \\'self\\' data:\"\\n-    if request.endpoint == \"editbook.edit_book\" or config.config_use_google_drive:\\n+    csp = \"default-src \\'self\\'\"\\n+    csp += \\'\\'.join([\\' \\' + host for host in config.config_trustedhosts.strip().split(\\',\\')])\\n+    csp += \" \\'unsafe-inline\\' \\'unsafe-eval\\'; font-src \\'self\\' data:; img-src \\'self\\' data:\"\\n+    resp.headers[\\'Content-Security-Policy\\'] = csp\\n+    if request.endpoint == \"edit-book.edit_book\" or config.config_use_google_drive:\\n         resp.headers[\\'Content-Security-Policy\\'] += \" *\"\\n     elif request.endpoint == \"web.read_book\":\\n         resp.headers[\\'Content-Security-Policy\\'] += \" blob:;style-src-elem \\'self\\' blob: \\'unsafe-inline\\';\"\\n@@ -93,6 +97,7 @@ def add_security_headers(resp):\\n     resp.headers[\\'Strict-Transport-Security\\'] = \\'max-age=31536000;\\'\\n     return resp\\n \\n+\\n web = Blueprint(\\'web\\', __name__)\\n log = logger.create()\\n \\n@@ -119,6 +124,7 @@ def inner(*args, **kwargs):\\n \\n     return inner\\n \\n+\\n # ################################### data provider functions #########################################################\\n \\n \\n@@ -140,11 +146,11 @@ def set_bookmark(book_id, book_format):\\n         ub.session_commit()\\n         return \"\", 204\\n \\n-    lbookmark = ub.Bookmark(user_id=current_user.id,\\n-                            book_id=book_id,\\n-                            format=book_format,\\n-                            bookmark_key=bookmark_key)\\n-    ub.session.merge(lbookmark)\\n+    l_bookmark = ub.Bookmark(user_id=current_user.id,\\n+                             book_id=book_id,\\n+                             format=book_format,\\n+                             bookmark_key=bookmark_key)\\n+    ub.session.merge(l_bookmark)\\n     ub.session_commit(\"Bookmark for user {} in book {} created\".format(current_user.id, book_id))\\n     return \"\", 201\\n \\n@@ -162,7 +168,7 @@ def toggle_read(book_id):\\n @web.route(\"/ajax/togglearchived/<int:book_id>\", methods=[\\'POST\\'])\\n @login_required\\n def toggle_archived(book_id):\\n-    is_archived = change_archived_books(book_id, message=\"Book {} archivebit toggled\".format(book_id))\\n+    is_archived = change_archived_books(book_id, message=\"Book {} archive bit toggled\".format(book_id))\\n     if is_archived:\\n         remove_synced_book(book_id)\\n     return \"\"\\n@@ -230,6 +236,7 @@ def get_comic_book(book_id, book_format, page):\\n         return \"\", 204\\n \\'\\'\\'\\n \\n+\\n # ################################### Typeahead ##################################################################\\n \\n \\n@@ -297,6 +304,12 @@ def get_matching_tags():\\n     return json_dumps\\n \\n \\n+def generate_char_list(data_colum, db_link):\\n+    return (calibre_db.session.query(func.upper(func.substr(data_colum, 1, 1)).label(\\'char\\'))\\n+            .join(db_link).join(db.Books).filter(calibre_db.common_filters())\\n+            .group_by(func.upper(func.substr(data_colum, 1, 1))).all())\\n+\\n+\\n def get_sort_function(sort_param, data):\\n     order = [db.Books.timestamp.desc()]\\n     if sort_param == \\'stored\\':\\n@@ -373,7 +386,7 @@ def render_books_list(data, sort_param, book_id, page):\\n     else:\\n         website = data or \"newest\"\\n         entries, random, pagination = calibre_db.fill_indexpage(page, 0, db.Books, True, order[0],\\n-        \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFalse, 0,\\n+                                                                False, 0,\\n                                                                 db.books_series_link,\\n                                                                 db.Books.id == db.books_series_link.c.book,\\n                                                                 db.Series)\\n@@ -407,32 +420,33 @@ def render_discover_books(page, book_id):\\n     else:\\n         abort(404)\\n \\n+\\n def render_hot_books(page, order):\\n     if current_user.check_visibility(constants.SIDEBAR_HOT):\\n         if order[1] not in [\\'hotasc\\', \\'hotdesc\\']:\\n-        # Unary expression comparsion only working (for this expression) in sqlalchemy 1.4+\\n-        #if not (order[0][0].compare(func.count(ub.Downloads.book_id).desc()) or\\n-        #        order[0][0].compare(func.count(ub.Downloads.book_id).asc())):\\n+            # Unary expression comparsion only working (for this expression) in sqlalchemy 1.4+\\n+            # if not (order[0][0].compare(func.count(ub.Downloads.book_id).desc()) or\\n+            #        order[0][0].compare(func.count(ub.Downloads.book_id).asc())):\\n             order = [func.count(ub.Downloads.book_id).desc()], \\'hotdesc\\'\\n         if current_user.show_detail_random():\\n             random = calibre_db.session.query(db.Books).filter(calibre_db.common_filters()) \\\\\\n                 .order_by(func.random()).limit(config.config_random_books)\\n         else:\\n             random = false()\\n         off = int(int(config.config_books_per_page) * (page - 1))\\n-        all_books = ub.session.query(ub.Downloads, func.count(ub.Downloads.book_id))\\\\\\n+        all_books = ub.session.query(ub.Downloads, func.count(ub.Downloads.book_id)) \\\\\\n             .order_by(*order[0]).group_by(ub.Downloads.book_id)\\n         hot_books = all_books.offset(off).limit(config.config_books_per_page)\\n         entries = list()\\n         for book in hot_books:\\n-            downloadBook = calibre_db.session.query(db.Books).filter(calibre_db.common_filters()).filter(\\n+            download_book = calibre_db.session.query(db.Books).filter(calibre_db.common_filters()).filter(\\n                 db.Books.id == book.Downloads.book_id).first()\\n-            if downloadBook:\\n-                entries.append(downloadBook)\\n+            if download_book:\\n+                entries.append(download_book)\\n             else:\\n                 ub.delete_download(book.Downloads.book_id)\\n-        numBooks = entries.__len__()\\n-        pagination = Pagination(page, config.config_books_per_page, numBooks)\\n+        num_books = entries.__len__()\\n+        pagination = Pagination(page, config.config_books_per_page, num_books)\\n         return render_title_template(\\'index.html\\', random=random, entries=entries, pagination=pagination,\\n                                      title=_(u\"Hot Books (Most Downloaded)\"), page=\"hot\", order=order[1])\\n     else:\\n@@ -462,16 +476,16 @@ def render_downloaded_books(page, order, user_id):\\n                                                             db.Series,\\n                                                             ub.Downloads, db.Books.id == ub.Downloads.book_id)\\n         for book in entries:\\n-            if not calibre_db.session.query(db.Books).filter(calibre_db.common_filters()) \\\\\\n-                             .filter(db.Books.id == book.id).first():\\n+            if not calibre_db.session.query(db.Books).\\\\\\n+                                            filter(calibre_db.common_filters()).filter(db.Books.id == book.id).first():\\n                 ub.delete_download(book.id)\\n         user = ub.session.query(ub.User).filter(ub.User.id == user_id).first()\\n         return render_title_template(\\'index.html\\',\\n                                      random=random,\\n                                      entries=entries,\\n                                      pagination=pagination,\\n                                      id=user_id,\\n-                                     title=_(u\"Downloaded books by %(user)s\",user=user.name),\\n+                                     title=_(u\"Downloaded books by %(user)s\", user=user.name),\\n                                      page=\"download\",\\n                                      order=order[1])\\n     else:\\n@@ -639,29 +653,27 @@ def render_read_books(page, are_read, as_xml=False, order=None):\\n                         column=config.config_read_column),\\n                       category=\"error\")\\n                 return redirect(url_for(\"web.index\"))\\n-            return [] # ToDo: Handle error Case for opds\\n+            return []  # ToDo: Handle error Case for opds\\n \\n     if as_xml:\\n         return entries, pagination\\n     else:\\n         if are_read:\\n             name = _(u\\'Read Books\\') + \\' (\\' + str(pagination.total_count) + \\')\\'\\n-            pagename = \"read\"\\n+            page_name = \"read\"\\n         else:\\n             name = _(u\\'Unread Books\\') + \\' (\\' + str(pagination.total_count) + \\')\\'\\n-            pagename = \"unread\"\\n+            page_name = \"unread\"\\n         return render_title_template(\\'index.html\\', random=random, entries=entries, pagination=pagination,\\n-                                     title=name, page=pagename, order=order[1])\\n+                                     title=name, page=page_name, order=order[1])\\n \\n \\n def render_archived_books(page, sort_param):\\n     order = sort_param[0] or []\\n-    archived_books = (\\n-        ub.session.query(ub.ArchivedBook)\\n-        .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n-        .filter(ub.ArchivedBook.is_archived == True)\\n-        .all()\\n-    )\\n+    archived_books = (ub.session.query(ub.ArchivedBook)\\n+                      .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n+                      .filter(ub.ArchivedBook.is_archived == True)\\n+                      .all())\\n     archived_book_ids = [archived_book.book_id for archived_book in archived_books]\\n \\n     archived_filter = db.Books.id.in_(archived_book_ids)\\n@@ -674,40 +686,40 @@ def render_archived_books(page, sort_param):\\n                                                                                 False, 0)\\n \\n     name = _(u\\'Archived Books\\') + \\' (\\' + str(len(archived_book_ids)) + \\')\\'\\n-    pagename = \"archived\"\\n+    page_name = \"archived\"\\n     return render_title_template(\\'index.html\\', random=random, entries=entries, pagination=pagination,\\n-                                 title=name, page=pagename, order=sort_param[1])\\n+                                 title=name, page=page_name, order=sort_param[1])\\n \\n \\n def render_prepare_search_form(cc):\\n     # prepare data for search-form\\n-    tags = calibre_db.session.query(db.Tags)\\\\\\n-        .join(db.books_tags_link)\\\\\\n-        .join(db.Books)\\\\\\n+    tags = calibre_db.session.query(db.Tags) \\\\\\n+        .join(db.books_tags_link) \\\\\\n+        .join(db.Books) \\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n-        .group_by(text(\\'books_tags_link.tag\\'))\\\\\\n+        .group_by(text(\\'books_tags_link.tag\\')) \\\\\\n         .order_by(db.Tags.name).all()\\n-    series = calibre_db.session.query(db.Series)\\\\\\n-        .join(db.books_series_link)\\\\\\n-        .join(db.Books)\\\\\\n+    series = calibre_db.session.query(db.Series) \\\\\\n+        .join(db.books_series_link) \\\\\\n+        .join(db.Books) \\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n-        .group_by(text(\\'books_series_link.series\\'))\\\\\\n-        .order_by(db.Series.name)\\\\\\n+        .group_by(text(\\'books_series_link.series\\')) \\\\\\n+        .order_by(db.Series.name) \\\\\\n         .filter(calibre_db.common_filters()).all()\\n-    shelves = ub.session.query(ub.Shelf)\\\\\\n-        .filter(or_(ub.Shelf.is_public == 1, ub.Shelf.user_id == int(current_user.id)))\\\\\\n+    shelves = ub.session.query(ub.Shelf) \\\\\\n+        .filter(or_(ub.Shelf.is_public == 1, ub.Shelf.user_id == int(current_user.id))) \\\\\\n         .order_by(ub.Shelf.name).all()\\n-    extensions = calibre_db.session.query(db.Data)\\\\\\n-        .join(db.Books)\\\\\\n+    extensions = calibre_db.session.query(db.Data) \\\\\\n+        .join(db.Books) \\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n-        .group_by(db.Data.format)\\\\\\n+        .group_by(db.Data.format) \\\\\\n         .order_by(db.Data.format).all()\\n     if current_user.filter_language() == u\"all\":\\n         languages = calibre_db.speaking_language()\\n     else:\\n         languages = None\\n     return render_title_template(\\'search_form.html\\', tags=tags, languages=languages, extensions=extensions,\\n-                                 series=series,shelves=shelves, title=_(u\"Advanced Search\"), cc=cc, page=\"advsearch\")\\n+                                 series=series, shelves=shelves, title=_(u\"Advanced Search\"), cc=cc, page=\"advsearch\")\\n \\n \\n def render_search_results(term, offset=None, order=None, limit=None):\\n@@ -716,7 +728,6 @@ def render_search_results(term, offset=None, order=None, limit=None):\\n                                                                       offset,\\n                                                                       order,\\n                                                                       limit,\\n-                                                                      False,\\n                                                                       config.config_read_column,\\n                                                                       *join)\\n     return render_title_template(\\'search.html\\',\\n@@ -759,12 +770,13 @@ def books_table():\\n     return render_title_template(\\'book_table.html\\', title=_(u\"Books List\"), cc=cc, page=\"book_table\",\\n                                  visiblility=visibility)\\n \\n+\\n @web.route(\"/ajax/listbooks\")\\n @login_required\\n def list_books():\\n     off = int(request.args.get(\"offset\") or 0)\\n     limit = int(request.args.get(\"limit\") or config.config_books_per_page)\\n-    search = request.args.get(\"search\")\\n+    search_param = request.args.get(\"search\")\\n     sort_param = request.args.get(\"sort\", \"id\")\\n     order = request.args.get(\"order\", \"\").lower()\\n     state = None\\n@@ -784,8 +796,8 @@ def list_books():\\n     elif sort_param == \"authors\":\\n         order = [db.Authors.name.asc(), db.Series.name, db.Books.series_index] if order == \"asc\" \\\\\\n             else [db.Authors.name.desc(), db.Series.name.desc(), db.Books.series_index.desc()]\\n-        join = db.books_authors_link, db.Books.id == db.books_authors_link.c.book, db.Authors, \\\\\\n-               db.books_series_link, db.Books.id == db.books_series_link.c.book, db.Series\\n+        join = db.books_authors_link, db.Books.id == db.books_authors_link.c.book, db.Authors, db.books_series_link, \\\\\\n+            db.Books.id == db.books_series_link.c.book, db.Series\\n     elif sort_param == \"languages\":\\n         order = [db.Languages.lang_code.asc()] if order == \"asc\" else [db.Languages.lang_code.desc()]\\n         join = db.books_languages_link, db.Books.id == db.books_languages_link.c.book, db.Languages\\n@@ -794,10 +806,11 @@ def list_books():\\n     elif not state:\\n         order = [db.Books.timestamp.desc()]\\n \\n-    total_count = filtered_count = calibre_db.session.query(db.Books).filter(calibre_db.common_filters(allow_show_archived=True)).count()\\n+    total_count = filtered_count = calibre_db.session.query(db.Books).filter(\\n+        calibre_db.common_filters(allow_show_archived=True)).count()\\n     if state is not None:\\n-        if search:\\n-            books = calibre_db.search_query(search, config.config_read_column).all()\\n+        if search_param:\\n+            books = calibre_db.search_query(search_param, config.config_read_column).all()\\n             filtered_count = len(books)\\n         else:\\n             if not config.config_read_column:\\n@@ -818,15 +831,14 @@ def list_books():\\n                     # Skip linking read column and return None instead of read status\\n                     books = calibre_db.session.query(db.Books, None, ub.ArchivedBook.is_archived)\\n             books = (books.outerjoin(ub.ArchivedBook, and_(db.Books.id == ub.ArchivedBook.book_id,\\n-                                                          int(current_user.id) == ub.ArchivedBook.user_id))\\n+                                                           int(current_user.id) == ub.ArchivedBook.user_id))\\n                      .filter(calibre_db.common_filters(allow_show_archived=True)).all())\\n         entries = calibre_db.get_checkbox_sorted(books, state, off, limit, order, True)\\n-    elif search:\\n-        entries, filtered_count, __ = calibre_db.get_search_results(search,\\n+    elif search_param:\\n+        entries, filtered_count, __ = calibre_db.get_search_results(search_param,\\n                                                                     off,\\n-                                                                    [order,\\'\\'],\\n+                                                                    [order, \\'\\'],\\n                                                                     limit,\\n-                                                                    True,\\n                                                                     config.config_read_column,\\n                                                                     *join)\\n     else:\\n@@ -845,9 +857,9 @@ def list_books():\\n         val = entry[0]\\n         val.read_status = entry[1] == ub.ReadBook.STATUS_FINISHED\\n         val.is_archived = entry[2] is True\\n-        for index in range(0, len(val.languages)):\\n-            val.languages[index].language_name = isoLanguages.get_language_name(get_locale(), val.languages[\\n-                index].lang_code)\\n+        for lang_index in range(0, len(val.languages)):\\n+            val.languages[lang_index].language_name = isoLanguages.get_language_name(get_locale(), val.languages[\\n+                lang_index].lang_code)\\n         result.append(val)\\n \\n     table_entries = {\\'totalNotFiltered\\': total_count, \\'total\\': filtered_count, \"rows\": result}\\n@@ -857,6 +869,7 @@ def list_books():\\n     response.headers[\"Content-Type\"] = \"application/json; charset=utf-8\"\\n     return response\\n \\n+\\n @web.route(\"/ajax/table_settings\", methods=[\\'POST\\'])\\n @login_required\\n def update_table_settings():\\n@@ -886,19 +899,18 @@ def author_list():\\n         entries = calibre_db.session.query(db.Authors, func.count(\\'books_authors_link.book\\').label(\\'count\\')) \\\\\\n             .join(db.books_authors_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_authors_link.author\\')).order_by(order).all()\\n-        charlist = calibre_db.session.query(func.upper(func.substr(db.Authors.sort, 1, 1)).label(\\'char\\')) \\\\\\n-            .join(db.books_authors_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-            .group_by(func.upper(func.substr(db.Authors.sort, 1, 1))).all()\\n+        char_list = generate_char_list(db.Authors.sort, db.books_authors_link)\\n         # If not creating a copy, readonly databases can not display authornames with \"|\" in it as changing the name\\n         # starts a change session\\n-        autor_copy = copy.deepcopy(entries)\\n-        for entry in autor_copy:\\n+        author_copy = copy.deepcopy(entries)\\n+        for entry in author_copy:\\n             entry.Authors.name = entry.Authors.name.replace(\\'|\\', \\',\\')\\n-        return render_title_template(\\'list.html\\', entries=autor_copy, folder=\\'web.books_list\\', charlist=charlist,\\n+        return render_title_template(\\'list.html\\', entries=author_copy, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=u\"Authors\", page=\"authorlist\", data=\\'author\\', order=order_no)\\n     else:\\n         abort(404)\\n \\n+\\n @web.route(\"/downloadlist\")\\n @login_required_if_no_ano\\n def download_list():\\n@@ -909,12 +921,12 @@ def download_list():\\n         order = ub.User.name.asc()\\n         order_no = 1\\n     if current_user.check_visibility(constants.SIDEBAR_DOWNLOAD) and current_user.role_admin():\\n-        entries = ub.session.query(ub.User, func.count(ub.Downloads.book_id).label(\\'count\\'))\\\\\\n+        entries = ub.session.query(ub.User, func.count(ub.Downloads.book_id).label(\\'count\\')) \\\\\\n             .join(ub.Downloads).group_by(ub.Downloads.user_id).order_by(order).all()\\n-        charlist = ub.session.query(func.upper(func.substr(ub.User.name, 1, 1)).label(\\'char\\')) \\\\\\n+        char_list = ub.session.query(func.upper(func.substr(ub.User.name, 1, 1)).label(\\'char\\')) \\\\\\n             .filter(ub.User.role.op(\\'&\\')(constants.ROLE_ANONYMOUS) != constants.ROLE_ANONYMOUS) \\\\\\n             .group_by(func.upper(func.substr(ub.User.name, 1, 1))).all()\\n-        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=_(u\"Downloads\"), page=\"downloadlist\", data=\"download\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -933,10 +945,8 @@ def publisher_list():\\n         entries = calibre_db.session.query(db.Publishers, func.count(\\'books_publishers_link.book\\').label(\\'count\\')) \\\\\\n             .join(db.books_publishers_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_publishers_link.publisher\\')).order_by(order).all()\\n-        charlist = calibre_db.session.query(func.upper(func.substr(db.Publishers.name, 1, 1)).label(\\'char\\')) \\\\\\n-            .join(db.books_publishers_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-            .group_by(func.upper(func.substr(db.Publishers.name, 1, 1))).all()\\n-        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+        char_list = generate_char_list(db.Publishers.name, db.books_publishers_link)\\n+        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=_(u\"Publishers\"), page=\"publisherlist\", data=\"publisher\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -952,25 +962,19 @@ def series_list():\\n         else:\\n             order = db.Series.sort.asc()\\n             order_no = 1\\n+        char_list = generate_char_list(db.Series.sort, db.books_series_link)\\n         if current_user.get_view_property(\\'series\\', \\'series_view\\') == \\'list\\':\\n             entries = calibre_db.session.query(db.Series, func.count(\\'books_series_link.book\\').label(\\'count\\')) \\\\\\n                 .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n                 .group_by(text(\\'books_series_link.series\\')).order_by(order).all()\\n-            charlist = calibre_db.session.query(func.upper(func.substr(db.Series.sort, 1, 1)).label(\\'char\\')) \\\\\\n-                .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-                .group_by(func.upper(func.substr(db.Series.sort, 1, 1))).all()\\n-            return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+            return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                          title=_(u\"Series\"), page=\"serieslist\", data=\"series\", order=order_no)\\n         else:\\n             entries = calibre_db.session.query(db.Books, func.count(\\'books_series_link\\').label(\\'count\\'),\\n                                                func.max(db.Books.series_index), db.Books.id) \\\\\\n-                .join(db.books_series_link).join(db.Series).filter(calibre_db.common_filters())\\\\\\n+                .join(db.books_series_link).join(db.Series).filter(calibre_db.common_filters()) \\\\\\n                 .group_by(text(\\'books_series_link.series\\')).order_by(order).all()\\n-            charlist = calibre_db.session.query(func.upper(func.substr(db.Series.sort, 1, 1)).label(\\'char\\')) \\\\\\n-                .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-                .group_by(func.upper(func.substr(db.Series.sort, 1, 1))).all()\\n-\\n-            return render_title_template(\\'grid.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+            return render_title_template(\\'grid.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                          title=_(u\"Series\"), page=\"serieslist\", data=\"series\", bodyClass=\"grid-view\",\\n                                          order=order_no)\\n     else:\\n@@ -988,7 +992,7 @@ def ratings_list():\\n             order = db.Ratings.rating.asc()\\n             order_no = 1\\n         entries = calibre_db.session.query(db.Ratings, func.count(\\'books_ratings_link.book\\').label(\\'count\\'),\\n-                                   (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n+                                           (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n             .join(db.books_ratings_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_ratings_link.rating\\')).order_by(order).all()\\n         return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=list(),\\n@@ -1023,14 +1027,14 @@ def formats_list():\\n def language_overview():\\n     if current_user.check_visibility(constants.SIDEBAR_LANGUAGE) and current_user.filter_language() == u\"all\":\\n         order_no = 0 if current_user.get_view_property(\\'language\\', \\'dir\\') == \\'desc\\' else 1\\n-        charlist = list()\\n+        char_list = list()\\n         languages = calibre_db.speaking_language(reverse_order=not order_no, with_count=True)\\n         for lang in languages:\\n             upper_lang = lang[0].name[0].upper()\\n-            if upper_lang not in charlist:\\n-                charlist.append(upper_lang)\\n+            if upper_lang not in char_list:\\n+                char_list.append(upper_lang)\\n         return render_title_template(\\'languages.html\\', languages=languages,\\n-                                     charlist=charlist, title=_(u\"Languages\"), page=\"langlist\",\\n+                                     charlist=char_list, title=_(u\"Languages\"), page=\"langlist\",\\n                                      data=\"language\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -1049,10 +1053,8 @@ def category_list():\\n         entries = calibre_db.session.query(db.Tags, func.count(\\'books_tags_link.book\\').label(\\'count\\')) \\\\\\n             .join(db.books_tags_link).join(db.Books).order_by(order).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_tags_link.tag\\')).all()\\n-        charlist = calibre_db.session.query(func.upper(func.substr(db.Tags.name, 1, 1)).label(\\'char\\')) \\\\\\n-            .join(db.books_tags_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-            .group_by(func.upper(func.substr(db.Tags.name, 1, 1))).all()\\n-        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+        char_list = generate_char_list(db.Tags.name, db.books_tags_link)\\n+        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=_(u\"Categories\"), page=\"catlist\", data=\"category\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -1176,7 +1178,15 @@ def adv_search_read_status(q, read_status):\\n     return q\\n \\n \\n-def adv_search_extension(q, include_extension_inputs, exclude_extension_inputs):\\n+def adv_search_text(q, include_inputs, exclude_inputs, data_value):\\n+    for inp in include_inputs:\\n+        q = q.filter(db.Books.data.any(data_value == inp))\\n+    for excl in exclude_inputs:\\n+        q = q.filter(not_(db.Books.data.any(data_value == excl)))\\n+    return q\\n+\\n+\\n+\\'\\'\\'def adv_search_extension(q, include_extension_inputs, exclude_extension_inputs):\\n     for extension in include_extension_inputs:\\n         q = q.filter(db.Books.data.any(db.Data.format == extension))\\n     for extension in exclude_extension_inputs:\\n@@ -1197,15 +1207,17 @@ def adv_search_serie(q, include_series_inputs, exclude_series_inputs):\\n         q = q.filter(db.Books.series.any(db.Series.id == serie))\\n     for serie in exclude_series_inputs:\\n         q = q.filter(not_(db.Books.series.any(db.Series.id == serie)))\\n-    return q\\n+    return q\\'\\'\\'\\n+\\n \\n def adv_search_shelf(q, include_shelf_inputs, exclude_shelf_inputs):\\n-    q = q.outerjoin(ub.BookShelf, db.Books.id == ub.BookShelf.book_id)\\\\\\n+    q = q.outerjoin(ub.BookShelf, db.Books.id == ub.BookShelf.book_id) \\\\\\n         .filter(or_(ub.BookShelf.shelf == None, ub.BookShelf.shelf.notin_(exclude_shelf_inputs)))\\n     if len(include_shelf_inputs) > 0:\\n         q = q.filter(ub.BookShelf.shelf.in_(include_shelf_inputs))\\n     return q\\n \\n+\\n def extend_search_term(searchterm,\\n                        author_name,\\n                        book_title,\\n@@ -1232,7 +1244,7 @@ def extend_search_term(searchterm,\\n                                            format=\\'medium\\', locale=get_locale())])\\n         except ValueError:\\n             pub_end = u\"\"\\n-    elements = {\\'tag\\': db.Tags, \\'serie\\':db.Series, \\'shelf\\':ub.Shelf}\\n+    elements = {\\'tag\\': db.Tags, \\'serie\\': db.Series, \\'shelf\\': ub.Shelf}\\n     for key, db_element in elements.items():\\n         tag_names = calibre_db.session.query(db_element).filter(db_element.id.in_(tags[\\'include_\\' + key])).all()\\n         searchterm.extend(tag.name for tag in tag_names)\\n@@ -1284,8 +1296,8 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n     query = query.outerjoin(ub.ArchivedBook, and_(db.Books.id == ub.ArchivedBook.book_id,\\n                                                   int(current_user.id) == ub.ArchivedBook.user_id))\\n \\n-    q = query.outerjoin(db.books_series_link, db.Books.id == db.books_series_link.c.book)\\\\\\n-        .outerjoin(db.Series)\\\\\\n+    q = query.outerjoin(db.books_series_link, db.Books.id == db.books_series_link.c.book) \\\\\\n+        .outerjoin(db.Series) \\\\\\n         .filter(calibre_db.common_filters(True))\\n \\n     # parse multiselects to a complete dict\\n@@ -1311,43 +1323,43 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n     if publisher:\\n         publisher = publisher.strip().lower()\\n \\n-    searchterm = []\\n+    search_term = []\\n     cc_present = False\\n     for c in cc:\\n         if c.datatype == \"datetime\":\\n             column_start = term.get(\\'custom_column_\\' + str(c.id) + \\'_start\\')\\n             column_end = term.get(\\'custom_column_\\' + str(c.id) + \\'_end\\')\\n             if column_start:\\n-                searchterm.extend([u\"{} >= {}\".format(c.name,\\n-                                                      format_date(datetime.strptime(column_start, \"%Y-%m-%d\").date(),\\n-                                                                      format=\\'medium\\',\\n-                                                                      locale=get_locale())\\n-                                                      )])\\n+                search_term.extend([u\"{} >= {}\".format(c.name,\\n+                                                       format_date(datetime.strptime(column_start, \"%Y-%m-%d\").date(),\\n+                                                                   format=\\'medium\\',\\n+                                                                   locale=get_locale())\\n+                                                       )])\\n                 cc_present = True\\n             if column_end:\\n-                searchterm.extend([u\"{} <= {}\".format(c.name,\\n-                                                      format_date(datetime.strptime(column_end, \"%Y-%m-%d\").date(),\\n-                                                                      format=\\'medium\\',\\n-                                                                      locale=get_locale())\\n-                                                      )])\\n+                search_term.extend([u\"{} <= {}\".format(c.name,\\n+                                                       format_date(datetime.strptime(column_end, \"%Y-%m-%d\").date(),\\n+                                                                   format=\\'medium\\',\\n+                                                                   locale=get_locale())\\n+                                                       )])\\n                 cc_present = True\\n         elif term.get(\\'custom_column_\\' + str(c.id)):\\n-            searchterm.extend([(u\"{}: {}\".format(c.name, term.get(\\'custom_column_\\' + str(c.id))))])\\n+            search_term.extend([(u\"{}: {}\".format(c.name, term.get(\\'custom_column_\\' + str(c.id))))])\\n             cc_present = True\\n \\n-\\n-    if any(tags.values()) or author_name or book_title or publisher or pub_start or pub_end or rating_low \\\\\\n-       or rating_high or description or cc_present or read_status:\\n-        searchterm, pub_start, pub_end = extend_search_term(searchterm,\\n-                                                            author_name,\\n-                                                            book_title,\\n-                                                            publisher,\\n-                                                            pub_start,\\n-                                                            pub_end,\\n-                                                            tags,\\n-                                                            rating_high,\\n-                                                            rating_low,\\n-                                                            read_status)\\n+    if any(tags.values()) or author_name or book_title or \\\\\\n+        publisher or pub_start or pub_end or rating_low or rating_high \\\\\\n+            or description or cc_present or read_status:\\n+        search_term, pub_start, pub_end = extend_search_term(search_term,\\n+                                                             author_name,\\n+                                                             book_title,\\n+                                                             publisher,\\n+                                                             pub_start,\\n+                                                             pub_end,\\n+                                                             tags,\\n+                                                             rating_high,\\n+                                                             rating_low,\\n+                                                             read_status)\\n         # q = q.filter()\\n         if author_name:\\n             q = q.filter(db.Books.authors.any(func.lower(db.Authors.name).ilike(\"%\" + author_name + \"%\")))\\n@@ -1360,12 +1372,12 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n         q = adv_search_read_status(q, read_status)\\n         if publisher:\\n             q = q.filter(db.Books.publishers.any(func.lower(db.Publishers.name).ilike(\"%\" + publisher + \"%\")))\\n-        q = adv_search_tag(q, tags[\\'include_tag\\'], tags[\\'exclude_tag\\'])\\n-        q = adv_search_serie(q, tags[\\'include_serie\\'], tags[\\'exclude_serie\\'])\\n+        q = adv_search_text(q, tags[\\'include_tag\\'], tags[\\'exclude_tag\\'], db.Tags.id)\\n+        q = adv_search_text(q, tags[\\'include_serie\\'], tags[\\'exclude_serie\\'], db.Series.id)\\n+        q = adv_search_text(q, tags[\\'include_extension\\'], tags[\\'exclude_extension\\'], db.Data.format)\\n         q = adv_search_shelf(q, tags[\\'include_shelf\\'], tags[\\'exclude_shelf\\'])\\n-        q = adv_search_extension(q, tags[\\'include_extension\\'], tags[\\'exclude_extension\\'])\\n-        q = adv_search_language(q, tags[\\'include_language\\'], tags[\\'exclude_language\\'])\\n-        q = adv_search_ratings(q, rating_high, rating_low)\\n+        q = adv_search_language(q, tags[\\'include_language\\'], tags[\\'exclude_language\\'], )\\n+        q = adv_search_ratings(q, rating_high, rating_low, )\\n \\n         if description:\\n             q = q.filter(db.Books.comments.any(func.lower(db.Comments.text).ilike(\"%\" + description + \"%\")))\\n@@ -1390,7 +1402,7 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n         limit_all = result_count\\n     entries = calibre_db.order_authors(q[offset:limit_all], list_return=True, combined=True)\\n     return render_title_template(\\'search.html\\',\\n-                                 adv_searchterm=searchterm,\\n+                                 adv_searchterm=search_term,\\n                                  pagination=pagination,\\n                                  entries=entries,\\n                                  result_count=result_count,\\n@@ -1414,10 +1426,12 @@ def advanced_search_form():\\n def get_cover(book_id):\\n     return get_book_cover(book_id)\\n \\n+\\n @web.route(\"/robots.txt\")\\n def get_robots():\\n     return send_from_directory(constants.STATIC_DIR, \"robots.txt\")\\n \\n+\\n @web.route(\"/show/<int:book_id>/<book_format>\", defaults={\\'anyname\\': \\'None\\'})\\n @web.route(\"/show/<int:book_id>/<book_format>/<anyname>\")\\n @login_required_if_no_ano\\n@@ -1561,7 +1575,7 @@ def login():\\n                       category=\"success\")\\n                 return redirect_back(url_for(\"web.index\"))\\n             elif login_result is None and user and check_password_hash(str(user.password), form[\\'password\\']) \\\\\\n-                and user.name != \"Guest\":\\n+                    and user.name != \"Guest\":\\n                 login_user(user, remember=bool(form.get(\\'remember_me\\')))\\n                 ub.store_user_session()\\n                 log.info(\"Local Fallback Login as: \\'%s\\'\", user.name)\\n@@ -1573,23 +1587,23 @@ def login():\\n                 log.info(error)\\n                 flash(_(u\"Could not login: %(message)s\", message=error), category=\"error\")\\n             else:\\n-                ip_Address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n-                log.warning(\\'LDAP Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_Address)\\n+                ip_address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n+                log.warning(\\'LDAP Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_address)\\n                 flash(_(u\"Wrong Username or Password\"), category=\"error\")\\n         else:\\n-            ip_Address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n+            ip_address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n             if \\'forgot\\' in form and form[\\'forgot\\'] == \\'forgot\\':\\n                 if user is not None and user.name != \"Guest\":\\n                     ret, __ = reset_password(user.id)\\n                     if ret == 1:\\n                         flash(_(u\"New Password was send to your email address\"), category=\"info\")\\n-                        log.info(\\'Password reset for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_Address)\\n+                        log.info(\\'Password reset for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_address)\\n                     else:\\n                         log.error(u\"An unknown error occurred. Please try again later\")\\n                         flash(_(u\"An unknown error occurred. Please try again later.\"), category=\"error\")\\n                 else:\\n                     flash(_(u\"Please enter valid username to reset password\"), category=\"error\")\\n-                    log.warning(\\'Username missing for password reset IP-address: %s\\', ip_Address)\\n+                    log.warning(\\'Username missing for password reset IP-address: %s\\', ip_address)\\n             else:\\n                 if user and check_password_hash(str(user.password), form[\\'password\\']) and user.name != \"Guest\":\\n                     login_user(user, remember=bool(form.get(\\'remember_me\\')))\\n@@ -1599,7 +1613,7 @@ def login():\\n                     config.config_is_initial = False\\n                     return redirect_back(url_for(\"web.index\"))\\n                 else:\\n-                    log.warning(\\'Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_Address)\\n+                    log.warning(\\'Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_address)\\n                     flash(_(u\"Wrong Username or Password\"), category=\"error\")\\n \\n     next_url = request.args.get(\\'next\\', default=url_for(\"web.index\"), type=str)\\n@@ -1617,7 +1631,7 @@ def login():\\n @login_required\\n def logout():\\n     if current_user is not None and current_user.is_authenticated:\\n-        ub.delete_user_session(current_user.id, flask_session.get(\\'_id\\',\"\"))\\n+        ub.delete_user_session(current_user.id, flask_session.get(\\'_id\\', \"\"))\\n         logout_user()\\n         if feature_support[\\'oauth\\'] and (config.config_login_type == 2 or config.config_login_type == 3):\\n             logout_oauth_user()\\n@@ -1639,7 +1653,7 @@ def change_profile(kobo_support, local_oauth_check, oauth_status, translations,\\n             current_user.email = check_email(to_save[\"email\"])\\n         if current_user.role_admin():\\n             if to_save.get(\"name\", current_user.name) != current_user.name:\\n-                # Query User name, if not existing, change\\n+                # Query username, if not existing, change\\n                 current_user.name = check_username(to_save[\"name\"])\\n         current_user.random_books = 1 if to_save.get(\"show_random\") == \"on\" else 0\\n         if to_save.get(\"default_language\"):\\n@@ -1693,7 +1707,7 @@ def change_profile(kobo_support, local_oauth_check, oauth_status, translations,\\n @login_required\\n def profile():\\n     languages = calibre_db.speaking_language()\\n-    translations = babel.list_translations() + [LC(\\'en\\')]\\n+    translations = babel.list_translations() + [Locale(\\'en\\')]\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     if feature_support[\\'oauth\\'] and config.config_login_type == 2:\\n         oauth_status = get_oauth_status()\\n@@ -1727,7 +1741,8 @@ def read_book(book_id, book_format):\\n     book.ordered_authors = calibre_db.order_authors([book], False)\\n \\n     if not book:\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         log.debug(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\")\\n         return redirect(url_for(\"web.index\"))\\n \\n@@ -1768,7 +1783,8 @@ def read_book(book_id, book_format):\\n                 return render_title_template(\\'readcbr.html\\', comicfile=all_name, title=title,\\n                                              extension=fileExt)\\n         log.debug(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\")\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         return redirect(url_for(\"web.index\"))\\n \\n \\n@@ -1782,14 +1798,14 @@ def show_book(book_id):\\n         entry = entries[0]\\n         entry.read_status = read_book == ub.ReadBook.STATUS_FINISHED\\n         entry.is_archived = archived_book\\n-        for index in range(0, len(entry.languages)):\\n-            entry.languages[index].language_name = isoLanguages.get_language_name(get_locale(), entry.languages[\\n-                index].lang_code)\\n+        for lang_index in range(0, len(entry.languages)):\\n+            entry.languages[lang_index].language_name = isoLanguages.get_language_name(get_locale(), entry.languages[\\n+                lang_index].lang_code)\\n         cc = get_cc_columns(filter_config_custom_read=True)\\n-        book_in_shelfs = []\\n+        book_in_shelves = []\\n         shelfs = ub.session.query(ub.BookShelf).filter(ub.BookShelf.book_id == book_id).all()\\n         for sh in shelfs:\\n-            book_in_shelfs.append(sh.shelf)\\n+            book_in_shelves.append(sh.shelf)\\n \\n         entry.tags = sort(entry.tags, key=lambda tag: tag.name)\\n \\n@@ -1806,9 +1822,9 @@ def show_book(book_id):\\n         return render_title_template(\\'detail.html\\',\\n                                      entry=entry,\\n                                      cc=cc,\\n-                                     is_xhr=request.headers.get(\\'X-Requested-With\\')==\\'XMLHttpRequest\\',\\n+                                     is_xhr=request.headers.get(\\'X-Requested-With\\') == \\'XMLHttpRequest\\',\\n                                      title=entry.title,\\n-                                     books_shelfs=book_in_shelfs,\\n+                                     books_shelfs=book_in_shelves,\\n                                      page=\"book\")\\n     else:\\n         log.debug(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\")', '@@ -22,7 +22,7 @@\\n \\n {%  if source_formats|length > 0 and conversion_formats|length > 0 %}\\n   <div class=\"text-center more-stuff\"><h4>{{_(\\'Convert book format:\\')}}</h4>\\n-      <form class=\"padded-bottom\" action=\"{{ url_for(\\'editbook.convert_bookformat\\', book_id=book.id) }}\" method=\"post\" id=\"book_convert_frm\">\\n+      <form class=\"padded-bottom\" action=\"{{ url_for(\\'edit-book.convert_bookformat\\', book_id=book.id) }}\" method=\"post\" id=\"book_convert_frm\">\\n           <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n           <div class=\"form-group\">\\n               <div class=\"text-left\">\\n@@ -48,7 +48,7 @@\\n {% endif %}\\n \\n   </div>\\n-<form role=\"form\" action=\"{{ url_for(\\'editbook.edit_book\\', book_id=book.id) }}\" method=\"post\" enctype=\"multipart/form-data\" id=\"book_edit_frm\">\\n+<form role=\"form\" action=\"{{ url_for(\\'edit-book.edit_book\\', book_id=book.id) }}\" method=\"post\" enctype=\"multipart/form-data\" id=\"book_edit_frm\">\\n   <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n   <div class=\"col-sm-9 col-xs-12\">\\n     <div class=\"form-group\">', '@@ -138,7 +138,7 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n         <p>\\n         <span class=\"glyphicon glyphicon-link\"></span>\\n         {% for identifier in entry.identifiers %}\\n-          <a href=\"{{identifier}}\" target=\"_blank\" class=\"btn btn-xs btn-success\" role=\"button\">{{identifier.formatType()}}</a>\\n+          <a href=\"{{identifier}}\" target=\"_blank\" class=\"btn btn-xs btn-success\" role=\"button\">{{identifier.format_type()}}</a>\\n         {%endfor%}\\n       </p>\\n       </div>\\n@@ -295,7 +295,7 @@ <h3 id=\"decription\">{{_(\\'Description:\\')}}</h3>\\n       {% if g.user.role_edit() %}\\n       <div class=\"btn-toolbar\" role=\"toolbar\">\\n         <div class=\"btn-group\" role=\"group\" aria-label=\"Edit/Delete book\">\\n-          <a href=\"{{ url_for(\\'editbook.edit_book\\', book_id=entry.id) }}\" class=\"btn btn-sm btn-primary\" id=\"edit_book\" role=\"button\"><span class=\"glyphicon glyphicon-edit\"></span> {{_(\\'Edit Metadata\\')}}</a>\\n+          <a href=\"{{ url_for(\\'edit-book.edit_book\\', book_id=entry.id) }}\" class=\"btn btn-sm btn-primary\" id=\"edit_book\" role=\"button\"><span class=\"glyphicon glyphicon-edit\"></span> {{_(\\'Edit Metadata\\')}}</a>\\n         </div>\\n       </div>\\n       {% endif %}', '@@ -23,11 +23,10 @@\\n import re\\n import shutil\\n import socket\\n-import unicodedata\\n from datetime import datetime, timedelta\\n from tempfile import gettempdir\\n-from urllib.parse import urlparse\\n import requests\\n+import unidecode\\n \\n from babel.dates import format_datetime\\n from babel.units import format_unit\\n@@ -41,15 +40,19 @@\\n from markupsafe import escape\\n from urllib.parse import quote\\n \\n+\\n try:\\n-    import unidecode\\n-    use_unidecode = True\\n+    import advocate\\n+    from advocate.exceptions import UnacceptableAddressException\\n+    use_advocate = True\\n except ImportError:\\n-    use_unidecode = False\\n+    use_advocate = False\\n+    advocate = requests\\n+    UnacceptableAddressException = MissingSchema = BaseException\\n \\n from . import calibre_db, cli\\n from .tasks.convert import TaskConvert\\n-from . import logger, config, get_locale, db, ub, kobo_sync_status\\n+from . import logger, config, get_locale, db, ub\\n from . import gdriveutils as gd\\n from .constants import STATIC_DIR as _STATIC_DIR\\n from .subproc_wrapper import process_wait\\n@@ -143,7 +146,7 @@ def check_send_to_kindle_with_converter(formats):\\n                             \\'text\\': _(\\'Convert %(orig)s to %(format)s and send to Kindle\\',\\n                                       orig=\\'Epub\\',\\n                                       format=\\'Mobi\\')})\\n-    if \\'AZW3\\' in formats and not \\'MOBI\\' in formats:\\n+    if \\'AZW3\\' in formats and \\'MOBI\\' not in formats:\\n         bookformats.append({\\'format\\': \\'Mobi\\',\\n                             \\'convert\\': 2,\\n                             \\'text\\': _(\\'Convert %(orig)s to %(format)s and send to Kindle\\',\\n@@ -185,11 +188,11 @@ def check_send_to_kindle(entry):\\n # Check if a reader is existing for any of the book formats, if not, return empty list, otherwise return\\n # list with supported formats\\n def check_read_formats(entry):\\n-    EXTENSIONS_READER = {\\'TXT\\', \\'PDF\\', \\'EPUB\\', \\'CBZ\\', \\'CBT\\', \\'CBR\\', \\'DJVU\\'}\\n+    extensions_reader = {\\'TXT\\', \\'PDF\\', \\'EPUB\\', \\'CBZ\\', \\'CBT\\', \\'CBR\\', \\'DJVU\\'}\\n     bookformats = list()\\n     if len(entry.data):\\n         for ele in iter(entry.data):\\n-            if ele.format.upper() in EXTENSIONS_READER:\\n+            if ele.format.upper() in extensions_reader:\\n                 bookformats.append(ele.format.lower())\\n     return bookformats\\n \\n@@ -213,10 +216,10 @@ def send_mail(book_id, book_format, convert, kindle_mail, calibrepath, user_id):\\n         if entry.format.upper() == book_format.upper():\\n             converted_file_name = entry.name + \\'.\\' + book_format.lower()\\n             link = \\'<a href=\"{}\">{}</a>\\'.format(url_for(\\'web.show_book\\', book_id=book_id), escape(book.title))\\n-            EmailText = _(u\"%(book)s send to Kindle\", book=link)\\n+            email_text = _(u\"%(book)s send to Kindle\", book=link)\\n             WorkerThread.add(user_id, TaskEmail(_(u\"Send to Kindle\"), book.path, converted_file_name,\\n                              config.get_mail_settings(), kindle_mail,\\n-                             EmailText, _(u\\'This e-mail has been sent via Calibre-Web.\\')))\\n+                             email_text, _(u\\'This e-mail has been sent via Calibre-Web.\\')))\\n             return\\n     return _(u\"The requested file could not be read. Maybe wrong permissions?\")\\n \\n@@ -229,15 +232,8 @@ def get_valid_filename(value, replace_whitespace=True, chars=128):\\n     if value[-1:] == u\\'.\\':\\n         value = value[:-1]+u\\'_\\'\\n     value = value.replace(\"/\", \"_\").replace(\":\", \"_\").strip(\\'\\\\0\\')\\n-    if use_unidecode:\\n-        if config.config_unicode_filename:\\n-            value = (unidecode.unidecode(value))\\n-    else:\\n-        value = value.replace(u\\'Â§\\', u\\'SS\\')\\n-        value = value.replace(u\\'ÃŸ\\', u\\'ss\\')\\n-        value = unicodedata.normalize(\\'NFKD\\', value)\\n-        re_slugify = re.compile(r\\'[\\\\W\\\\s-]\\', re.UNICODE)\\n-        value = re_slugify.sub(\\'\\', value)\\n+    if config.config_unicode_filename:\\n+        value = (unidecode.unidecode(value))\\n     if replace_whitespace:\\n         #  *+:\\\\\"/<>? are replaced by _\\n         value = re.sub(r\\'[*+:\\\\\\\\\\\\\"/<>?]+\\', u\\'_\\', value, flags=re.U)\\n@@ -266,6 +262,7 @@ def split_authors(values):\\n \\n \\n def get_sorted_author(value):\\n+    value2 = None\\n     try:\\n         if \\',\\' not in value:\\n             regexes = [r\"^(JR|SR)\\\\.?$\", r\"^I{1,3}\\\\.?$\", r\"^IV\\\\.?$\"]\\n@@ -290,6 +287,7 @@ def get_sorted_author(value):\\n             value2 = value\\n     return value2\\n \\n+\\n def edit_book_read_status(book_id, read_status=None):\\n     if not config.config_read_column:\\n         book = ub.session.query(ub.ReadBook).filter(and_(ub.ReadBook.user_id == int(current_user.id),\\n@@ -303,9 +301,9 @@ def edit_book_read_status(book_id, read_status=None):\\n             else:\\n                 book.read_status = ub.ReadBook.STATUS_FINISHED if read_status else ub.ReadBook.STATUS_UNREAD\\n         else:\\n-            readBook = ub.ReadBook(user_id=current_user.id, book_id = book_id)\\n-            readBook.read_status = ub.ReadBook.STATUS_FINISHED\\n-            book = readBook\\n+            read_book = ub.ReadBook(user_id=current_user.id, book_id=book_id)\\n+            read_book.read_status = ub.ReadBook.STATUS_FINISHED\\n+            book = read_book\\n         if not book.kobo_reading_state:\\n             kobo_reading_state = ub.KoboReadingState(user_id=current_user.id, book_id=book_id)\\n             kobo_reading_state.current_bookmark = ub.KoboBookmark()\\n@@ -332,12 +330,13 @@ def edit_book_read_status(book_id, read_status=None):\\n         except (KeyError, AttributeError):\\n             log.error(u\"Custom Column No.%d is not existing in calibre database\", config.config_read_column)\\n             return \"Custom Column No.{} is not existing in calibre database\".format(config.config_read_column)\\n-        except (OperationalError, InvalidRequestError) as e:\\n+        except (OperationalError, InvalidRequestError) as ex:\\n             calibre_db.session.rollback()\\n-            log.error(u\"Read status could not set: {}\".format(e))\\n-            return _(\"Read status could not set: {}\".format(e.orig))\\n+            log.error(u\"Read status could not set: {}\".format(ex))\\n+            return _(\"Read status could not set: {}\".format(ex.orig))\\n     return \"\"\\n \\n+\\n # Deletes a book fro the local filestorage, returns True if deleting is successfull, otherwise false\\n def delete_book_file(book, calibrepath, book_format=None):\\n     # check that path is 2 elements deep, check that target path has no subfolders\\n@@ -361,15 +360,15 @@ def delete_book_file(book, calibrepath, book_format=None):\\n                                            id=book.id,\\n                                            path=book.path)\\n                     shutil.rmtree(path)\\n-                except (IOError, OSError) as e:\\n-                    log.error(\"Deleting book %s failed: %s\", book.id, e)\\n-                    return False, _(\"Deleting book %(id)s failed: %(message)s\", id=book.id, message=e)\\n+                except (IOError, OSError) as ex:\\n+                    log.error(\"Deleting book %s failed: %s\", book.id, ex)\\n+                    return False, _(\"Deleting book %(id)s failed: %(message)s\", id=book.id, message=ex)\\n                 authorpath = os.path.join(calibrepath, os.path.split(book.path)[0])\\n                 if not os.listdir(authorpath):\\n                     try:\\n                         shutil.rmtree(authorpath)\\n-                    except (IOError, OSError) as e:\\n-                        log.error(\"Deleting authorpath for book %s failed: %s\", book.id, e)\\n+                    except (IOError, OSError) as ex:\\n+                        log.error(\"Deleting authorpath for book %s failed: %s\", book.id, ex)\\n                 return True, None\\n \\n     log.error(\"Deleting book %s from database only, book path in database not valid: %s\",\\n@@ -395,21 +394,21 @@ def clean_author_database(renamed_author, calibre_path=\"\", local_book=None, gdri\\n                 all_titledir = book.path.split(\\'/\\')[1]\\n                 all_new_path = os.path.join(calibre_path, all_new_authordir, all_titledir)\\n                 all_new_name = get_valid_filename(book.title, chars=42) + \\' - \\' \\\\\\n-                               + get_valid_filename(new_author.name, chars=42)\\n+                    + get_valid_filename(new_author.name, chars=42)\\n                 # change location in database to new author/title path\\n                 book.path = os.path.join(all_new_authordir, all_titledir).replace(\\'\\\\\\\\\\', \\'/\\')\\n                 for file_format in book.data:\\n                     if not gdrive:\\n                         shutil.move(os.path.normcase(os.path.join(all_new_path,\\n                                                                   file_format.name + \\'.\\' + file_format.format.lower())),\\n-                            os.path.normcase(os.path.join(all_new_path,\\n-                                                          all_new_name + \\'.\\' + file_format.format.lower())))\\n+                                    os.path.normcase(os.path.join(all_new_path,\\n+                                                                  all_new_name + \\'.\\' + file_format.format.lower())))\\n                     else:\\n-                        gFile = gd.getFileFromEbooksFolder(all_new_path,\\n-                                                           file_format.name + \\'.\\' + file_format.format.lower())\\n-                        if gFile:\\n-                            gd.moveGdriveFileRemote(gFile, all_new_name + u\\'.\\' + file_format.format.lower())\\n-                            gd.updateDatabaseOnEdit(gFile[\\'id\\'], all_new_name + u\\'.\\' + file_format.format.lower())\\n+                        g_file = gd.getFileFromEbooksFolder(all_new_path,\\n+                                                            file_format.name + \\'.\\' + file_format.format.lower())\\n+                        if g_file:\\n+                            gd.moveGdriveFileRemote(g_file, all_new_name + u\\'.\\' + file_format.format.lower())\\n+                            gd.updateDatabaseOnEdit(g_file[\\'id\\'], all_new_name + u\\'.\\' + file_format.format.lower())\\n                         else:\\n                             log.error(\"File {} not found on gdrive\"\\n                                       .format(all_new_path, file_format.name + \\'.\\' + file_format.format.lower()))\\n@@ -426,16 +425,16 @@ def rename_all_authors(first_author, renamed_author, calibre_path=\"\", localbook=\\n             old_author_dir = get_valid_filename(r, chars=96)\\n             new_author_rename_dir = get_valid_filename(new_author.name, chars=96)\\n             if gdrive:\\n-                gFile = gd.getFileFromEbooksFolder(None, old_author_dir)\\n-                if gFile:\\n-                    gd.moveGdriveFolderRemote(gFile, new_author_rename_dir)\\n+                g_file = gd.getFileFromEbooksFolder(None, old_author_dir)\\n+                if g_file:\\n+                    gd.moveGdriveFolderRemote(g_file, new_author_rename_dir)\\n             else:\\n                 if os.path.isdir(os.path.join(calibre_path, old_author_dir)):\\n                     try:\\n                         old_author_path = os.path.join(calibre_path, old_author_dir)\\n                         new_author_path = os.path.join(calibre_path, new_author_rename_dir)\\n                         shutil.move(os.path.normcase(old_author_path), os.path.normcase(new_author_path))\\n-                    except (OSError) as ex:\\n+                    except OSError as ex:\\n                         log.error(\"Rename author from: %s to %s: %s\", old_author_path, new_author_path, ex)\\n                         log.debug(ex, exc_info=True)\\n                         return _(\"Rename author from: \\'%(src)s\\' to \\'%(dest)s\\' failed with error: %(error)s\",\\n@@ -444,6 +443,7 @@ def rename_all_authors(first_author, renamed_author, calibre_path=\"\", localbook=\\n         new_authordir = get_valid_filename(localbook.authors[0].name, chars=96)\\n     return new_authordir\\n \\n+\\n # Moves files in file storage during author/title rename, or from temp dir to file storage\\n def update_dir_structure_file(book_id, calibre_path, first_author, original_filepath, db_filename, renamed_author):\\n     # get book database entry from id, if original path overwrite source with original_filepath\\n@@ -483,11 +483,9 @@ def update_dir_structure_file(book_id, calibre_path, first_author, original_file\\n \\n \\n def upload_new_file_gdrive(book_id, first_author, renamed_author, title, title_dir, original_filepath, filename_ext):\\n-    error = False\\n     book = calibre_db.get_book(book_id)\\n     file_name = get_valid_filename(title, chars=42) + \\' - \\' + \\\\\\n-                get_valid_filename(first_author, chars=42) + \\\\\\n-                filename_ext\\n+        get_valid_filename(first_author, chars=42) + filename_ext\\n     rename_all_authors(first_author, renamed_author, gdrive=True)\\n     gdrive_path = os.path.join(get_valid_filename(first_author, chars=96),\\n                                title_dir + \" (\" + str(book_id) + \")\")\\n@@ -505,20 +503,20 @@ def update_dir_structure_gdrive(book_id, first_author, renamed_author):\\n     new_titledir = get_valid_filename(book.title, chars=96) + u\" (\" + str(book_id) + u\")\"\\n \\n     if titledir != new_titledir:\\n-        gFile = gd.getFileFromEbooksFolder(os.path.dirname(book.path), titledir)\\n-        if gFile:\\n-            gd.moveGdriveFileRemote(gFile, new_titledir)\\n+        g_file = gd.getFileFromEbooksFolder(os.path.dirname(book.path), titledir)\\n+        if g_file:\\n+            gd.moveGdriveFileRemote(g_file, new_titledir)\\n             book.path = book.path.split(\\'/\\')[0] + u\\'/\\' + new_titledir\\n-            gd.updateDatabaseOnEdit(gFile[\\'id\\'], book.path)     # only child folder affected\\n+            gd.updateDatabaseOnEdit(g_file[\\'id\\'], book.path)     # only child folder affected\\n         else:\\n             return _(u\\'File %(file)s not found on Google Drive\\', file=book.path)  # file not found\\n \\n     if authordir != new_authordir and authordir not in renamed_author:\\n-        gFile = gd.getFileFromEbooksFolder(os.path.dirname(book.path), new_titledir)\\n-        if gFile:\\n-            gd.moveGdriveFolderRemote(gFile, new_authordir)\\n+        g_file = gd.getFileFromEbooksFolder(os.path.dirname(book.path), new_titledir)\\n+        if g_file:\\n+            gd.moveGdriveFolderRemote(g_file, new_authordir)\\n             book.path = new_authordir + u\\'/\\' + book.path.split(\\'/\\')[1]\\n-            gd.updateDatabaseOnEdit(gFile[\\'id\\'], book.path)\\n+            gd.updateDatabaseOnEdit(g_file[\\'id\\'], book.path)\\n         else:\\n             return _(u\\'File %(file)s not found on Google Drive\\', file=authordir)  # file not found\\n \\n@@ -542,15 +540,15 @@ def move_files_on_change(calibre_path, new_authordir, new_titledir, localbook, d\\n                 # move original path to new path\\n                 log.debug(\"Moving title: %s to %s\", path, new_path)\\n                 shutil.move(os.path.normcase(path), os.path.normcase(new_path))\\n-            else: # path is valid copy only files to new location (merge)\\n+            else:  # path is valid copy only files to new location (merge)\\n                 log.info(\"Moving title: %s into existing: %s\", path, new_path)\\n                 # Take all files and subfolder from old path (strange command)\\n                 for dir_name, __, file_list in os.walk(path):\\n                     for file in file_list:\\n                         shutil.move(os.path.normcase(os.path.join(dir_name, file)),\\n-                                        os.path.normcase(os.path.join(new_path + dir_name[len(path):], file)))\\n+                                    os.path.normcase(os.path.join(new_path + dir_name[len(path):], file)))\\n         # change location in database to new author/title path\\n-        localbook.path = os.path.join(new_authordir, new_titledir).replace(\\'\\\\\\\\\\',\\'/\\')\\n+        localbook.path = os.path.join(new_authordir, new_titledir).replace(\\'\\\\\\\\\\', \\'/\\')\\n     except OSError as ex:\\n         log.error(\"Rename title from: %s to %s: %s\", path, new_path, ex)\\n         log.debug(ex, exc_info=True)\\n@@ -587,12 +585,12 @@ def delete_book_gdrive(book, book_format):\\n         for entry in book.data:\\n             if entry.format.upper() == book_format:\\n                 name = entry.name + \\'.\\' + book_format\\n-        gFile = gd.getFileFromEbooksFolder(book.path, name)\\n+        g_file = gd.getFileFromEbooksFolder(book.path, name)\\n     else:\\n-        gFile = gd.getFileFromEbooksFolder(os.path.dirname(book.path), book.path.split(\\'/\\')[1])\\n-    if gFile:\\n-        gd.deleteDatabaseEntry(gFile[\\'id\\'])\\n-        gFile.Trash()\\n+        g_file = gd.getFileFromEbooksFolder(os.path.dirname(book.path), book.path.split(\\'/\\')[1])\\n+    if g_file:\\n+        gd.deleteDatabaseEntry(g_file[\\'id\\'])\\n+        g_file.Trash()\\n     else:\\n         error = _(u\\'Book path %(path)s not found on Google Drive\\', path=book.path)  # file not found\\n \\n@@ -624,12 +622,13 @@ def generate_random_password():\\n \\n def uniq(inpt):\\n     output = []\\n-    inpt = [ \" \".join(inp.split()) for inp in inpt]\\n+    inpt = [\" \".join(inp.split()) for inp in inpt]\\n     for x in inpt:\\n         if x not in output:\\n             output.append(x)\\n     return output\\n \\n+\\n def check_email(email):\\n     email = valid_email(email)\\n     if ub.session.query(ub.User).filter(func.lower(ub.User.email) == email.lower()).first():\\n@@ -642,7 +641,7 @@ def check_username(username):\\n     username = username.strip()\\n     if ub.session.query(ub.User).filter(func.lower(ub.User.name) == username.lower()).scalar():\\n         log.error(u\"This username is already taken\")\\n-        raise Exception (_(u\"This username is already taken\"))\\n+        raise Exception(_(u\"This username is already taken\"))\\n     return username\\n \\n \\n@@ -728,13 +727,13 @@ def get_book_cover_internal(book, use_generic_cover_on_failure):\\n # saves book cover from url\\n def save_cover_from_url(url, book_path):\\n     try:\\n-        if not cli.allow_localhost:\\n-            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\\n-            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\\n-            if ip.startswith(\"127.\") or ip.startswith(\\'::ffff:7f\\') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\\n-                log.error(\"Localhost was accessed for cover upload\")\\n-                return False, _(\"You are not allowed to access localhost for cover uploads\")\\n-        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\\n+        if cli.allow_localhost:\\n+            img = requests.get(url, timeout=(10, 200), allow_redirects=False)  # ToDo: Error Handling\\n+        elif use_advocate:\\n+            img = advocate.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\\n+        else:\\n+            log.error(\"python modul advocate is not installed but is needed\")\\n+            return False, _(\"Python modul \\'advocate\\' is not installed but is needed for cover downloads\")\\n         img.raise_for_status()\\n         return save_cover(img, book_path)\\n     except (socket.gaierror,\\n@@ -746,6 +745,9 @@ def save_cover_from_url(url, book_path):\\n     except MissingDelegateError as ex:\\n         log.info(u\\'File Format Error %s\\', ex)\\n         return False, _(\"Cover Format Error\")\\n+    except UnacceptableAddressException:\\n+        log.error(\"Localhost was accessed for cover upload\")\\n+        return False, _(\"You are not allowed to access localhost for cover uploads\")\\n \\n \\n def save_cover_from_filestorage(filepath, saved_filename, img):\\n@@ -808,7 +810,7 @@ def save_cover(img, book_path):\\n             os.mkdir(tmp_dir)\\n         ret, message = save_cover_from_filestorage(tmp_dir, \"uploaded_cover.jpg\", img)\\n         if ret is True:\\n-            gd.uploadFileToEbooksFolder(os.path.join(book_path, \\'cover.jpg\\').replace(\"\\\\\\\\\",\"/\"),\\n+            gd.uploadFileToEbooksFolder(os.path.join(book_path, \\'cover.jpg\\').replace(\"\\\\\\\\\", \"/\"),\\n                                         os.path.join(tmp_dir, \"uploaded_cover.jpg\"))\\n             log.info(\"Cover is saved on Google Drive\")\\n             return True, None\\n@@ -820,9 +822,9 @@ def save_cover(img, book_path):\\n \\n def do_download_file(book, book_format, client, data, headers):\\n     if config.config_use_google_drive:\\n-        #startTime = time.time()\\n+        # startTime = time.time()\\n         df = gd.getFileFromEbooksFolder(book.path, data.name + \".\" + book_format)\\n-        #log.debug(\\'%s\\', time.time() - startTime)\\n+        # log.debug(\\'%s\\', time.time() - startTime)\\n         if df:\\n             return gd.do_gdrive_download(df, headers)\\n         else:\\n@@ -846,16 +848,16 @@ def do_download_file(book, book_format, client, data, headers):\\n ##################################\\n \\n \\n-def check_unrar(unrarLocation):\\n-    if not unrarLocation:\\n+def check_unrar(unrar_location):\\n+    if not unrar_location:\\n         return\\n \\n-    if not os.path.exists(unrarLocation):\\n+    if not os.path.exists(unrar_location):\\n         return _(\\'Unrar binary file not found\\')\\n \\n     try:\\n-        unrarLocation = [unrarLocation]\\n-        value = process_wait(unrarLocation, pattern=\\'UNRAR (.*) freeware\\')\\n+        unrar_location = [unrar_location]\\n+        value = process_wait(unrar_location, pattern=\\'UNRAR (.*) freeware\\')\\n         if value:\\n             version = value.group(1)\\n             log.debug(\"unrar version %s\", version)\\n@@ -882,19 +884,19 @@ def json_serial(obj):\\n \\n # helper function for displaying the runtime of tasks\\n def format_runtime(runtime):\\n-    retVal = \"\"\\n+    ret_val = \"\"\\n     if runtime.days:\\n-        retVal = format_unit(runtime.days, \\'duration-day\\', length=\"long\", locale=get_locale()) + \\', \\'\\n+        ret_val = format_unit(runtime.days, \\'duration-day\\', length=\"long\", locale=get_locale()) + \\', \\'\\n     mins, seconds = divmod(runtime.seconds, 60)\\n     hours, minutes = divmod(mins, 60)\\n     # ToDo: locale.number_symbols._data[\\'timeSeparator\\'] -> localize time separator ?\\n     if hours:\\n-        retVal += \\'{:d}:{:02d}:{:02d}s\\'.format(hours, minutes, seconds)\\n+        ret_val += \\'{:d}:{:02d}:{:02d}s\\'.format(hours, minutes, seconds)\\n     elif minutes:\\n-        retVal += \\'{:2d}:{:02d}s\\'.format(minutes, seconds)\\n+        ret_val += \\'{:2d}:{:02d}s\\'.format(minutes, seconds)\\n     else:\\n-        retVal += \\'{:2d}s\\'.format(seconds)\\n-    return retVal\\n+        ret_val += \\'{:2d}s\\'.format(seconds)\\n+    return ret_val\\n \\n \\n # helper function to apply localize status information in tasklist entries\\n@@ -951,8 +953,8 @@ def check_valid_domain(domain_text):\\n \\n \\n def get_cc_columns(filter_config_custom_read=False):\\n-    tmpcc = calibre_db.session.query(db.Custom_Columns)\\\\\\n-        .filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    tmpcc = calibre_db.session.query(db.CustomColumns)\\\\\\n+        .filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     cc = []\\n     r = None\\n     if config.config_columns_to_ignore:\\n@@ -971,6 +973,7 @@ def get_cc_columns(filter_config_custom_read=False):\\n def get_download_link(book_id, book_format, client):\\n     book_format = book_format.split(\".\")[0]\\n     book = calibre_db.get_filtered_book(book_id, allow_show_archived=True)\\n+    data1= \"\"\\n     if book:\\n         data1 = calibre_db.get_book_format(book.id, book_format.upper())\\n     else:', '@@ -28,7 +28,6 @@\\n from flask_login import current_user\\n from sqlalchemy.sql.expression import func, text, or_, and_, true\\n from werkzeug.security import check_password_hash\\n-from tornado.httputil import HTTPServerRequest\\n from . import constants, logger, config, db, calibre_db, ub, services, get_locale, isoLanguages\\n from .helper import get_download_link, get_book_cover\\n from .pagination import Pagination\\n@@ -99,26 +98,7 @@ def feed_normal_search():\\n @opds.route(\"/opds/books\")\\n @requires_basic_auth_if_no_ano\\n def feed_booksindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Books.sort, 1, 1)).label(\\'id\\'))\\\\\\n-        .filter(calibre_db.common_filters()).group_by(func.upper(func.substr(db.Books.sort, 1, 1))).all()\\n-\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n-\\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_books\\',\\n-                               pagination=pagination)\\n+    return render_element_index(db.Books.sort, None, \\'opds.feed_letter_books\\')\\n \\n \\n @opds.route(\"/opds/books/letter/<book_id>\")\\n@@ -171,43 +151,23 @@ def feed_hot():\\n     hot_books = all_books.offset(off).limit(config.config_books_per_page)\\n     entries = list()\\n     for book in hot_books:\\n-        downloadBook = calibre_db.get_book(book.Downloads.book_id)\\n-        if downloadBook:\\n+        download_book = calibre_db.get_book(book.Downloads.book_id)\\n+        if download_book:\\n             entries.append(\\n                 calibre_db.get_filtered_book(book.Downloads.book_id)\\n             )\\n         else:\\n             ub.delete_download(book.Downloads.book_id)\\n-    numBooks = entries.__len__()\\n+    num_books = entries.__len__()\\n     pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1),\\n-                            config.config_books_per_page, numBooks)\\n+                            config.config_books_per_page, num_books)\\n     return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n \\n \\n @opds.route(\"/opds/author\")\\n @requires_basic_auth_if_no_ano\\n def feed_authorindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Authors.sort, 1, 1)).label(\\'id\\'))\\\\\\n-        .join(db.books_authors_link).join(db.Books).filter(calibre_db.common_filters())\\\\\\n-        .group_by(func.upper(func.substr(db.Authors.sort, 1, 1))).all()\\n-\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n-\\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_author\\',\\n-                               pagination=pagination)\\n+    return render_element_index(db.Authors.sort, db.books_authors_link, \\'opds.feed_letter_author\\')\\n \\n \\n @opds.route(\"/opds/author/letter/<book_id>\")\\n@@ -228,12 +188,7 @@ def feed_letter_author(book_id):\\n @opds.route(\"/opds/author/<int:book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_author(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.authors.any(db.Authors.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Authors, book_id)\\n \\n \\n @opds.route(\"/opds/publisher\")\\n@@ -254,37 +209,14 @@ def feed_publisherindex():\\n @opds.route(\"/opds/publisher/<int:book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_publisher(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.publishers.any(db.Publishers.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Publishers, book_id)\\n \\n \\n @opds.route(\"/opds/category\")\\n @requires_basic_auth_if_no_ano\\n def feed_categoryindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Tags.name, 1, 1)).label(\\'id\\'))\\\\\\n-        .join(db.books_tags_link).join(db.Books).filter(calibre_db.common_filters())\\\\\\n-        .group_by(func.upper(func.substr(db.Tags.name, 1, 1))).all()\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n+    return render_element_index(db.Tags.name, db.books_tags_link, \\'opds.feed_letter_category\\')\\n \\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_category\\',\\n-                               pagination=pagination)\\n \\n @opds.route(\"/opds/category/letter/<book_id>\")\\n @requires_basic_auth_if_no_ano\\n@@ -306,36 +238,14 @@ def feed_letter_category(book_id):\\n @opds.route(\"/opds/category/<int:book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_category(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.tags.any(db.Tags.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Tags, book_id)\\n \\n \\n @opds.route(\"/opds/series\")\\n @requires_basic_auth_if_no_ano\\n def feed_seriesindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Series.sort, 1, 1)).label(\\'id\\'))\\\\\\n-        .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters())\\\\\\n-        .group_by(func.upper(func.substr(db.Series.sort, 1, 1))).all()\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_series\\',\\n-                               pagination=pagination)\\n+    return render_element_index(db.Series.sort, db.books_series_link, \\'opds.feed_letter_series\\')\\n+\\n \\n @opds.route(\"/opds/series/letter/<book_id>\")\\n @requires_basic_auth_if_no_ano\\n@@ -370,7 +280,7 @@ def feed_series(book_id):\\n def feed_ratingindex():\\n     off = request.args.get(\"offset\") or 0\\n     entries = calibre_db.session.query(db.Ratings, func.count(\\'books_ratings_link.book\\').label(\\'count\\'),\\n-                               (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n+                                       (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n         .join(db.books_ratings_link)\\\\\\n         .join(db.Books)\\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n@@ -388,12 +298,7 @@ def feed_ratingindex():\\n @opds.route(\"/opds/ratings/<book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_ratings(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.ratings.any(db.Ratings.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Tags, book_id)\\n \\n \\n @opds.route(\"/opds/formats\")\\n@@ -491,7 +396,7 @@ def feed_shelf(book_id):\\n @requires_basic_auth_if_no_ano\\n def opds_download_link(book_id, book_format):\\n     # I gave up with this: With enabled ldap login, the user doesn\\'t get logged in, therefore it\\'s always guest\\n-    # workaround, loading the user from the request and checking it\\'s download rights here\\n+    # workaround, loading the user from the request and checking its download rights here\\n     # in case of anonymous browsing user is None\\n     user = load_user_from_request(request) or current_user\\n     if not user.role_download():\\n@@ -517,6 +422,31 @@ def get_metadata_calibre_companion(uuid, library):\\n         return \"\"\\n \\n \\n+@opds.route(\"/opds/thumb_240_240/<book_id>\")\\n+@opds.route(\"/opds/cover_240_240/<book_id>\")\\n+@opds.route(\"/opds/cover_90_90/<book_id>\")\\n+@opds.route(\"/opds/cover/<book_id>\")\\n+@requires_basic_auth_if_no_ano\\n+def feed_get_cover(book_id):\\n+    return get_book_cover(book_id)\\n+\\n+\\n+@opds.route(\"/opds/readbooks\")\\n+@requires_basic_auth_if_no_ano\\n+def feed_read_books():\\n+    off = request.args.get(\"offset\") or 0\\n+    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, True, True)\\n+    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+\\n+\\n+@opds.route(\"/opds/unreadbooks\")\\n+@requires_basic_auth_if_no_ano\\n+def feed_unread_books():\\n+    off = request.args.get(\"offset\") or 0\\n+    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, False, True)\\n+    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+\\n+\\n def feed_search(term):\\n     if term:\\n         entries, __, ___ = calibre_db.get_search_results(term, config_read_column=config.config_read_column)\\n@@ -538,8 +468,8 @@ def check_auth(username, password):\\n     if bool(user and check_password_hash(str(user.password), password)):\\n         return True\\n     else:\\n-        ip_Address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n-        log.warning(\\'OPDS Login failed for user \"%s\" IP-address: %s\\', username.decode(\\'utf-8\\'), ip_Address)\\n+        ip_address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n+        log.warning(\\'OPDS Login failed for user \"%s\" IP-address: %s\\', username.decode(\\'utf-8\\'), ip_address)\\n         return False\\n \\n \\n@@ -559,26 +489,33 @@ def render_xml_template(*args, **kwargs):\\n     return response\\n \\n \\n-@opds.route(\"/opds/thumb_240_240/<book_id>\")\\n-@opds.route(\"/opds/cover_240_240/<book_id>\")\\n-@opds.route(\"/opds/cover_90_90/<book_id>\")\\n-@opds.route(\"/opds/cover/<book_id>\")\\n-@requires_basic_auth_if_no_ano\\n-def feed_get_cover(book_id):\\n-    return get_book_cover(book_id)\\n-\\n-\\n-@opds.route(\"/opds/readbooks\")\\n-@requires_basic_auth_if_no_ano\\n-def feed_read_books():\\n+def render_xml_dataset(data_table, book_id):\\n     off = request.args.get(\"offset\") or 0\\n-    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, True, True)\\n-    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n+                                                        db.Books,\\n+                                                        data_table.any(data_table.id == book_id),\\n+                                                        [db.Books.timestamp.desc()])\\n+    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n \\n \\n-@opds.route(\"/opds/unreadbooks\")\\n-@requires_basic_auth_if_no_ano\\n-def feed_unread_books():\\n-    off = request.args.get(\"offset\") or 0\\n-    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, False, True)\\n-    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+def render_element_index(database_column, linked_table, folder):\\n+    shift = 0\\n+    off = int(request.args.get(\"offset\") or 0)\\n+    entries = calibre_db.session.query(func.upper(func.substr(database_column, 1, 1)).label(\\'id\\'))\\n+    if linked_table:\\n+        entries = entries.join(linked_table).join(db.Books)\\n+    entries = entries.filter(calibre_db.common_filters()).group_by(func.upper(func.substr(database_column, 1, 1))).all()\\n+    elements = []\\n+    if off == 0:\\n+        elements.append({\\'id\\': \"00\", \\'name\\': _(\"All\")})\\n+        shift = 1\\n+    for entry in entries[\\n+                 off + shift - 1:\\n+                 int(off + int(config.config_books_per_page) - shift)]:\\n+        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n+    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n+                            len(entries) + 1)\\n+    return render_xml_template(\\'feed.xml\\',\\n+                               letterelements=elements,\\n+                               folder=folder,\\n+                               pagination=pagination)', '@@ -6,7 +6,7 @@\\n     data-escape=\"true\"\\n     {% if g.user.role_edit() %}\\n         data-editable-type=\"text\"\\n-        data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=parameter)}}\"\\n+        data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=parameter)}}\"\\n         data-editable-title=\"{{ edit_text }}\"\\n         data-edit=\"true\"\\n         {% if validate %}data-edit-validate=\"{{ _(\\'This Field is Required\\') }}\" {% endif %}\\n@@ -66,30 +66,30 @@ <h2 class=\"{{page}}\">{{_(title)}}</h2>\\n             {{ text_table_row(\\'authors\\', _(\\'Enter Authors\\'),_(\\'Authors\\'), true, true) }}\\n             {{ text_table_row(\\'tags\\', _(\\'Enter Categories\\'),_(\\'Categories\\'), false, true) }}\\n             {{ text_table_row(\\'series\\', _(\\'Enter Series\\'),_(\\'Series\\'), false, true) }}\\n-            <th data-field=\"series_index\" id=\"series_index\" data-visible=\"{{visiblility.get(\\'series_index\\')}}\" data-edit-validate=\"{{ _(\\'This Field is Required\\') }}\" data-sortable=\"true\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-min=\"0\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'series_index\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter Title\\')}}\"{% endif %}>{{_(\\'Series Index\\')}}</th>\\n+            <th data-field=\"series_index\" id=\"series_index\" data-visible=\"{{visiblility.get(\\'series_index\\')}}\" data-edit-validate=\"{{ _(\\'This Field is Required\\') }}\" data-sortable=\"true\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-min=\"0\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'series_index\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter Title\\')}}\"{% endif %}>{{_(\\'Series Index\\')}}</th>\\n             {{ text_table_row(\\'languages\\', _(\\'Enter Languages\\'),_(\\'Languages\\'), false, true) }}\\n             <!--th data-field=\"pubdate\" data-type=\"date\" data-visible=\"{{visiblility.get(\\'pubdate\\')}}\" data-viewformat=\"dd.mm.yyyy\" id=\"pubdate\" data-sortable=\"true\">{{_(\\'Publishing Date\\')}}</th-->\\n             {{ text_table_row(\\'publishers\\', _(\\'Enter Publishers\\'),_(\\'Publishers\\'), false, true) }}\\n-            <th data-field=\"comments\" id=\"comments\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'comments\\')}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'comments\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter comments\\')}}\"{% endif %}>{{_(\\'Comments\\')}}</th>\\n+            <th data-field=\"comments\" id=\"comments\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'comments\\')}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'comments\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter comments\\')}}\"{% endif %}>{{_(\\'Comments\\')}}</th>\\n             {% if g.user.check_visibility(32768) %}\\n                 {{ book_checkbox_row(\\'is_archived\\', _(\\'Archiv Status\\'), false)}}\\n             {%  endif %}\\n             {{ book_checkbox_row(\\'read_status\\', _(\\'Read Status\\'), false)}}\\n             {% for c in cc %}\\n               {% if c.datatype == \"int\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"1\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"1\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"rating\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-formatter=\"ratingFormatter\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.5\" data-editable-step=\"1\" data-editable-min=\"1\" data-editable-max=\"5\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-formatter=\"ratingFormatter\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.5\" data-editable-step=\"1\" data-editable-min=\"1\" data-editable-max=\"5\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"float\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"enumeration\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"select\" data-editable-source={{ url_for(\\'editbook.table_get_custom_enum\\', c_id=c.id)  }} data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"select\" data-editable-source={{ url_for(\\'edit-book.table_get_custom_enum\\', c_id=c.id)  }} data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype in [\"datetime\"] %}\\n                   <!-- missing -->\\n               {% elif c.datatype == \"text\" %}\\n                  {{ text_table_row(\\'custom_column_\\' + c.id|string, _(\\'Enter \\') + c.name, c.name, false, false) }}\\n               {% elif c.datatype == \"comments\" %}\\n-                  <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                  <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"bool\" %}\\n                   {{ book_checkbox_row(\\'custom_column_\\' + c.id|string, c.name, false)}}\\n               {% else %}', '@@ -40,7 +40,7 @@\\n from cps.shelf import shelf\\n from cps.admin import admi\\n from cps.gdrive import gdrive\\n-from cps.editbooks import editbook\\n+from cps.editbooks import EditBook\\n from cps.remotelogin import remotelogin\\n from cps.search_metadata import meta\\n from cps.error_handler import init_errorhandler\\n@@ -73,7 +73,7 @@ def main():\\n     app.register_blueprint(remotelogin)\\n     app.register_blueprint(meta)\\n     app.register_blueprint(gdrive)\\n-    app.register_blueprint(editbook)\\n+    app.register_blueprint(EditBook)\\n     if kobo_available:\\n         app.register_blueprint(kobo)\\n         app.register_blueprint(kobo_auth)', '@@ -27,8 +27,9 @@\\n import time\\n import operator\\n from datetime import datetime, timedelta\\n+from functools import wraps\\n \\n-from babel import Locale as LC\\n+from babel import Locale\\n from babel.dates import format_datetime\\n from flask import Blueprint, flash, redirect, url_for, abort, request, make_response, send_from_directory, g, Response\\n from flask_login import login_required, current_user, logout_user, confirm_login\\n@@ -47,7 +48,6 @@\\n from .render_template import render_title_template, get_sidebar_config\\n from . import debug_info, _BABEL_TRANSLATIONS\\n \\n-from functools import wraps\\n \\n log = logger.create()\\n \\n@@ -189,10 +189,10 @@ def admin():\\n         else:\\n             commit = version[\\'version\\']\\n \\n-    allUser = ub.session.query(ub.User).all()\\n+    all_user = ub.session.query(ub.User).all()\\n     email_settings = config.get_mail_settings()\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n-    return render_title_template(\"admin.html\", allUser=allUser, email=email_settings, config=config, commit=commit,\\n+    return render_title_template(\"admin.html\", allUser=all_user, email=email_settings, config=config, commit=commit,\\n                                  feature_support=feature_support, kobo_support=kobo_support,\\n                                  title=_(u\"Admin page\"), page=\"admin\")\\n \\n@@ -242,12 +242,12 @@ def calibreweb_alive():\\n @login_required\\n @admin_required\\n def view_configuration():\\n-    read_column = calibre_db.session.query(db.Custom_Columns)\\\\\\n-        .filter(and_(db.Custom_Columns.datatype == \\'bool\\', db.Custom_Columns.mark_for_delete == 0)).all()\\n-    restrict_columns = calibre_db.session.query(db.Custom_Columns)\\\\\\n-        .filter(and_(db.Custom_Columns.datatype == \\'text\\', db.Custom_Columns.mark_for_delete == 0)).all()\\n+    read_column = calibre_db.session.query(db.CustomColumns)\\\\\\n+        .filter(and_(db.CustomColumns.datatype == \\'bool\\', db.CustomColumns.mark_for_delete == 0)).all()\\n+    restrict_columns = calibre_db.session.query(db.CustomColumns)\\\\\\n+        .filter(and_(db.CustomColumns.datatype == \\'text\\', db.CustomColumns.mark_for_delete == 0)).all()\\n     languages = calibre_db.speaking_language()\\n-    translations = [LC(\\'en\\')] + babel.list_translations()\\n+    translations = [Locale(\\'en\\')] + babel.list_translations()\\n     return render_title_template(\"config_view_edit.html\", conf=config, readColumns=read_column,\\n                                  restrictColumns=restrict_columns,\\n                                  languages=languages,\\n@@ -261,8 +261,8 @@ def view_configuration():\\n def edit_user_table():\\n     visibility = current_user.view_settings.get(\\'useredit\\', {})\\n     languages = calibre_db.speaking_language()\\n-    translations = babel.list_translations() + [LC(\\'en\\')]\\n-    allUser = ub.session.query(ub.User)\\n+    translations = babel.list_translations() + [Locale(\\'en\\')]\\n+    all_user = ub.session.query(ub.User)\\n     tags = calibre_db.session.query(db.Tags)\\\\\\n         .join(db.books_tags_link)\\\\\\n         .join(db.Books)\\\\\\n@@ -274,10 +274,10 @@ def edit_user_table():\\n     else:\\n         custom_values = []\\n     if not config.config_anonbrowse:\\n-        allUser = allUser.filter(ub.User.role.op(\\'&\\')(constants.ROLE_ANONYMOUS) != constants.ROLE_ANONYMOUS)\\n+        all_user = all_user.filter(ub.User.role.op(\\'&\\')(constants.ROLE_ANONYMOUS) != constants.ROLE_ANONYMOUS)\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     return render_title_template(\"user_table.html\",\\n-                                 users=allUser.all(),\\n+                                 users=all_user.all(),\\n                                  tags=tags,\\n                                  custom_values=custom_values,\\n                                  translations=translations,\\n@@ -332,7 +332,7 @@ def list_users():\\n         if user.default_language == \"all\":\\n             user.default = _(\"All\")\\n         else:\\n-            user.default = LC.parse(user.default_language).get_language_name(get_locale())\\n+            user.default = Locale.parse(user.default_language).get_language_name(get_locale())\\n \\n     table_entries = {\\'totalNotFiltered\\': total_count, \\'total\\': filtered_count, \"rows\": users}\\n     js_list = json.dumps(table_entries, cls=db.AlchemyEncoder)\\n@@ -380,7 +380,7 @@ def delete_user():\\n @login_required\\n @admin_required\\n def table_get_locale():\\n-    locale = babel.list_translations() + [LC(\\'en\\')]\\n+    locale = babel.list_translations() + [Locale(\\'en\\')]\\n     ret = list()\\n     current_locale = get_locale()\\n     for loc in locale:\\n@@ -444,7 +444,7 @@ def edit_list_user(param):\\n                 elif param.endswith(\\'role\\'):\\n                     value = int(vals[\\'field_index\\'])\\n                     if user.name == \"Guest\" and value in \\\\\\n-                                 [constants.ROLE_ADMIN, constants.ROLE_PASSWD, constants.ROLE_EDIT_SHELFS]:\\n+                            [constants.ROLE_ADMIN, constants.ROLE_PASSWD, constants.ROLE_EDIT_SHELFS]:\\n                         raise Exception(_(\"Guest can\\'t have this role\"))\\n                     # check for valid value, last on checks for power of 2 value\\n                     if value > 0 and value <= constants.ROLE_VIEWER and (value & value-1 == 0 or value == 1):\\n@@ -524,16 +524,16 @@ def update_table_settings():\\n \\n def check_valid_read_column(column):\\n     if column != \"0\":\\n-        if not calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.id == column) \\\\\\n-              .filter(and_(db.Custom_Columns.datatype == \\'bool\\', db.Custom_Columns.mark_for_delete == 0)).all():\\n+        if not calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.id == column) \\\\\\n+              .filter(and_(db.CustomColumns.datatype == \\'bool\\', db.CustomColumns.mark_for_delete == 0)).all():\\n             return False\\n     return True\\n \\n \\n def check_valid_restricted_column(column):\\n     if column != \"0\":\\n-        if not calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.id == column) \\\\\\n-              .filter(and_(db.Custom_Columns.datatype == \\'text\\', db.Custom_Columns.mark_for_delete == 0)).all():\\n+        if not calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.id == column) \\\\\\n+              .filter(and_(db.CustomColumns.datatype == \\'text\\', db.CustomColumns.mark_for_delete == 0)).all():\\n             return False\\n     return True\\n \\n@@ -1078,12 +1078,12 @@ def _configuration_oauth_helper(to_save):\\n     reboot_required = False\\n     for element in oauthblueprints:\\n         if to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_id\"] != element[\\'oauth_client_id\\'] \\\\\\n-            or to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"] != element[\\'oauth_client_secret\\']:\\n+                or to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"] != element[\\'oauth_client_secret\\']:\\n             reboot_required = True\\n             element[\\'oauth_client_id\\'] = to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_id\"]\\n             element[\\'oauth_client_secret\\'] = to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"]\\n         if to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_id\"] \\\\\\n-            and to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"]:\\n+                and to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"]:\\n             active_oauths += 1\\n             element[\"active\"] = 1\\n         else:\\n@@ -1136,7 +1136,7 @@ def _configuration_ldap_helper(to_save):\\n     if not config.config_ldap_provider_url \\\\\\n         or not config.config_ldap_port \\\\\\n         or not config.config_ldap_dn \\\\\\n-        or not config.config_ldap_user_object:\\n+            or not config.config_ldap_user_object:\\n         return reboot_required, _configuration_result(_(\\'Please Enter a LDAP Provider, \\'\\n                                                         \\'Port, DN and User Object Identifier\\'))\\n \\n@@ -1211,6 +1211,7 @@ def _db_configuration_update_helper():\\n                                            \\'\\',\\n                                            to_save[\\'config_calibre_dir\\'],\\n                                            flags=re.IGNORECASE)\\n+    db_valid = False\\n     try:\\n         db_change, db_valid = _db_simulate_change()\\n \\n@@ -1229,11 +1230,11 @@ def _db_configuration_update_helper():\\n         return _db_configuration_result(\\'{}\\'.format(ex), gdrive_error)\\n \\n     if db_change or not db_valid or not config.db_configured \\\\\\n-          or config.config_calibre_dir != to_save[\"config_calibre_dir\"]:\\n+            or config.config_calibre_dir != to_save[\"config_calibre_dir\"]:\\n         if not calibre_db.setup_db(to_save[\\'config_calibre_dir\\'], ub.app_DB_path):\\n             return _db_configuration_result(_(\\'DB Location is not Valid, Please Enter Correct Path\\'),\\n                                             gdrive_error)\\n-        config.store_calibre_uuid(calibre_db, db.Library_Id)\\n+        config.store_calibre_uuid(calibre_db, db.LibraryId)\\n         # if db changed -> delete shelfs, delete download books, delete read books, kobo sync...\\n         if db_change:\\n             log.info(\"Calibre Database changed, delete all Calibre-Web info related to old Database\")\\n@@ -1272,7 +1273,7 @@ def _configuration_update_helper():\\n         _config_checkbox_int(to_save, \"config_unicode_filename\")\\n         # Reboot on config_anonbrowse with enabled ldap, as decoraters are changed in this case\\n         reboot_required |= (_config_checkbox_int(to_save, \"config_anonbrowse\")\\n-                             and config.config_login_type == constants.LOGIN_LDAP)\\n+                            and config.config_login_type == constants.LOGIN_LDAP)\\n         _config_checkbox_int(to_save, \"config_public_reg\")\\n         _config_checkbox_int(to_save, \"config_register_email\")\\n         reboot_required |= _config_checkbox_int(to_save, \"config_kobo_sync\")\\n@@ -1560,7 +1561,7 @@ def _handle_edit_user(to_save, content, languages, translations, kobo_support):\\n def new_user():\\n     content = ub.User()\\n     languages = calibre_db.speaking_language()\\n-    translations = [LC(\\'en\\')] + babel.list_translations()\\n+    translations = [Locale(\\'en\\')] + babel.list_translations()\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     if request.method == \"POST\":\\n         to_save = request.form.to_dict()\\n@@ -1647,7 +1648,7 @@ def edit_user(user_id):\\n         flash(_(u\"User not found\"), category=\"error\")\\n         return redirect(url_for(\\'admin.admin\\'))\\n     languages = calibre_db.speaking_language(return_all_languages=True)\\n-    translations = babel.list_translations() + [LC(\\'en\\')]\\n+    translations = babel.list_translations() + [Locale(\\'en\\')]\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     if request.method == \"POST\":\\n         to_save = request.form.to_dict()', '@@ -17,13 +17,13 @@\\n #  You should have received a copy of the GNU General Public License\\n #  along with this program. If not, see <http://www.gnu.org/licenses/>.\\n \\n-import copy\\n import os\\n import re\\n import ast\\n import json\\n from datetime import datetime\\n from urllib.parse import quote\\n+import unidecode\\n \\n from sqlalchemy import create_engine\\n from sqlalchemy import Table, Column, ForeignKey, CheckConstraint\\n@@ -49,11 +49,6 @@\\n \\n from weakref import WeakSet\\n \\n-try:\\n-    import unidecode\\n-    use_unidecode = True\\n-except ImportError:\\n-    use_unidecode = False\\n \\n log = logger.create()\\n \\n@@ -93,7 +88,7 @@\\n                               )\\n \\n \\n-class Library_Id(Base):\\n+class LibraryId(Base):\\n     __tablename__ = \\'library_id\\'\\n     id = Column(Integer, primary_key=True)\\n     uuid = Column(String, nullable=False)\\n@@ -112,7 +107,7 @@ def __init__(self, val, id_type, book):\\n         self.type = id_type\\n         self.book = book\\n \\n-    def formatType(self):\\n+    def format_type(self):\\n         format_type = self.type.lower()\\n         if format_type == \\'amazon\\':\\n             return u\"Amazon\"\\n@@ -184,8 +179,8 @@ class Comments(Base):\\n     book = Column(Integer, ForeignKey(\\'books.id\\'), nullable=False, unique=True)\\n     text = Column(String(collation=\\'NOCASE\\'), nullable=False)\\n \\n-    def __init__(self, text, book):\\n-        self.text = text\\n+    def __init__(self, comment, book):\\n+        self.text = comment\\n         self.book = book\\n \\n     def get(self):\\n@@ -367,18 +362,17 @@ def __init__(self, title, sort, author_sort, timestamp, pubdate, series_index, l\\n         self.path = path\\n         self.has_cover = (has_cover != None)\\n \\n-\\n     def __repr__(self):\\n         return u\"<Books(\\'{0},{1}{2}{3}{4}{5}{6}{7}{8}\\')>\".format(self.title, self.sort, self.author_sort,\\n                                                                  self.timestamp, self.pubdate, self.series_index,\\n                                                                  self.last_modified, self.path, self.has_cover)\\n \\n     @property\\n     def atom_timestamp(self):\\n-        return (self.timestamp.strftime(\\'%Y-%m-%dT%H:%M:%S+00:00\\') or \\'\\')\\n+        return self.timestamp.strftime(\\'%Y-%m-%dT%H:%M:%S+00:00\\') or \\'\\'\\n \\n \\n-class Custom_Columns(Base):\\n+class CustomColumns(Base):\\n     __tablename__ = \\'custom_columns\\'\\n \\n     id = Column(Integer, primary_key=True)\\n@@ -436,7 +430,7 @@ def default(self, o):\\n         return json.JSONEncoder.default(self, o)\\n \\n \\n-class CalibreDB():\\n+class CalibreDB:\\n     _init = False\\n     engine = None\\n     config = None\\n@@ -450,17 +444,17 @@ def __init__(self, expire_on_commit=True):\\n         \"\"\"\\n         self.session = None\\n         if self._init:\\n-            self.initSession(expire_on_commit)\\n+            self.init_session(expire_on_commit)\\n \\n         self.instances.add(self)\\n \\n-    def initSession(self, expire_on_commit=True):\\n+    def init_session(self, expire_on_commit=True):\\n         self.session = self.session_factory()\\n         self.session.expire_on_commit = expire_on_commit\\n         self.update_title_sort(self.config)\\n \\n     @classmethod\\n-    def setup_db_cc_classes(self, cc):\\n+    def setup_db_cc_classes(cls, cc):\\n         cc_ids = []\\n         books_custom_column_links = {}\\n         for row in cc:\\n@@ -539,16 +533,16 @@ def check_valid_db(cls, config_calibre_dir, app_db_path, config_calibre_uuid):\\n             return False, False\\n         try:\\n             check_engine = create_engine(\\'sqlite://\\',\\n-                          echo=False,\\n-                          isolation_level=\"SERIALIZABLE\",\\n-                          connect_args={\\'check_same_thread\\': False},\\n-                          poolclass=StaticPool)\\n+                                         echo=False,\\n+                                         isolation_level=\"SERIALIZABLE\",\\n+                                         connect_args={\\'check_same_thread\\': False},\\n+                                         poolclass=StaticPool)\\n             with check_engine.begin() as connection:\\n                 connection.execute(text(\"attach database \\'{}\\' as calibre;\".format(dbpath)))\\n                 connection.execute(text(\"attach database \\'{}\\' as app_settings;\".format(app_db_path)))\\n                 local_session = scoped_session(sessionmaker())\\n                 local_session.configure(bind=connection)\\n-                database_uuid = local_session().query(Library_Id).one_or_none()\\n+                database_uuid = local_session().query(LibraryId).one_or_none()\\n                 # local_session.dispose()\\n \\n             check_engine.connect()\\n@@ -603,7 +597,7 @@ def setup_db(cls, config_calibre_dir, app_db_path):\\n                                                           autoflush=True,\\n                                                           bind=cls.engine))\\n         for inst in cls.instances:\\n-            inst.initSession()\\n+            inst.init_session()\\n \\n         cls._init = True\\n         return True\\n@@ -644,12 +638,10 @@ def get_book_format(self, book_id, file_format):\\n     # Language and content filters for displaying in the UI\\n     def common_filters(self, allow_show_archived=False, return_all_languages=False):\\n         if not allow_show_archived:\\n-            archived_books = (\\n-                ub.session.query(ub.ArchivedBook)\\n-                    .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n-                    .filter(ub.ArchivedBook.is_archived == True)\\n-                    .all()\\n-            )\\n+            archived_books = (ub.session.query(ub.ArchivedBook)\\n+                              .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n+                              .filter(ub.ArchivedBook.is_archived == True)\\n+                              .all())\\n             archived_book_ids = [archived_book.book_id for archived_book in archived_books]\\n             archived_filter = Books.id.notin_(archived_book_ids)\\n         else:\\n@@ -668,11 +660,11 @@ def common_filters(self, allow_show_archived=False, return_all_languages=False):\\n                 pos_cc_list = current_user.allowed_column_value.split(\\',\\')\\n                 pos_content_cc_filter = true() if pos_cc_list == [\\'\\'] else \\\\\\n                     getattr(Books, \\'custom_column_\\' + str(self.config.config_restricted_column)). \\\\\\n-                        any(cc_classes[self.config.config_restricted_column].value.in_(pos_cc_list))\\n+                    any(cc_classes[self.config.config_restricted_column].value.in_(pos_cc_list))\\n                 neg_cc_list = current_user.denied_column_value.split(\\',\\')\\n                 neg_content_cc_filter = false() if neg_cc_list == [\\'\\'] else \\\\\\n                     getattr(Books, \\'custom_column_\\' + str(self.config.config_restricted_column)). \\\\\\n-                        any(cc_classes[self.config.config_restricted_column].value.in_(neg_cc_list))\\n+                    any(cc_classes[self.config.config_restricted_column].value.in_(neg_cc_list))\\n             except (KeyError, AttributeError):\\n                 pos_content_cc_filter = false()\\n                 neg_content_cc_filter = true()\\n@@ -728,7 +720,7 @@ def fill_indexpage_with_archived_books(self, page, database, pagesize, db_filter\\n                 query = (self.session.query(database, ub.ReadBook.read_status, ub.ArchivedBook.is_archived)\\n                          .select_from(Books)\\n                          .outerjoin(ub.ReadBook,\\n-                               and_(ub.ReadBook.user_id == int(current_user.id), ub.ReadBook.book_id == Books.id)))\\n+                                    and_(ub.ReadBook.user_id == int(current_user.id), ub.ReadBook.book_id == Books.id)))\\n             else:\\n                 try:\\n                     read_column = cc_classes[config_read_column]\\n@@ -738,7 +730,7 @@ def fill_indexpage_with_archived_books(self, page, database, pagesize, db_filter\\n                 except (KeyError, AttributeError):\\n                     log.error(\"Custom Column No.%d is not existing in calibre database\", read_column)\\n                     # Skip linking read column and return None instead of read status\\n-                    query =self.session.query(database, None, ub.ArchivedBook.is_archived)\\n+                    query = self.session.query(database, None, ub.ArchivedBook.is_archived)\\n             query = query.outerjoin(ub.ArchivedBook, and_(Books.id == ub.ArchivedBook.book_id,\\n                                                           int(current_user.id) == ub.ArchivedBook.user_id))\\n         else:\\n@@ -812,7 +804,6 @@ def order_authors(self, entries, list_return=False, combined=False):\\n                 return authors_ordered\\n         return entries\\n \\n-\\n     def get_typeahead(self, database, query, replace=(\\'\\', \\'\\'), tag_filter=true()):\\n         query = query or \\'\\'\\n         self.session.connection().connection.connection.create_function(\"lower\", 1, lcase)\\n@@ -872,7 +863,7 @@ def search_query(self, term, config_read_column, *join):\\n                 ))\\n \\n     # read search results from calibre-database and return it (function is used for feed and simple search\\n-    def get_search_results(self, term, offset=None, order=None, limit=None, allow_show_archived=False,\\n+    def get_search_results(self, term, offset=None, order=None, limit=None,\\n                            config_read_column=False, *join):\\n         order = order[0] if order else [Books.sort]\\n         pagination = None\\n@@ -915,7 +906,6 @@ def speaking_language(self, languages=None, return_all_languages=False, with_cou\\n                 lang.name = isoLanguages.get_language_name(get_locale(), lang.lang_code)\\n             return sorted(languages, key=lambda x: x.name, reverse=reverse_order)\\n \\n-\\n     def update_title_sort(self, config, conn=None):\\n         # user defined sort function for calibre databases (Series, etc.)\\n         def _title_sort(title):\\n@@ -973,6 +963,6 @@ def lcase(s):\\n     try:\\n         return unidecode.unidecode(s.lower())\\n     except Exception as ex:\\n-        log = logger.create()\\n-        log.error_or_exception(ex)\\n+        _log = logger.create()\\n+        _log.error_or_exception(ex)\\n         return s.lower()', '@@ -31,7 +31,7 @@\\n try:\\n     from lxml.html.clean import clean_html\\n except ImportError:\\n-    pass\\n+    clean_html = None\\n \\n from flask import Blueprint, request, flash, redirect, url_for, abort, Markup, Response\\n from flask_babel import gettext as _\\n@@ -48,7 +48,7 @@\\n from .kobo_sync_status import change_archived_books\\n \\n \\n-editbook = Blueprint(\\'editbook\\', __name__)\\n+EditBook = Blueprint(\\'edit-book\\', __name__)\\n log = logger.create()\\n \\n \\n@@ -61,6 +61,7 @@ def inner(*args, **kwargs):\\n \\n     return inner\\n \\n+\\n def edit_required(f):\\n     @wraps(f)\\n     def inner(*args, **kwargs):\\n@@ -70,6 +71,7 @@ def inner(*args, **kwargs):\\n \\n     return inner\\n \\n+\\n def search_objects_remove(db_book_object, db_type, input_elements):\\n     del_elements = []\\n     for c_elements in db_book_object:\\n@@ -119,6 +121,7 @@ def remove_objects(db_book_object, db_session, del_elements):\\n                 db_session.delete(del_element)\\n     return changed\\n \\n+\\n def add_objects(db_book_object, db_object, db_session, db_type, add_elements):\\n     changed = False\\n     if db_type == \\'languages\\':\\n@@ -128,7 +131,7 @@ def add_objects(db_book_object, db_object, db_session, db_type, add_elements):\\n     else:\\n         db_filter = db_object.name\\n     for add_element in add_elements:\\n-        # check if a element with that name exists\\n+        # check if an element with that name exists\\n         db_element = db_session.query(db_object).filter(db_filter == add_element).first()\\n         # if no element is found add it\\n         if db_type == \\'author\\':\\n@@ -147,7 +150,6 @@ def add_objects(db_book_object, db_object, db_session, db_type, add_elements):\\n             db_book_object.append(new_element)\\n         else:\\n             db_element = create_objects_for_addition(db_element, add_element, db_type)\\n-            changed = True\\n             # add element to book\\n             changed = True\\n             db_book_object.append(db_element)\\n@@ -178,7 +180,7 @@ def create_objects_for_addition(db_element, add_element, db_type):\\n     return db_element\\n \\n \\n-# Modifies different Database objects, first check if elements if elements have to be deleted,\\n+# Modifies different Database objects, first check if elements have to be deleted,\\n # because they are no longer used, than check if elements have to be added to database\\n def modify_database_object(input_elements, db_book_object, db_object, db_session, db_type):\\n     # passing input_elements not as a list may lead to undesired results\\n@@ -207,7 +209,7 @@ def modify_identifiers(input_identifiers, db_identifiers, db_session):\\n     input_dict = dict([(identifier.type.lower(), identifier) for identifier in input_identifiers])\\n     if len(input_identifiers) != len(input_dict):\\n         error = True\\n-    db_dict = dict([(identifier.type.lower(), identifier) for identifier in db_identifiers ])\\n+    db_dict = dict([(identifier.type.lower(), identifier) for identifier in db_identifiers])\\n     # delete db identifiers not present in input or modify them with input val\\n     for identifier_type, identifier in db_dict.items():\\n         if identifier_type not in input_dict.keys():\\n@@ -224,14 +226,15 @@ def modify_identifiers(input_identifiers, db_identifiers, db_session):\\n             changed = True\\n     return changed, error\\n \\n-@editbook.route(\"/ajax/delete/<int:book_id>\", methods=[\"POST\"])\\n+\\n+@EditBook.route(\"/ajax/delete/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_from_details(book_id):\\n     return Response(delete_book_from_table(book_id, \"\", True), mimetype=\\'application/json\\')\\n \\n \\n-@editbook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"}, methods=[\"POST\"])\\n-@editbook.route(\"/delete/<int:book_id>/<string:book_format>\", methods=[\"POST\"])\\n+@EditBook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"}, methods=[\"POST\"])\\n+@EditBook.route(\"/delete/<int:book_id>/<string:book_format>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_ajax(book_id, book_format):\\n     return delete_book_from_table(book_id, book_format, False)\\n@@ -252,8 +255,8 @@ def delete_whole_book(book_id, book):\\n     modify_database_object([u\\'\\'], book.languages, db.Languages, calibre_db.session, \\'languages\\')\\n     modify_database_object([u\\'\\'], book.publishers, db.Publishers, calibre_db.session, \\'publishers\\')\\n \\n-    cc = calibre_db.session.query(db.Custom_Columns). \\\\\\n-        filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    cc = calibre_db.session.query(db.CustomColumns). \\\\\\n+        filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     for c in cc:\\n         cc_string = \"custom_column_\" + str(c.id)\\n         if not c.is_multiple:\\n@@ -283,18 +286,18 @@ def delete_whole_book(book_id, book):\\n     calibre_db.session.query(db.Books).filter(db.Books.id == book_id).delete()\\n \\n \\n-def render_delete_book_result(book_format, jsonResponse, warning, book_id):\\n+def render_delete_book_result(book_format, json_response, warning, book_id):\\n     if book_format:\\n-        if jsonResponse:\\n-            return json.dumps([warning, {\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n+        if json_response:\\n+            return json.dumps([warning, {\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n                                          \"type\": \"success\",\\n                                          \"format\": book_format,\\n                                          \"message\": _(\\'Book Format Successfully Deleted\\')}])\\n         else:\\n             flash(_(\\'Book Format Successfully Deleted\\'), category=\"success\")\\n-            return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+            return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n     else:\\n-        if jsonResponse:\\n+        if json_response:\\n             return json.dumps([warning, {\"location\": url_for(\\'web.index\\'),\\n                                          \"type\": \"success\",\\n                                          \"format\": book_format,\\n@@ -304,28 +307,28 @@ def render_delete_book_result(book_format, jsonResponse, warning, book_id):\\n             return redirect(url_for(\\'web.index\\'))\\n \\n \\n-def delete_book_from_table(book_id, book_format, jsonResponse):\\n+def delete_book_from_table(book_id, book_format, json_response):\\n     warning = {}\\n     if current_user.role_delete_books():\\n         book = calibre_db.get_book(book_id)\\n         if book:\\n             try:\\n                 result, error = helper.delete_book(book, config.config_calibre_dir, book_format=book_format.upper())\\n                 if not result:\\n-                    if jsonResponse:\\n-                        return json.dumps([{\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n-                                           \"type\": \"danger\",\\n-                                           \"format\": \"\",\\n-                                           \"message\": error}])\\n+                    if json_response:\\n+                        return json.dumps([{\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n+                                            \"type\": \"danger\",\\n+                                            \"format\": \"\",\\n+                                            \"message\": error}])\\n                     else:\\n                         flash(error, category=\"error\")\\n-                        return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+                        return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n                 if error:\\n-                    if jsonResponse:\\n-                        warning = {\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n-                                                \"type\": \"warning\",\\n-                                                \"format\": \"\",\\n-                                                \"message\": error}\\n+                    if json_response:\\n+                        warning = {\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n+                                   \"type\": \"warning\",\\n+                                   \"format\": \"\",\\n+                                   \"message\": error}\\n                     else:\\n                         flash(error, category=\"warning\")\\n                 if not book_format:\\n@@ -339,35 +342,36 @@ def delete_book_from_table(book_id, book_format, jsonResponse):\\n             except Exception as ex:\\n                 log.error_or_exception(ex)\\n                 calibre_db.session.rollback()\\n-                if jsonResponse:\\n-                    return json.dumps([{\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n+                if json_response:\\n+                    return json.dumps([{\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n                                         \"type\": \"danger\",\\n                                         \"format\": \"\",\\n                                         \"message\": ex}])\\n                 else:\\n                     flash(str(ex), category=\"error\")\\n-                    return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+                    return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n         else:\\n             # book not found\\n             log.error(\\'Book with id \"%s\" could not be deleted: not found\\', book_id)\\n-        return render_delete_book_result(book_format, jsonResponse, warning, book_id)\\n+        return render_delete_book_result(book_format, json_response, warning, book_id)\\n     message = _(\"You are missing permissions to delete books\")\\n-    if jsonResponse:\\n-        return json.dumps({\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n+    if json_response:\\n+        return json.dumps({\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n                            \"type\": \"danger\",\\n                            \"format\": \"\",\\n                            \"message\": message})\\n     else:\\n         flash(message, category=\"error\")\\n-        return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+        return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n \\n def render_edit_book(book_id):\\n-    cc = calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    cc = calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     book = calibre_db.get_filtered_book(book_id, allow_show_archived=True)\\n     if not book:\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         return redirect(url_for(\"web.index\"))\\n \\n     for lang in book.languages:\\n@@ -380,9 +384,9 @@ def render_edit_book(book_id):\\n         author_names.append(authr.name.replace(\\'|\\', \\',\\'))\\n \\n     # Option for showing convertbook button\\n-    valid_source_formats=list()\\n+    valid_source_formats = list()\\n     allowed_conversion_formats = list()\\n-    kepub_possible=None\\n+    kepub_possible = None\\n     if config.config_converterpath:\\n         for file in book.data:\\n             if file.format.lower() in constants.EXTENSIONS_CONVERT_FROM:\\n@@ -430,6 +434,7 @@ def edit_book_ratings(to_save, book):\\n             changed = True\\n     return changed\\n \\n+\\n def edit_book_tags(tags, book):\\n     input_tags = tags.split(\\',\\')\\n     input_tags = list(map(lambda it: it.strip(), input_tags))\\n@@ -446,48 +451,48 @@ def edit_book_series(series, book):\\n \\n def edit_book_series_index(series_index, book):\\n     # Add default series_index to book\\n-    modif_date = False\\n+    modify_date = False\\n     series_index = series_index or \\'1\\'\\n     if not series_index.replace(\\'.\\', \\'\\', 1).isdigit():\\n         flash(_(\"%(seriesindex)s is not a valid number, skipping\", seriesindex=series_index), category=\"warning\")\\n         return False\\n     if str(book.series_index) != series_index:\\n         book.series_index = series_index\\n-        modif_date = True\\n-    return modif_date\\n+        modify_date = True\\n+    return modify_date\\n \\n \\n # Handle book comments/description\\n def edit_book_comments(comments, book):\\n-    modif_date = False\\n+    modify_date = False\\n     if comments:\\n         comments = clean_html(comments)\\n     if len(book.comments):\\n         if book.comments[0].text != comments:\\n             book.comments[0].text = comments\\n-            modif_date = True\\n+            modify_date = True\\n     else:\\n         if comments:\\n-            book.comments.append(db.Comments(text=comments, book=book.id))\\n-            modif_date = True\\n-    return modif_date\\n+            book.comments.append(db.Comments(comment=comments, book=book.id))\\n+            modify_date = True\\n+    return modify_date\\n \\n \\n-def edit_book_languages(languages, book, upload=False, invalid=None):\\n+def edit_book_languages(languages, book, upload_mode=False, invalid=None):\\n     input_languages = languages.split(\\',\\')\\n     unknown_languages = []\\n-    if not upload:\\n+    if not upload_mode:\\n         input_l = isoLanguages.get_language_codes(get_locale(), input_languages, unknown_languages)\\n     else:\\n         input_l = isoLanguages.get_valid_language_codes(get_locale(), input_languages, unknown_languages)\\n-    for l in unknown_languages:\\n-        log.error(\"\\'%s\\' is not a valid language\", l)\\n+    for lang in unknown_languages:\\n+        log.error(\"\\'%s\\' is not a valid language\", lang)\\n         if isinstance(invalid, list):\\n-            invalid.append(l)\\n+            invalid.append(lang)\\n         else:\\n-            raise ValueError(_(u\"\\'%(langname)s\\' is not a valid language\", langname=l))\\n+            raise ValueError(_(u\"\\'%(langname)s\\' is not a valid language\", langname=lang))\\n     # ToDo: Not working correct\\n-    if upload and len(input_l) == 1:\\n+    if upload_mode and len(input_l) == 1:\\n         # If the language of the file is excluded from the users view, it\\'s not imported, to allow the user to view\\n         # the book it\\'s language is set to the filter language\\n         if input_l[0] != current_user.filter_language() and current_user.filter_language() != \"all\":\\n@@ -571,17 +576,20 @@ def edit_cc_data_string(book, c, to_save, cc_db_value, cc_string):\\n         getattr(book, cc_string).append(new_cc)\\n     return changed, to_save\\n \\n+\\n def edit_single_cc_data(book_id, book, column_id, to_save):\\n-    cc = (calibre_db.session.query(db.Custom_Columns)\\n-          .filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions))\\n-          .filter(db.Custom_Columns.id == column_id)\\n+    cc = (calibre_db.session.query(db.CustomColumns)\\n+          .filter(db.CustomColumns.datatype.notin_(db.cc_exceptions))\\n+          .filter(db.CustomColumns.id == column_id)\\n           .all())\\n     return edit_cc_data(book_id, book, to_save, cc)\\n \\n+\\n def edit_all_cc_data(book_id, book, to_save):\\n-    cc = calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    cc = calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     return edit_cc_data(book_id, book, to_save, cc)\\n \\n+\\n def edit_cc_data(book_id, book, to_save, cc):\\n     changed = False\\n     for c in cc:\\n@@ -614,10 +622,11 @@ def edit_cc_data(book_id, book, to_save, cc):\\n                                               \\'custom\\')\\n     return changed\\n \\n-def upload_single_file(request, book, book_id):\\n+\\n+def upload_single_file(file_request, book, book_id):\\n     # Check and handle Uploaded file\\n-    if \\'btn-upload-format\\' in request.files:\\n-        requested_file = request.files[\\'btn-upload-format\\']\\n+    if \\'btn-upload-format\\' in file_request.files:\\n+        requested_file = file_request.files[\\'btn-upload-format\\']\\n         # check for empty request\\n         if requested_file.filename != \\'\\':\\n             if not current_user.role_upload():\\n@@ -669,17 +678,17 @@ def upload_single_file(request, book, book_id):\\n \\n             # Queue uploader info\\n             link = \\'<a href=\"{}\">{}</a>\\'.format(url_for(\\'web.show_book\\', book_id=book.id), escape(book.title))\\n-            uploadText=_(u\"File format %(ext)s added to %(book)s\", ext=file_ext.upper(), book=link)\\n-            WorkerThread.add(current_user.name, TaskUpload(uploadText, escape(book.title)))\\n+            upload_text = _(u\"File format %(ext)s added to %(book)s\", ext=file_ext.upper(), book=link)\\n+            WorkerThread.add(current_user.name, TaskUpload(upload_text, escape(book.title)))\\n \\n             return uploader.process(\\n                 saved_filename, *os.path.splitext(requested_file.filename),\\n                 rarExecutable=config.config_rarfile_location)\\n \\n \\n-def upload_cover(request, book):\\n-    if \\'btn-upload-cover\\' in request.files:\\n-        requested_file = request.files[\\'btn-upload-cover\\']\\n+def upload_cover(cover_request, book):\\n+    if \\'btn-upload-cover\\' in cover_request.files:\\n+        requested_file = cover_request.files[\\'btn-upload-cover\\']\\n         # check for empty request\\n         if requested_file.filename != \\'\\':\\n             if not current_user.role_upload():\\n@@ -706,8 +715,8 @@ def handle_title_on_edit(book, book_title):\\n \\n def handle_author_on_edit(book, author_name, update_stored=True):\\n     # handle author(s)\\n-    # renamed = False\\n-    input_authors = author_name.split(\\'&\\')\\n+    input_authors, renamed = prepare_authors(author_name)\\n+    \\'\\'\\'input_authors = author_name.split(\\'&\\')\\n     input_authors = list(map(lambda it: it.strip().replace(\\',\\', \\'|\\'), input_authors))\\n     # Remove duplicates in authors list\\n     input_authors = helper.uniq(input_authors)\\n@@ -725,7 +734,7 @@ def handle_author_on_edit(book, author_name, update_stored=True):\\n             sorted_renamed_author = helper.get_sorted_author(renamed_author.name)\\n             sorted_old_author = helper.get_sorted_author(in_aut)\\n             for one_book in all_books:\\n-                one_book.author_sort = one_book.author_sort.replace(sorted_renamed_author, sorted_old_author)\\n+                one_book.author_sort = one_book.author_sort.replace(sorted_renamed_author, sorted_old_author)\\'\\'\\'\\n \\n     change = modify_database_object(input_authors, book.authors, db.Authors, calibre_db.session, \\'author\\')\\n \\n@@ -746,11 +755,11 @@ def handle_author_on_edit(book, author_name, update_stored=True):\\n     return input_authors, change, renamed\\n \\n \\n-@editbook.route(\"/admin/book/<int:book_id>\", methods=[\\'GET\\', \\'POST\\'])\\n+@EditBook.route(\"/admin/book/<int:book_id>\", methods=[\\'GET\\', \\'POST\\'])\\n @login_required_if_no_ano\\n @edit_required\\n def edit_book(book_id):\\n-    modif_date = False\\n+    modify_date = False\\n \\n     # create the function for sorting...\\n     try:\\n@@ -767,13 +776,14 @@ def edit_book(book_id):\\n \\n     # Book not found\\n     if not book:\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         return redirect(url_for(\"web.index\"))\\n \\n     meta = upload_single_file(request, book, book_id)\\n     if upload_cover(request, book) is True:\\n         book.has_cover = 1\\n-        modif_date = True\\n+        modify_date = True\\n     try:\\n         to_save = request.form.to_dict()\\n         merge_metadata(to_save, meta)\\n@@ -786,15 +796,15 @@ def edit_book(book_id):\\n         input_authors, authorchange, renamed = handle_author_on_edit(book, to_save[\"author_name\"])\\n         if authorchange or title_change:\\n             edited_books_id = book.id\\n-            modif_date = True\\n+            modify_date = True\\n \\n         if config.config_use_google_drive:\\n             gdriveutils.updateGdriveCalibreFromLocal()\\n \\n-        error = False\\n+        error = \"\"\\n         if edited_books_id:\\n             error = helper.update_dir_structure(edited_books_id, config.config_calibre_dir, input_authors[0],\\n-                                               renamed_author=renamed)\\n+                                                renamed_author=renamed)\\n \\n         if not error:\\n             if \"cover_url\" in to_save:\\n@@ -808,32 +818,32 @@ def edit_book(book_id):\\n                         result, error = helper.save_cover_from_url(to_save[\"cover_url\"], book.path)\\n                         if result is True:\\n                             book.has_cover = 1\\n-                            modif_date = True\\n+                            modify_date = True\\n                         else:\\n                             flash(error, category=\"error\")\\n \\n             # Add default series_index to book\\n-            modif_date |= edit_book_series_index(to_save[\"series_index\"], book)\\n+            modify_date |= edit_book_series_index(to_save[\"series_index\"], book)\\n             # Handle book comments/description\\n-            modif_date |= edit_book_comments(Markup(to_save[\\'description\\']).unescape(), book)\\n+            modify_date |= edit_book_comments(Markup(to_save[\\'description\\']).unescape(), book)\\n             # Handle identifiers\\n             input_identifiers = identifier_list(to_save, book)\\n             modification, warning = modify_identifiers(input_identifiers, book.identifiers, calibre_db.session)\\n             if warning:\\n                 flash(_(\"Identifiers are not Case Sensitive, Overwriting Old Identifier\"), category=\"warning\")\\n-            modif_date |= modification\\n+            modify_date |= modification\\n             # Handle book tags\\n-            modif_date |= edit_book_tags(to_save[\\'tags\\'], book)\\n+            modify_date |= edit_book_tags(to_save[\\'tags\\'], book)\\n             # Handle book series\\n-            modif_date |= edit_book_series(to_save[\"series\"], book)\\n+            modify_date |= edit_book_series(to_save[\"series\"], book)\\n             # handle book publisher\\n-            modif_date |= edit_book_publisher(to_save[\\'publisher\\'], book)\\n+            modify_date |= edit_book_publisher(to_save[\\'publisher\\'], book)\\n             # handle book languages\\n-            modif_date |= edit_book_languages(to_save[\\'languages\\'], book)\\n+            modify_date |= edit_book_languages(to_save[\\'languages\\'], book)\\n             # handle book ratings\\n-            modif_date |= edit_book_ratings(to_save, book)\\n+            modify_date |= edit_book_ratings(to_save, book)\\n             # handle cc data\\n-            modif_date |= edit_all_cc_data(book_id, book, to_save)\\n+            modify_date |= edit_all_cc_data(book_id, book, to_save)\\n \\n             if to_save[\"pubdate\"]:\\n                 try:\\n@@ -843,7 +853,7 @@ def edit_book(book_id):\\n             else:\\n                 book.pubdate = db.Books.DEFAULT_PUBDATE\\n \\n-            if modif_date:\\n+            if modify_date:\\n                 book.last_modified = datetime.utcnow()\\n                 kobo_sync_status.remove_synced_book(edited_books_id, all=True)\\n \\n@@ -905,14 +915,7 @@ def identifier_list(to_save, book):\\n     return result\\n \\n \\n-def prepare_authors_on_upload(title, authr):\\n-    if title != _(u\\'Unknown\\') and authr != _(u\\'Unknown\\'):\\n-        entry = calibre_db.check_exists_book(authr, title)\\n-        if entry:\\n-            log.info(\"Uploaded book probably exists in library\")\\n-            flash(_(u\"Uploaded book probably exists in the library, consider to change before upload new: \")\\n-                  + Markup(render_title_template(\\'book_exists_flash.html\\', entry=entry)), category=\"warning\")\\n-\\n+def prepare_authors(authr):\\n     # handle authors\\n     input_authors = authr.split(\\'&\\')\\n     # handle_authors(input_authors)\\n@@ -935,6 +938,18 @@ def prepare_authors_on_upload(title, authr):\\n             sorted_old_author = helper.get_sorted_author(in_aut)\\n             for one_book in all_books:\\n                 one_book.author_sort = one_book.author_sort.replace(sorted_renamed_author, sorted_old_author)\\n+    return input_authors, renamed\\n+\\n+\\n+def prepare_authors_on_upload(title, authr):\\n+    if title != _(u\\'Unknown\\') and authr != _(u\\'Unknown\\'):\\n+        entry = calibre_db.check_exists_book(authr, title)\\n+        if entry:\\n+            log.info(\"Uploaded book probably exists in library\")\\n+            flash(_(u\"Uploaded book probably exists in the library, consider to change before upload new: \")\\n+                  + Markup(render_title_template(\\'book_exists_flash.html\\', entry=entry)), category=\"warning\")\\n+\\n+    input_authors, renamed = prepare_authors(authr)\\n \\n     sort_authors_list = list()\\n     db_author = None\\n@@ -955,42 +970,42 @@ def prepare_authors_on_upload(title, authr):\\n     return sort_authors, input_authors, db_author, renamed\\n \\n \\n-def create_book_on_upload(modif_date, meta):\\n+def create_book_on_upload(modify_date, meta):\\n     title = meta.title\\n     authr = meta.author\\n     sort_authors, input_authors, db_author, renamed_authors = prepare_authors_on_upload(title, authr)\\n \\n     title_dir = helper.get_valid_filename(title, chars=96)\\n     author_dir = helper.get_valid_filename(db_author.name, chars=96)\\n \\n-    # combine path and normalize path from windows systems\\n+    # combine path and normalize path from Windows systems\\n     path = os.path.join(author_dir, title_dir).replace(\\'\\\\\\\\\\', \\'/\\')\\n \\n     # Calibre adds books with utc as timezone\\n     db_book = db.Books(title, \"\", sort_authors, datetime.utcnow(), datetime(101, 1, 1),\\n                        \\'1\\', datetime.utcnow(), path, meta.cover, db_author, [], \"\")\\n \\n-    modif_date |= modify_database_object(input_authors, db_book.authors, db.Authors, calibre_db.session,\\n-                                         \\'author\\')\\n+    modify_date |= modify_database_object(input_authors, db_book.authors, db.Authors, calibre_db.session,\\n+                                          \\'author\\')\\n \\n     # Add series_index to book\\n-    modif_date |= edit_book_series_index(meta.series_id, db_book)\\n+    modify_date |= edit_book_series_index(meta.series_id, db_book)\\n \\n     # add languages\\n-    invalid=[]\\n-    modif_date |= edit_book_languages(meta.languages, db_book, upload=True, invalid=invalid)\\n+    invalid = []\\n+    modify_date |= edit_book_languages(meta.languages, db_book, upload_mode=True, invalid=invalid)\\n     if invalid:\\n-        for l in invalid:\\n-            flash(_(u\"\\'%(langname)s\\' is not a valid language\", langname=l), category=\"warning\")\\n+        for lang in invalid:\\n+            flash(_(u\"\\'%(langname)s\\' is not a valid language\", langname=lang), category=\"warning\")\\n \\n     # handle tags\\n-    modif_date |= edit_book_tags(meta.tags, db_book)\\n+    modify_date |= edit_book_tags(meta.tags, db_book)\\n \\n     # handle publisher\\n-    modif_date |= edit_book_publisher(meta.publisher, db_book)\\n+    modify_date |= edit_book_publisher(meta.publisher, db_book)\\n \\n     # handle series\\n-    modif_date |= edit_book_series(meta.series, db_book)\\n+    modify_date |= edit_book_series(meta.series, db_book)\\n \\n     # Add file to book\\n     file_size = os.path.getsize(meta.file_path)\\n@@ -1002,6 +1017,7 @@ def create_book_on_upload(modif_date, meta):\\n     calibre_db.session.flush()\\n     return db_book, input_authors, title_dir, renamed_authors\\n \\n+\\n def file_handling_on_upload(requested_file):\\n     # check if file extension is correct\\n     if \\'.\\' in requested_file.filename:\\n@@ -1045,7 +1061,7 @@ def move_coverfile(meta, db_book):\\n               category=\"error\")\\n \\n \\n-@editbook.route(\"/upload\", methods=[\"POST\"])\\n+@EditBook.route(\"/upload\", methods=[\"POST\"])\\n @login_required_if_no_ano\\n @upload_required\\n def upload():\\n@@ -1054,7 +1070,7 @@ def upload():\\n     if request.method == \\'POST\\' and \\'btn-upload\\' in request.files:\\n         for requested_file in request.files.getlist(\"btn-upload\"):\\n             try:\\n-                modif_date = False\\n+                modify_date = False\\n                 # create the function for sorting...\\n                 calibre_db.update_title_sort(config)\\n                 calibre_db.session.connection().connection.connection.create_function(\\'uuid4\\', 0, lambda: str(uuid4()))\\n@@ -1063,10 +1079,10 @@ def upload():\\n                 if error:\\n                     return error\\n \\n-                db_book, input_authors, title_dir, renamed_authors = create_book_on_upload(modif_date, meta)\\n+                db_book, input_authors, title_dir, renamed_authors = create_book_on_upload(modify_date, meta)\\n \\n-                # Comments needs book id therefore only possible after flush\\n-                modif_date |= edit_book_comments(Markup(meta.description).unescape(), db_book)\\n+                # Comments need book id therefore only possible after flush\\n+                modify_date |= edit_book_comments(Markup(meta.description).unescape(), db_book)\\n \\n                 book_id = db_book.id\\n                 title = db_book.title\\n@@ -1096,12 +1112,12 @@ def upload():\\n                 if error:\\n                     flash(error, category=\"error\")\\n                 link = \\'<a href=\"{}\">{}</a>\\'.format(url_for(\\'web.show_book\\', book_id=book_id), escape(title))\\n-                uploadText = _(u\"File %(file)s uploaded\", file=link)\\n-                WorkerThread.add(current_user.name, TaskUpload(uploadText, escape(title)))\\n+                upload_text = _(u\"File %(file)s uploaded\", file=link)\\n+                WorkerThread.add(current_user.name, TaskUpload(upload_text, escape(title)))\\n \\n                 if len(request.files.getlist(\"btn-upload\")) < 2:\\n                     if current_user.role_edit() or current_user.role_admin():\\n-                        resp = {\"location\": url_for(\\'editbook.edit_book\\', book_id=book_id)}\\n+                        resp = {\"location\": url_for(\\'edit-book.edit_book\\', book_id=book_id)}\\n                         return Response(json.dumps(resp), mimetype=\\'application/json\\')\\n                     else:\\n                         resp = {\"location\": url_for(\\'web.show_book\\', book_id=book_id)}\\n@@ -1113,7 +1129,7 @@ def upload():\\n         return Response(json.dumps({\"location\": url_for(\"web.index\")}), mimetype=\\'application/json\\')\\n \\n \\n-@editbook.route(\"/admin/book/convert/<int:book_id>\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/admin/book/convert/<int:book_id>\", methods=[\\'POST\\'])\\n @login_required_if_no_ano\\n @edit_required\\n def convert_bookformat(book_id):\\n@@ -1123,39 +1139,41 @@ def convert_bookformat(book_id):\\n \\n     if (book_format_from is None) or (book_format_to is None):\\n         flash(_(u\"Source or destination format for conversion missing\"), category=\"error\")\\n-        return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+        return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n     log.info(\\'converting: book id: %s from: %s to: %s\\', book_id, book_format_from, book_format_to)\\n     rtn = helper.convert_book_format(book_id, config.config_calibre_dir, book_format_from.upper(),\\n                                      book_format_to.upper(), current_user.name)\\n \\n     if rtn is None:\\n         flash(_(u\"Book successfully queued for converting to %(book_format)s\",\\n-                    book_format=book_format_to),\\n-                    category=\"success\")\\n+                book_format=book_format_to),\\n+              category=\"success\")\\n     else:\\n         flash(_(u\"There was an error converting this book: %(res)s\", res=rtn), category=\"error\")\\n-    return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+    return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n-@editbook.route(\"/ajax/getcustomenum/<int:c_id>\")\\n+\\n+@EditBook.route(\"/ajax/getcustomenum/<int:c_id>\")\\n @login_required\\n def table_get_custom_enum(c_id):\\n     ret = list()\\n-    cc = (calibre_db.session.query(db.Custom_Columns)\\n-              .filter(db.Custom_Columns.id == c_id)\\n-              .filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).one_or_none())\\n+    cc = (calibre_db.session.query(db.CustomColumns)\\n+          .filter(db.CustomColumns.id == c_id)\\n+          .filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).one_or_none())\\n     ret.append({\\'value\\': \"\", \\'text\\': \"\"})\\n     for idx, en in enumerate(cc.get_display_dict()[\\'enum_values\\']):\\n         ret.append({\\'value\\': en, \\'text\\': en})\\n     return json.dumps(ret)\\n \\n \\n-@editbook.route(\"/ajax/editbooks/<param>\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/editbooks/<param>\", methods=[\\'POST\\'])\\n @login_required_if_no_ano\\n @edit_required\\n def edit_list_book(param):\\n     vals = request.form.to_dict()\\n     book = calibre_db.get_book(vals[\\'pk\\'])\\n+    sort_param = \"\"\\n     # ret = \"\"\\n     try:\\n         if param == \\'series_index\\':\\n@@ -1172,7 +1190,7 @@ def edit_list_book(param):\\n         elif param == \\'publishers\\':\\n             edit_book_publisher(vals[\\'value\\'], book)\\n             ret = Response(json.dumps({\\'success\\': True,\\n-                                        \\'newValue\\': \\', \\'.join([publisher.name for publisher in book.publishers])}),\\n+                                       \\'newValue\\': \\', \\'.join([publisher.name for publisher in book.publishers])}),\\n                            mimetype=\\'application/json\\')\\n         elif param == \\'languages\\':\\n             invalid = list()\\n@@ -1186,13 +1204,13 @@ def edit_list_book(param):\\n                 for lang in book.languages:\\n                     lang_names.append(isoLanguages.get_language_name(get_locale(), lang.lang_code))\\n                 ret = Response(json.dumps({\\'success\\': True, \\'newValue\\':  \\', \\'.join(lang_names)}),\\n-                                mimetype=\\'application/json\\')\\n+                               mimetype=\\'application/json\\')\\n         elif param == \\'author_sort\\':\\n             book.author_sort = vals[\\'value\\']\\n             ret = Response(json.dumps({\\'success\\': True, \\'newValue\\':  book.author_sort}),\\n                            mimetype=\\'application/json\\')\\n         elif param == \\'title\\':\\n-            sort = book.sort\\n+            sort_param = book.sort\\n             handle_title_on_edit(book, vals.get(\\'value\\', \"\"))\\n             helper.update_dir_structure(book.id, config.config_calibre_dir)\\n             ret = Response(json.dumps({\\'success\\': True, \\'newValue\\':  book.title}),\\n@@ -1208,12 +1226,13 @@ def edit_list_book(param):\\n         elif param == \\'authors\\':\\n             input_authors, __, renamed = handle_author_on_edit(book, vals[\\'value\\'], vals.get(\\'checkA\\', None) == \"true\")\\n             helper.update_dir_structure(book.id, config.config_calibre_dir, input_authors[0], renamed_author=renamed)\\n-            ret = Response(json.dumps({\\'success\\': True,\\n-                                       \\'newValue\\':  \\' & \\'.join([author.replace(\\'|\\',\\',\\') for author in input_authors])}),\\n-                           mimetype=\\'application/json\\')\\n+            ret = Response(json.dumps({\\n+                \\'success\\': True,\\n+                \\'newValue\\':  \\' & \\'.join([author.replace(\\'|\\', \\',\\') for author in input_authors])}),\\n+                mimetype=\\'application/json\\')\\n         elif param == \\'is_archived\\':\\n             is_archived = change_archived_books(book.id, vals[\\'value\\'] == \"True\",\\n-                                                message=\"Book {} archivebit set to: {}\".format(book.id, vals[\\'value\\']))\\n+                                                message=\"Book {} archive bit set to: {}\".format(book.id, vals[\\'value\\']))\\n             if is_archived:\\n                 kobo_sync_status.remove_synced_book(book.id)\\n             return \"\"\\n@@ -1238,7 +1257,7 @@ def edit_list_book(param):\\n         calibre_db.session.commit()\\n         # revert change for sort if automatic fields link is deactivated\\n         if param == \\'title\\' and vals.get(\\'checkT\\') == \"false\":\\n-            book.sort = sort\\n+            book.sort = sort_param\\n             calibre_db.session.commit()\\n     except (OperationalError, IntegrityError) as e:\\n         calibre_db.session.rollback()\\n@@ -1249,7 +1268,7 @@ def edit_list_book(param):\\n     return ret\\n \\n \\n-@editbook.route(\"/ajax/sort_value/<field>/<int:bookid>\")\\n+@EditBook.route(\"/ajax/sort_value/<field>/<int:bookid>\")\\n @login_required\\n def get_sorted_entry(field, bookid):\\n     if field in [\\'title\\', \\'authors\\', \\'sort\\', \\'author_sort\\']:\\n@@ -1266,7 +1285,7 @@ def get_sorted_entry(field, bookid):\\n     return \"\"\\n \\n \\n-@editbook.route(\"/ajax/simulatemerge\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/simulatemerge\", methods=[\\'POST\\'])\\n @login_required\\n @edit_required\\n def simulate_merge_list_book():\\n@@ -1282,7 +1301,7 @@ def simulate_merge_list_book():\\n     return \"\"\\n \\n \\n-@editbook.route(\"/ajax/mergebooks\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/mergebooks\", methods=[\\'POST\\'])\\n @login_required\\n @edit_required\\n def merge_list_book():\\n@@ -1295,8 +1314,9 @@ def merge_list_book():\\n         if to_book:\\n             for file in to_book.data:\\n                 to_file.append(file.format)\\n-            to_name = helper.get_valid_filename(to_book.title, chars=96) + \\' - \\' + \\\\\\n-                      helper.get_valid_filename(to_book.authors[0].name, chars=96)\\n+            to_name = helper.get_valid_filename(to_book.title,\\n+                                                chars=96) + \\' - \\' + helper.get_valid_filename(to_book.authors[0].name,\\n+                                                                                              chars=96)\\n             for book_id in vals:\\n                 from_book = calibre_db.get_book(book_id)\\n                 if from_book:\\n@@ -1314,19 +1334,20 @@ def merge_list_book():\\n                                                         element.format,\\n                                                         element.uncompressed_size,\\n                                                         to_name))\\n-                    delete_book_from_table(from_book.id,\"\", True)\\n+                    delete_book_from_table(from_book.id, \"\", True)\\n                     return json.dumps({\\'success\\': True})\\n     return \"\"\\n \\n \\n-@editbook.route(\"/ajax/xchange\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/xchange\", methods=[\\'POST\\'])\\n @login_required\\n @edit_required\\n def table_xchange_author_title():\\n     vals = request.get_json().get(\\'xchange\\')\\n+    edited_books_id = False\\n     if vals:\\n         for val in vals:\\n-            modif_date = False\\n+            modify_date = False\\n             book = calibre_db.get_book(val)\\n             authors = book.title\\n             book.authors = calibre_db.order_authors([book])\\n@@ -1338,15 +1359,15 @@ def table_xchange_author_title():\\n             input_authors, authorchange, renamed = handle_author_on_edit(book, authors)\\n             if authorchange or title_change:\\n                 edited_books_id = book.id\\n-                modif_date = True\\n+                modify_date = True\\n \\n             if config.config_use_google_drive:\\n                 gdriveutils.updateGdriveCalibreFromLocal()\\n \\n             if edited_books_id:\\n                 helper.update_dir_structure(edited_books_id, config.config_calibre_dir, input_authors[0],\\n-                                           renamed_author=renamed)\\n-            if modif_date:\\n+                                            renamed_author=renamed)\\n+            if modify_date:\\n                 book.last_modified = datetime.utcnow()\\n             try:\\n                 calibre_db.session.commit()', '@@ -60,7 +60,7 @@\\n             {% if g.user.is_authenticated or g.allow_anonymous %}\\n               {% if g.user.role_upload() and g.allow_upload %}\\n                   <li>\\n-                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'editbook.upload\\') }}\" data-title=\"{{_(\\'Uploading...\\')}}\" data-footer=\"{{_(\\'Close\\')}}\" data-failed=\"{{_(\\'Error\\')}}\" data-message=\"{{_(\\'Upload done, processing, please wait...\\')}}\" method=\"post\" enctype=\"multipart/form-data\">\\n+                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'edit-book.upload\\') }}\" data-title=\"{{_(\\'Uploading...\\')}}\" data-footer=\"{{_(\\'Close\\')}}\" data-failed=\"{{_(\\'Error\\')}}\" data-message=\"{{_(\\'Upload done, processing, please wait...\\')}}\" method=\"post\" enctype=\"multipart/form-data\">\\n                       <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n                       <div class=\"form-group\">\\n                         <span class=\"btn btn-default btn-file\">{{_(\\'Upload\\')}}<input id=\"btn-upload\" name=\"btn-upload\"', \"@@ -22,6 +22,7 @@\\n \\n import json\\n from datetime import datetime\\n+from functools import wraps\\n \\n from flask import Blueprint, request, make_response, abort, url_for, flash, redirect\\n from flask_login import login_required, current_user, login_user\\n@@ -31,10 +32,6 @@\\n from . import config, logger, ub\\n from .render_template import render_title_template\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We're not using Python 3\\n \\n remotelogin = Blueprint('remotelogin', __name__)\\n log = logger.create()\", \"@@ -18,17 +18,14 @@\\n \\n import base64\\n import binascii\\n+from functools import wraps\\n \\n from sqlalchemy.sql.expression import func\\n from werkzeug.security import check_password_hash\\n from flask_login import login_required, login_user\\n \\n from . import lm, ub, config, constants, services\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We're not using Python 3\\n \\n def login_required_if_no_ano(func):\\n     @wraps(func)\", '@@ -57,10 +57,10 @@ def has_prev(self):\\n     def has_next(self):\\n         return self.page < self.pages\\n \\n-    # right_edge: last right_edges count of all pages are shown as number, means, if 10 pages are paginated -> 9,10 shwn\\n-    # left_edge: first left_edges count of all pages are shown as number                                    -> 1,2 shwn\\n-    # left_current: left_current count below current page are shown as number, means if current page 5      -> 3,4 shwn\\n-    # left_current: right_current count above current page are shown as number, means if current page 5     -> 6,7 shwn\\n+    # right_edge: last right_edges count of all pages are shown as number, means, if 10 pages are paginated -> 9,10 shown\\n+    # left_edge: first left_edges count of all pages are shown as number                                    -> 1,2 shown\\n+    # left_current: left_current count below current page are shown as number, means if current page 5      -> 3,4 shown\\n+    # left_current: right_current count above current page are shown as number, means if current page 5     -> 6,7 shown\\n     def iter_pages(self, left_edge=2, left_current=2,\\n                    right_current=4, right_edge=2):\\n         last = 0'], 'file': ['cps/epub.py', 'cps/web.py', 'cps/templates/book_edit.html', 'cps/templates/detail.html', 'cps/helper.py', 'cps/opds.py', 'cps/templates/book_table.html', 'cps.py', 'cps/admin.py', 'cps/db.py', 'cps/editbooks.py', 'cps/templates/layout.html', 'cps/remotelogin.py', 'cps/usermanagement.py', 'cps/pagination.py'], 'language': ['Python', 'Python', 'HTML', 'HTML', 'Python', 'Python', 'HTML', 'Python', 'Python', 'Python', 'Python', 'HTML', 'Python', 'Python', 'Python'], 'temp_id': [UUID('c334ba32-cbb1-4f4f-a929-82adc0b333d6'), UUID('7f20d3dd-7942-4cb6-8be2-f4a6eae2aed7'), UUID('688e96ab-c40f-4435-8715-31e5490399ad'), UUID('c46e3b59-d59e-4e18-b863-4cf6c3d2e21a'), UUID('8cf529dc-2dc1-45fd-8d98-fc8804002ee3'), UUID('b033871b-e4cb-4df3-bbaf-b2f22d9fa6ff'), UUID('0e21d787-fca7-4d0a-ba63-5a2f6e3b208c'), UUID('9932b474-3c9a-456a-bd2e-436ec05bd410'), UUID('21d1e6cb-288e-418b-b895-f1e4fe9945cd'), UUID('d9f030d9-6905-43db-9d6e-2edd5ecb7cd0'), UUID('2e810e0c-1bc5-46f0-9649-dcec73395617'), UUID('58f95a16-f16b-46c2-8584-11ae7a857a33'), UUID('aead2e6b-219e-45a1-98d2-5fe5c47a24ab'), UUID('9265fc9d-a468-4ad8-86c1-df3b317efd40'), UUID('0a4c4b29-9900-4d43-acff-588e25e68f1b')]}\n",
      "ERROR:root:Error in {'repo': 'janeczku/calibre-web', 'vulnerability_id': '2022-0990', 'commit': '4545f4a20d9ff90b99bbd4e3e34b6de4441d6367', 'commit_source': 'github', 'cwe_id': ['CWE-918'], 'patch': ['@@ -53,11 +53,11 @@ def get_epub_info(tmp_file_path, original_file_name, original_file_extension):\\n \\n     txt = epub_zip.read(\\'META-INF/container.xml\\')\\n     tree = etree.fromstring(txt)\\n-    cfname = tree.xpath(\\'n:rootfiles/n:rootfile/@full-path\\', namespaces=ns)[0]\\n-    cf = epub_zip.read(cfname)\\n+    cf_name = tree.xpath(\\'n:rootfiles/n:rootfile/@full-path\\', namespaces=ns)[0]\\n+    cf = epub_zip.read(cf_name)\\n     tree = etree.fromstring(cf)\\n \\n-    coverpath = os.path.dirname(cfname)\\n+    cover_path = os.path.dirname(cf_name)\\n \\n     p = tree.xpath(\\'/pkg:package/pkg:metadata\\', namespaces=ns)[0]\\n \\n@@ -90,7 +90,7 @@ def get_epub_info(tmp_file_path, original_file_name, original_file_extension):\\n \\n     epub_metadata = parse_epub_series(ns, tree, epub_metadata)\\n \\n-    cover_file = parse_epub_cover(ns, tree, epub_zip, coverpath, tmp_file_path)\\n+    cover_file = parse_epub_cover(ns, tree, epub_zip, cover_path, tmp_file_path)\\n \\n     if not epub_metadata[\\'title\\']:\\n         title = original_file_name\\n@@ -114,9 +114,12 @@ def get_epub_info(tmp_file_path, original_file_name, original_file_extension):\\n def parse_epub_cover(ns, tree, epub_zip, cover_path, tmp_file_path):\\n     cover_section = tree.xpath(\"/pkg:package/pkg:manifest/pkg:item[@id=\\'cover-image\\']/@href\", namespaces=ns)\\n     cover_file = None\\n-    if len(cover_section) > 0:\\n-        cover_file = _extract_cover(epub_zip, cover_section[0], cover_path, tmp_file_path)\\n-    else:\\n+    # if len(cover_section) > 0:\\n+    for cs in cover_section:\\n+        cover_file = _extract_cover(epub_zip, cs, cover_path, tmp_file_path)\\n+        if cover_file:\\n+            break\\n+    if not cover_file:\\n         meta_cover = tree.xpath(\"/pkg:package/pkg:metadata/pkg:meta[@name=\\'cover\\']/@content\", namespaces=ns)\\n         if len(meta_cover) > 0:\\n             cover_section = tree.xpath(\\n@@ -143,8 +146,7 @@ def parse_epub_cover(ns, tree, epub_zip, cover_path, tmp_file_path):\\n                     cover_file = _extract_cover(epub_zip, filename, \"\", tmp_file_path)\\n             else:\\n                 cover_file = _extract_cover(epub_zip, cs, cover_path, tmp_file_path)\\n-            if cover_file:\\n-                break\\n+            if cover_file: break\\n     return cover_file\\n \\n ', '@@ -29,7 +29,7 @@\\n from functools import wraps\\n \\n from babel.dates import format_date\\n-from babel import Locale as LC\\n+from babel import Locale\\n from flask import Blueprint, jsonify\\n from flask import request, redirect, send_from_directory, make_response, flash, abort, url_for\\n from flask import session as flask_session\\n@@ -60,7 +60,6 @@\\n from .render_template import render_title_template\\n from .kobo_sync_status import change_archived_books\\n \\n-\\n feature_support = {\\n     \\'ldap\\': bool(services.ldap),\\n     \\'goodreads\\': bool(services.goodreads_support),\\n@@ -69,10 +68,12 @@\\n \\n try:\\n     from .oauth_bb import oauth_check, register_user_with_oauth, logout_oauth_user, get_oauth_status\\n+\\n     feature_support[\\'oauth\\'] = True\\n except ImportError:\\n     feature_support[\\'oauth\\'] = False\\n     oauth_check = {}\\n+    register_user_with_oauth = logout_oauth_user = get_oauth_status = None\\n \\n try:\\n     from natsort import natsorted as sort\\n@@ -82,8 +83,11 @@\\n \\n @app.after_request\\n def add_security_headers(resp):\\n-    resp.headers[\\'Content-Security-Policy\\'] = \"default-src \\'self\\'\" + \\'\\'.join([\\' \\'+host for host in config.config_trustedhosts.strip().split(\\',\\')]) + \" \\'unsafe-inline\\' \\'unsafe-eval\\'; font-src \\'self\\' data:; img-src \\'self\\' data:\"\\n-    if request.endpoint == \"editbook.edit_book\" or config.config_use_google_drive:\\n+    csp = \"default-src \\'self\\'\"\\n+    csp += \\'\\'.join([\\' \\' + host for host in config.config_trustedhosts.strip().split(\\',\\')])\\n+    csp += \" \\'unsafe-inline\\' \\'unsafe-eval\\'; font-src \\'self\\' data:; img-src \\'self\\' data:\"\\n+    resp.headers[\\'Content-Security-Policy\\'] = csp\\n+    if request.endpoint == \"edit-book.edit_book\" or config.config_use_google_drive:\\n         resp.headers[\\'Content-Security-Policy\\'] += \" *\"\\n     elif request.endpoint == \"web.read_book\":\\n         resp.headers[\\'Content-Security-Policy\\'] += \" blob:;style-src-elem \\'self\\' blob: \\'unsafe-inline\\';\"\\n@@ -93,6 +97,7 @@ def add_security_headers(resp):\\n     resp.headers[\\'Strict-Transport-Security\\'] = \\'max-age=31536000;\\'\\n     return resp\\n \\n+\\n web = Blueprint(\\'web\\', __name__)\\n log = logger.create()\\n \\n@@ -119,6 +124,7 @@ def inner(*args, **kwargs):\\n \\n     return inner\\n \\n+\\n # ################################### data provider functions #########################################################\\n \\n \\n@@ -140,11 +146,11 @@ def set_bookmark(book_id, book_format):\\n         ub.session_commit()\\n         return \"\", 204\\n \\n-    lbookmark = ub.Bookmark(user_id=current_user.id,\\n-                            book_id=book_id,\\n-                            format=book_format,\\n-                            bookmark_key=bookmark_key)\\n-    ub.session.merge(lbookmark)\\n+    l_bookmark = ub.Bookmark(user_id=current_user.id,\\n+                             book_id=book_id,\\n+                             format=book_format,\\n+                             bookmark_key=bookmark_key)\\n+    ub.session.merge(l_bookmark)\\n     ub.session_commit(\"Bookmark for user {} in book {} created\".format(current_user.id, book_id))\\n     return \"\", 201\\n \\n@@ -162,7 +168,7 @@ def toggle_read(book_id):\\n @web.route(\"/ajax/togglearchived/<int:book_id>\", methods=[\\'POST\\'])\\n @login_required\\n def toggle_archived(book_id):\\n-    is_archived = change_archived_books(book_id, message=\"Book {} archivebit toggled\".format(book_id))\\n+    is_archived = change_archived_books(book_id, message=\"Book {} archive bit toggled\".format(book_id))\\n     if is_archived:\\n         remove_synced_book(book_id)\\n     return \"\"\\n@@ -230,6 +236,7 @@ def get_comic_book(book_id, book_format, page):\\n         return \"\", 204\\n \\'\\'\\'\\n \\n+\\n # ################################### Typeahead ##################################################################\\n \\n \\n@@ -297,6 +304,12 @@ def get_matching_tags():\\n     return json_dumps\\n \\n \\n+def generate_char_list(data_colum, db_link):\\n+    return (calibre_db.session.query(func.upper(func.substr(data_colum, 1, 1)).label(\\'char\\'))\\n+            .join(db_link).join(db.Books).filter(calibre_db.common_filters())\\n+            .group_by(func.upper(func.substr(data_colum, 1, 1))).all())\\n+\\n+\\n def get_sort_function(sort_param, data):\\n     order = [db.Books.timestamp.desc()]\\n     if sort_param == \\'stored\\':\\n@@ -373,7 +386,7 @@ def render_books_list(data, sort_param, book_id, page):\\n     else:\\n         website = data or \"newest\"\\n         entries, random, pagination = calibre_db.fill_indexpage(page, 0, db.Books, True, order[0],\\n-        \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFalse, 0,\\n+                                                                False, 0,\\n                                                                 db.books_series_link,\\n                                                                 db.Books.id == db.books_series_link.c.book,\\n                                                                 db.Series)\\n@@ -407,32 +420,33 @@ def render_discover_books(page, book_id):\\n     else:\\n         abort(404)\\n \\n+\\n def render_hot_books(page, order):\\n     if current_user.check_visibility(constants.SIDEBAR_HOT):\\n         if order[1] not in [\\'hotasc\\', \\'hotdesc\\']:\\n-        # Unary expression comparsion only working (for this expression) in sqlalchemy 1.4+\\n-        #if not (order[0][0].compare(func.count(ub.Downloads.book_id).desc()) or\\n-        #        order[0][0].compare(func.count(ub.Downloads.book_id).asc())):\\n+            # Unary expression comparsion only working (for this expression) in sqlalchemy 1.4+\\n+            # if not (order[0][0].compare(func.count(ub.Downloads.book_id).desc()) or\\n+            #        order[0][0].compare(func.count(ub.Downloads.book_id).asc())):\\n             order = [func.count(ub.Downloads.book_id).desc()], \\'hotdesc\\'\\n         if current_user.show_detail_random():\\n             random = calibre_db.session.query(db.Books).filter(calibre_db.common_filters()) \\\\\\n                 .order_by(func.random()).limit(config.config_random_books)\\n         else:\\n             random = false()\\n         off = int(int(config.config_books_per_page) * (page - 1))\\n-        all_books = ub.session.query(ub.Downloads, func.count(ub.Downloads.book_id))\\\\\\n+        all_books = ub.session.query(ub.Downloads, func.count(ub.Downloads.book_id)) \\\\\\n             .order_by(*order[0]).group_by(ub.Downloads.book_id)\\n         hot_books = all_books.offset(off).limit(config.config_books_per_page)\\n         entries = list()\\n         for book in hot_books:\\n-            downloadBook = calibre_db.session.query(db.Books).filter(calibre_db.common_filters()).filter(\\n+            download_book = calibre_db.session.query(db.Books).filter(calibre_db.common_filters()).filter(\\n                 db.Books.id == book.Downloads.book_id).first()\\n-            if downloadBook:\\n-                entries.append(downloadBook)\\n+            if download_book:\\n+                entries.append(download_book)\\n             else:\\n                 ub.delete_download(book.Downloads.book_id)\\n-        numBooks = entries.__len__()\\n-        pagination = Pagination(page, config.config_books_per_page, numBooks)\\n+        num_books = entries.__len__()\\n+        pagination = Pagination(page, config.config_books_per_page, num_books)\\n         return render_title_template(\\'index.html\\', random=random, entries=entries, pagination=pagination,\\n                                      title=_(u\"Hot Books (Most Downloaded)\"), page=\"hot\", order=order[1])\\n     else:\\n@@ -462,16 +476,16 @@ def render_downloaded_books(page, order, user_id):\\n                                                             db.Series,\\n                                                             ub.Downloads, db.Books.id == ub.Downloads.book_id)\\n         for book in entries:\\n-            if not calibre_db.session.query(db.Books).filter(calibre_db.common_filters()) \\\\\\n-                             .filter(db.Books.id == book.id).first():\\n+            if not calibre_db.session.query(db.Books).\\\\\\n+                                            filter(calibre_db.common_filters()).filter(db.Books.id == book.id).first():\\n                 ub.delete_download(book.id)\\n         user = ub.session.query(ub.User).filter(ub.User.id == user_id).first()\\n         return render_title_template(\\'index.html\\',\\n                                      random=random,\\n                                      entries=entries,\\n                                      pagination=pagination,\\n                                      id=user_id,\\n-                                     title=_(u\"Downloaded books by %(user)s\",user=user.name),\\n+                                     title=_(u\"Downloaded books by %(user)s\", user=user.name),\\n                                      page=\"download\",\\n                                      order=order[1])\\n     else:\\n@@ -639,29 +653,27 @@ def render_read_books(page, are_read, as_xml=False, order=None):\\n                         column=config.config_read_column),\\n                       category=\"error\")\\n                 return redirect(url_for(\"web.index\"))\\n-            return [] # ToDo: Handle error Case for opds\\n+            return []  # ToDo: Handle error Case for opds\\n \\n     if as_xml:\\n         return entries, pagination\\n     else:\\n         if are_read:\\n             name = _(u\\'Read Books\\') + \\' (\\' + str(pagination.total_count) + \\')\\'\\n-            pagename = \"read\"\\n+            page_name = \"read\"\\n         else:\\n             name = _(u\\'Unread Books\\') + \\' (\\' + str(pagination.total_count) + \\')\\'\\n-            pagename = \"unread\"\\n+            page_name = \"unread\"\\n         return render_title_template(\\'index.html\\', random=random, entries=entries, pagination=pagination,\\n-                                     title=name, page=pagename, order=order[1])\\n+                                     title=name, page=page_name, order=order[1])\\n \\n \\n def render_archived_books(page, sort_param):\\n     order = sort_param[0] or []\\n-    archived_books = (\\n-        ub.session.query(ub.ArchivedBook)\\n-        .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n-        .filter(ub.ArchivedBook.is_archived == True)\\n-        .all()\\n-    )\\n+    archived_books = (ub.session.query(ub.ArchivedBook)\\n+                      .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n+                      .filter(ub.ArchivedBook.is_archived == True)\\n+                      .all())\\n     archived_book_ids = [archived_book.book_id for archived_book in archived_books]\\n \\n     archived_filter = db.Books.id.in_(archived_book_ids)\\n@@ -674,40 +686,40 @@ def render_archived_books(page, sort_param):\\n                                                                                 False, 0)\\n \\n     name = _(u\\'Archived Books\\') + \\' (\\' + str(len(archived_book_ids)) + \\')\\'\\n-    pagename = \"archived\"\\n+    page_name = \"archived\"\\n     return render_title_template(\\'index.html\\', random=random, entries=entries, pagination=pagination,\\n-                                 title=name, page=pagename, order=sort_param[1])\\n+                                 title=name, page=page_name, order=sort_param[1])\\n \\n \\n def render_prepare_search_form(cc):\\n     # prepare data for search-form\\n-    tags = calibre_db.session.query(db.Tags)\\\\\\n-        .join(db.books_tags_link)\\\\\\n-        .join(db.Books)\\\\\\n+    tags = calibre_db.session.query(db.Tags) \\\\\\n+        .join(db.books_tags_link) \\\\\\n+        .join(db.Books) \\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n-        .group_by(text(\\'books_tags_link.tag\\'))\\\\\\n+        .group_by(text(\\'books_tags_link.tag\\')) \\\\\\n         .order_by(db.Tags.name).all()\\n-    series = calibre_db.session.query(db.Series)\\\\\\n-        .join(db.books_series_link)\\\\\\n-        .join(db.Books)\\\\\\n+    series = calibre_db.session.query(db.Series) \\\\\\n+        .join(db.books_series_link) \\\\\\n+        .join(db.Books) \\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n-        .group_by(text(\\'books_series_link.series\\'))\\\\\\n-        .order_by(db.Series.name)\\\\\\n+        .group_by(text(\\'books_series_link.series\\')) \\\\\\n+        .order_by(db.Series.name) \\\\\\n         .filter(calibre_db.common_filters()).all()\\n-    shelves = ub.session.query(ub.Shelf)\\\\\\n-        .filter(or_(ub.Shelf.is_public == 1, ub.Shelf.user_id == int(current_user.id)))\\\\\\n+    shelves = ub.session.query(ub.Shelf) \\\\\\n+        .filter(or_(ub.Shelf.is_public == 1, ub.Shelf.user_id == int(current_user.id))) \\\\\\n         .order_by(ub.Shelf.name).all()\\n-    extensions = calibre_db.session.query(db.Data)\\\\\\n-        .join(db.Books)\\\\\\n+    extensions = calibre_db.session.query(db.Data) \\\\\\n+        .join(db.Books) \\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n-        .group_by(db.Data.format)\\\\\\n+        .group_by(db.Data.format) \\\\\\n         .order_by(db.Data.format).all()\\n     if current_user.filter_language() == u\"all\":\\n         languages = calibre_db.speaking_language()\\n     else:\\n         languages = None\\n     return render_title_template(\\'search_form.html\\', tags=tags, languages=languages, extensions=extensions,\\n-                                 series=series,shelves=shelves, title=_(u\"Advanced Search\"), cc=cc, page=\"advsearch\")\\n+                                 series=series, shelves=shelves, title=_(u\"Advanced Search\"), cc=cc, page=\"advsearch\")\\n \\n \\n def render_search_results(term, offset=None, order=None, limit=None):\\n@@ -716,7 +728,6 @@ def render_search_results(term, offset=None, order=None, limit=None):\\n                                                                       offset,\\n                                                                       order,\\n                                                                       limit,\\n-                                                                      False,\\n                                                                       config.config_read_column,\\n                                                                       *join)\\n     return render_title_template(\\'search.html\\',\\n@@ -759,12 +770,13 @@ def books_table():\\n     return render_title_template(\\'book_table.html\\', title=_(u\"Books List\"), cc=cc, page=\"book_table\",\\n                                  visiblility=visibility)\\n \\n+\\n @web.route(\"/ajax/listbooks\")\\n @login_required\\n def list_books():\\n     off = int(request.args.get(\"offset\") or 0)\\n     limit = int(request.args.get(\"limit\") or config.config_books_per_page)\\n-    search = request.args.get(\"search\")\\n+    search_param = request.args.get(\"search\")\\n     sort_param = request.args.get(\"sort\", \"id\")\\n     order = request.args.get(\"order\", \"\").lower()\\n     state = None\\n@@ -784,8 +796,8 @@ def list_books():\\n     elif sort_param == \"authors\":\\n         order = [db.Authors.name.asc(), db.Series.name, db.Books.series_index] if order == \"asc\" \\\\\\n             else [db.Authors.name.desc(), db.Series.name.desc(), db.Books.series_index.desc()]\\n-        join = db.books_authors_link, db.Books.id == db.books_authors_link.c.book, db.Authors, \\\\\\n-               db.books_series_link, db.Books.id == db.books_series_link.c.book, db.Series\\n+        join = db.books_authors_link, db.Books.id == db.books_authors_link.c.book, db.Authors, db.books_series_link, \\\\\\n+            db.Books.id == db.books_series_link.c.book, db.Series\\n     elif sort_param == \"languages\":\\n         order = [db.Languages.lang_code.asc()] if order == \"asc\" else [db.Languages.lang_code.desc()]\\n         join = db.books_languages_link, db.Books.id == db.books_languages_link.c.book, db.Languages\\n@@ -794,10 +806,11 @@ def list_books():\\n     elif not state:\\n         order = [db.Books.timestamp.desc()]\\n \\n-    total_count = filtered_count = calibre_db.session.query(db.Books).filter(calibre_db.common_filters(allow_show_archived=True)).count()\\n+    total_count = filtered_count = calibre_db.session.query(db.Books).filter(\\n+        calibre_db.common_filters(allow_show_archived=True)).count()\\n     if state is not None:\\n-        if search:\\n-            books = calibre_db.search_query(search, config.config_read_column).all()\\n+        if search_param:\\n+            books = calibre_db.search_query(search_param, config.config_read_column).all()\\n             filtered_count = len(books)\\n         else:\\n             if not config.config_read_column:\\n@@ -818,15 +831,14 @@ def list_books():\\n                     # Skip linking read column and return None instead of read status\\n                     books = calibre_db.session.query(db.Books, None, ub.ArchivedBook.is_archived)\\n             books = (books.outerjoin(ub.ArchivedBook, and_(db.Books.id == ub.ArchivedBook.book_id,\\n-                                                          int(current_user.id) == ub.ArchivedBook.user_id))\\n+                                                           int(current_user.id) == ub.ArchivedBook.user_id))\\n                      .filter(calibre_db.common_filters(allow_show_archived=True)).all())\\n         entries = calibre_db.get_checkbox_sorted(books, state, off, limit, order, True)\\n-    elif search:\\n-        entries, filtered_count, __ = calibre_db.get_search_results(search,\\n+    elif search_param:\\n+        entries, filtered_count, __ = calibre_db.get_search_results(search_param,\\n                                                                     off,\\n-                                                                    [order,\\'\\'],\\n+                                                                    [order, \\'\\'],\\n                                                                     limit,\\n-                                                                    True,\\n                                                                     config.config_read_column,\\n                                                                     *join)\\n     else:\\n@@ -845,9 +857,9 @@ def list_books():\\n         val = entry[0]\\n         val.read_status = entry[1] == ub.ReadBook.STATUS_FINISHED\\n         val.is_archived = entry[2] is True\\n-        for index in range(0, len(val.languages)):\\n-            val.languages[index].language_name = isoLanguages.get_language_name(get_locale(), val.languages[\\n-                index].lang_code)\\n+        for lang_index in range(0, len(val.languages)):\\n+            val.languages[lang_index].language_name = isoLanguages.get_language_name(get_locale(), val.languages[\\n+                lang_index].lang_code)\\n         result.append(val)\\n \\n     table_entries = {\\'totalNotFiltered\\': total_count, \\'total\\': filtered_count, \"rows\": result}\\n@@ -857,6 +869,7 @@ def list_books():\\n     response.headers[\"Content-Type\"] = \"application/json; charset=utf-8\"\\n     return response\\n \\n+\\n @web.route(\"/ajax/table_settings\", methods=[\\'POST\\'])\\n @login_required\\n def update_table_settings():\\n@@ -886,19 +899,18 @@ def author_list():\\n         entries = calibre_db.session.query(db.Authors, func.count(\\'books_authors_link.book\\').label(\\'count\\')) \\\\\\n             .join(db.books_authors_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_authors_link.author\\')).order_by(order).all()\\n-        charlist = calibre_db.session.query(func.upper(func.substr(db.Authors.sort, 1, 1)).label(\\'char\\')) \\\\\\n-            .join(db.books_authors_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-            .group_by(func.upper(func.substr(db.Authors.sort, 1, 1))).all()\\n+        char_list = generate_char_list(db.Authors.sort, db.books_authors_link)\\n         # If not creating a copy, readonly databases can not display authornames with \"|\" in it as changing the name\\n         # starts a change session\\n-        autor_copy = copy.deepcopy(entries)\\n-        for entry in autor_copy:\\n+        author_copy = copy.deepcopy(entries)\\n+        for entry in author_copy:\\n             entry.Authors.name = entry.Authors.name.replace(\\'|\\', \\',\\')\\n-        return render_title_template(\\'list.html\\', entries=autor_copy, folder=\\'web.books_list\\', charlist=charlist,\\n+        return render_title_template(\\'list.html\\', entries=author_copy, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=u\"Authors\", page=\"authorlist\", data=\\'author\\', order=order_no)\\n     else:\\n         abort(404)\\n \\n+\\n @web.route(\"/downloadlist\")\\n @login_required_if_no_ano\\n def download_list():\\n@@ -909,12 +921,12 @@ def download_list():\\n         order = ub.User.name.asc()\\n         order_no = 1\\n     if current_user.check_visibility(constants.SIDEBAR_DOWNLOAD) and current_user.role_admin():\\n-        entries = ub.session.query(ub.User, func.count(ub.Downloads.book_id).label(\\'count\\'))\\\\\\n+        entries = ub.session.query(ub.User, func.count(ub.Downloads.book_id).label(\\'count\\')) \\\\\\n             .join(ub.Downloads).group_by(ub.Downloads.user_id).order_by(order).all()\\n-        charlist = ub.session.query(func.upper(func.substr(ub.User.name, 1, 1)).label(\\'char\\')) \\\\\\n+        char_list = ub.session.query(func.upper(func.substr(ub.User.name, 1, 1)).label(\\'char\\')) \\\\\\n             .filter(ub.User.role.op(\\'&\\')(constants.ROLE_ANONYMOUS) != constants.ROLE_ANONYMOUS) \\\\\\n             .group_by(func.upper(func.substr(ub.User.name, 1, 1))).all()\\n-        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=_(u\"Downloads\"), page=\"downloadlist\", data=\"download\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -933,10 +945,8 @@ def publisher_list():\\n         entries = calibre_db.session.query(db.Publishers, func.count(\\'books_publishers_link.book\\').label(\\'count\\')) \\\\\\n             .join(db.books_publishers_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_publishers_link.publisher\\')).order_by(order).all()\\n-        charlist = calibre_db.session.query(func.upper(func.substr(db.Publishers.name, 1, 1)).label(\\'char\\')) \\\\\\n-            .join(db.books_publishers_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-            .group_by(func.upper(func.substr(db.Publishers.name, 1, 1))).all()\\n-        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+        char_list = generate_char_list(db.Publishers.name, db.books_publishers_link)\\n+        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=_(u\"Publishers\"), page=\"publisherlist\", data=\"publisher\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -952,25 +962,19 @@ def series_list():\\n         else:\\n             order = db.Series.sort.asc()\\n             order_no = 1\\n+        char_list = generate_char_list(db.Series.sort, db.books_series_link)\\n         if current_user.get_view_property(\\'series\\', \\'series_view\\') == \\'list\\':\\n             entries = calibre_db.session.query(db.Series, func.count(\\'books_series_link.book\\').label(\\'count\\')) \\\\\\n                 .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n                 .group_by(text(\\'books_series_link.series\\')).order_by(order).all()\\n-            charlist = calibre_db.session.query(func.upper(func.substr(db.Series.sort, 1, 1)).label(\\'char\\')) \\\\\\n-                .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-                .group_by(func.upper(func.substr(db.Series.sort, 1, 1))).all()\\n-            return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+            return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                          title=_(u\"Series\"), page=\"serieslist\", data=\"series\", order=order_no)\\n         else:\\n             entries = calibre_db.session.query(db.Books, func.count(\\'books_series_link\\').label(\\'count\\'),\\n                                                func.max(db.Books.series_index), db.Books.id) \\\\\\n-                .join(db.books_series_link).join(db.Series).filter(calibre_db.common_filters())\\\\\\n+                .join(db.books_series_link).join(db.Series).filter(calibre_db.common_filters()) \\\\\\n                 .group_by(text(\\'books_series_link.series\\')).order_by(order).all()\\n-            charlist = calibre_db.session.query(func.upper(func.substr(db.Series.sort, 1, 1)).label(\\'char\\')) \\\\\\n-                .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-                .group_by(func.upper(func.substr(db.Series.sort, 1, 1))).all()\\n-\\n-            return render_title_template(\\'grid.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+            return render_title_template(\\'grid.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                          title=_(u\"Series\"), page=\"serieslist\", data=\"series\", bodyClass=\"grid-view\",\\n                                          order=order_no)\\n     else:\\n@@ -988,7 +992,7 @@ def ratings_list():\\n             order = db.Ratings.rating.asc()\\n             order_no = 1\\n         entries = calibre_db.session.query(db.Ratings, func.count(\\'books_ratings_link.book\\').label(\\'count\\'),\\n-                                   (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n+                                           (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n             .join(db.books_ratings_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_ratings_link.rating\\')).order_by(order).all()\\n         return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=list(),\\n@@ -1023,14 +1027,14 @@ def formats_list():\\n def language_overview():\\n     if current_user.check_visibility(constants.SIDEBAR_LANGUAGE) and current_user.filter_language() == u\"all\":\\n         order_no = 0 if current_user.get_view_property(\\'language\\', \\'dir\\') == \\'desc\\' else 1\\n-        charlist = list()\\n+        char_list = list()\\n         languages = calibre_db.speaking_language(reverse_order=not order_no, with_count=True)\\n         for lang in languages:\\n             upper_lang = lang[0].name[0].upper()\\n-            if upper_lang not in charlist:\\n-                charlist.append(upper_lang)\\n+            if upper_lang not in char_list:\\n+                char_list.append(upper_lang)\\n         return render_title_template(\\'languages.html\\', languages=languages,\\n-                                     charlist=charlist, title=_(u\"Languages\"), page=\"langlist\",\\n+                                     charlist=char_list, title=_(u\"Languages\"), page=\"langlist\",\\n                                      data=\"language\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -1049,10 +1053,8 @@ def category_list():\\n         entries = calibre_db.session.query(db.Tags, func.count(\\'books_tags_link.book\\').label(\\'count\\')) \\\\\\n             .join(db.books_tags_link).join(db.Books).order_by(order).filter(calibre_db.common_filters()) \\\\\\n             .group_by(text(\\'books_tags_link.tag\\')).all()\\n-        charlist = calibre_db.session.query(func.upper(func.substr(db.Tags.name, 1, 1)).label(\\'char\\')) \\\\\\n-            .join(db.books_tags_link).join(db.Books).filter(calibre_db.common_filters()) \\\\\\n-            .group_by(func.upper(func.substr(db.Tags.name, 1, 1))).all()\\n-        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=charlist,\\n+        char_list = generate_char_list(db.Tags.name, db.books_tags_link)\\n+        return render_title_template(\\'list.html\\', entries=entries, folder=\\'web.books_list\\', charlist=char_list,\\n                                      title=_(u\"Categories\"), page=\"catlist\", data=\"category\", order=order_no)\\n     else:\\n         abort(404)\\n@@ -1176,7 +1178,15 @@ def adv_search_read_status(q, read_status):\\n     return q\\n \\n \\n-def adv_search_extension(q, include_extension_inputs, exclude_extension_inputs):\\n+def adv_search_text(q, include_inputs, exclude_inputs, data_value):\\n+    for inp in include_inputs:\\n+        q = q.filter(db.Books.data.any(data_value == inp))\\n+    for excl in exclude_inputs:\\n+        q = q.filter(not_(db.Books.data.any(data_value == excl)))\\n+    return q\\n+\\n+\\n+\\'\\'\\'def adv_search_extension(q, include_extension_inputs, exclude_extension_inputs):\\n     for extension in include_extension_inputs:\\n         q = q.filter(db.Books.data.any(db.Data.format == extension))\\n     for extension in exclude_extension_inputs:\\n@@ -1197,15 +1207,17 @@ def adv_search_serie(q, include_series_inputs, exclude_series_inputs):\\n         q = q.filter(db.Books.series.any(db.Series.id == serie))\\n     for serie in exclude_series_inputs:\\n         q = q.filter(not_(db.Books.series.any(db.Series.id == serie)))\\n-    return q\\n+    return q\\'\\'\\'\\n+\\n \\n def adv_search_shelf(q, include_shelf_inputs, exclude_shelf_inputs):\\n-    q = q.outerjoin(ub.BookShelf, db.Books.id == ub.BookShelf.book_id)\\\\\\n+    q = q.outerjoin(ub.BookShelf, db.Books.id == ub.BookShelf.book_id) \\\\\\n         .filter(or_(ub.BookShelf.shelf == None, ub.BookShelf.shelf.notin_(exclude_shelf_inputs)))\\n     if len(include_shelf_inputs) > 0:\\n         q = q.filter(ub.BookShelf.shelf.in_(include_shelf_inputs))\\n     return q\\n \\n+\\n def extend_search_term(searchterm,\\n                        author_name,\\n                        book_title,\\n@@ -1232,7 +1244,7 @@ def extend_search_term(searchterm,\\n                                            format=\\'medium\\', locale=get_locale())])\\n         except ValueError:\\n             pub_end = u\"\"\\n-    elements = {\\'tag\\': db.Tags, \\'serie\\':db.Series, \\'shelf\\':ub.Shelf}\\n+    elements = {\\'tag\\': db.Tags, \\'serie\\': db.Series, \\'shelf\\': ub.Shelf}\\n     for key, db_element in elements.items():\\n         tag_names = calibre_db.session.query(db_element).filter(db_element.id.in_(tags[\\'include_\\' + key])).all()\\n         searchterm.extend(tag.name for tag in tag_names)\\n@@ -1284,8 +1296,8 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n     query = query.outerjoin(ub.ArchivedBook, and_(db.Books.id == ub.ArchivedBook.book_id,\\n                                                   int(current_user.id) == ub.ArchivedBook.user_id))\\n \\n-    q = query.outerjoin(db.books_series_link, db.Books.id == db.books_series_link.c.book)\\\\\\n-        .outerjoin(db.Series)\\\\\\n+    q = query.outerjoin(db.books_series_link, db.Books.id == db.books_series_link.c.book) \\\\\\n+        .outerjoin(db.Series) \\\\\\n         .filter(calibre_db.common_filters(True))\\n \\n     # parse multiselects to a complete dict\\n@@ -1311,43 +1323,43 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n     if publisher:\\n         publisher = publisher.strip().lower()\\n \\n-    searchterm = []\\n+    search_term = []\\n     cc_present = False\\n     for c in cc:\\n         if c.datatype == \"datetime\":\\n             column_start = term.get(\\'custom_column_\\' + str(c.id) + \\'_start\\')\\n             column_end = term.get(\\'custom_column_\\' + str(c.id) + \\'_end\\')\\n             if column_start:\\n-                searchterm.extend([u\"{} >= {}\".format(c.name,\\n-                                                      format_date(datetime.strptime(column_start, \"%Y-%m-%d\").date(),\\n-                                                                      format=\\'medium\\',\\n-                                                                      locale=get_locale())\\n-                                                      )])\\n+                search_term.extend([u\"{} >= {}\".format(c.name,\\n+                                                       format_date(datetime.strptime(column_start, \"%Y-%m-%d\").date(),\\n+                                                                   format=\\'medium\\',\\n+                                                                   locale=get_locale())\\n+                                                       )])\\n                 cc_present = True\\n             if column_end:\\n-                searchterm.extend([u\"{} <= {}\".format(c.name,\\n-                                                      format_date(datetime.strptime(column_end, \"%Y-%m-%d\").date(),\\n-                                                                      format=\\'medium\\',\\n-                                                                      locale=get_locale())\\n-                                                      )])\\n+                search_term.extend([u\"{} <= {}\".format(c.name,\\n+                                                       format_date(datetime.strptime(column_end, \"%Y-%m-%d\").date(),\\n+                                                                   format=\\'medium\\',\\n+                                                                   locale=get_locale())\\n+                                                       )])\\n                 cc_present = True\\n         elif term.get(\\'custom_column_\\' + str(c.id)):\\n-            searchterm.extend([(u\"{}: {}\".format(c.name, term.get(\\'custom_column_\\' + str(c.id))))])\\n+            search_term.extend([(u\"{}: {}\".format(c.name, term.get(\\'custom_column_\\' + str(c.id))))])\\n             cc_present = True\\n \\n-\\n-    if any(tags.values()) or author_name or book_title or publisher or pub_start or pub_end or rating_low \\\\\\n-       or rating_high or description or cc_present or read_status:\\n-        searchterm, pub_start, pub_end = extend_search_term(searchterm,\\n-                                                            author_name,\\n-                                                            book_title,\\n-                                                            publisher,\\n-                                                            pub_start,\\n-                                                            pub_end,\\n-                                                            tags,\\n-                                                            rating_high,\\n-                                                            rating_low,\\n-                                                            read_status)\\n+    if any(tags.values()) or author_name or book_title or \\\\\\n+        publisher or pub_start or pub_end or rating_low or rating_high \\\\\\n+            or description or cc_present or read_status:\\n+        search_term, pub_start, pub_end = extend_search_term(search_term,\\n+                                                             author_name,\\n+                                                             book_title,\\n+                                                             publisher,\\n+                                                             pub_start,\\n+                                                             pub_end,\\n+                                                             tags,\\n+                                                             rating_high,\\n+                                                             rating_low,\\n+                                                             read_status)\\n         # q = q.filter()\\n         if author_name:\\n             q = q.filter(db.Books.authors.any(func.lower(db.Authors.name).ilike(\"%\" + author_name + \"%\")))\\n@@ -1360,12 +1372,12 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n         q = adv_search_read_status(q, read_status)\\n         if publisher:\\n             q = q.filter(db.Books.publishers.any(func.lower(db.Publishers.name).ilike(\"%\" + publisher + \"%\")))\\n-        q = adv_search_tag(q, tags[\\'include_tag\\'], tags[\\'exclude_tag\\'])\\n-        q = adv_search_serie(q, tags[\\'include_serie\\'], tags[\\'exclude_serie\\'])\\n+        q = adv_search_text(q, tags[\\'include_tag\\'], tags[\\'exclude_tag\\'], db.Tags.id)\\n+        q = adv_search_text(q, tags[\\'include_serie\\'], tags[\\'exclude_serie\\'], db.Series.id)\\n+        q = adv_search_text(q, tags[\\'include_extension\\'], tags[\\'exclude_extension\\'], db.Data.format)\\n         q = adv_search_shelf(q, tags[\\'include_shelf\\'], tags[\\'exclude_shelf\\'])\\n-        q = adv_search_extension(q, tags[\\'include_extension\\'], tags[\\'exclude_extension\\'])\\n-        q = adv_search_language(q, tags[\\'include_language\\'], tags[\\'exclude_language\\'])\\n-        q = adv_search_ratings(q, rating_high, rating_low)\\n+        q = adv_search_language(q, tags[\\'include_language\\'], tags[\\'exclude_language\\'], )\\n+        q = adv_search_ratings(q, rating_high, rating_low, )\\n \\n         if description:\\n             q = q.filter(db.Books.comments.any(func.lower(db.Comments.text).ilike(\"%\" + description + \"%\")))\\n@@ -1390,7 +1402,7 @@ def render_adv_search_results(term, offset=None, order=None, limit=None):\\n         limit_all = result_count\\n     entries = calibre_db.order_authors(q[offset:limit_all], list_return=True, combined=True)\\n     return render_title_template(\\'search.html\\',\\n-                                 adv_searchterm=searchterm,\\n+                                 adv_searchterm=search_term,\\n                                  pagination=pagination,\\n                                  entries=entries,\\n                                  result_count=result_count,\\n@@ -1414,10 +1426,12 @@ def advanced_search_form():\\n def get_cover(book_id):\\n     return get_book_cover(book_id)\\n \\n+\\n @web.route(\"/robots.txt\")\\n def get_robots():\\n     return send_from_directory(constants.STATIC_DIR, \"robots.txt\")\\n \\n+\\n @web.route(\"/show/<int:book_id>/<book_format>\", defaults={\\'anyname\\': \\'None\\'})\\n @web.route(\"/show/<int:book_id>/<book_format>/<anyname>\")\\n @login_required_if_no_ano\\n@@ -1561,7 +1575,7 @@ def login():\\n                       category=\"success\")\\n                 return redirect_back(url_for(\"web.index\"))\\n             elif login_result is None and user and check_password_hash(str(user.password), form[\\'password\\']) \\\\\\n-                and user.name != \"Guest\":\\n+                    and user.name != \"Guest\":\\n                 login_user(user, remember=bool(form.get(\\'remember_me\\')))\\n                 ub.store_user_session()\\n                 log.info(\"Local Fallback Login as: \\'%s\\'\", user.name)\\n@@ -1573,23 +1587,23 @@ def login():\\n                 log.info(error)\\n                 flash(_(u\"Could not login: %(message)s\", message=error), category=\"error\")\\n             else:\\n-                ip_Address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n-                log.warning(\\'LDAP Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_Address)\\n+                ip_address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n+                log.warning(\\'LDAP Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_address)\\n                 flash(_(u\"Wrong Username or Password\"), category=\"error\")\\n         else:\\n-            ip_Address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n+            ip_address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n             if \\'forgot\\' in form and form[\\'forgot\\'] == \\'forgot\\':\\n                 if user is not None and user.name != \"Guest\":\\n                     ret, __ = reset_password(user.id)\\n                     if ret == 1:\\n                         flash(_(u\"New Password was send to your email address\"), category=\"info\")\\n-                        log.info(\\'Password reset for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_Address)\\n+                        log.info(\\'Password reset for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_address)\\n                     else:\\n                         log.error(u\"An unknown error occurred. Please try again later\")\\n                         flash(_(u\"An unknown error occurred. Please try again later.\"), category=\"error\")\\n                 else:\\n                     flash(_(u\"Please enter valid username to reset password\"), category=\"error\")\\n-                    log.warning(\\'Username missing for password reset IP-address: %s\\', ip_Address)\\n+                    log.warning(\\'Username missing for password reset IP-address: %s\\', ip_address)\\n             else:\\n                 if user and check_password_hash(str(user.password), form[\\'password\\']) and user.name != \"Guest\":\\n                     login_user(user, remember=bool(form.get(\\'remember_me\\')))\\n@@ -1599,7 +1613,7 @@ def login():\\n                     config.config_is_initial = False\\n                     return redirect_back(url_for(\"web.index\"))\\n                 else:\\n-                    log.warning(\\'Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_Address)\\n+                    log.warning(\\'Login failed for user \"%s\" IP-address: %s\\', form[\\'username\\'], ip_address)\\n                     flash(_(u\"Wrong Username or Password\"), category=\"error\")\\n \\n     next_url = request.args.get(\\'next\\', default=url_for(\"web.index\"), type=str)\\n@@ -1617,7 +1631,7 @@ def login():\\n @login_required\\n def logout():\\n     if current_user is not None and current_user.is_authenticated:\\n-        ub.delete_user_session(current_user.id, flask_session.get(\\'_id\\',\"\"))\\n+        ub.delete_user_session(current_user.id, flask_session.get(\\'_id\\', \"\"))\\n         logout_user()\\n         if feature_support[\\'oauth\\'] and (config.config_login_type == 2 or config.config_login_type == 3):\\n             logout_oauth_user()\\n@@ -1639,7 +1653,7 @@ def change_profile(kobo_support, local_oauth_check, oauth_status, translations,\\n             current_user.email = check_email(to_save[\"email\"])\\n         if current_user.role_admin():\\n             if to_save.get(\"name\", current_user.name) != current_user.name:\\n-                # Query User name, if not existing, change\\n+                # Query username, if not existing, change\\n                 current_user.name = check_username(to_save[\"name\"])\\n         current_user.random_books = 1 if to_save.get(\"show_random\") == \"on\" else 0\\n         if to_save.get(\"default_language\"):\\n@@ -1693,7 +1707,7 @@ def change_profile(kobo_support, local_oauth_check, oauth_status, translations,\\n @login_required\\n def profile():\\n     languages = calibre_db.speaking_language()\\n-    translations = babel.list_translations() + [LC(\\'en\\')]\\n+    translations = babel.list_translations() + [Locale(\\'en\\')]\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     if feature_support[\\'oauth\\'] and config.config_login_type == 2:\\n         oauth_status = get_oauth_status()\\n@@ -1727,7 +1741,8 @@ def read_book(book_id, book_format):\\n     book.ordered_authors = calibre_db.order_authors([book], False)\\n \\n     if not book:\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         log.debug(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\")\\n         return redirect(url_for(\"web.index\"))\\n \\n@@ -1768,7 +1783,8 @@ def read_book(book_id, book_format):\\n                 return render_title_template(\\'readcbr.html\\', comicfile=all_name, title=title,\\n                                              extension=fileExt)\\n         log.debug(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\")\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         return redirect(url_for(\"web.index\"))\\n \\n \\n@@ -1782,14 +1798,14 @@ def show_book(book_id):\\n         entry = entries[0]\\n         entry.read_status = read_book == ub.ReadBook.STATUS_FINISHED\\n         entry.is_archived = archived_book\\n-        for index in range(0, len(entry.languages)):\\n-            entry.languages[index].language_name = isoLanguages.get_language_name(get_locale(), entry.languages[\\n-                index].lang_code)\\n+        for lang_index in range(0, len(entry.languages)):\\n+            entry.languages[lang_index].language_name = isoLanguages.get_language_name(get_locale(), entry.languages[\\n+                lang_index].lang_code)\\n         cc = get_cc_columns(filter_config_custom_read=True)\\n-        book_in_shelfs = []\\n+        book_in_shelves = []\\n         shelfs = ub.session.query(ub.BookShelf).filter(ub.BookShelf.book_id == book_id).all()\\n         for sh in shelfs:\\n-            book_in_shelfs.append(sh.shelf)\\n+            book_in_shelves.append(sh.shelf)\\n \\n         entry.tags = sort(entry.tags, key=lambda tag: tag.name)\\n \\n@@ -1806,9 +1822,9 @@ def show_book(book_id):\\n         return render_title_template(\\'detail.html\\',\\n                                      entry=entry,\\n                                      cc=cc,\\n-                                     is_xhr=request.headers.get(\\'X-Requested-With\\')==\\'XMLHttpRequest\\',\\n+                                     is_xhr=request.headers.get(\\'X-Requested-With\\') == \\'XMLHttpRequest\\',\\n                                      title=entry.title,\\n-                                     books_shelfs=book_in_shelfs,\\n+                                     books_shelfs=book_in_shelves,\\n                                      page=\"book\")\\n     else:\\n         log.debug(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\")', '@@ -22,7 +22,7 @@\\n \\n {%  if source_formats|length > 0 and conversion_formats|length > 0 %}\\n   <div class=\"text-center more-stuff\"><h4>{{_(\\'Convert book format:\\')}}</h4>\\n-      <form class=\"padded-bottom\" action=\"{{ url_for(\\'editbook.convert_bookformat\\', book_id=book.id) }}\" method=\"post\" id=\"book_convert_frm\">\\n+      <form class=\"padded-bottom\" action=\"{{ url_for(\\'edit-book.convert_bookformat\\', book_id=book.id) }}\" method=\"post\" id=\"book_convert_frm\">\\n           <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n           <div class=\"form-group\">\\n               <div class=\"text-left\">\\n@@ -48,7 +48,7 @@\\n {% endif %}\\n \\n   </div>\\n-<form role=\"form\" action=\"{{ url_for(\\'editbook.edit_book\\', book_id=book.id) }}\" method=\"post\" enctype=\"multipart/form-data\" id=\"book_edit_frm\">\\n+<form role=\"form\" action=\"{{ url_for(\\'edit-book.edit_book\\', book_id=book.id) }}\" method=\"post\" enctype=\"multipart/form-data\" id=\"book_edit_frm\">\\n   <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n   <div class=\"col-sm-9 col-xs-12\">\\n     <div class=\"form-group\">', '@@ -138,7 +138,7 @@ <h2 id=\"title\">{{entry.title}}</h2>\\n         <p>\\n         <span class=\"glyphicon glyphicon-link\"></span>\\n         {% for identifier in entry.identifiers %}\\n-          <a href=\"{{identifier}}\" target=\"_blank\" class=\"btn btn-xs btn-success\" role=\"button\">{{identifier.formatType()}}</a>\\n+          <a href=\"{{identifier}}\" target=\"_blank\" class=\"btn btn-xs btn-success\" role=\"button\">{{identifier.format_type()}}</a>\\n         {%endfor%}\\n       </p>\\n       </div>\\n@@ -295,7 +295,7 @@ <h3 id=\"decription\">{{_(\\'Description:\\')}}</h3>\\n       {% if g.user.role_edit() %}\\n       <div class=\"btn-toolbar\" role=\"toolbar\">\\n         <div class=\"btn-group\" role=\"group\" aria-label=\"Edit/Delete book\">\\n-          <a href=\"{{ url_for(\\'editbook.edit_book\\', book_id=entry.id) }}\" class=\"btn btn-sm btn-primary\" id=\"edit_book\" role=\"button\"><span class=\"glyphicon glyphicon-edit\"></span> {{_(\\'Edit Metadata\\')}}</a>\\n+          <a href=\"{{ url_for(\\'edit-book.edit_book\\', book_id=entry.id) }}\" class=\"btn btn-sm btn-primary\" id=\"edit_book\" role=\"button\"><span class=\"glyphicon glyphicon-edit\"></span> {{_(\\'Edit Metadata\\')}}</a>\\n         </div>\\n       </div>\\n       {% endif %}', '@@ -23,11 +23,10 @@\\n import re\\n import shutil\\n import socket\\n-import unicodedata\\n from datetime import datetime, timedelta\\n from tempfile import gettempdir\\n-from urllib.parse import urlparse\\n import requests\\n+import unidecode\\n \\n from babel.dates import format_datetime\\n from babel.units import format_unit\\n@@ -41,15 +40,19 @@\\n from markupsafe import escape\\n from urllib.parse import quote\\n \\n+\\n try:\\n-    import unidecode\\n-    use_unidecode = True\\n+    import advocate\\n+    from advocate.exceptions import UnacceptableAddressException\\n+    use_advocate = True\\n except ImportError:\\n-    use_unidecode = False\\n+    use_advocate = False\\n+    advocate = requests\\n+    UnacceptableAddressException = MissingSchema = BaseException\\n \\n from . import calibre_db, cli\\n from .tasks.convert import TaskConvert\\n-from . import logger, config, get_locale, db, ub, kobo_sync_status\\n+from . import logger, config, get_locale, db, ub\\n from . import gdriveutils as gd\\n from .constants import STATIC_DIR as _STATIC_DIR\\n from .subproc_wrapper import process_wait\\n@@ -143,7 +146,7 @@ def check_send_to_kindle_with_converter(formats):\\n                             \\'text\\': _(\\'Convert %(orig)s to %(format)s and send to Kindle\\',\\n                                       orig=\\'Epub\\',\\n                                       format=\\'Mobi\\')})\\n-    if \\'AZW3\\' in formats and not \\'MOBI\\' in formats:\\n+    if \\'AZW3\\' in formats and \\'MOBI\\' not in formats:\\n         bookformats.append({\\'format\\': \\'Mobi\\',\\n                             \\'convert\\': 2,\\n                             \\'text\\': _(\\'Convert %(orig)s to %(format)s and send to Kindle\\',\\n@@ -185,11 +188,11 @@ def check_send_to_kindle(entry):\\n # Check if a reader is existing for any of the book formats, if not, return empty list, otherwise return\\n # list with supported formats\\n def check_read_formats(entry):\\n-    EXTENSIONS_READER = {\\'TXT\\', \\'PDF\\', \\'EPUB\\', \\'CBZ\\', \\'CBT\\', \\'CBR\\', \\'DJVU\\'}\\n+    extensions_reader = {\\'TXT\\', \\'PDF\\', \\'EPUB\\', \\'CBZ\\', \\'CBT\\', \\'CBR\\', \\'DJVU\\'}\\n     bookformats = list()\\n     if len(entry.data):\\n         for ele in iter(entry.data):\\n-            if ele.format.upper() in EXTENSIONS_READER:\\n+            if ele.format.upper() in extensions_reader:\\n                 bookformats.append(ele.format.lower())\\n     return bookformats\\n \\n@@ -213,10 +216,10 @@ def send_mail(book_id, book_format, convert, kindle_mail, calibrepath, user_id):\\n         if entry.format.upper() == book_format.upper():\\n             converted_file_name = entry.name + \\'.\\' + book_format.lower()\\n             link = \\'<a href=\"{}\">{}</a>\\'.format(url_for(\\'web.show_book\\', book_id=book_id), escape(book.title))\\n-            EmailText = _(u\"%(book)s send to Kindle\", book=link)\\n+            email_text = _(u\"%(book)s send to Kindle\", book=link)\\n             WorkerThread.add(user_id, TaskEmail(_(u\"Send to Kindle\"), book.path, converted_file_name,\\n                              config.get_mail_settings(), kindle_mail,\\n-                             EmailText, _(u\\'This e-mail has been sent via Calibre-Web.\\')))\\n+                             email_text, _(u\\'This e-mail has been sent via Calibre-Web.\\')))\\n             return\\n     return _(u\"The requested file could not be read. Maybe wrong permissions?\")\\n \\n@@ -229,15 +232,8 @@ def get_valid_filename(value, replace_whitespace=True, chars=128):\\n     if value[-1:] == u\\'.\\':\\n         value = value[:-1]+u\\'_\\'\\n     value = value.replace(\"/\", \"_\").replace(\":\", \"_\").strip(\\'\\\\0\\')\\n-    if use_unidecode:\\n-        if config.config_unicode_filename:\\n-            value = (unidecode.unidecode(value))\\n-    else:\\n-        value = value.replace(u\\'Â§\\', u\\'SS\\')\\n-        value = value.replace(u\\'ÃŸ\\', u\\'ss\\')\\n-        value = unicodedata.normalize(\\'NFKD\\', value)\\n-        re_slugify = re.compile(r\\'[\\\\W\\\\s-]\\', re.UNICODE)\\n-        value = re_slugify.sub(\\'\\', value)\\n+    if config.config_unicode_filename:\\n+        value = (unidecode.unidecode(value))\\n     if replace_whitespace:\\n         #  *+:\\\\\"/<>? are replaced by _\\n         value = re.sub(r\\'[*+:\\\\\\\\\\\\\"/<>?]+\\', u\\'_\\', value, flags=re.U)\\n@@ -266,6 +262,7 @@ def split_authors(values):\\n \\n \\n def get_sorted_author(value):\\n+    value2 = None\\n     try:\\n         if \\',\\' not in value:\\n             regexes = [r\"^(JR|SR)\\\\.?$\", r\"^I{1,3}\\\\.?$\", r\"^IV\\\\.?$\"]\\n@@ -290,6 +287,7 @@ def get_sorted_author(value):\\n             value2 = value\\n     return value2\\n \\n+\\n def edit_book_read_status(book_id, read_status=None):\\n     if not config.config_read_column:\\n         book = ub.session.query(ub.ReadBook).filter(and_(ub.ReadBook.user_id == int(current_user.id),\\n@@ -303,9 +301,9 @@ def edit_book_read_status(book_id, read_status=None):\\n             else:\\n                 book.read_status = ub.ReadBook.STATUS_FINISHED if read_status else ub.ReadBook.STATUS_UNREAD\\n         else:\\n-            readBook = ub.ReadBook(user_id=current_user.id, book_id = book_id)\\n-            readBook.read_status = ub.ReadBook.STATUS_FINISHED\\n-            book = readBook\\n+            read_book = ub.ReadBook(user_id=current_user.id, book_id=book_id)\\n+            read_book.read_status = ub.ReadBook.STATUS_FINISHED\\n+            book = read_book\\n         if not book.kobo_reading_state:\\n             kobo_reading_state = ub.KoboReadingState(user_id=current_user.id, book_id=book_id)\\n             kobo_reading_state.current_bookmark = ub.KoboBookmark()\\n@@ -332,12 +330,13 @@ def edit_book_read_status(book_id, read_status=None):\\n         except (KeyError, AttributeError):\\n             log.error(u\"Custom Column No.%d is not existing in calibre database\", config.config_read_column)\\n             return \"Custom Column No.{} is not existing in calibre database\".format(config.config_read_column)\\n-        except (OperationalError, InvalidRequestError) as e:\\n+        except (OperationalError, InvalidRequestError) as ex:\\n             calibre_db.session.rollback()\\n-            log.error(u\"Read status could not set: {}\".format(e))\\n-            return _(\"Read status could not set: {}\".format(e.orig))\\n+            log.error(u\"Read status could not set: {}\".format(ex))\\n+            return _(\"Read status could not set: {}\".format(ex.orig))\\n     return \"\"\\n \\n+\\n # Deletes a book fro the local filestorage, returns True if deleting is successfull, otherwise false\\n def delete_book_file(book, calibrepath, book_format=None):\\n     # check that path is 2 elements deep, check that target path has no subfolders\\n@@ -361,15 +360,15 @@ def delete_book_file(book, calibrepath, book_format=None):\\n                                            id=book.id,\\n                                            path=book.path)\\n                     shutil.rmtree(path)\\n-                except (IOError, OSError) as e:\\n-                    log.error(\"Deleting book %s failed: %s\", book.id, e)\\n-                    return False, _(\"Deleting book %(id)s failed: %(message)s\", id=book.id, message=e)\\n+                except (IOError, OSError) as ex:\\n+                    log.error(\"Deleting book %s failed: %s\", book.id, ex)\\n+                    return False, _(\"Deleting book %(id)s failed: %(message)s\", id=book.id, message=ex)\\n                 authorpath = os.path.join(calibrepath, os.path.split(book.path)[0])\\n                 if not os.listdir(authorpath):\\n                     try:\\n                         shutil.rmtree(authorpath)\\n-                    except (IOError, OSError) as e:\\n-                        log.error(\"Deleting authorpath for book %s failed: %s\", book.id, e)\\n+                    except (IOError, OSError) as ex:\\n+                        log.error(\"Deleting authorpath for book %s failed: %s\", book.id, ex)\\n                 return True, None\\n \\n     log.error(\"Deleting book %s from database only, book path in database not valid: %s\",\\n@@ -395,21 +394,21 @@ def clean_author_database(renamed_author, calibre_path=\"\", local_book=None, gdri\\n                 all_titledir = book.path.split(\\'/\\')[1]\\n                 all_new_path = os.path.join(calibre_path, all_new_authordir, all_titledir)\\n                 all_new_name = get_valid_filename(book.title, chars=42) + \\' - \\' \\\\\\n-                               + get_valid_filename(new_author.name, chars=42)\\n+                    + get_valid_filename(new_author.name, chars=42)\\n                 # change location in database to new author/title path\\n                 book.path = os.path.join(all_new_authordir, all_titledir).replace(\\'\\\\\\\\\\', \\'/\\')\\n                 for file_format in book.data:\\n                     if not gdrive:\\n                         shutil.move(os.path.normcase(os.path.join(all_new_path,\\n                                                                   file_format.name + \\'.\\' + file_format.format.lower())),\\n-                            os.path.normcase(os.path.join(all_new_path,\\n-                                                          all_new_name + \\'.\\' + file_format.format.lower())))\\n+                                    os.path.normcase(os.path.join(all_new_path,\\n+                                                                  all_new_name + \\'.\\' + file_format.format.lower())))\\n                     else:\\n-                        gFile = gd.getFileFromEbooksFolder(all_new_path,\\n-                                                           file_format.name + \\'.\\' + file_format.format.lower())\\n-                        if gFile:\\n-                            gd.moveGdriveFileRemote(gFile, all_new_name + u\\'.\\' + file_format.format.lower())\\n-                            gd.updateDatabaseOnEdit(gFile[\\'id\\'], all_new_name + u\\'.\\' + file_format.format.lower())\\n+                        g_file = gd.getFileFromEbooksFolder(all_new_path,\\n+                                                            file_format.name + \\'.\\' + file_format.format.lower())\\n+                        if g_file:\\n+                            gd.moveGdriveFileRemote(g_file, all_new_name + u\\'.\\' + file_format.format.lower())\\n+                            gd.updateDatabaseOnEdit(g_file[\\'id\\'], all_new_name + u\\'.\\' + file_format.format.lower())\\n                         else:\\n                             log.error(\"File {} not found on gdrive\"\\n                                       .format(all_new_path, file_format.name + \\'.\\' + file_format.format.lower()))\\n@@ -426,16 +425,16 @@ def rename_all_authors(first_author, renamed_author, calibre_path=\"\", localbook=\\n             old_author_dir = get_valid_filename(r, chars=96)\\n             new_author_rename_dir = get_valid_filename(new_author.name, chars=96)\\n             if gdrive:\\n-                gFile = gd.getFileFromEbooksFolder(None, old_author_dir)\\n-                if gFile:\\n-                    gd.moveGdriveFolderRemote(gFile, new_author_rename_dir)\\n+                g_file = gd.getFileFromEbooksFolder(None, old_author_dir)\\n+                if g_file:\\n+                    gd.moveGdriveFolderRemote(g_file, new_author_rename_dir)\\n             else:\\n                 if os.path.isdir(os.path.join(calibre_path, old_author_dir)):\\n                     try:\\n                         old_author_path = os.path.join(calibre_path, old_author_dir)\\n                         new_author_path = os.path.join(calibre_path, new_author_rename_dir)\\n                         shutil.move(os.path.normcase(old_author_path), os.path.normcase(new_author_path))\\n-                    except (OSError) as ex:\\n+                    except OSError as ex:\\n                         log.error(\"Rename author from: %s to %s: %s\", old_author_path, new_author_path, ex)\\n                         log.debug(ex, exc_info=True)\\n                         return _(\"Rename author from: \\'%(src)s\\' to \\'%(dest)s\\' failed with error: %(error)s\",\\n@@ -444,6 +443,7 @@ def rename_all_authors(first_author, renamed_author, calibre_path=\"\", localbook=\\n         new_authordir = get_valid_filename(localbook.authors[0].name, chars=96)\\n     return new_authordir\\n \\n+\\n # Moves files in file storage during author/title rename, or from temp dir to file storage\\n def update_dir_structure_file(book_id, calibre_path, first_author, original_filepath, db_filename, renamed_author):\\n     # get book database entry from id, if original path overwrite source with original_filepath\\n@@ -483,11 +483,9 @@ def update_dir_structure_file(book_id, calibre_path, first_author, original_file\\n \\n \\n def upload_new_file_gdrive(book_id, first_author, renamed_author, title, title_dir, original_filepath, filename_ext):\\n-    error = False\\n     book = calibre_db.get_book(book_id)\\n     file_name = get_valid_filename(title, chars=42) + \\' - \\' + \\\\\\n-                get_valid_filename(first_author, chars=42) + \\\\\\n-                filename_ext\\n+        get_valid_filename(first_author, chars=42) + filename_ext\\n     rename_all_authors(first_author, renamed_author, gdrive=True)\\n     gdrive_path = os.path.join(get_valid_filename(first_author, chars=96),\\n                                title_dir + \" (\" + str(book_id) + \")\")\\n@@ -505,20 +503,20 @@ def update_dir_structure_gdrive(book_id, first_author, renamed_author):\\n     new_titledir = get_valid_filename(book.title, chars=96) + u\" (\" + str(book_id) + u\")\"\\n \\n     if titledir != new_titledir:\\n-        gFile = gd.getFileFromEbooksFolder(os.path.dirname(book.path), titledir)\\n-        if gFile:\\n-            gd.moveGdriveFileRemote(gFile, new_titledir)\\n+        g_file = gd.getFileFromEbooksFolder(os.path.dirname(book.path), titledir)\\n+        if g_file:\\n+            gd.moveGdriveFileRemote(g_file, new_titledir)\\n             book.path = book.path.split(\\'/\\')[0] + u\\'/\\' + new_titledir\\n-            gd.updateDatabaseOnEdit(gFile[\\'id\\'], book.path)     # only child folder affected\\n+            gd.updateDatabaseOnEdit(g_file[\\'id\\'], book.path)     # only child folder affected\\n         else:\\n             return _(u\\'File %(file)s not found on Google Drive\\', file=book.path)  # file not found\\n \\n     if authordir != new_authordir and authordir not in renamed_author:\\n-        gFile = gd.getFileFromEbooksFolder(os.path.dirname(book.path), new_titledir)\\n-        if gFile:\\n-            gd.moveGdriveFolderRemote(gFile, new_authordir)\\n+        g_file = gd.getFileFromEbooksFolder(os.path.dirname(book.path), new_titledir)\\n+        if g_file:\\n+            gd.moveGdriveFolderRemote(g_file, new_authordir)\\n             book.path = new_authordir + u\\'/\\' + book.path.split(\\'/\\')[1]\\n-            gd.updateDatabaseOnEdit(gFile[\\'id\\'], book.path)\\n+            gd.updateDatabaseOnEdit(g_file[\\'id\\'], book.path)\\n         else:\\n             return _(u\\'File %(file)s not found on Google Drive\\', file=authordir)  # file not found\\n \\n@@ -542,15 +540,15 @@ def move_files_on_change(calibre_path, new_authordir, new_titledir, localbook, d\\n                 # move original path to new path\\n                 log.debug(\"Moving title: %s to %s\", path, new_path)\\n                 shutil.move(os.path.normcase(path), os.path.normcase(new_path))\\n-            else: # path is valid copy only files to new location (merge)\\n+            else:  # path is valid copy only files to new location (merge)\\n                 log.info(\"Moving title: %s into existing: %s\", path, new_path)\\n                 # Take all files and subfolder from old path (strange command)\\n                 for dir_name, __, file_list in os.walk(path):\\n                     for file in file_list:\\n                         shutil.move(os.path.normcase(os.path.join(dir_name, file)),\\n-                                        os.path.normcase(os.path.join(new_path + dir_name[len(path):], file)))\\n+                                    os.path.normcase(os.path.join(new_path + dir_name[len(path):], file)))\\n         # change location in database to new author/title path\\n-        localbook.path = os.path.join(new_authordir, new_titledir).replace(\\'\\\\\\\\\\',\\'/\\')\\n+        localbook.path = os.path.join(new_authordir, new_titledir).replace(\\'\\\\\\\\\\', \\'/\\')\\n     except OSError as ex:\\n         log.error(\"Rename title from: %s to %s: %s\", path, new_path, ex)\\n         log.debug(ex, exc_info=True)\\n@@ -587,12 +585,12 @@ def delete_book_gdrive(book, book_format):\\n         for entry in book.data:\\n             if entry.format.upper() == book_format:\\n                 name = entry.name + \\'.\\' + book_format\\n-        gFile = gd.getFileFromEbooksFolder(book.path, name)\\n+        g_file = gd.getFileFromEbooksFolder(book.path, name)\\n     else:\\n-        gFile = gd.getFileFromEbooksFolder(os.path.dirname(book.path), book.path.split(\\'/\\')[1])\\n-    if gFile:\\n-        gd.deleteDatabaseEntry(gFile[\\'id\\'])\\n-        gFile.Trash()\\n+        g_file = gd.getFileFromEbooksFolder(os.path.dirname(book.path), book.path.split(\\'/\\')[1])\\n+    if g_file:\\n+        gd.deleteDatabaseEntry(g_file[\\'id\\'])\\n+        g_file.Trash()\\n     else:\\n         error = _(u\\'Book path %(path)s not found on Google Drive\\', path=book.path)  # file not found\\n \\n@@ -624,12 +622,13 @@ def generate_random_password():\\n \\n def uniq(inpt):\\n     output = []\\n-    inpt = [ \" \".join(inp.split()) for inp in inpt]\\n+    inpt = [\" \".join(inp.split()) for inp in inpt]\\n     for x in inpt:\\n         if x not in output:\\n             output.append(x)\\n     return output\\n \\n+\\n def check_email(email):\\n     email = valid_email(email)\\n     if ub.session.query(ub.User).filter(func.lower(ub.User.email) == email.lower()).first():\\n@@ -642,7 +641,7 @@ def check_username(username):\\n     username = username.strip()\\n     if ub.session.query(ub.User).filter(func.lower(ub.User.name) == username.lower()).scalar():\\n         log.error(u\"This username is already taken\")\\n-        raise Exception (_(u\"This username is already taken\"))\\n+        raise Exception(_(u\"This username is already taken\"))\\n     return username\\n \\n \\n@@ -728,13 +727,13 @@ def get_book_cover_internal(book, use_generic_cover_on_failure):\\n # saves book cover from url\\n def save_cover_from_url(url, book_path):\\n     try:\\n-        if not cli.allow_localhost:\\n-            # 127.0.x.x, localhost, [::1], [::ffff:7f00:1]\\n-            ip = socket.getaddrinfo(urlparse(url).hostname, 0)[0][4][0]\\n-            if ip.startswith(\"127.\") or ip.startswith(\\'::ffff:7f\\') or ip == \"::1\" or ip == \"0.0.0.0\" or ip == \"::\":\\n-                log.error(\"Localhost was accessed for cover upload\")\\n-                return False, _(\"You are not allowed to access localhost for cover uploads\")\\n-        img = requests.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\\n+        if cli.allow_localhost:\\n+            img = requests.get(url, timeout=(10, 200), allow_redirects=False)  # ToDo: Error Handling\\n+        elif use_advocate:\\n+            img = advocate.get(url, timeout=(10, 200), allow_redirects=False)      # ToDo: Error Handling\\n+        else:\\n+            log.error(\"python modul advocate is not installed but is needed\")\\n+            return False, _(\"Python modul \\'advocate\\' is not installed but is needed for cover downloads\")\\n         img.raise_for_status()\\n         return save_cover(img, book_path)\\n     except (socket.gaierror,\\n@@ -746,6 +745,9 @@ def save_cover_from_url(url, book_path):\\n     except MissingDelegateError as ex:\\n         log.info(u\\'File Format Error %s\\', ex)\\n         return False, _(\"Cover Format Error\")\\n+    except UnacceptableAddressException:\\n+        log.error(\"Localhost was accessed for cover upload\")\\n+        return False, _(\"You are not allowed to access localhost for cover uploads\")\\n \\n \\n def save_cover_from_filestorage(filepath, saved_filename, img):\\n@@ -808,7 +810,7 @@ def save_cover(img, book_path):\\n             os.mkdir(tmp_dir)\\n         ret, message = save_cover_from_filestorage(tmp_dir, \"uploaded_cover.jpg\", img)\\n         if ret is True:\\n-            gd.uploadFileToEbooksFolder(os.path.join(book_path, \\'cover.jpg\\').replace(\"\\\\\\\\\",\"/\"),\\n+            gd.uploadFileToEbooksFolder(os.path.join(book_path, \\'cover.jpg\\').replace(\"\\\\\\\\\", \"/\"),\\n                                         os.path.join(tmp_dir, \"uploaded_cover.jpg\"))\\n             log.info(\"Cover is saved on Google Drive\")\\n             return True, None\\n@@ -820,9 +822,9 @@ def save_cover(img, book_path):\\n \\n def do_download_file(book, book_format, client, data, headers):\\n     if config.config_use_google_drive:\\n-        #startTime = time.time()\\n+        # startTime = time.time()\\n         df = gd.getFileFromEbooksFolder(book.path, data.name + \".\" + book_format)\\n-        #log.debug(\\'%s\\', time.time() - startTime)\\n+        # log.debug(\\'%s\\', time.time() - startTime)\\n         if df:\\n             return gd.do_gdrive_download(df, headers)\\n         else:\\n@@ -846,16 +848,16 @@ def do_download_file(book, book_format, client, data, headers):\\n ##################################\\n \\n \\n-def check_unrar(unrarLocation):\\n-    if not unrarLocation:\\n+def check_unrar(unrar_location):\\n+    if not unrar_location:\\n         return\\n \\n-    if not os.path.exists(unrarLocation):\\n+    if not os.path.exists(unrar_location):\\n         return _(\\'Unrar binary file not found\\')\\n \\n     try:\\n-        unrarLocation = [unrarLocation]\\n-        value = process_wait(unrarLocation, pattern=\\'UNRAR (.*) freeware\\')\\n+        unrar_location = [unrar_location]\\n+        value = process_wait(unrar_location, pattern=\\'UNRAR (.*) freeware\\')\\n         if value:\\n             version = value.group(1)\\n             log.debug(\"unrar version %s\", version)\\n@@ -882,19 +884,19 @@ def json_serial(obj):\\n \\n # helper function for displaying the runtime of tasks\\n def format_runtime(runtime):\\n-    retVal = \"\"\\n+    ret_val = \"\"\\n     if runtime.days:\\n-        retVal = format_unit(runtime.days, \\'duration-day\\', length=\"long\", locale=get_locale()) + \\', \\'\\n+        ret_val = format_unit(runtime.days, \\'duration-day\\', length=\"long\", locale=get_locale()) + \\', \\'\\n     mins, seconds = divmod(runtime.seconds, 60)\\n     hours, minutes = divmod(mins, 60)\\n     # ToDo: locale.number_symbols._data[\\'timeSeparator\\'] -> localize time separator ?\\n     if hours:\\n-        retVal += \\'{:d}:{:02d}:{:02d}s\\'.format(hours, minutes, seconds)\\n+        ret_val += \\'{:d}:{:02d}:{:02d}s\\'.format(hours, minutes, seconds)\\n     elif minutes:\\n-        retVal += \\'{:2d}:{:02d}s\\'.format(minutes, seconds)\\n+        ret_val += \\'{:2d}:{:02d}s\\'.format(minutes, seconds)\\n     else:\\n-        retVal += \\'{:2d}s\\'.format(seconds)\\n-    return retVal\\n+        ret_val += \\'{:2d}s\\'.format(seconds)\\n+    return ret_val\\n \\n \\n # helper function to apply localize status information in tasklist entries\\n@@ -951,8 +953,8 @@ def check_valid_domain(domain_text):\\n \\n \\n def get_cc_columns(filter_config_custom_read=False):\\n-    tmpcc = calibre_db.session.query(db.Custom_Columns)\\\\\\n-        .filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    tmpcc = calibre_db.session.query(db.CustomColumns)\\\\\\n+        .filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     cc = []\\n     r = None\\n     if config.config_columns_to_ignore:\\n@@ -971,6 +973,7 @@ def get_cc_columns(filter_config_custom_read=False):\\n def get_download_link(book_id, book_format, client):\\n     book_format = book_format.split(\".\")[0]\\n     book = calibre_db.get_filtered_book(book_id, allow_show_archived=True)\\n+    data1= \"\"\\n     if book:\\n         data1 = calibre_db.get_book_format(book.id, book_format.upper())\\n     else:', '@@ -28,7 +28,6 @@\\n from flask_login import current_user\\n from sqlalchemy.sql.expression import func, text, or_, and_, true\\n from werkzeug.security import check_password_hash\\n-from tornado.httputil import HTTPServerRequest\\n from . import constants, logger, config, db, calibre_db, ub, services, get_locale, isoLanguages\\n from .helper import get_download_link, get_book_cover\\n from .pagination import Pagination\\n@@ -99,26 +98,7 @@ def feed_normal_search():\\n @opds.route(\"/opds/books\")\\n @requires_basic_auth_if_no_ano\\n def feed_booksindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Books.sort, 1, 1)).label(\\'id\\'))\\\\\\n-        .filter(calibre_db.common_filters()).group_by(func.upper(func.substr(db.Books.sort, 1, 1))).all()\\n-\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n-\\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_books\\',\\n-                               pagination=pagination)\\n+    return render_element_index(db.Books.sort, None, \\'opds.feed_letter_books\\')\\n \\n \\n @opds.route(\"/opds/books/letter/<book_id>\")\\n@@ -171,43 +151,23 @@ def feed_hot():\\n     hot_books = all_books.offset(off).limit(config.config_books_per_page)\\n     entries = list()\\n     for book in hot_books:\\n-        downloadBook = calibre_db.get_book(book.Downloads.book_id)\\n-        if downloadBook:\\n+        download_book = calibre_db.get_book(book.Downloads.book_id)\\n+        if download_book:\\n             entries.append(\\n                 calibre_db.get_filtered_book(book.Downloads.book_id)\\n             )\\n         else:\\n             ub.delete_download(book.Downloads.book_id)\\n-    numBooks = entries.__len__()\\n+    num_books = entries.__len__()\\n     pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1),\\n-                            config.config_books_per_page, numBooks)\\n+                            config.config_books_per_page, num_books)\\n     return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n \\n \\n @opds.route(\"/opds/author\")\\n @requires_basic_auth_if_no_ano\\n def feed_authorindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Authors.sort, 1, 1)).label(\\'id\\'))\\\\\\n-        .join(db.books_authors_link).join(db.Books).filter(calibre_db.common_filters())\\\\\\n-        .group_by(func.upper(func.substr(db.Authors.sort, 1, 1))).all()\\n-\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n-\\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_author\\',\\n-                               pagination=pagination)\\n+    return render_element_index(db.Authors.sort, db.books_authors_link, \\'opds.feed_letter_author\\')\\n \\n \\n @opds.route(\"/opds/author/letter/<book_id>\")\\n@@ -228,12 +188,7 @@ def feed_letter_author(book_id):\\n @opds.route(\"/opds/author/<int:book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_author(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.authors.any(db.Authors.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Authors, book_id)\\n \\n \\n @opds.route(\"/opds/publisher\")\\n@@ -254,37 +209,14 @@ def feed_publisherindex():\\n @opds.route(\"/opds/publisher/<int:book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_publisher(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.publishers.any(db.Publishers.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Publishers, book_id)\\n \\n \\n @opds.route(\"/opds/category\")\\n @requires_basic_auth_if_no_ano\\n def feed_categoryindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Tags.name, 1, 1)).label(\\'id\\'))\\\\\\n-        .join(db.books_tags_link).join(db.Books).filter(calibre_db.common_filters())\\\\\\n-        .group_by(func.upper(func.substr(db.Tags.name, 1, 1))).all()\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n+    return render_element_index(db.Tags.name, db.books_tags_link, \\'opds.feed_letter_category\\')\\n \\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_category\\',\\n-                               pagination=pagination)\\n \\n @opds.route(\"/opds/category/letter/<book_id>\")\\n @requires_basic_auth_if_no_ano\\n@@ -306,36 +238,14 @@ def feed_letter_category(book_id):\\n @opds.route(\"/opds/category/<int:book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_category(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.tags.any(db.Tags.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Tags, book_id)\\n \\n \\n @opds.route(\"/opds/series\")\\n @requires_basic_auth_if_no_ano\\n def feed_seriesindex():\\n-    shift = 0\\n-    off = int(request.args.get(\"offset\") or 0)\\n-    entries = calibre_db.session.query(func.upper(func.substr(db.Series.sort, 1, 1)).label(\\'id\\'))\\\\\\n-        .join(db.books_series_link).join(db.Books).filter(calibre_db.common_filters())\\\\\\n-        .group_by(func.upper(func.substr(db.Series.sort, 1, 1))).all()\\n-    elements = []\\n-    if off == 0:\\n-        elements.append({\\'id\\': \"00\", \\'name\\':_(\"All\")})\\n-        shift = 1\\n-    for entry in entries[\\n-                 off + shift - 1:\\n-                 int(off + int(config.config_books_per_page) - shift)]:\\n-        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n-    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n-                            len(entries) + 1)\\n-    return render_xml_template(\\'feed.xml\\',\\n-                               letterelements=elements,\\n-                               folder=\\'opds.feed_letter_series\\',\\n-                               pagination=pagination)\\n+    return render_element_index(db.Series.sort, db.books_series_link, \\'opds.feed_letter_series\\')\\n+\\n \\n @opds.route(\"/opds/series/letter/<book_id>\")\\n @requires_basic_auth_if_no_ano\\n@@ -370,7 +280,7 @@ def feed_series(book_id):\\n def feed_ratingindex():\\n     off = request.args.get(\"offset\") or 0\\n     entries = calibre_db.session.query(db.Ratings, func.count(\\'books_ratings_link.book\\').label(\\'count\\'),\\n-                               (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n+                                       (db.Ratings.rating / 2).label(\\'name\\')) \\\\\\n         .join(db.books_ratings_link)\\\\\\n         .join(db.Books)\\\\\\n         .filter(calibre_db.common_filters()) \\\\\\n@@ -388,12 +298,7 @@ def feed_ratingindex():\\n @opds.route(\"/opds/ratings/<book_id>\")\\n @requires_basic_auth_if_no_ano\\n def feed_ratings(book_id):\\n-    off = request.args.get(\"offset\") or 0\\n-    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n-                                                        db.Books,\\n-                                                        db.Books.ratings.any(db.Ratings.id == book_id),\\n-                                                        [db.Books.timestamp.desc()])\\n-    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n+    return render_xml_dataset(db.Tags, book_id)\\n \\n \\n @opds.route(\"/opds/formats\")\\n@@ -491,7 +396,7 @@ def feed_shelf(book_id):\\n @requires_basic_auth_if_no_ano\\n def opds_download_link(book_id, book_format):\\n     # I gave up with this: With enabled ldap login, the user doesn\\'t get logged in, therefore it\\'s always guest\\n-    # workaround, loading the user from the request and checking it\\'s download rights here\\n+    # workaround, loading the user from the request and checking its download rights here\\n     # in case of anonymous browsing user is None\\n     user = load_user_from_request(request) or current_user\\n     if not user.role_download():\\n@@ -517,6 +422,31 @@ def get_metadata_calibre_companion(uuid, library):\\n         return \"\"\\n \\n \\n+@opds.route(\"/opds/thumb_240_240/<book_id>\")\\n+@opds.route(\"/opds/cover_240_240/<book_id>\")\\n+@opds.route(\"/opds/cover_90_90/<book_id>\")\\n+@opds.route(\"/opds/cover/<book_id>\")\\n+@requires_basic_auth_if_no_ano\\n+def feed_get_cover(book_id):\\n+    return get_book_cover(book_id)\\n+\\n+\\n+@opds.route(\"/opds/readbooks\")\\n+@requires_basic_auth_if_no_ano\\n+def feed_read_books():\\n+    off = request.args.get(\"offset\") or 0\\n+    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, True, True)\\n+    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+\\n+\\n+@opds.route(\"/opds/unreadbooks\")\\n+@requires_basic_auth_if_no_ano\\n+def feed_unread_books():\\n+    off = request.args.get(\"offset\") or 0\\n+    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, False, True)\\n+    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+\\n+\\n def feed_search(term):\\n     if term:\\n         entries, __, ___ = calibre_db.get_search_results(term, config_read_column=config.config_read_column)\\n@@ -538,8 +468,8 @@ def check_auth(username, password):\\n     if bool(user and check_password_hash(str(user.password), password)):\\n         return True\\n     else:\\n-        ip_Address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n-        log.warning(\\'OPDS Login failed for user \"%s\" IP-address: %s\\', username.decode(\\'utf-8\\'), ip_Address)\\n+        ip_address = request.headers.get(\\'X-Forwarded-For\\', request.remote_addr)\\n+        log.warning(\\'OPDS Login failed for user \"%s\" IP-address: %s\\', username.decode(\\'utf-8\\'), ip_address)\\n         return False\\n \\n \\n@@ -559,26 +489,33 @@ def render_xml_template(*args, **kwargs):\\n     return response\\n \\n \\n-@opds.route(\"/opds/thumb_240_240/<book_id>\")\\n-@opds.route(\"/opds/cover_240_240/<book_id>\")\\n-@opds.route(\"/opds/cover_90_90/<book_id>\")\\n-@opds.route(\"/opds/cover/<book_id>\")\\n-@requires_basic_auth_if_no_ano\\n-def feed_get_cover(book_id):\\n-    return get_book_cover(book_id)\\n-\\n-\\n-@opds.route(\"/opds/readbooks\")\\n-@requires_basic_auth_if_no_ano\\n-def feed_read_books():\\n+def render_xml_dataset(data_table, book_id):\\n     off = request.args.get(\"offset\") or 0\\n-    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, True, True)\\n-    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+    entries, __, pagination = calibre_db.fill_indexpage((int(off) / (int(config.config_books_per_page)) + 1), 0,\\n+                                                        db.Books,\\n+                                                        data_table.any(data_table.id == book_id),\\n+                                                        [db.Books.timestamp.desc()])\\n+    return render_xml_template(\\'feed.xml\\', entries=entries, pagination=pagination)\\n \\n \\n-@opds.route(\"/opds/unreadbooks\")\\n-@requires_basic_auth_if_no_ano\\n-def feed_unread_books():\\n-    off = request.args.get(\"offset\") or 0\\n-    result, pagination = render_read_books(int(off) / (int(config.config_books_per_page)) + 1, False, True)\\n-    return render_xml_template(\\'feed.xml\\', entries=result, pagination=pagination)\\n+def render_element_index(database_column, linked_table, folder):\\n+    shift = 0\\n+    off = int(request.args.get(\"offset\") or 0)\\n+    entries = calibre_db.session.query(func.upper(func.substr(database_column, 1, 1)).label(\\'id\\'))\\n+    if linked_table:\\n+        entries = entries.join(linked_table).join(db.Books)\\n+    entries = entries.filter(calibre_db.common_filters()).group_by(func.upper(func.substr(database_column, 1, 1))).all()\\n+    elements = []\\n+    if off == 0:\\n+        elements.append({\\'id\\': \"00\", \\'name\\': _(\"All\")})\\n+        shift = 1\\n+    for entry in entries[\\n+                 off + shift - 1:\\n+                 int(off + int(config.config_books_per_page) - shift)]:\\n+        elements.append({\\'id\\': entry.id, \\'name\\': entry.id})\\n+    pagination = Pagination((int(off) / (int(config.config_books_per_page)) + 1), config.config_books_per_page,\\n+                            len(entries) + 1)\\n+    return render_xml_template(\\'feed.xml\\',\\n+                               letterelements=elements,\\n+                               folder=folder,\\n+                               pagination=pagination)', '@@ -6,7 +6,7 @@\\n     data-escape=\"true\"\\n     {% if g.user.role_edit() %}\\n         data-editable-type=\"text\"\\n-        data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=parameter)}}\"\\n+        data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=parameter)}}\"\\n         data-editable-title=\"{{ edit_text }}\"\\n         data-edit=\"true\"\\n         {% if validate %}data-edit-validate=\"{{ _(\\'This Field is Required\\') }}\" {% endif %}\\n@@ -66,30 +66,30 @@ <h2 class=\"{{page}}\">{{_(title)}}</h2>\\n             {{ text_table_row(\\'authors\\', _(\\'Enter Authors\\'),_(\\'Authors\\'), true, true) }}\\n             {{ text_table_row(\\'tags\\', _(\\'Enter Categories\\'),_(\\'Categories\\'), false, true) }}\\n             {{ text_table_row(\\'series\\', _(\\'Enter Series\\'),_(\\'Series\\'), false, true) }}\\n-            <th data-field=\"series_index\" id=\"series_index\" data-visible=\"{{visiblility.get(\\'series_index\\')}}\" data-edit-validate=\"{{ _(\\'This Field is Required\\') }}\" data-sortable=\"true\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-min=\"0\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'series_index\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter Title\\')}}\"{% endif %}>{{_(\\'Series Index\\')}}</th>\\n+            <th data-field=\"series_index\" id=\"series_index\" data-visible=\"{{visiblility.get(\\'series_index\\')}}\" data-edit-validate=\"{{ _(\\'This Field is Required\\') }}\" data-sortable=\"true\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-min=\"0\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'series_index\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter Title\\')}}\"{% endif %}>{{_(\\'Series Index\\')}}</th>\\n             {{ text_table_row(\\'languages\\', _(\\'Enter Languages\\'),_(\\'Languages\\'), false, true) }}\\n             <!--th data-field=\"pubdate\" data-type=\"date\" data-visible=\"{{visiblility.get(\\'pubdate\\')}}\" data-viewformat=\"dd.mm.yyyy\" id=\"pubdate\" data-sortable=\"true\">{{_(\\'Publishing Date\\')}}</th-->\\n             {{ text_table_row(\\'publishers\\', _(\\'Enter Publishers\\'),_(\\'Publishers\\'), false, true) }}\\n-            <th data-field=\"comments\" id=\"comments\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'comments\\')}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'comments\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter comments\\')}}\"{% endif %}>{{_(\\'Comments\\')}}</th>\\n+            <th data-field=\"comments\" id=\"comments\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'comments\\')}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'comments\\')}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter comments\\')}}\"{% endif %}>{{_(\\'Comments\\')}}</th>\\n             {% if g.user.check_visibility(32768) %}\\n                 {{ book_checkbox_row(\\'is_archived\\', _(\\'Archiv Status\\'), false)}}\\n             {%  endif %}\\n             {{ book_checkbox_row(\\'read_status\\', _(\\'Read Status\\'), false)}}\\n             {% for c in cc %}\\n               {% if c.datatype == \"int\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"1\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"1\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"rating\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-formatter=\"ratingFormatter\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.5\" data-editable-step=\"1\" data-editable-min=\"1\" data-editable-max=\"5\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-formatter=\"ratingFormatter\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.5\" data-editable-step=\"1\" data-editable-min=\"1\" data-editable-max=\"5\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"float\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"number\" data-editable-placeholder=\"1\" data-editable-step=\"0.01\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"enumeration\" %}\\n-                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"select\" data-editable-source={{ url_for(\\'editbook.table_get_custom_enum\\', c_id=c.id)  }} data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"select\" data-editable-source={{ url_for(\\'edit-book.table_get_custom_enum\\', c_id=c.id)  }} data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype in [\"datetime\"] %}\\n                   <!-- missing -->\\n               {% elif c.datatype == \"text\" %}\\n                  {{ text_table_row(\\'custom_column_\\' + c.id|string, _(\\'Enter \\') + c.name, c.name, false, false) }}\\n               {% elif c.datatype == \"comments\" %}\\n-                  <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'editbook.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n+                  <th data-field=\"custom_column_{{ c.id|string }}\" id=\"custom_column_{{ c.id|string }}\" data-escape=\"true\" data-editable-mode=\"popup\"  data-visible=\"{{visiblility.get(\\'custom_column_\\'+ c.id|string)}}\" data-sortable=\"false\" {% if g.user.role_edit() %} data-editable-type=\"wysihtml5\" data-editable-url=\"{{ url_for(\\'edit-book.edit_list_book\\', param=\\'custom_column_\\'+ c.id|string)}}\" data-edit=\"true\" data-editable-title=\"{{_(\\'Enter \\') + c.name}}\"{% endif %}>{{c.name}}</th>\\n               {% elif c.datatype == \"bool\" %}\\n                   {{ book_checkbox_row(\\'custom_column_\\' + c.id|string, c.name, false)}}\\n               {% else %}', '@@ -40,7 +40,7 @@\\n from cps.shelf import shelf\\n from cps.admin import admi\\n from cps.gdrive import gdrive\\n-from cps.editbooks import editbook\\n+from cps.editbooks import EditBook\\n from cps.remotelogin import remotelogin\\n from cps.search_metadata import meta\\n from cps.error_handler import init_errorhandler\\n@@ -73,7 +73,7 @@ def main():\\n     app.register_blueprint(remotelogin)\\n     app.register_blueprint(meta)\\n     app.register_blueprint(gdrive)\\n-    app.register_blueprint(editbook)\\n+    app.register_blueprint(EditBook)\\n     if kobo_available:\\n         app.register_blueprint(kobo)\\n         app.register_blueprint(kobo_auth)', '@@ -27,8 +27,9 @@\\n import time\\n import operator\\n from datetime import datetime, timedelta\\n+from functools import wraps\\n \\n-from babel import Locale as LC\\n+from babel import Locale\\n from babel.dates import format_datetime\\n from flask import Blueprint, flash, redirect, url_for, abort, request, make_response, send_from_directory, g, Response\\n from flask_login import login_required, current_user, logout_user, confirm_login\\n@@ -47,7 +48,6 @@\\n from .render_template import render_title_template, get_sidebar_config\\n from . import debug_info, _BABEL_TRANSLATIONS\\n \\n-from functools import wraps\\n \\n log = logger.create()\\n \\n@@ -189,10 +189,10 @@ def admin():\\n         else:\\n             commit = version[\\'version\\']\\n \\n-    allUser = ub.session.query(ub.User).all()\\n+    all_user = ub.session.query(ub.User).all()\\n     email_settings = config.get_mail_settings()\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n-    return render_title_template(\"admin.html\", allUser=allUser, email=email_settings, config=config, commit=commit,\\n+    return render_title_template(\"admin.html\", allUser=all_user, email=email_settings, config=config, commit=commit,\\n                                  feature_support=feature_support, kobo_support=kobo_support,\\n                                  title=_(u\"Admin page\"), page=\"admin\")\\n \\n@@ -242,12 +242,12 @@ def calibreweb_alive():\\n @login_required\\n @admin_required\\n def view_configuration():\\n-    read_column = calibre_db.session.query(db.Custom_Columns)\\\\\\n-        .filter(and_(db.Custom_Columns.datatype == \\'bool\\', db.Custom_Columns.mark_for_delete == 0)).all()\\n-    restrict_columns = calibre_db.session.query(db.Custom_Columns)\\\\\\n-        .filter(and_(db.Custom_Columns.datatype == \\'text\\', db.Custom_Columns.mark_for_delete == 0)).all()\\n+    read_column = calibre_db.session.query(db.CustomColumns)\\\\\\n+        .filter(and_(db.CustomColumns.datatype == \\'bool\\', db.CustomColumns.mark_for_delete == 0)).all()\\n+    restrict_columns = calibre_db.session.query(db.CustomColumns)\\\\\\n+        .filter(and_(db.CustomColumns.datatype == \\'text\\', db.CustomColumns.mark_for_delete == 0)).all()\\n     languages = calibre_db.speaking_language()\\n-    translations = [LC(\\'en\\')] + babel.list_translations()\\n+    translations = [Locale(\\'en\\')] + babel.list_translations()\\n     return render_title_template(\"config_view_edit.html\", conf=config, readColumns=read_column,\\n                                  restrictColumns=restrict_columns,\\n                                  languages=languages,\\n@@ -261,8 +261,8 @@ def view_configuration():\\n def edit_user_table():\\n     visibility = current_user.view_settings.get(\\'useredit\\', {})\\n     languages = calibre_db.speaking_language()\\n-    translations = babel.list_translations() + [LC(\\'en\\')]\\n-    allUser = ub.session.query(ub.User)\\n+    translations = babel.list_translations() + [Locale(\\'en\\')]\\n+    all_user = ub.session.query(ub.User)\\n     tags = calibre_db.session.query(db.Tags)\\\\\\n         .join(db.books_tags_link)\\\\\\n         .join(db.Books)\\\\\\n@@ -274,10 +274,10 @@ def edit_user_table():\\n     else:\\n         custom_values = []\\n     if not config.config_anonbrowse:\\n-        allUser = allUser.filter(ub.User.role.op(\\'&\\')(constants.ROLE_ANONYMOUS) != constants.ROLE_ANONYMOUS)\\n+        all_user = all_user.filter(ub.User.role.op(\\'&\\')(constants.ROLE_ANONYMOUS) != constants.ROLE_ANONYMOUS)\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     return render_title_template(\"user_table.html\",\\n-                                 users=allUser.all(),\\n+                                 users=all_user.all(),\\n                                  tags=tags,\\n                                  custom_values=custom_values,\\n                                  translations=translations,\\n@@ -332,7 +332,7 @@ def list_users():\\n         if user.default_language == \"all\":\\n             user.default = _(\"All\")\\n         else:\\n-            user.default = LC.parse(user.default_language).get_language_name(get_locale())\\n+            user.default = Locale.parse(user.default_language).get_language_name(get_locale())\\n \\n     table_entries = {\\'totalNotFiltered\\': total_count, \\'total\\': filtered_count, \"rows\": users}\\n     js_list = json.dumps(table_entries, cls=db.AlchemyEncoder)\\n@@ -380,7 +380,7 @@ def delete_user():\\n @login_required\\n @admin_required\\n def table_get_locale():\\n-    locale = babel.list_translations() + [LC(\\'en\\')]\\n+    locale = babel.list_translations() + [Locale(\\'en\\')]\\n     ret = list()\\n     current_locale = get_locale()\\n     for loc in locale:\\n@@ -444,7 +444,7 @@ def edit_list_user(param):\\n                 elif param.endswith(\\'role\\'):\\n                     value = int(vals[\\'field_index\\'])\\n                     if user.name == \"Guest\" and value in \\\\\\n-                                 [constants.ROLE_ADMIN, constants.ROLE_PASSWD, constants.ROLE_EDIT_SHELFS]:\\n+                            [constants.ROLE_ADMIN, constants.ROLE_PASSWD, constants.ROLE_EDIT_SHELFS]:\\n                         raise Exception(_(\"Guest can\\'t have this role\"))\\n                     # check for valid value, last on checks for power of 2 value\\n                     if value > 0 and value <= constants.ROLE_VIEWER and (value & value-1 == 0 or value == 1):\\n@@ -524,16 +524,16 @@ def update_table_settings():\\n \\n def check_valid_read_column(column):\\n     if column != \"0\":\\n-        if not calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.id == column) \\\\\\n-              .filter(and_(db.Custom_Columns.datatype == \\'bool\\', db.Custom_Columns.mark_for_delete == 0)).all():\\n+        if not calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.id == column) \\\\\\n+              .filter(and_(db.CustomColumns.datatype == \\'bool\\', db.CustomColumns.mark_for_delete == 0)).all():\\n             return False\\n     return True\\n \\n \\n def check_valid_restricted_column(column):\\n     if column != \"0\":\\n-        if not calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.id == column) \\\\\\n-              .filter(and_(db.Custom_Columns.datatype == \\'text\\', db.Custom_Columns.mark_for_delete == 0)).all():\\n+        if not calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.id == column) \\\\\\n+              .filter(and_(db.CustomColumns.datatype == \\'text\\', db.CustomColumns.mark_for_delete == 0)).all():\\n             return False\\n     return True\\n \\n@@ -1078,12 +1078,12 @@ def _configuration_oauth_helper(to_save):\\n     reboot_required = False\\n     for element in oauthblueprints:\\n         if to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_id\"] != element[\\'oauth_client_id\\'] \\\\\\n-            or to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"] != element[\\'oauth_client_secret\\']:\\n+                or to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"] != element[\\'oauth_client_secret\\']:\\n             reboot_required = True\\n             element[\\'oauth_client_id\\'] = to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_id\"]\\n             element[\\'oauth_client_secret\\'] = to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"]\\n         if to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_id\"] \\\\\\n-            and to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"]:\\n+                and to_save[\"config_\" + str(element[\\'id\\']) + \"_oauth_client_secret\"]:\\n             active_oauths += 1\\n             element[\"active\"] = 1\\n         else:\\n@@ -1136,7 +1136,7 @@ def _configuration_ldap_helper(to_save):\\n     if not config.config_ldap_provider_url \\\\\\n         or not config.config_ldap_port \\\\\\n         or not config.config_ldap_dn \\\\\\n-        or not config.config_ldap_user_object:\\n+            or not config.config_ldap_user_object:\\n         return reboot_required, _configuration_result(_(\\'Please Enter a LDAP Provider, \\'\\n                                                         \\'Port, DN and User Object Identifier\\'))\\n \\n@@ -1211,6 +1211,7 @@ def _db_configuration_update_helper():\\n                                            \\'\\',\\n                                            to_save[\\'config_calibre_dir\\'],\\n                                            flags=re.IGNORECASE)\\n+    db_valid = False\\n     try:\\n         db_change, db_valid = _db_simulate_change()\\n \\n@@ -1229,11 +1230,11 @@ def _db_configuration_update_helper():\\n         return _db_configuration_result(\\'{}\\'.format(ex), gdrive_error)\\n \\n     if db_change or not db_valid or not config.db_configured \\\\\\n-          or config.config_calibre_dir != to_save[\"config_calibre_dir\"]:\\n+            or config.config_calibre_dir != to_save[\"config_calibre_dir\"]:\\n         if not calibre_db.setup_db(to_save[\\'config_calibre_dir\\'], ub.app_DB_path):\\n             return _db_configuration_result(_(\\'DB Location is not Valid, Please Enter Correct Path\\'),\\n                                             gdrive_error)\\n-        config.store_calibre_uuid(calibre_db, db.Library_Id)\\n+        config.store_calibre_uuid(calibre_db, db.LibraryId)\\n         # if db changed -> delete shelfs, delete download books, delete read books, kobo sync...\\n         if db_change:\\n             log.info(\"Calibre Database changed, delete all Calibre-Web info related to old Database\")\\n@@ -1272,7 +1273,7 @@ def _configuration_update_helper():\\n         _config_checkbox_int(to_save, \"config_unicode_filename\")\\n         # Reboot on config_anonbrowse with enabled ldap, as decoraters are changed in this case\\n         reboot_required |= (_config_checkbox_int(to_save, \"config_anonbrowse\")\\n-                             and config.config_login_type == constants.LOGIN_LDAP)\\n+                            and config.config_login_type == constants.LOGIN_LDAP)\\n         _config_checkbox_int(to_save, \"config_public_reg\")\\n         _config_checkbox_int(to_save, \"config_register_email\")\\n         reboot_required |= _config_checkbox_int(to_save, \"config_kobo_sync\")\\n@@ -1560,7 +1561,7 @@ def _handle_edit_user(to_save, content, languages, translations, kobo_support):\\n def new_user():\\n     content = ub.User()\\n     languages = calibre_db.speaking_language()\\n-    translations = [LC(\\'en\\')] + babel.list_translations()\\n+    translations = [Locale(\\'en\\')] + babel.list_translations()\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     if request.method == \"POST\":\\n         to_save = request.form.to_dict()\\n@@ -1647,7 +1648,7 @@ def edit_user(user_id):\\n         flash(_(u\"User not found\"), category=\"error\")\\n         return redirect(url_for(\\'admin.admin\\'))\\n     languages = calibre_db.speaking_language(return_all_languages=True)\\n-    translations = babel.list_translations() + [LC(\\'en\\')]\\n+    translations = babel.list_translations() + [Locale(\\'en\\')]\\n     kobo_support = feature_support[\\'kobo\\'] and config.config_kobo_sync\\n     if request.method == \"POST\":\\n         to_save = request.form.to_dict()', '@@ -17,13 +17,13 @@\\n #  You should have received a copy of the GNU General Public License\\n #  along with this program. If not, see <http://www.gnu.org/licenses/>.\\n \\n-import copy\\n import os\\n import re\\n import ast\\n import json\\n from datetime import datetime\\n from urllib.parse import quote\\n+import unidecode\\n \\n from sqlalchemy import create_engine\\n from sqlalchemy import Table, Column, ForeignKey, CheckConstraint\\n@@ -49,11 +49,6 @@\\n \\n from weakref import WeakSet\\n \\n-try:\\n-    import unidecode\\n-    use_unidecode = True\\n-except ImportError:\\n-    use_unidecode = False\\n \\n log = logger.create()\\n \\n@@ -93,7 +88,7 @@\\n                               )\\n \\n \\n-class Library_Id(Base):\\n+class LibraryId(Base):\\n     __tablename__ = \\'library_id\\'\\n     id = Column(Integer, primary_key=True)\\n     uuid = Column(String, nullable=False)\\n@@ -112,7 +107,7 @@ def __init__(self, val, id_type, book):\\n         self.type = id_type\\n         self.book = book\\n \\n-    def formatType(self):\\n+    def format_type(self):\\n         format_type = self.type.lower()\\n         if format_type == \\'amazon\\':\\n             return u\"Amazon\"\\n@@ -184,8 +179,8 @@ class Comments(Base):\\n     book = Column(Integer, ForeignKey(\\'books.id\\'), nullable=False, unique=True)\\n     text = Column(String(collation=\\'NOCASE\\'), nullable=False)\\n \\n-    def __init__(self, text, book):\\n-        self.text = text\\n+    def __init__(self, comment, book):\\n+        self.text = comment\\n         self.book = book\\n \\n     def get(self):\\n@@ -367,18 +362,17 @@ def __init__(self, title, sort, author_sort, timestamp, pubdate, series_index, l\\n         self.path = path\\n         self.has_cover = (has_cover != None)\\n \\n-\\n     def __repr__(self):\\n         return u\"<Books(\\'{0},{1}{2}{3}{4}{5}{6}{7}{8}\\')>\".format(self.title, self.sort, self.author_sort,\\n                                                                  self.timestamp, self.pubdate, self.series_index,\\n                                                                  self.last_modified, self.path, self.has_cover)\\n \\n     @property\\n     def atom_timestamp(self):\\n-        return (self.timestamp.strftime(\\'%Y-%m-%dT%H:%M:%S+00:00\\') or \\'\\')\\n+        return self.timestamp.strftime(\\'%Y-%m-%dT%H:%M:%S+00:00\\') or \\'\\'\\n \\n \\n-class Custom_Columns(Base):\\n+class CustomColumns(Base):\\n     __tablename__ = \\'custom_columns\\'\\n \\n     id = Column(Integer, primary_key=True)\\n@@ -436,7 +430,7 @@ def default(self, o):\\n         return json.JSONEncoder.default(self, o)\\n \\n \\n-class CalibreDB():\\n+class CalibreDB:\\n     _init = False\\n     engine = None\\n     config = None\\n@@ -450,17 +444,17 @@ def __init__(self, expire_on_commit=True):\\n         \"\"\"\\n         self.session = None\\n         if self._init:\\n-            self.initSession(expire_on_commit)\\n+            self.init_session(expire_on_commit)\\n \\n         self.instances.add(self)\\n \\n-    def initSession(self, expire_on_commit=True):\\n+    def init_session(self, expire_on_commit=True):\\n         self.session = self.session_factory()\\n         self.session.expire_on_commit = expire_on_commit\\n         self.update_title_sort(self.config)\\n \\n     @classmethod\\n-    def setup_db_cc_classes(self, cc):\\n+    def setup_db_cc_classes(cls, cc):\\n         cc_ids = []\\n         books_custom_column_links = {}\\n         for row in cc:\\n@@ -539,16 +533,16 @@ def check_valid_db(cls, config_calibre_dir, app_db_path, config_calibre_uuid):\\n             return False, False\\n         try:\\n             check_engine = create_engine(\\'sqlite://\\',\\n-                          echo=False,\\n-                          isolation_level=\"SERIALIZABLE\",\\n-                          connect_args={\\'check_same_thread\\': False},\\n-                          poolclass=StaticPool)\\n+                                         echo=False,\\n+                                         isolation_level=\"SERIALIZABLE\",\\n+                                         connect_args={\\'check_same_thread\\': False},\\n+                                         poolclass=StaticPool)\\n             with check_engine.begin() as connection:\\n                 connection.execute(text(\"attach database \\'{}\\' as calibre;\".format(dbpath)))\\n                 connection.execute(text(\"attach database \\'{}\\' as app_settings;\".format(app_db_path)))\\n                 local_session = scoped_session(sessionmaker())\\n                 local_session.configure(bind=connection)\\n-                database_uuid = local_session().query(Library_Id).one_or_none()\\n+                database_uuid = local_session().query(LibraryId).one_or_none()\\n                 # local_session.dispose()\\n \\n             check_engine.connect()\\n@@ -603,7 +597,7 @@ def setup_db(cls, config_calibre_dir, app_db_path):\\n                                                           autoflush=True,\\n                                                           bind=cls.engine))\\n         for inst in cls.instances:\\n-            inst.initSession()\\n+            inst.init_session()\\n \\n         cls._init = True\\n         return True\\n@@ -644,12 +638,10 @@ def get_book_format(self, book_id, file_format):\\n     # Language and content filters for displaying in the UI\\n     def common_filters(self, allow_show_archived=False, return_all_languages=False):\\n         if not allow_show_archived:\\n-            archived_books = (\\n-                ub.session.query(ub.ArchivedBook)\\n-                    .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n-                    .filter(ub.ArchivedBook.is_archived == True)\\n-                    .all()\\n-            )\\n+            archived_books = (ub.session.query(ub.ArchivedBook)\\n+                              .filter(ub.ArchivedBook.user_id == int(current_user.id))\\n+                              .filter(ub.ArchivedBook.is_archived == True)\\n+                              .all())\\n             archived_book_ids = [archived_book.book_id for archived_book in archived_books]\\n             archived_filter = Books.id.notin_(archived_book_ids)\\n         else:\\n@@ -668,11 +660,11 @@ def common_filters(self, allow_show_archived=False, return_all_languages=False):\\n                 pos_cc_list = current_user.allowed_column_value.split(\\',\\')\\n                 pos_content_cc_filter = true() if pos_cc_list == [\\'\\'] else \\\\\\n                     getattr(Books, \\'custom_column_\\' + str(self.config.config_restricted_column)). \\\\\\n-                        any(cc_classes[self.config.config_restricted_column].value.in_(pos_cc_list))\\n+                    any(cc_classes[self.config.config_restricted_column].value.in_(pos_cc_list))\\n                 neg_cc_list = current_user.denied_column_value.split(\\',\\')\\n                 neg_content_cc_filter = false() if neg_cc_list == [\\'\\'] else \\\\\\n                     getattr(Books, \\'custom_column_\\' + str(self.config.config_restricted_column)). \\\\\\n-                        any(cc_classes[self.config.config_restricted_column].value.in_(neg_cc_list))\\n+                    any(cc_classes[self.config.config_restricted_column].value.in_(neg_cc_list))\\n             except (KeyError, AttributeError):\\n                 pos_content_cc_filter = false()\\n                 neg_content_cc_filter = true()\\n@@ -728,7 +720,7 @@ def fill_indexpage_with_archived_books(self, page, database, pagesize, db_filter\\n                 query = (self.session.query(database, ub.ReadBook.read_status, ub.ArchivedBook.is_archived)\\n                          .select_from(Books)\\n                          .outerjoin(ub.ReadBook,\\n-                               and_(ub.ReadBook.user_id == int(current_user.id), ub.ReadBook.book_id == Books.id)))\\n+                                    and_(ub.ReadBook.user_id == int(current_user.id), ub.ReadBook.book_id == Books.id)))\\n             else:\\n                 try:\\n                     read_column = cc_classes[config_read_column]\\n@@ -738,7 +730,7 @@ def fill_indexpage_with_archived_books(self, page, database, pagesize, db_filter\\n                 except (KeyError, AttributeError):\\n                     log.error(\"Custom Column No.%d is not existing in calibre database\", read_column)\\n                     # Skip linking read column and return None instead of read status\\n-                    query =self.session.query(database, None, ub.ArchivedBook.is_archived)\\n+                    query = self.session.query(database, None, ub.ArchivedBook.is_archived)\\n             query = query.outerjoin(ub.ArchivedBook, and_(Books.id == ub.ArchivedBook.book_id,\\n                                                           int(current_user.id) == ub.ArchivedBook.user_id))\\n         else:\\n@@ -812,7 +804,6 @@ def order_authors(self, entries, list_return=False, combined=False):\\n                 return authors_ordered\\n         return entries\\n \\n-\\n     def get_typeahead(self, database, query, replace=(\\'\\', \\'\\'), tag_filter=true()):\\n         query = query or \\'\\'\\n         self.session.connection().connection.connection.create_function(\"lower\", 1, lcase)\\n@@ -872,7 +863,7 @@ def search_query(self, term, config_read_column, *join):\\n                 ))\\n \\n     # read search results from calibre-database and return it (function is used for feed and simple search\\n-    def get_search_results(self, term, offset=None, order=None, limit=None, allow_show_archived=False,\\n+    def get_search_results(self, term, offset=None, order=None, limit=None,\\n                            config_read_column=False, *join):\\n         order = order[0] if order else [Books.sort]\\n         pagination = None\\n@@ -915,7 +906,6 @@ def speaking_language(self, languages=None, return_all_languages=False, with_cou\\n                 lang.name = isoLanguages.get_language_name(get_locale(), lang.lang_code)\\n             return sorted(languages, key=lambda x: x.name, reverse=reverse_order)\\n \\n-\\n     def update_title_sort(self, config, conn=None):\\n         # user defined sort function for calibre databases (Series, etc.)\\n         def _title_sort(title):\\n@@ -973,6 +963,6 @@ def lcase(s):\\n     try:\\n         return unidecode.unidecode(s.lower())\\n     except Exception as ex:\\n-        log = logger.create()\\n-        log.error_or_exception(ex)\\n+        _log = logger.create()\\n+        _log.error_or_exception(ex)\\n         return s.lower()', '@@ -31,7 +31,7 @@\\n try:\\n     from lxml.html.clean import clean_html\\n except ImportError:\\n-    pass\\n+    clean_html = None\\n \\n from flask import Blueprint, request, flash, redirect, url_for, abort, Markup, Response\\n from flask_babel import gettext as _\\n@@ -48,7 +48,7 @@\\n from .kobo_sync_status import change_archived_books\\n \\n \\n-editbook = Blueprint(\\'editbook\\', __name__)\\n+EditBook = Blueprint(\\'edit-book\\', __name__)\\n log = logger.create()\\n \\n \\n@@ -61,6 +61,7 @@ def inner(*args, **kwargs):\\n \\n     return inner\\n \\n+\\n def edit_required(f):\\n     @wraps(f)\\n     def inner(*args, **kwargs):\\n@@ -70,6 +71,7 @@ def inner(*args, **kwargs):\\n \\n     return inner\\n \\n+\\n def search_objects_remove(db_book_object, db_type, input_elements):\\n     del_elements = []\\n     for c_elements in db_book_object:\\n@@ -119,6 +121,7 @@ def remove_objects(db_book_object, db_session, del_elements):\\n                 db_session.delete(del_element)\\n     return changed\\n \\n+\\n def add_objects(db_book_object, db_object, db_session, db_type, add_elements):\\n     changed = False\\n     if db_type == \\'languages\\':\\n@@ -128,7 +131,7 @@ def add_objects(db_book_object, db_object, db_session, db_type, add_elements):\\n     else:\\n         db_filter = db_object.name\\n     for add_element in add_elements:\\n-        # check if a element with that name exists\\n+        # check if an element with that name exists\\n         db_element = db_session.query(db_object).filter(db_filter == add_element).first()\\n         # if no element is found add it\\n         if db_type == \\'author\\':\\n@@ -147,7 +150,6 @@ def add_objects(db_book_object, db_object, db_session, db_type, add_elements):\\n             db_book_object.append(new_element)\\n         else:\\n             db_element = create_objects_for_addition(db_element, add_element, db_type)\\n-            changed = True\\n             # add element to book\\n             changed = True\\n             db_book_object.append(db_element)\\n@@ -178,7 +180,7 @@ def create_objects_for_addition(db_element, add_element, db_type):\\n     return db_element\\n \\n \\n-# Modifies different Database objects, first check if elements if elements have to be deleted,\\n+# Modifies different Database objects, first check if elements have to be deleted,\\n # because they are no longer used, than check if elements have to be added to database\\n def modify_database_object(input_elements, db_book_object, db_object, db_session, db_type):\\n     # passing input_elements not as a list may lead to undesired results\\n@@ -207,7 +209,7 @@ def modify_identifiers(input_identifiers, db_identifiers, db_session):\\n     input_dict = dict([(identifier.type.lower(), identifier) for identifier in input_identifiers])\\n     if len(input_identifiers) != len(input_dict):\\n         error = True\\n-    db_dict = dict([(identifier.type.lower(), identifier) for identifier in db_identifiers ])\\n+    db_dict = dict([(identifier.type.lower(), identifier) for identifier in db_identifiers])\\n     # delete db identifiers not present in input or modify them with input val\\n     for identifier_type, identifier in db_dict.items():\\n         if identifier_type not in input_dict.keys():\\n@@ -224,14 +226,15 @@ def modify_identifiers(input_identifiers, db_identifiers, db_session):\\n             changed = True\\n     return changed, error\\n \\n-@editbook.route(\"/ajax/delete/<int:book_id>\", methods=[\"POST\"])\\n+\\n+@EditBook.route(\"/ajax/delete/<int:book_id>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_from_details(book_id):\\n     return Response(delete_book_from_table(book_id, \"\", True), mimetype=\\'application/json\\')\\n \\n \\n-@editbook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"}, methods=[\"POST\"])\\n-@editbook.route(\"/delete/<int:book_id>/<string:book_format>\", methods=[\"POST\"])\\n+@EditBook.route(\"/delete/<int:book_id>\", defaults={\\'book_format\\': \"\"}, methods=[\"POST\"])\\n+@EditBook.route(\"/delete/<int:book_id>/<string:book_format>\", methods=[\"POST\"])\\n @login_required\\n def delete_book_ajax(book_id, book_format):\\n     return delete_book_from_table(book_id, book_format, False)\\n@@ -252,8 +255,8 @@ def delete_whole_book(book_id, book):\\n     modify_database_object([u\\'\\'], book.languages, db.Languages, calibre_db.session, \\'languages\\')\\n     modify_database_object([u\\'\\'], book.publishers, db.Publishers, calibre_db.session, \\'publishers\\')\\n \\n-    cc = calibre_db.session.query(db.Custom_Columns). \\\\\\n-        filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    cc = calibre_db.session.query(db.CustomColumns). \\\\\\n+        filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     for c in cc:\\n         cc_string = \"custom_column_\" + str(c.id)\\n         if not c.is_multiple:\\n@@ -283,18 +286,18 @@ def delete_whole_book(book_id, book):\\n     calibre_db.session.query(db.Books).filter(db.Books.id == book_id).delete()\\n \\n \\n-def render_delete_book_result(book_format, jsonResponse, warning, book_id):\\n+def render_delete_book_result(book_format, json_response, warning, book_id):\\n     if book_format:\\n-        if jsonResponse:\\n-            return json.dumps([warning, {\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n+        if json_response:\\n+            return json.dumps([warning, {\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n                                          \"type\": \"success\",\\n                                          \"format\": book_format,\\n                                          \"message\": _(\\'Book Format Successfully Deleted\\')}])\\n         else:\\n             flash(_(\\'Book Format Successfully Deleted\\'), category=\"success\")\\n-            return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+            return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n     else:\\n-        if jsonResponse:\\n+        if json_response:\\n             return json.dumps([warning, {\"location\": url_for(\\'web.index\\'),\\n                                          \"type\": \"success\",\\n                                          \"format\": book_format,\\n@@ -304,28 +307,28 @@ def render_delete_book_result(book_format, jsonResponse, warning, book_id):\\n             return redirect(url_for(\\'web.index\\'))\\n \\n \\n-def delete_book_from_table(book_id, book_format, jsonResponse):\\n+def delete_book_from_table(book_id, book_format, json_response):\\n     warning = {}\\n     if current_user.role_delete_books():\\n         book = calibre_db.get_book(book_id)\\n         if book:\\n             try:\\n                 result, error = helper.delete_book(book, config.config_calibre_dir, book_format=book_format.upper())\\n                 if not result:\\n-                    if jsonResponse:\\n-                        return json.dumps([{\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n-                                           \"type\": \"danger\",\\n-                                           \"format\": \"\",\\n-                                           \"message\": error}])\\n+                    if json_response:\\n+                        return json.dumps([{\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n+                                            \"type\": \"danger\",\\n+                                            \"format\": \"\",\\n+                                            \"message\": error}])\\n                     else:\\n                         flash(error, category=\"error\")\\n-                        return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+                        return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n                 if error:\\n-                    if jsonResponse:\\n-                        warning = {\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n-                                                \"type\": \"warning\",\\n-                                                \"format\": \"\",\\n-                                                \"message\": error}\\n+                    if json_response:\\n+                        warning = {\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n+                                   \"type\": \"warning\",\\n+                                   \"format\": \"\",\\n+                                   \"message\": error}\\n                     else:\\n                         flash(error, category=\"warning\")\\n                 if not book_format:\\n@@ -339,35 +342,36 @@ def delete_book_from_table(book_id, book_format, jsonResponse):\\n             except Exception as ex:\\n                 log.error_or_exception(ex)\\n                 calibre_db.session.rollback()\\n-                if jsonResponse:\\n-                    return json.dumps([{\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n+                if json_response:\\n+                    return json.dumps([{\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n                                         \"type\": \"danger\",\\n                                         \"format\": \"\",\\n                                         \"message\": ex}])\\n                 else:\\n                     flash(str(ex), category=\"error\")\\n-                    return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+                    return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n         else:\\n             # book not found\\n             log.error(\\'Book with id \"%s\" could not be deleted: not found\\', book_id)\\n-        return render_delete_book_result(book_format, jsonResponse, warning, book_id)\\n+        return render_delete_book_result(book_format, json_response, warning, book_id)\\n     message = _(\"You are missing permissions to delete books\")\\n-    if jsonResponse:\\n-        return json.dumps({\"location\": url_for(\"editbook.edit_book\", book_id=book_id),\\n+    if json_response:\\n+        return json.dumps({\"location\": url_for(\"edit-book.edit_book\", book_id=book_id),\\n                            \"type\": \"danger\",\\n                            \"format\": \"\",\\n                            \"message\": message})\\n     else:\\n         flash(message, category=\"error\")\\n-        return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+        return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n \\n def render_edit_book(book_id):\\n-    cc = calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    cc = calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     book = calibre_db.get_filtered_book(book_id, allow_show_archived=True)\\n     if not book:\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         return redirect(url_for(\"web.index\"))\\n \\n     for lang in book.languages:\\n@@ -380,9 +384,9 @@ def render_edit_book(book_id):\\n         author_names.append(authr.name.replace(\\'|\\', \\',\\'))\\n \\n     # Option for showing convertbook button\\n-    valid_source_formats=list()\\n+    valid_source_formats = list()\\n     allowed_conversion_formats = list()\\n-    kepub_possible=None\\n+    kepub_possible = None\\n     if config.config_converterpath:\\n         for file in book.data:\\n             if file.format.lower() in constants.EXTENSIONS_CONVERT_FROM:\\n@@ -430,6 +434,7 @@ def edit_book_ratings(to_save, book):\\n             changed = True\\n     return changed\\n \\n+\\n def edit_book_tags(tags, book):\\n     input_tags = tags.split(\\',\\')\\n     input_tags = list(map(lambda it: it.strip(), input_tags))\\n@@ -446,48 +451,48 @@ def edit_book_series(series, book):\\n \\n def edit_book_series_index(series_index, book):\\n     # Add default series_index to book\\n-    modif_date = False\\n+    modify_date = False\\n     series_index = series_index or \\'1\\'\\n     if not series_index.replace(\\'.\\', \\'\\', 1).isdigit():\\n         flash(_(\"%(seriesindex)s is not a valid number, skipping\", seriesindex=series_index), category=\"warning\")\\n         return False\\n     if str(book.series_index) != series_index:\\n         book.series_index = series_index\\n-        modif_date = True\\n-    return modif_date\\n+        modify_date = True\\n+    return modify_date\\n \\n \\n # Handle book comments/description\\n def edit_book_comments(comments, book):\\n-    modif_date = False\\n+    modify_date = False\\n     if comments:\\n         comments = clean_html(comments)\\n     if len(book.comments):\\n         if book.comments[0].text != comments:\\n             book.comments[0].text = comments\\n-            modif_date = True\\n+            modify_date = True\\n     else:\\n         if comments:\\n-            book.comments.append(db.Comments(text=comments, book=book.id))\\n-            modif_date = True\\n-    return modif_date\\n+            book.comments.append(db.Comments(comment=comments, book=book.id))\\n+            modify_date = True\\n+    return modify_date\\n \\n \\n-def edit_book_languages(languages, book, upload=False, invalid=None):\\n+def edit_book_languages(languages, book, upload_mode=False, invalid=None):\\n     input_languages = languages.split(\\',\\')\\n     unknown_languages = []\\n-    if not upload:\\n+    if not upload_mode:\\n         input_l = isoLanguages.get_language_codes(get_locale(), input_languages, unknown_languages)\\n     else:\\n         input_l = isoLanguages.get_valid_language_codes(get_locale(), input_languages, unknown_languages)\\n-    for l in unknown_languages:\\n-        log.error(\"\\'%s\\' is not a valid language\", l)\\n+    for lang in unknown_languages:\\n+        log.error(\"\\'%s\\' is not a valid language\", lang)\\n         if isinstance(invalid, list):\\n-            invalid.append(l)\\n+            invalid.append(lang)\\n         else:\\n-            raise ValueError(_(u\"\\'%(langname)s\\' is not a valid language\", langname=l))\\n+            raise ValueError(_(u\"\\'%(langname)s\\' is not a valid language\", langname=lang))\\n     # ToDo: Not working correct\\n-    if upload and len(input_l) == 1:\\n+    if upload_mode and len(input_l) == 1:\\n         # If the language of the file is excluded from the users view, it\\'s not imported, to allow the user to view\\n         # the book it\\'s language is set to the filter language\\n         if input_l[0] != current_user.filter_language() and current_user.filter_language() != \"all\":\\n@@ -571,17 +576,20 @@ def edit_cc_data_string(book, c, to_save, cc_db_value, cc_string):\\n         getattr(book, cc_string).append(new_cc)\\n     return changed, to_save\\n \\n+\\n def edit_single_cc_data(book_id, book, column_id, to_save):\\n-    cc = (calibre_db.session.query(db.Custom_Columns)\\n-          .filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions))\\n-          .filter(db.Custom_Columns.id == column_id)\\n+    cc = (calibre_db.session.query(db.CustomColumns)\\n+          .filter(db.CustomColumns.datatype.notin_(db.cc_exceptions))\\n+          .filter(db.CustomColumns.id == column_id)\\n           .all())\\n     return edit_cc_data(book_id, book, to_save, cc)\\n \\n+\\n def edit_all_cc_data(book_id, book, to_save):\\n-    cc = calibre_db.session.query(db.Custom_Columns).filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).all()\\n+    cc = calibre_db.session.query(db.CustomColumns).filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).all()\\n     return edit_cc_data(book_id, book, to_save, cc)\\n \\n+\\n def edit_cc_data(book_id, book, to_save, cc):\\n     changed = False\\n     for c in cc:\\n@@ -614,10 +622,11 @@ def edit_cc_data(book_id, book, to_save, cc):\\n                                               \\'custom\\')\\n     return changed\\n \\n-def upload_single_file(request, book, book_id):\\n+\\n+def upload_single_file(file_request, book, book_id):\\n     # Check and handle Uploaded file\\n-    if \\'btn-upload-format\\' in request.files:\\n-        requested_file = request.files[\\'btn-upload-format\\']\\n+    if \\'btn-upload-format\\' in file_request.files:\\n+        requested_file = file_request.files[\\'btn-upload-format\\']\\n         # check for empty request\\n         if requested_file.filename != \\'\\':\\n             if not current_user.role_upload():\\n@@ -669,17 +678,17 @@ def upload_single_file(request, book, book_id):\\n \\n             # Queue uploader info\\n             link = \\'<a href=\"{}\">{}</a>\\'.format(url_for(\\'web.show_book\\', book_id=book.id), escape(book.title))\\n-            uploadText=_(u\"File format %(ext)s added to %(book)s\", ext=file_ext.upper(), book=link)\\n-            WorkerThread.add(current_user.name, TaskUpload(uploadText, escape(book.title)))\\n+            upload_text = _(u\"File format %(ext)s added to %(book)s\", ext=file_ext.upper(), book=link)\\n+            WorkerThread.add(current_user.name, TaskUpload(upload_text, escape(book.title)))\\n \\n             return uploader.process(\\n                 saved_filename, *os.path.splitext(requested_file.filename),\\n                 rarExecutable=config.config_rarfile_location)\\n \\n \\n-def upload_cover(request, book):\\n-    if \\'btn-upload-cover\\' in request.files:\\n-        requested_file = request.files[\\'btn-upload-cover\\']\\n+def upload_cover(cover_request, book):\\n+    if \\'btn-upload-cover\\' in cover_request.files:\\n+        requested_file = cover_request.files[\\'btn-upload-cover\\']\\n         # check for empty request\\n         if requested_file.filename != \\'\\':\\n             if not current_user.role_upload():\\n@@ -706,8 +715,8 @@ def handle_title_on_edit(book, book_title):\\n \\n def handle_author_on_edit(book, author_name, update_stored=True):\\n     # handle author(s)\\n-    # renamed = False\\n-    input_authors = author_name.split(\\'&\\')\\n+    input_authors, renamed = prepare_authors(author_name)\\n+    \\'\\'\\'input_authors = author_name.split(\\'&\\')\\n     input_authors = list(map(lambda it: it.strip().replace(\\',\\', \\'|\\'), input_authors))\\n     # Remove duplicates in authors list\\n     input_authors = helper.uniq(input_authors)\\n@@ -725,7 +734,7 @@ def handle_author_on_edit(book, author_name, update_stored=True):\\n             sorted_renamed_author = helper.get_sorted_author(renamed_author.name)\\n             sorted_old_author = helper.get_sorted_author(in_aut)\\n             for one_book in all_books:\\n-                one_book.author_sort = one_book.author_sort.replace(sorted_renamed_author, sorted_old_author)\\n+                one_book.author_sort = one_book.author_sort.replace(sorted_renamed_author, sorted_old_author)\\'\\'\\'\\n \\n     change = modify_database_object(input_authors, book.authors, db.Authors, calibre_db.session, \\'author\\')\\n \\n@@ -746,11 +755,11 @@ def handle_author_on_edit(book, author_name, update_stored=True):\\n     return input_authors, change, renamed\\n \\n \\n-@editbook.route(\"/admin/book/<int:book_id>\", methods=[\\'GET\\', \\'POST\\'])\\n+@EditBook.route(\"/admin/book/<int:book_id>\", methods=[\\'GET\\', \\'POST\\'])\\n @login_required_if_no_ano\\n @edit_required\\n def edit_book(book_id):\\n-    modif_date = False\\n+    modify_date = False\\n \\n     # create the function for sorting...\\n     try:\\n@@ -767,13 +776,14 @@ def edit_book(book_id):\\n \\n     # Book not found\\n     if not book:\\n-        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"), category=\"error\")\\n+        flash(_(u\"Oops! Selected book title is unavailable. File does not exist or is not accessible\"),\\n+              category=\"error\")\\n         return redirect(url_for(\"web.index\"))\\n \\n     meta = upload_single_file(request, book, book_id)\\n     if upload_cover(request, book) is True:\\n         book.has_cover = 1\\n-        modif_date = True\\n+        modify_date = True\\n     try:\\n         to_save = request.form.to_dict()\\n         merge_metadata(to_save, meta)\\n@@ -786,15 +796,15 @@ def edit_book(book_id):\\n         input_authors, authorchange, renamed = handle_author_on_edit(book, to_save[\"author_name\"])\\n         if authorchange or title_change:\\n             edited_books_id = book.id\\n-            modif_date = True\\n+            modify_date = True\\n \\n         if config.config_use_google_drive:\\n             gdriveutils.updateGdriveCalibreFromLocal()\\n \\n-        error = False\\n+        error = \"\"\\n         if edited_books_id:\\n             error = helper.update_dir_structure(edited_books_id, config.config_calibre_dir, input_authors[0],\\n-                                               renamed_author=renamed)\\n+                                                renamed_author=renamed)\\n \\n         if not error:\\n             if \"cover_url\" in to_save:\\n@@ -808,32 +818,32 @@ def edit_book(book_id):\\n                         result, error = helper.save_cover_from_url(to_save[\"cover_url\"], book.path)\\n                         if result is True:\\n                             book.has_cover = 1\\n-                            modif_date = True\\n+                            modify_date = True\\n                         else:\\n                             flash(error, category=\"error\")\\n \\n             # Add default series_index to book\\n-            modif_date |= edit_book_series_index(to_save[\"series_index\"], book)\\n+            modify_date |= edit_book_series_index(to_save[\"series_index\"], book)\\n             # Handle book comments/description\\n-            modif_date |= edit_book_comments(Markup(to_save[\\'description\\']).unescape(), book)\\n+            modify_date |= edit_book_comments(Markup(to_save[\\'description\\']).unescape(), book)\\n             # Handle identifiers\\n             input_identifiers = identifier_list(to_save, book)\\n             modification, warning = modify_identifiers(input_identifiers, book.identifiers, calibre_db.session)\\n             if warning:\\n                 flash(_(\"Identifiers are not Case Sensitive, Overwriting Old Identifier\"), category=\"warning\")\\n-            modif_date |= modification\\n+            modify_date |= modification\\n             # Handle book tags\\n-            modif_date |= edit_book_tags(to_save[\\'tags\\'], book)\\n+            modify_date |= edit_book_tags(to_save[\\'tags\\'], book)\\n             # Handle book series\\n-            modif_date |= edit_book_series(to_save[\"series\"], book)\\n+            modify_date |= edit_book_series(to_save[\"series\"], book)\\n             # handle book publisher\\n-            modif_date |= edit_book_publisher(to_save[\\'publisher\\'], book)\\n+            modify_date |= edit_book_publisher(to_save[\\'publisher\\'], book)\\n             # handle book languages\\n-            modif_date |= edit_book_languages(to_save[\\'languages\\'], book)\\n+            modify_date |= edit_book_languages(to_save[\\'languages\\'], book)\\n             # handle book ratings\\n-            modif_date |= edit_book_ratings(to_save, book)\\n+            modify_date |= edit_book_ratings(to_save, book)\\n             # handle cc data\\n-            modif_date |= edit_all_cc_data(book_id, book, to_save)\\n+            modify_date |= edit_all_cc_data(book_id, book, to_save)\\n \\n             if to_save[\"pubdate\"]:\\n                 try:\\n@@ -843,7 +853,7 @@ def edit_book(book_id):\\n             else:\\n                 book.pubdate = db.Books.DEFAULT_PUBDATE\\n \\n-            if modif_date:\\n+            if modify_date:\\n                 book.last_modified = datetime.utcnow()\\n                 kobo_sync_status.remove_synced_book(edited_books_id, all=True)\\n \\n@@ -905,14 +915,7 @@ def identifier_list(to_save, book):\\n     return result\\n \\n \\n-def prepare_authors_on_upload(title, authr):\\n-    if title != _(u\\'Unknown\\') and authr != _(u\\'Unknown\\'):\\n-        entry = calibre_db.check_exists_book(authr, title)\\n-        if entry:\\n-            log.info(\"Uploaded book probably exists in library\")\\n-            flash(_(u\"Uploaded book probably exists in the library, consider to change before upload new: \")\\n-                  + Markup(render_title_template(\\'book_exists_flash.html\\', entry=entry)), category=\"warning\")\\n-\\n+def prepare_authors(authr):\\n     # handle authors\\n     input_authors = authr.split(\\'&\\')\\n     # handle_authors(input_authors)\\n@@ -935,6 +938,18 @@ def prepare_authors_on_upload(title, authr):\\n             sorted_old_author = helper.get_sorted_author(in_aut)\\n             for one_book in all_books:\\n                 one_book.author_sort = one_book.author_sort.replace(sorted_renamed_author, sorted_old_author)\\n+    return input_authors, renamed\\n+\\n+\\n+def prepare_authors_on_upload(title, authr):\\n+    if title != _(u\\'Unknown\\') and authr != _(u\\'Unknown\\'):\\n+        entry = calibre_db.check_exists_book(authr, title)\\n+        if entry:\\n+            log.info(\"Uploaded book probably exists in library\")\\n+            flash(_(u\"Uploaded book probably exists in the library, consider to change before upload new: \")\\n+                  + Markup(render_title_template(\\'book_exists_flash.html\\', entry=entry)), category=\"warning\")\\n+\\n+    input_authors, renamed = prepare_authors(authr)\\n \\n     sort_authors_list = list()\\n     db_author = None\\n@@ -955,42 +970,42 @@ def prepare_authors_on_upload(title, authr):\\n     return sort_authors, input_authors, db_author, renamed\\n \\n \\n-def create_book_on_upload(modif_date, meta):\\n+def create_book_on_upload(modify_date, meta):\\n     title = meta.title\\n     authr = meta.author\\n     sort_authors, input_authors, db_author, renamed_authors = prepare_authors_on_upload(title, authr)\\n \\n     title_dir = helper.get_valid_filename(title, chars=96)\\n     author_dir = helper.get_valid_filename(db_author.name, chars=96)\\n \\n-    # combine path and normalize path from windows systems\\n+    # combine path and normalize path from Windows systems\\n     path = os.path.join(author_dir, title_dir).replace(\\'\\\\\\\\\\', \\'/\\')\\n \\n     # Calibre adds books with utc as timezone\\n     db_book = db.Books(title, \"\", sort_authors, datetime.utcnow(), datetime(101, 1, 1),\\n                        \\'1\\', datetime.utcnow(), path, meta.cover, db_author, [], \"\")\\n \\n-    modif_date |= modify_database_object(input_authors, db_book.authors, db.Authors, calibre_db.session,\\n-                                         \\'author\\')\\n+    modify_date |= modify_database_object(input_authors, db_book.authors, db.Authors, calibre_db.session,\\n+                                          \\'author\\')\\n \\n     # Add series_index to book\\n-    modif_date |= edit_book_series_index(meta.series_id, db_book)\\n+    modify_date |= edit_book_series_index(meta.series_id, db_book)\\n \\n     # add languages\\n-    invalid=[]\\n-    modif_date |= edit_book_languages(meta.languages, db_book, upload=True, invalid=invalid)\\n+    invalid = []\\n+    modify_date |= edit_book_languages(meta.languages, db_book, upload_mode=True, invalid=invalid)\\n     if invalid:\\n-        for l in invalid:\\n-            flash(_(u\"\\'%(langname)s\\' is not a valid language\", langname=l), category=\"warning\")\\n+        for lang in invalid:\\n+            flash(_(u\"\\'%(langname)s\\' is not a valid language\", langname=lang), category=\"warning\")\\n \\n     # handle tags\\n-    modif_date |= edit_book_tags(meta.tags, db_book)\\n+    modify_date |= edit_book_tags(meta.tags, db_book)\\n \\n     # handle publisher\\n-    modif_date |= edit_book_publisher(meta.publisher, db_book)\\n+    modify_date |= edit_book_publisher(meta.publisher, db_book)\\n \\n     # handle series\\n-    modif_date |= edit_book_series(meta.series, db_book)\\n+    modify_date |= edit_book_series(meta.series, db_book)\\n \\n     # Add file to book\\n     file_size = os.path.getsize(meta.file_path)\\n@@ -1002,6 +1017,7 @@ def create_book_on_upload(modif_date, meta):\\n     calibre_db.session.flush()\\n     return db_book, input_authors, title_dir, renamed_authors\\n \\n+\\n def file_handling_on_upload(requested_file):\\n     # check if file extension is correct\\n     if \\'.\\' in requested_file.filename:\\n@@ -1045,7 +1061,7 @@ def move_coverfile(meta, db_book):\\n               category=\"error\")\\n \\n \\n-@editbook.route(\"/upload\", methods=[\"POST\"])\\n+@EditBook.route(\"/upload\", methods=[\"POST\"])\\n @login_required_if_no_ano\\n @upload_required\\n def upload():\\n@@ -1054,7 +1070,7 @@ def upload():\\n     if request.method == \\'POST\\' and \\'btn-upload\\' in request.files:\\n         for requested_file in request.files.getlist(\"btn-upload\"):\\n             try:\\n-                modif_date = False\\n+                modify_date = False\\n                 # create the function for sorting...\\n                 calibre_db.update_title_sort(config)\\n                 calibre_db.session.connection().connection.connection.create_function(\\'uuid4\\', 0, lambda: str(uuid4()))\\n@@ -1063,10 +1079,10 @@ def upload():\\n                 if error:\\n                     return error\\n \\n-                db_book, input_authors, title_dir, renamed_authors = create_book_on_upload(modif_date, meta)\\n+                db_book, input_authors, title_dir, renamed_authors = create_book_on_upload(modify_date, meta)\\n \\n-                # Comments needs book id therefore only possible after flush\\n-                modif_date |= edit_book_comments(Markup(meta.description).unescape(), db_book)\\n+                # Comments need book id therefore only possible after flush\\n+                modify_date |= edit_book_comments(Markup(meta.description).unescape(), db_book)\\n \\n                 book_id = db_book.id\\n                 title = db_book.title\\n@@ -1096,12 +1112,12 @@ def upload():\\n                 if error:\\n                     flash(error, category=\"error\")\\n                 link = \\'<a href=\"{}\">{}</a>\\'.format(url_for(\\'web.show_book\\', book_id=book_id), escape(title))\\n-                uploadText = _(u\"File %(file)s uploaded\", file=link)\\n-                WorkerThread.add(current_user.name, TaskUpload(uploadText, escape(title)))\\n+                upload_text = _(u\"File %(file)s uploaded\", file=link)\\n+                WorkerThread.add(current_user.name, TaskUpload(upload_text, escape(title)))\\n \\n                 if len(request.files.getlist(\"btn-upload\")) < 2:\\n                     if current_user.role_edit() or current_user.role_admin():\\n-                        resp = {\"location\": url_for(\\'editbook.edit_book\\', book_id=book_id)}\\n+                        resp = {\"location\": url_for(\\'edit-book.edit_book\\', book_id=book_id)}\\n                         return Response(json.dumps(resp), mimetype=\\'application/json\\')\\n                     else:\\n                         resp = {\"location\": url_for(\\'web.show_book\\', book_id=book_id)}\\n@@ -1113,7 +1129,7 @@ def upload():\\n         return Response(json.dumps({\"location\": url_for(\"web.index\")}), mimetype=\\'application/json\\')\\n \\n \\n-@editbook.route(\"/admin/book/convert/<int:book_id>\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/admin/book/convert/<int:book_id>\", methods=[\\'POST\\'])\\n @login_required_if_no_ano\\n @edit_required\\n def convert_bookformat(book_id):\\n@@ -1123,39 +1139,41 @@ def convert_bookformat(book_id):\\n \\n     if (book_format_from is None) or (book_format_to is None):\\n         flash(_(u\"Source or destination format for conversion missing\"), category=\"error\")\\n-        return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+        return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n     log.info(\\'converting: book id: %s from: %s to: %s\\', book_id, book_format_from, book_format_to)\\n     rtn = helper.convert_book_format(book_id, config.config_calibre_dir, book_format_from.upper(),\\n                                      book_format_to.upper(), current_user.name)\\n \\n     if rtn is None:\\n         flash(_(u\"Book successfully queued for converting to %(book_format)s\",\\n-                    book_format=book_format_to),\\n-                    category=\"success\")\\n+                book_format=book_format_to),\\n+              category=\"success\")\\n     else:\\n         flash(_(u\"There was an error converting this book: %(res)s\", res=rtn), category=\"error\")\\n-    return redirect(url_for(\\'editbook.edit_book\\', book_id=book_id))\\n+    return redirect(url_for(\\'edit-book.edit_book\\', book_id=book_id))\\n \\n-@editbook.route(\"/ajax/getcustomenum/<int:c_id>\")\\n+\\n+@EditBook.route(\"/ajax/getcustomenum/<int:c_id>\")\\n @login_required\\n def table_get_custom_enum(c_id):\\n     ret = list()\\n-    cc = (calibre_db.session.query(db.Custom_Columns)\\n-              .filter(db.Custom_Columns.id == c_id)\\n-              .filter(db.Custom_Columns.datatype.notin_(db.cc_exceptions)).one_or_none())\\n+    cc = (calibre_db.session.query(db.CustomColumns)\\n+          .filter(db.CustomColumns.id == c_id)\\n+          .filter(db.CustomColumns.datatype.notin_(db.cc_exceptions)).one_or_none())\\n     ret.append({\\'value\\': \"\", \\'text\\': \"\"})\\n     for idx, en in enumerate(cc.get_display_dict()[\\'enum_values\\']):\\n         ret.append({\\'value\\': en, \\'text\\': en})\\n     return json.dumps(ret)\\n \\n \\n-@editbook.route(\"/ajax/editbooks/<param>\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/editbooks/<param>\", methods=[\\'POST\\'])\\n @login_required_if_no_ano\\n @edit_required\\n def edit_list_book(param):\\n     vals = request.form.to_dict()\\n     book = calibre_db.get_book(vals[\\'pk\\'])\\n+    sort_param = \"\"\\n     # ret = \"\"\\n     try:\\n         if param == \\'series_index\\':\\n@@ -1172,7 +1190,7 @@ def edit_list_book(param):\\n         elif param == \\'publishers\\':\\n             edit_book_publisher(vals[\\'value\\'], book)\\n             ret = Response(json.dumps({\\'success\\': True,\\n-                                        \\'newValue\\': \\', \\'.join([publisher.name for publisher in book.publishers])}),\\n+                                       \\'newValue\\': \\', \\'.join([publisher.name for publisher in book.publishers])}),\\n                            mimetype=\\'application/json\\')\\n         elif param == \\'languages\\':\\n             invalid = list()\\n@@ -1186,13 +1204,13 @@ def edit_list_book(param):\\n                 for lang in book.languages:\\n                     lang_names.append(isoLanguages.get_language_name(get_locale(), lang.lang_code))\\n                 ret = Response(json.dumps({\\'success\\': True, \\'newValue\\':  \\', \\'.join(lang_names)}),\\n-                                mimetype=\\'application/json\\')\\n+                               mimetype=\\'application/json\\')\\n         elif param == \\'author_sort\\':\\n             book.author_sort = vals[\\'value\\']\\n             ret = Response(json.dumps({\\'success\\': True, \\'newValue\\':  book.author_sort}),\\n                            mimetype=\\'application/json\\')\\n         elif param == \\'title\\':\\n-            sort = book.sort\\n+            sort_param = book.sort\\n             handle_title_on_edit(book, vals.get(\\'value\\', \"\"))\\n             helper.update_dir_structure(book.id, config.config_calibre_dir)\\n             ret = Response(json.dumps({\\'success\\': True, \\'newValue\\':  book.title}),\\n@@ -1208,12 +1226,13 @@ def edit_list_book(param):\\n         elif param == \\'authors\\':\\n             input_authors, __, renamed = handle_author_on_edit(book, vals[\\'value\\'], vals.get(\\'checkA\\', None) == \"true\")\\n             helper.update_dir_structure(book.id, config.config_calibre_dir, input_authors[0], renamed_author=renamed)\\n-            ret = Response(json.dumps({\\'success\\': True,\\n-                                       \\'newValue\\':  \\' & \\'.join([author.replace(\\'|\\',\\',\\') for author in input_authors])}),\\n-                           mimetype=\\'application/json\\')\\n+            ret = Response(json.dumps({\\n+                \\'success\\': True,\\n+                \\'newValue\\':  \\' & \\'.join([author.replace(\\'|\\', \\',\\') for author in input_authors])}),\\n+                mimetype=\\'application/json\\')\\n         elif param == \\'is_archived\\':\\n             is_archived = change_archived_books(book.id, vals[\\'value\\'] == \"True\",\\n-                                                message=\"Book {} archivebit set to: {}\".format(book.id, vals[\\'value\\']))\\n+                                                message=\"Book {} archive bit set to: {}\".format(book.id, vals[\\'value\\']))\\n             if is_archived:\\n                 kobo_sync_status.remove_synced_book(book.id)\\n             return \"\"\\n@@ -1238,7 +1257,7 @@ def edit_list_book(param):\\n         calibre_db.session.commit()\\n         # revert change for sort if automatic fields link is deactivated\\n         if param == \\'title\\' and vals.get(\\'checkT\\') == \"false\":\\n-            book.sort = sort\\n+            book.sort = sort_param\\n             calibre_db.session.commit()\\n     except (OperationalError, IntegrityError) as e:\\n         calibre_db.session.rollback()\\n@@ -1249,7 +1268,7 @@ def edit_list_book(param):\\n     return ret\\n \\n \\n-@editbook.route(\"/ajax/sort_value/<field>/<int:bookid>\")\\n+@EditBook.route(\"/ajax/sort_value/<field>/<int:bookid>\")\\n @login_required\\n def get_sorted_entry(field, bookid):\\n     if field in [\\'title\\', \\'authors\\', \\'sort\\', \\'author_sort\\']:\\n@@ -1266,7 +1285,7 @@ def get_sorted_entry(field, bookid):\\n     return \"\"\\n \\n \\n-@editbook.route(\"/ajax/simulatemerge\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/simulatemerge\", methods=[\\'POST\\'])\\n @login_required\\n @edit_required\\n def simulate_merge_list_book():\\n@@ -1282,7 +1301,7 @@ def simulate_merge_list_book():\\n     return \"\"\\n \\n \\n-@editbook.route(\"/ajax/mergebooks\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/mergebooks\", methods=[\\'POST\\'])\\n @login_required\\n @edit_required\\n def merge_list_book():\\n@@ -1295,8 +1314,9 @@ def merge_list_book():\\n         if to_book:\\n             for file in to_book.data:\\n                 to_file.append(file.format)\\n-            to_name = helper.get_valid_filename(to_book.title, chars=96) + \\' - \\' + \\\\\\n-                      helper.get_valid_filename(to_book.authors[0].name, chars=96)\\n+            to_name = helper.get_valid_filename(to_book.title,\\n+                                                chars=96) + \\' - \\' + helper.get_valid_filename(to_book.authors[0].name,\\n+                                                                                              chars=96)\\n             for book_id in vals:\\n                 from_book = calibre_db.get_book(book_id)\\n                 if from_book:\\n@@ -1314,19 +1334,20 @@ def merge_list_book():\\n                                                         element.format,\\n                                                         element.uncompressed_size,\\n                                                         to_name))\\n-                    delete_book_from_table(from_book.id,\"\", True)\\n+                    delete_book_from_table(from_book.id, \"\", True)\\n                     return json.dumps({\\'success\\': True})\\n     return \"\"\\n \\n \\n-@editbook.route(\"/ajax/xchange\", methods=[\\'POST\\'])\\n+@EditBook.route(\"/ajax/xchange\", methods=[\\'POST\\'])\\n @login_required\\n @edit_required\\n def table_xchange_author_title():\\n     vals = request.get_json().get(\\'xchange\\')\\n+    edited_books_id = False\\n     if vals:\\n         for val in vals:\\n-            modif_date = False\\n+            modify_date = False\\n             book = calibre_db.get_book(val)\\n             authors = book.title\\n             book.authors = calibre_db.order_authors([book])\\n@@ -1338,15 +1359,15 @@ def table_xchange_author_title():\\n             input_authors, authorchange, renamed = handle_author_on_edit(book, authors)\\n             if authorchange or title_change:\\n                 edited_books_id = book.id\\n-                modif_date = True\\n+                modify_date = True\\n \\n             if config.config_use_google_drive:\\n                 gdriveutils.updateGdriveCalibreFromLocal()\\n \\n             if edited_books_id:\\n                 helper.update_dir_structure(edited_books_id, config.config_calibre_dir, input_authors[0],\\n-                                           renamed_author=renamed)\\n-            if modif_date:\\n+                                            renamed_author=renamed)\\n+            if modify_date:\\n                 book.last_modified = datetime.utcnow()\\n             try:\\n                 calibre_db.session.commit()', '@@ -60,7 +60,7 @@\\n             {% if g.user.is_authenticated or g.allow_anonymous %}\\n               {% if g.user.role_upload() and g.allow_upload %}\\n                   <li>\\n-                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'editbook.upload\\') }}\" data-title=\"{{_(\\'Uploading...\\')}}\" data-footer=\"{{_(\\'Close\\')}}\" data-failed=\"{{_(\\'Error\\')}}\" data-message=\"{{_(\\'Upload done, processing, please wait...\\')}}\" method=\"post\" enctype=\"multipart/form-data\">\\n+                    <form id=\"form-upload\" class=\"navbar-form\" action=\"{{ url_for(\\'edit-book.upload\\') }}\" data-title=\"{{_(\\'Uploading...\\')}}\" data-footer=\"{{_(\\'Close\\')}}\" data-failed=\"{{_(\\'Error\\')}}\" data-message=\"{{_(\\'Upload done, processing, please wait...\\')}}\" method=\"post\" enctype=\"multipart/form-data\">\\n                       <input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\">\\n                       <div class=\"form-group\">\\n                         <span class=\"btn btn-default btn-file\">{{_(\\'Upload\\')}}<input id=\"btn-upload\" name=\"btn-upload\"', \"@@ -22,6 +22,7 @@\\n \\n import json\\n from datetime import datetime\\n+from functools import wraps\\n \\n from flask import Blueprint, request, make_response, abort, url_for, flash, redirect\\n from flask_login import login_required, current_user, login_user\\n@@ -31,10 +32,6 @@\\n from . import config, logger, ub\\n from .render_template import render_title_template\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We're not using Python 3\\n \\n remotelogin = Blueprint('remotelogin', __name__)\\n log = logger.create()\", \"@@ -18,17 +18,14 @@\\n \\n import base64\\n import binascii\\n+from functools import wraps\\n \\n from sqlalchemy.sql.expression import func\\n from werkzeug.security import check_password_hash\\n from flask_login import login_required, login_user\\n \\n from . import lm, ub, config, constants, services\\n \\n-try:\\n-    from functools import wraps\\n-except ImportError:\\n-    pass  # We're not using Python 3\\n \\n def login_required_if_no_ano(func):\\n     @wraps(func)\", '@@ -57,10 +57,10 @@ def has_prev(self):\\n     def has_next(self):\\n         return self.page < self.pages\\n \\n-    # right_edge: last right_edges count of all pages are shown as number, means, if 10 pages are paginated -> 9,10 shwn\\n-    # left_edge: first left_edges count of all pages are shown as number                                    -> 1,2 shwn\\n-    # left_current: left_current count below current page are shown as number, means if current page 5      -> 3,4 shwn\\n-    # left_current: right_current count above current page are shown as number, means if current page 5     -> 6,7 shwn\\n+    # right_edge: last right_edges count of all pages are shown as number, means, if 10 pages are paginated -> 9,10 shown\\n+    # left_edge: first left_edges count of all pages are shown as number                                    -> 1,2 shown\\n+    # left_current: left_current count below current page are shown as number, means if current page 5      -> 3,4 shown\\n+    # left_current: right_current count above current page are shown as number, means if current page 5     -> 6,7 shown\\n     def iter_pages(self, left_edge=2, left_current=2,\\n                    right_current=4, right_edge=2):\\n         last = 0'], 'file': ['cps/epub.py', 'cps/web.py', 'cps/templates/book_edit.html', 'cps/templates/detail.html', 'cps/helper.py', 'cps/opds.py', 'cps/templates/book_table.html', 'cps.py', 'cps/admin.py', 'cps/db.py', 'cps/editbooks.py', 'cps/templates/layout.html', 'cps/remotelogin.py', 'cps/usermanagement.py', 'cps/pagination.py'], 'language': ['Python', 'Python', 'HTML', 'HTML', 'Python', 'Python', 'HTML', 'Python', 'Python', 'Python', 'Python', 'HTML', 'Python', 'Python', 'Python'], 'temp_id': [UUID('c334ba32-cbb1-4f4f-a929-82adc0b333d6'), UUID('7f20d3dd-7942-4cb6-8be2-f4a6eae2aed7'), UUID('688e96ab-c40f-4435-8715-31e5490399ad'), UUID('c46e3b59-d59e-4e18-b863-4cf6c3d2e21a'), UUID('8cf529dc-2dc1-45fd-8d98-fc8804002ee3'), UUID('b033871b-e4cb-4df3-bbaf-b2f22d9fa6ff'), UUID('0e21d787-fca7-4d0a-ba63-5a2f6e3b208c'), UUID('9932b474-3c9a-456a-bd2e-436ec05bd410'), UUID('21d1e6cb-288e-418b-b895-f1e4fe9945cd'), UUID('d9f030d9-6905-43db-9d6e-2edd5ecb7cd0'), UUID('2e810e0c-1bc5-46f0-9649-dcec73395617'), UUID('58f95a16-f16b-46c2-8584-11ae7a857a33'), UUID('aead2e6b-219e-45a1-98d2-5fe5c47a24ab'), UUID('9265fc9d-a468-4ad8-86c1-df3b317efd40'), UUID('0a4c4b29-9900-4d43-acff-588e25e68f1b')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     register_user_with_oauth = logout_oauth_user = get_oauth_status = None\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     register_user_with_oauth = logout_oauth_user = get_oauth_status = None\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1225/1800 [10:36<05:37,  1.71it/s]ERROR:src.process_code_changes:Error processing commit b542d7465f7e6e02e1ea1aec059ac607a65cefe7\n",
      "ERROR:src.process_code_changes:{'repo': 'pulp/pulp', 'vulnerability_id': '2015-5263', 'commit': 'b542d7465f7e6e02e1ea1aec059ac607a65cefe7', 'commit_source': 'github', 'cwe_id': ['CWE-295'], 'patch': ['@@ -0,0 +1,16 @@\\n+from pulp.bindings.base import PulpAPI\\n+\\n+\\n+class StaticRequest(PulpAPI):\\n+    \"\"\"\\n+    Connection class to access static calls\\n+    \"\"\"\\n+\\n+    def get_server_key(self):\\n+        \"\"\"\\n+        Retrieve the server\\'s public key.\\n+\\n+        :return: rsa public key\\n+        :rtype:  str\\n+        \"\"\"\\n+        return self.server.GET(\\'/pulp/static/rsa_pub.key\\', ignore_prefix=True)', '@@ -91,43 +91,42 @@ def initialize(context):\\n     context.cli.add_command(StatusCommand(context, \\'status\\', _(d)))\\n \\n \\n-def download(url, location):\\n+def write_to_location(location, content):\\n     \"\"\"\\n-    Download files to the specified location.\\n-    :param url: The file URL.\\n-    :type url: str\\n-    :param location: The absolute path to where the downloaded\\n-        file is to be stored.\\n-    :type location: str\\n+    Write content to a path. Ensures that the entire path exists, creating directories if necessary.\\n+\\n+    :param location: path that should exist\\n+    :type  location: str\\n+    :param content: bits to be written to file\\n+    :type  content: str\\n     \"\"\"\\n-    request = urllib2.urlopen(url)\\n     try:\\n-        content = request.read()\\n+        os.makedirs(os.path.dirname(location))\\n+    except OSError, e:\\n+        if e.errno != errno.EEXIST:\\n+            raise\\n+    try:\\n         fp = open(location, \\'w+\\')\\n-        try:\\n-            fp.write(content)\\n-        finally:\\n-            fp.close()\\n+        fp.write(content)\\n     finally:\\n-        request.close()\\n+        fp.close()\\n \\n \\n-def update_server_key(conf):\\n+def update_server_key(command_inst):\\n     \"\"\"\\n-    Download the server\\'s RSA key and store in the location\\n-    specified in the configuration.\\n-    :param conf: The consumer configuration object.\\n-    :type conf: dict\\n+    Ensure that the server\\'s public key stored on the consumer is up to date.\\n+\\n+    :param command_inst: instance of a CLI command\\n+    :type  command_inst: pulp.client.extensions.extensions.PulpCliCommand\\n     \"\"\"\\n-    host = conf[\\'server\\'][\\'host\\']\\n-    location = conf[\\'server\\'][\\'rsa_pub\\']\\n-    url = \\'https://%s/pulp/static/rsa_pub.key\\' % host\\n     try:\\n-        os.makedirs(os.path.dirname(location))\\n-    except OSError, e:\\n-        if e.errno != errno.EEXIST:\\n-            raise\\n-    download(url, location)\\n+        key_reply = command_inst.context.server.static.get_server_key()\\n+    except Exception, e:\\n+        msg = _(\\'Download server RSA key failed [%(e)s]\\' % {\\'e\\': e})\\n+        command_inst.prompt.render_failure_message(msg)\\n+    else:\\n+        key_location = command_inst.context.config[\\'server\\'][\\'rsa_pub\\']\\n+        write_to_location(key_location, key_reply.response_body)\\n \\n \\n # -- common exceptions --------------------------------------------------------\\n@@ -201,14 +200,7 @@ def register(self, **kwargs):\\n         finally:\\n             fp.close()\\n \\n-        # download server public key\\n-\\n-        try:\\n-            update_server_key(self.context.config)\\n-        except Exception, e:\\n-            msg = _(\\'Download server RSA key failed [%(e)s]\\' % {\\'e\\': e})\\n-            self.prompt.render_failure_message(msg)\\n-\\n+        update_server_key(self)\\n         self.prompt.render_success_message(\\'Consumer [%s] successfully registered\\' % consumer_id)\\n \\n \\n@@ -250,11 +242,7 @@ def update(self, **kwargs):\\n             self.prompt.render_success_message(\\'Consumer [%s] successfully updated\\' % consumer_id)\\n             if not kwargs.get(OPTION_EXCHANGE_KEYS.keyword):\\n                 return\\n-            try:\\n-                update_server_key(self.context.config)\\n-            except Exception, e:\\n-                msg = _(\\'Download server RSA key failed [%(e)s]\\' % {\\'e\\': e})\\n-                self.prompt.render_failure_message(msg)\\n+            update_server_key(self)\\n         except NotFoundException:\\n             self.prompt.write(\\'Consumer [%s] does not exist on the server\\' % consumer_id, tag=\\'not-found\\')\\n ', '@@ -111,7 +111,7 @@ def list_schedules(self, action, consumer_id):\\n     def get_schedule(self, action, consumer_id, schedule_id):\\n         url = self.base_path % consumer_id + action + \\'/%s/\\' % schedule_id\\n         return self.server.GET(url)\\n-    \\n+\\n     def add_schedule(self, action, consumer_id, schedule, units, failure_threshold=UNSPECIFIED,\\n                      enabled=UNSPECIFIED, options=UNSPECIFIED):\\n         url = self.base_path % consumer_id + action + \\'/\\'\\n@@ -125,7 +125,7 @@ def add_schedule(self, action, consumer_id, schedule, units, failure_threshold=U\\n         # Strip out anything that wasn\\'t specified by the caller\\n         body = dict([(k, v) for k, v in body.items() if v is not UNSPECIFIED])\\n         return self.server.POST(url, body)\\n- \\n+\\n     def delete_schedule(self, action, consumer_id, schedule_id):\\n         url = self.base_path % consumer_id + action + \\'/%s/\\' % schedule_id\\n         return self.server.DELETE(url)\\n@@ -156,7 +156,7 @@ def find_by_id(self, consumer_id, repo_id=None):\\n         if repo_id:\\n             path += \\'%s/\\' % repo_id\\n         return self.server.GET(path)\\n-    \\n+\\n     def bind(self, consumer_id, repo_id, distributor_id, notify_agent=True, binding_config=None):\\n         path = self.BASE_PATH % consumer_id\\n         data = {\\n@@ -166,7 +166,7 @@ def bind(self, consumer_id, repo_id, distributor_id, notify_agent=True, binding_\\n             \\'binding_config\\': binding_config or {}\\n         }\\n         return self.server.POST(path, data)\\n-    \\n+\\n     def unbind(self, consumer_id, repo_id, distributor_id, force=False):\\n         path = self.BASE_PATH % consumer_id + \"%s/\" % repo_id + \"%s/\" % distributor_id\\n         body = dict(force=force)', '@@ -84,27 +84,29 @@ def __init__(self,\\n         self.verify_ssl = verify_ssl\\n         self.ca_path = ca_path\\n \\n-    def DELETE(self, path, body=None, log_request_body=True):\\n-        return self._request(\\'DELETE\\', path, body=body, log_request_body=log_request_body)\\n+    def DELETE(self, path, body=None, log_request_body=True, ignore_prefix=False):\\n+        return self._request(\\'DELETE\\', path, body=body, log_request_body=log_request_body,\\n+                             ignore_prefix=ignore_prefix)\\n \\n-    def GET(self, path, queries=()):\\n-        return self._request(\\'GET\\', path, queries)\\n+    def GET(self, path, queries=(), ignore_prefix=False):\\n+        return self._request(\\'GET\\', path, queries, ignore_prefix=ignore_prefix)\\n \\n-    def HEAD(self, path):\\n-        return self._request(\\'HEAD\\', path)\\n+    def HEAD(self, path, ignore_prefix=False):\\n+        return self._request(\\'HEAD\\', path, ignore_prefix=ignore_prefix)\\n \\n-    def POST(self, path, body=None, ensure_encoding=True, log_request_body=True):\\n+    def POST(self, path, body=None, ensure_encoding=True, log_request_body=True,\\n+             ignore_prefix=False):\\n         return self._request(\\'POST\\', path, body=body, ensure_encoding=ensure_encoding,\\n-                             log_request_body=log_request_body)\\n+                             log_request_body=log_request_body, ignore_prefix=ignore_prefix)\\n \\n-    def PUT(self, path, body, ensure_encoding=True, log_request_body=True):\\n+    def PUT(self, path, body, ensure_encoding=True, log_request_body=True, ignore_prefix=False):\\n         return self._request(\\'PUT\\', path, body=body, ensure_encoding=ensure_encoding,\\n-                             log_request_body=log_request_body)\\n+                             log_request_body=log_request_body, ignore_prefix=ignore_prefix)\\n \\n     # protected request utilities ---------------------------------------------\\n \\n     def _request(self, method, path, queries=(), body=None, ensure_encoding=True,\\n-                 log_request_body=True):\\n+                 log_request_body=True, ignore_prefix=False):\\n         \"\"\"\\n         make a HTTP request to the pulp server and return the response\\n \\n@@ -130,14 +132,17 @@ def _request(self, method, path, queries=(), body=None, ensure_encoding=True,\\n         :param log_request_body: Toggle logging of the request body, defaults to true\\n         :type log_request_body: bool\\n \\n+        :param ignore_prefix: when building the url, disregard the self.path_prefix\\n+        :type  ignore_prefix: bool\\n+\\n         :return:    Response object\\n         :rtype:     pulp.bindings.responses.Response\\n \\n         :raises:    ConnectionException or one of the RequestExceptions\\n                     (depending on response codes) in case of unsuccessful\\n                     request\\n         \"\"\"\\n-        url = self._build_url(path, queries)\\n+        url = self._build_url(path, queries, ignore_prefix)\\n         if ensure_encoding:\\n             body = self._process_body(body)\\n         if not isinstance(body, (NoneType, basestring)):\\n@@ -201,7 +206,7 @@ def _handle_exceptions(self, response_code, response_body):\\n         else:\\n             raise code_class_mappings[response_code](response_body)\\n \\n-    def _build_url(self, path, queries=()):\\n+    def _build_url(self, path, queries, ignore_prefix):\\n         \"\"\"\\n         Takes a relative path and query parameters, combines them with the\\n         base path, and returns the result. Handles utf-8 encoding as necessary.\\n@@ -217,13 +222,15 @@ def _build_url(self, path, queries=()):\\n                         in either case representing key-value pairs to be used\\n                         as query parameters on the URL.\\n         :type  queries: mapping object or sequence of 2-element tuples\\n+        :param ignore_prefix: when building the url, disregard the self.path_prefix\\n+        :type  ignore_prefix: bool\\n \\n         :return:    path that is a composite of self.path_prefix, path, and\\n                     queries. May be relative or absolute depending on the nature\\n                     of self.path_prefix\\n         \"\"\"\\n         # build the request url from the path and queries dict or tuple\\n-        if not path.startswith(self.path_prefix):\\n+        if not path.startswith(self.path_prefix) and not ignore_prefix:\\n             if path.startswith(\\'/\\'):\\n                 path = path[1:]\\n             path = \\'/\\'.join((self.path_prefix, path))', '@@ -19,6 +19,7 @@\\n from pulp.bindings.consumer_groups import *\\n from pulp.bindings.consumer import *\\n from pulp.bindings.server_info import ServerInfoAPI\\n+from pulp.bindings.static import StaticRequest\\n from pulp.bindings.tasks import TasksAPI, TaskSearchAPI\\n from pulp.bindings.upload import UploadAPI\\n from pulp.bindings.auth import *\\n@@ -67,6 +68,7 @@ def __init__(self, pulp_connection):\\n         self.repo_unit = RepositoryUnitAPI(pulp_connection)\\n         self.role = RoleAPI(pulp_connection)\\n         self.server_info = ServerInfoAPI(pulp_connection)\\n+        self.static = StaticRequest(pulp_connection)\\n         self.tasks = TasksAPI(pulp_connection)\\n         self.tasks_search = TaskSearchAPI(pulp_connection)\\n         self.uploads = UploadAPI(pulp_connection)'], 'file': ['bindings/pulp/bindings/static.py', 'client_consumer/pulp/client/consumer/cli.py', 'bindings/pulp/bindings/consumer.py', 'bindings/pulp/bindings/server.py', 'bindings/pulp/bindings/bindings.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('61e45e19-e242-4bc9-94e0-e465143e6196'), UUID('0945225c-ebe7-497f-8731-5bb26f782e35'), UUID('fa92ceb3-7a91-4cb8-a42f-3cc1233bfebe'), UUID('82429ffd-9129-4fb6-ae2e-6fbb47590a1e'), UUID('2a735f07-6938-434c-9d28-9c3577432d78')]}\n",
      "ERROR:root:Error in {'repo': 'pulp/pulp', 'vulnerability_id': '2015-5263', 'commit': 'b542d7465f7e6e02e1ea1aec059ac607a65cefe7', 'commit_source': 'github', 'cwe_id': ['CWE-295'], 'patch': ['@@ -0,0 +1,16 @@\\n+from pulp.bindings.base import PulpAPI\\n+\\n+\\n+class StaticRequest(PulpAPI):\\n+    \"\"\"\\n+    Connection class to access static calls\\n+    \"\"\"\\n+\\n+    def get_server_key(self):\\n+        \"\"\"\\n+        Retrieve the server\\'s public key.\\n+\\n+        :return: rsa public key\\n+        :rtype:  str\\n+        \"\"\"\\n+        return self.server.GET(\\'/pulp/static/rsa_pub.key\\', ignore_prefix=True)', '@@ -91,43 +91,42 @@ def initialize(context):\\n     context.cli.add_command(StatusCommand(context, \\'status\\', _(d)))\\n \\n \\n-def download(url, location):\\n+def write_to_location(location, content):\\n     \"\"\"\\n-    Download files to the specified location.\\n-    :param url: The file URL.\\n-    :type url: str\\n-    :param location: The absolute path to where the downloaded\\n-        file is to be stored.\\n-    :type location: str\\n+    Write content to a path. Ensures that the entire path exists, creating directories if necessary.\\n+\\n+    :param location: path that should exist\\n+    :type  location: str\\n+    :param content: bits to be written to file\\n+    :type  content: str\\n     \"\"\"\\n-    request = urllib2.urlopen(url)\\n     try:\\n-        content = request.read()\\n+        os.makedirs(os.path.dirname(location))\\n+    except OSError, e:\\n+        if e.errno != errno.EEXIST:\\n+            raise\\n+    try:\\n         fp = open(location, \\'w+\\')\\n-        try:\\n-            fp.write(content)\\n-        finally:\\n-            fp.close()\\n+        fp.write(content)\\n     finally:\\n-        request.close()\\n+        fp.close()\\n \\n \\n-def update_server_key(conf):\\n+def update_server_key(command_inst):\\n     \"\"\"\\n-    Download the server\\'s RSA key and store in the location\\n-    specified in the configuration.\\n-    :param conf: The consumer configuration object.\\n-    :type conf: dict\\n+    Ensure that the server\\'s public key stored on the consumer is up to date.\\n+\\n+    :param command_inst: instance of a CLI command\\n+    :type  command_inst: pulp.client.extensions.extensions.PulpCliCommand\\n     \"\"\"\\n-    host = conf[\\'server\\'][\\'host\\']\\n-    location = conf[\\'server\\'][\\'rsa_pub\\']\\n-    url = \\'https://%s/pulp/static/rsa_pub.key\\' % host\\n     try:\\n-        os.makedirs(os.path.dirname(location))\\n-    except OSError, e:\\n-        if e.errno != errno.EEXIST:\\n-            raise\\n-    download(url, location)\\n+        key_reply = command_inst.context.server.static.get_server_key()\\n+    except Exception, e:\\n+        msg = _(\\'Download server RSA key failed [%(e)s]\\' % {\\'e\\': e})\\n+        command_inst.prompt.render_failure_message(msg)\\n+    else:\\n+        key_location = command_inst.context.config[\\'server\\'][\\'rsa_pub\\']\\n+        write_to_location(key_location, key_reply.response_body)\\n \\n \\n # -- common exceptions --------------------------------------------------------\\n@@ -201,14 +200,7 @@ def register(self, **kwargs):\\n         finally:\\n             fp.close()\\n \\n-        # download server public key\\n-\\n-        try:\\n-            update_server_key(self.context.config)\\n-        except Exception, e:\\n-            msg = _(\\'Download server RSA key failed [%(e)s]\\' % {\\'e\\': e})\\n-            self.prompt.render_failure_message(msg)\\n-\\n+        update_server_key(self)\\n         self.prompt.render_success_message(\\'Consumer [%s] successfully registered\\' % consumer_id)\\n \\n \\n@@ -250,11 +242,7 @@ def update(self, **kwargs):\\n             self.prompt.render_success_message(\\'Consumer [%s] successfully updated\\' % consumer_id)\\n             if not kwargs.get(OPTION_EXCHANGE_KEYS.keyword):\\n                 return\\n-            try:\\n-                update_server_key(self.context.config)\\n-            except Exception, e:\\n-                msg = _(\\'Download server RSA key failed [%(e)s]\\' % {\\'e\\': e})\\n-                self.prompt.render_failure_message(msg)\\n+            update_server_key(self)\\n         except NotFoundException:\\n             self.prompt.write(\\'Consumer [%s] does not exist on the server\\' % consumer_id, tag=\\'not-found\\')\\n ', '@@ -111,7 +111,7 @@ def list_schedules(self, action, consumer_id):\\n     def get_schedule(self, action, consumer_id, schedule_id):\\n         url = self.base_path % consumer_id + action + \\'/%s/\\' % schedule_id\\n         return self.server.GET(url)\\n-    \\n+\\n     def add_schedule(self, action, consumer_id, schedule, units, failure_threshold=UNSPECIFIED,\\n                      enabled=UNSPECIFIED, options=UNSPECIFIED):\\n         url = self.base_path % consumer_id + action + \\'/\\'\\n@@ -125,7 +125,7 @@ def add_schedule(self, action, consumer_id, schedule, units, failure_threshold=U\\n         # Strip out anything that wasn\\'t specified by the caller\\n         body = dict([(k, v) for k, v in body.items() if v is not UNSPECIFIED])\\n         return self.server.POST(url, body)\\n- \\n+\\n     def delete_schedule(self, action, consumer_id, schedule_id):\\n         url = self.base_path % consumer_id + action + \\'/%s/\\' % schedule_id\\n         return self.server.DELETE(url)\\n@@ -156,7 +156,7 @@ def find_by_id(self, consumer_id, repo_id=None):\\n         if repo_id:\\n             path += \\'%s/\\' % repo_id\\n         return self.server.GET(path)\\n-    \\n+\\n     def bind(self, consumer_id, repo_id, distributor_id, notify_agent=True, binding_config=None):\\n         path = self.BASE_PATH % consumer_id\\n         data = {\\n@@ -166,7 +166,7 @@ def bind(self, consumer_id, repo_id, distributor_id, notify_agent=True, binding_\\n             \\'binding_config\\': binding_config or {}\\n         }\\n         return self.server.POST(path, data)\\n-    \\n+\\n     def unbind(self, consumer_id, repo_id, distributor_id, force=False):\\n         path = self.BASE_PATH % consumer_id + \"%s/\" % repo_id + \"%s/\" % distributor_id\\n         body = dict(force=force)', '@@ -84,27 +84,29 @@ def __init__(self,\\n         self.verify_ssl = verify_ssl\\n         self.ca_path = ca_path\\n \\n-    def DELETE(self, path, body=None, log_request_body=True):\\n-        return self._request(\\'DELETE\\', path, body=body, log_request_body=log_request_body)\\n+    def DELETE(self, path, body=None, log_request_body=True, ignore_prefix=False):\\n+        return self._request(\\'DELETE\\', path, body=body, log_request_body=log_request_body,\\n+                             ignore_prefix=ignore_prefix)\\n \\n-    def GET(self, path, queries=()):\\n-        return self._request(\\'GET\\', path, queries)\\n+    def GET(self, path, queries=(), ignore_prefix=False):\\n+        return self._request(\\'GET\\', path, queries, ignore_prefix=ignore_prefix)\\n \\n-    def HEAD(self, path):\\n-        return self._request(\\'HEAD\\', path)\\n+    def HEAD(self, path, ignore_prefix=False):\\n+        return self._request(\\'HEAD\\', path, ignore_prefix=ignore_prefix)\\n \\n-    def POST(self, path, body=None, ensure_encoding=True, log_request_body=True):\\n+    def POST(self, path, body=None, ensure_encoding=True, log_request_body=True,\\n+             ignore_prefix=False):\\n         return self._request(\\'POST\\', path, body=body, ensure_encoding=ensure_encoding,\\n-                             log_request_body=log_request_body)\\n+                             log_request_body=log_request_body, ignore_prefix=ignore_prefix)\\n \\n-    def PUT(self, path, body, ensure_encoding=True, log_request_body=True):\\n+    def PUT(self, path, body, ensure_encoding=True, log_request_body=True, ignore_prefix=False):\\n         return self._request(\\'PUT\\', path, body=body, ensure_encoding=ensure_encoding,\\n-                             log_request_body=log_request_body)\\n+                             log_request_body=log_request_body, ignore_prefix=ignore_prefix)\\n \\n     # protected request utilities ---------------------------------------------\\n \\n     def _request(self, method, path, queries=(), body=None, ensure_encoding=True,\\n-                 log_request_body=True):\\n+                 log_request_body=True, ignore_prefix=False):\\n         \"\"\"\\n         make a HTTP request to the pulp server and return the response\\n \\n@@ -130,14 +132,17 @@ def _request(self, method, path, queries=(), body=None, ensure_encoding=True,\\n         :param log_request_body: Toggle logging of the request body, defaults to true\\n         :type log_request_body: bool\\n \\n+        :param ignore_prefix: when building the url, disregard the self.path_prefix\\n+        :type  ignore_prefix: bool\\n+\\n         :return:    Response object\\n         :rtype:     pulp.bindings.responses.Response\\n \\n         :raises:    ConnectionException or one of the RequestExceptions\\n                     (depending on response codes) in case of unsuccessful\\n                     request\\n         \"\"\"\\n-        url = self._build_url(path, queries)\\n+        url = self._build_url(path, queries, ignore_prefix)\\n         if ensure_encoding:\\n             body = self._process_body(body)\\n         if not isinstance(body, (NoneType, basestring)):\\n@@ -201,7 +206,7 @@ def _handle_exceptions(self, response_code, response_body):\\n         else:\\n             raise code_class_mappings[response_code](response_body)\\n \\n-    def _build_url(self, path, queries=()):\\n+    def _build_url(self, path, queries, ignore_prefix):\\n         \"\"\"\\n         Takes a relative path and query parameters, combines them with the\\n         base path, and returns the result. Handles utf-8 encoding as necessary.\\n@@ -217,13 +222,15 @@ def _build_url(self, path, queries=()):\\n                         in either case representing key-value pairs to be used\\n                         as query parameters on the URL.\\n         :type  queries: mapping object or sequence of 2-element tuples\\n+        :param ignore_prefix: when building the url, disregard the self.path_prefix\\n+        :type  ignore_prefix: bool\\n \\n         :return:    path that is a composite of self.path_prefix, path, and\\n                     queries. May be relative or absolute depending on the nature\\n                     of self.path_prefix\\n         \"\"\"\\n         # build the request url from the path and queries dict or tuple\\n-        if not path.startswith(self.path_prefix):\\n+        if not path.startswith(self.path_prefix) and not ignore_prefix:\\n             if path.startswith(\\'/\\'):\\n                 path = path[1:]\\n             path = \\'/\\'.join((self.path_prefix, path))', '@@ -19,6 +19,7 @@\\n from pulp.bindings.consumer_groups import *\\n from pulp.bindings.consumer import *\\n from pulp.bindings.server_info import ServerInfoAPI\\n+from pulp.bindings.static import StaticRequest\\n from pulp.bindings.tasks import TasksAPI, TaskSearchAPI\\n from pulp.bindings.upload import UploadAPI\\n from pulp.bindings.auth import *\\n@@ -67,6 +68,7 @@ def __init__(self, pulp_connection):\\n         self.repo_unit = RepositoryUnitAPI(pulp_connection)\\n         self.role = RoleAPI(pulp_connection)\\n         self.server_info = ServerInfoAPI(pulp_connection)\\n+        self.static = StaticRequest(pulp_connection)\\n         self.tasks = TasksAPI(pulp_connection)\\n         self.tasks_search = TaskSearchAPI(pulp_connection)\\n         self.uploads = UploadAPI(pulp_connection)'], 'file': ['bindings/pulp/bindings/static.py', 'client_consumer/pulp/client/consumer/cli.py', 'bindings/pulp/bindings/consumer.py', 'bindings/pulp/bindings/server.py', 'bindings/pulp/bindings/bindings.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('61e45e19-e242-4bc9-94e0-e465143e6196'), UUID('0945225c-ebe7-497f-8731-5bb26f782e35'), UUID('fa92ceb3-7a91-4cb8-a42f-3cc1233bfebe'), UUID('82429ffd-9129-4fb6-ae2e-6fbb47590a1e'), UUID('2a735f07-6938-434c-9d28-9c3577432d78')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0: class BindingsAPI(PulpAPI):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 13:0: <line number missing in source>\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1237/1800 [10:38<03:56,  2.38it/s]ERROR:src.process_code_changes:Error processing commit a82098f4f90cd86297131b5663c3dec6a34470e8\n",
      "ERROR:src.process_code_changes:{'repo': 'Exiv2/exiv2', 'vulnerability_id': '2019-20421', 'commit': 'a82098f4f90cd86297131b5663c3dec6a34470e8', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -18,10 +18,6 @@\\n  * Foundation, Inc., 51 Franklin Street, 5th Floor, Boston, MA 02110-1301 USA.\\n  */\\n \\n-/*\\n-  File:      jp2image.cpp\\n-*/\\n-\\n // *****************************************************************************\\n \\n // included header files\\n@@ -197,6 +193,16 @@ namespace Exiv2\\n         return result;\\n     }\\n \\n+static void boxes_check(size_t b,size_t m)\\n+{\\n+    if ( b > m ) {\\n+#ifdef EXIV2_DEBUG_MESSAGES\\n+        std::cout << \"Exiv2::Jp2Image::readMetadata box maximum exceeded\" << std::endl;\\n+#endif\\n+        throw Error(kerCorruptedMetadata);\\n+    }\\n+}\\n+\\n     void Jp2Image::readMetadata()\\n     {\\n #ifdef EXIV2_DEBUG_MESSAGES\\n@@ -219,9 +225,12 @@ namespace Exiv2\\n         Jp2BoxHeader      subBox    = {0,0};\\n         Jp2ImageHeaderBox ihdr      = {0,0,0,0,0,0,0,0};\\n         Jp2UuidBox        uuid      = {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}};\\n+        size_t            boxes     = 0 ;\\n+        size_t            boxem     = 1000 ; // boxes max\\n \\n         while (io_->read((byte*)&box, sizeof(box)) == sizeof(box))\\n         {\\n+            boxes_check(boxes++,boxem );\\n             position   = io_->tell();\\n             box.length = getLong((byte*)&box.length, bigEndian);\\n             box.type   = getLong((byte*)&box.type, bigEndian);\\n@@ -251,8 +260,12 @@ namespace Exiv2\\n \\n                     while (io_->read((byte*)&subBox, sizeof(subBox)) == sizeof(subBox) && subBox.length )\\n                     {\\n+                        boxes_check(boxes++, boxem) ;\\n                         subBox.length = getLong((byte*)&subBox.length, bigEndian);\\n                         subBox.type   = getLong((byte*)&subBox.type, bigEndian);\\n+                        if (subBox.length > io_->size() ) {\\n+                            throw Error(kerCorruptedMetadata);\\n+                        }\\n #ifdef EXIV2_DEBUG_MESSAGES\\n                         std::cout << \"Exiv2::Jp2Image::readMetadata: \"\\n                         << \"subBox = \" << toAscii(subBox.type) << \" length = \" << subBox.length << std::endl;\\n@@ -308,7 +321,9 @@ namespace Exiv2\\n                         }\\n \\n                         io_->seek(restore,BasicIo::beg);\\n-                        io_->seek(subBox.length, Exiv2::BasicIo::cur);\\n+                        if ( io_->seek(subBox.length, Exiv2::BasicIo::cur) != 0 ) {\\n+                            throw Error(kerCorruptedMetadata);\\n+                        }\\n                         restore = io_->tell();\\n                     }\\n                     break;'], 'file': ['src/jp2image.cpp'], 'language': ['C/C++'], 'temp_id': [UUID('2e17b649-1ac4-4c00-a36c-5d3aa8a34816')]}\n",
      "ERROR:root:Error in {'repo': 'Exiv2/exiv2', 'vulnerability_id': '2019-20421', 'commit': 'a82098f4f90cd86297131b5663c3dec6a34470e8', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -18,10 +18,6 @@\\n  * Foundation, Inc., 51 Franklin Street, 5th Floor, Boston, MA 02110-1301 USA.\\n  */\\n \\n-/*\\n-  File:      jp2image.cpp\\n-*/\\n-\\n // *****************************************************************************\\n \\n // included header files\\n@@ -197,6 +193,16 @@ namespace Exiv2\\n         return result;\\n     }\\n \\n+static void boxes_check(size_t b,size_t m)\\n+{\\n+    if ( b > m ) {\\n+#ifdef EXIV2_DEBUG_MESSAGES\\n+        std::cout << \"Exiv2::Jp2Image::readMetadata box maximum exceeded\" << std::endl;\\n+#endif\\n+        throw Error(kerCorruptedMetadata);\\n+    }\\n+}\\n+\\n     void Jp2Image::readMetadata()\\n     {\\n #ifdef EXIV2_DEBUG_MESSAGES\\n@@ -219,9 +225,12 @@ namespace Exiv2\\n         Jp2BoxHeader      subBox    = {0,0};\\n         Jp2ImageHeaderBox ihdr      = {0,0,0,0,0,0,0,0};\\n         Jp2UuidBox        uuid      = {{0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0}};\\n+        size_t            boxes     = 0 ;\\n+        size_t            boxem     = 1000 ; // boxes max\\n \\n         while (io_->read((byte*)&box, sizeof(box)) == sizeof(box))\\n         {\\n+            boxes_check(boxes++,boxem );\\n             position   = io_->tell();\\n             box.length = getLong((byte*)&box.length, bigEndian);\\n             box.type   = getLong((byte*)&box.type, bigEndian);\\n@@ -251,8 +260,12 @@ namespace Exiv2\\n \\n                     while (io_->read((byte*)&subBox, sizeof(subBox)) == sizeof(subBox) && subBox.length )\\n                     {\\n+                        boxes_check(boxes++, boxem) ;\\n                         subBox.length = getLong((byte*)&subBox.length, bigEndian);\\n                         subBox.type   = getLong((byte*)&subBox.type, bigEndian);\\n+                        if (subBox.length > io_->size() ) {\\n+                            throw Error(kerCorruptedMetadata);\\n+                        }\\n #ifdef EXIV2_DEBUG_MESSAGES\\n                         std::cout << \"Exiv2::Jp2Image::readMetadata: \"\\n                         << \"subBox = \" << toAscii(subBox.type) << \" length = \" << subBox.length << std::endl;\\n@@ -308,7 +321,9 @@ namespace Exiv2\\n                         }\\n \\n                         io_->seek(restore,BasicIo::beg);\\n-                        io_->seek(subBox.length, Exiv2::BasicIo::cur);\\n+                        if ( io_->seek(subBox.length, Exiv2::BasicIo::cur) != 0 ) {\\n+                            throw Error(kerCorruptedMetadata);\\n+                        }\\n                         restore = io_->tell();\\n                     }\\n                     break;'], 'file': ['src/jp2image.cpp'], 'language': ['C/C++'], 'temp_id': [UUID('2e17b649-1ac4-4c00-a36c-5d3aa8a34816')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 143, in get_changes\n",
      "    local_repo = Repo.clone_from(repo.clone_url, f\"{REPOS_PATH}/{repo_name}\")\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1541, in clone_from\n",
      "    return cls._clone(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/repo/base.py\", line 1412, in _clone\n",
      "    finalize_process(proc, stderr=stderr)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/util.py\", line 504, in finalize_process\n",
      "    proc.wait(**kwargs)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/git/cmd.py\", line 834, in wait\n",
      "    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\n",
      "git.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git clone -v -- https://github.com/Exiv2/exiv2.git /Users/somen/repos/Exiv2/exiv2\n",
      "  stderr: 'Cloning into '/Users/somen/repos/Exiv2/exiv2'...\n",
      "POST git-upload-pack (175 bytes)\n",
      "POST git-upload-pack (gzip 4967 to 2402 bytes)\n",
      "error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly: CANCEL (err 8)\n",
      "error: 20123 bytes of body are still expected\n",
      "fetch-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: early EOF\n",
      "fatal: fetch-pack: invalid index-pack output\n",
      "'\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1270/1800 [16:00<43:27,  4.92s/it]  ERROR:src.process_code_changes:Error processing commit 84d84549c6c6d765abc9243ac7e85d810f32d6e7\n",
      "ERROR:src.process_code_changes:{'repo': 'joajfreitas/marcador', 'vulnerability_id': '2022-28470', 'commit': '84d84549c6c6d765abc9243ac7e85d810f32d6e7', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -1 +1 @@\\n-version = \\'0.16\\'\\n+version = \"0.16\"', '@@ -1,268 +0,0 @@\\n-#!/usr/bin/env python3\\n-\\n-from pathlib import Path\\n-from pprint import pprint\\n-\\n-import click\\n-import jinja2\\n-from appdirs import user_data_dir\\n-from selenium import webdriver\\n-\\n-from marcador import version\\n-from marcador.marcador_lib import (Bookmark, BookmarkTag, Database, Tag,\\n-                                   bookmark_to_str, get_session)\\n-from marcador.rofi_marcador import RofiMarcador\\n-\\n-\\n-from flask import Flask, jsonify, Response, send_from_directory, render_template, request, redirect\\n-from flask_cors import CORS\\n-\\n-\\n-def get_user_data_dir():\\n-    appauthor = \"joajfreitas\"\\n-    appname = \"marcador\"\\n-    \\n-    return user_data_dir(appname, appauthor)\\n-\\n-def get_db_path():\\n-    return Path(get_user_data_dir()) / Path(\"marcador.sqlite\")\\n-\\n-@click.command(name=\\'open\\')\\n-@click.argument(\\'url\\')\\n-def open_bookmark_cmd(url):\\n-    session = get_session(get_db_path())\\n-    bookmark = session.query(Bookmark).filter(Bookmark.url == url).one()\\n-    print(bookmark)\\n-\\n-    import webbrowser\\n-    webbrowser.open(bookmark.url)\\n-\\n-@click.command()\\n-@click.argument(\\'url\\')\\n-@click.argument(\\'tags\\', nargs=-1)\\n-def add(url, tags):\\n-    session = get_session(get_db_path())\\n-    \\n-    bookmark = Bookmark( \\n-            url = url, \\n-            description = \"\", \\n-            count = 0, \\n-            thumbnail = \"\", \\n-            score = 0)\\n-\\n-    session.add(bookmark)\\n-\\n-    for tag in tags:\\n-        tag = Tag(tag=tag)\\n-        session.add(tag)\\n-\\n-        bookmark_tag = BookmarkTag(url=url, tag=tag.tag)\\n-        session.add(bookmark_tag)\\n-    \\n-    session.commit()\\n-\\n-\\n-@click.command(name=\\'bookmarks\\')\\n-def print_bookmarks():\\n-    session = get_session(get_db_path())\\n-    \\n-    bookmarks = session.query(Bookmark).all()\\n-    pprint(bookmarks)\\n-\\n-@click.command(name=\\'bookmark\\')\\n-@click.argument(\\'url\\')\\n-def print_bookmark(url):\\n-    session = get_session(get_db_path())\\n-    \\n-    bookmark = session.query(Bookmark).filter(Bookmark.url == url).one()\\n-\\n-    pprint(bookmark)\\n-    pprint([bt.tag for bt in session.query(BookmarkTag).filter(BookmarkTag.url == url).all()])\\n-\\n-\\n-@click.command(name=\\'tags\\')\\n-def print_tags():\\n-    session = get_session(get_db_path())\\n-    \\n-    tags = session.query(Tag).all()\\n-    pprint(tags)\\n-\\n-\\n-@click.command(name=\\'tag\\')\\n-@click.argument(\\'tag\\')\\n-def print_tag(tag):\\n-    session = get_session(get_db_path())\\n-    \\n-    tag = session.query(Tag).filter(Tag.tag == tag).one()\\n-    pprint(tag)\\n-    \\n-    pprint([bt.url for bt in session.query(BookmarkTag).filter(BookmarkTag.tag == tag.tag).all()])\\n-\\n-@click.command()\\n-@click.argument(\\'url\\')\\n-def delete(url):\\n-    session = get_session(get_db_path())\\n-    session.query(Bookmark).filter(Bookmark.url == url).delete()\\n-    session.query(BookmarkTag).filter(BookmarkTag.url == url).delete()\\n-\\n-    session.commit()\\n-\\n-\\n-@click.command(name=\\'url\\')\\n-@click.argument(\\'id\\')\\n-def get_url(id):\\n-    session = get_session(get_db_path())\\n-    print(session.query(Bookmark).filter(Bookmark.identifier==id).one().url)\\n-\\n-\\n-@click.command(name=\\'bookmark\\')\\n-@click.argument(\\'id\\')\\n-def get_bookmark(id):\\n-    session = get_session(get_db_path())\\n-    print(session.query(Bookmark).filter(Bookmark.identifier == id).one())\\n-\\n-@click.command()\\n-@click.argument(\\'filename\\')\\n-@click.argument(\\'id\\')\\n-def edit(filename, id):\\n-    db = Database(filename)\\n-    db.edit_bookmark(id)\\n-\\n-@click.command(name=\\'html\\')\\n-@click.argument(\\'template\\')\\n-def html(template):\\n-    app = Flask(__name__)\\n-    app.config.from_object(__name__)\\n-\\n-    CORS(app, resources={r\\'/*\\': {\\'origins\\': \\'*\\'}})\\n-\\n-    with open(template) as t:\\n-        template = jinja2.Template(t.read())\\n-\\n-    @app.route(\\'/\\')\\n-    def index():\\n-        session = get_session(get_db_path())\\n-        bookmarks = session.query(Bookmark).order_by(Bookmark.score.desc()).all()\\n-\\n-        bookmarks = [(\"/redirect?url=\" + book.url, book.thumbnail) for book in bookmarks]\\n-\\n-        return template.render(bookmarks=bookmarks)\\n-\\n-    \\n-    @app.route(\\'/bookmarks\\')\\n-    def bookmarks():\\n-        session = get_session(get_db_path())\\n-        bookmarks = session.query(Bookmark).order_by(Bookmark.score.desc()).all()\\n-        \\n-        bookmarks_list = []\\n-        for bookmark in bookmarks:\\n-            bookmark = {\\n-                \"url\": bookmark.url,\\n-                \"thumb\": \"127.0.0.1:5000/\" + bookmark.thumbnail,\\n-                \"score\": bookmark.score,\\n-            }\\n-\\n-            bookmarks_list.append(bookmark)\\n-\\n-        return jsonify(bookmarks_list)\\n-\\n-\\n-    # sanity check route\\n-    @app.route(\\'/ping\\', methods=[\\'GET\\'])\\n-    def ping_pong():\\n-        return jsonify(\\'pong!\\')\\n-\\n-\\n-    @app.route(\\'/thumbnails/<filepath>\\', methods=[\\'GET\\'])\\n-    def thumbnails(filepath):  # pragma: no cover\\n-        return send_from_directory(str(get_user_data_dir() / Path(\".thumbnails\")), filepath)\\n-\\n-    @app.route(\\'/redirect\\')\\n-    def _redirect():\\n-        session = get_session(get_db_path())\\n-        url = request.args[\\'url\\']\\n-\\n-        bookmark = session.query(Bookmark).filter(Bookmark.url == url).one()\\n-\\n-        if bookmark.score == None:\\n-            bookmark.score = 0\\n-\\n-        if bookmark.count == None:\\n-            bookmark.count = 0\\n-\\n-        bookmark.score += 1\\n-        bookmark.count += 1\\n-\\n-        session.commit()\\n-\\n-        return redirect(url)\\n-\\n-    app.run()\\n-\\n-\\n-@click.command(name=\"gen_thumbnails\")\\n-def gen_thumbnails():\\n-    session = get_session(get_db_path())\\n-    user_data_dir = Path(get_user_data_dir())\\n-\\n-    bookmarks = session.query(Bookmark).all()\\n-\\n-    thumbnail_dir  = user_data_dir / Path(\".thumbnails/\")\\n-    thumbnail_dir.mkdir(parents=True, exist_ok=True)\\n-\\n-    thumbnails = thumbnail_dir.glob(\"*\")\\n-\\n-    driver = webdriver.Firefox()\\n-    for bookmark in bookmarks:\\n-        image_path = str(hash(bookmark.url)) + \".png\"\\n-        if image_path in thumbnails:\\n-            print(\"skiped\", bookmark.url)\\n-            continue\\n-        print(\"getting thumbnail for:\", bookmark.url)\\n-        try:\\n-            driver.get(bookmark.url)\\n-            thumbnail_path = image_path\\n-            driver.save_screenshot(str(thumbnail_dir / thumbnail_path))\\n-            bookmark.thumbnail = str(Path(\"thumbnails\") / thumbnail_path)\\n-        except Exception as e:\\n-            print(\"Error: \" + str(e))\\n-            continue\\n-    \\n-    session.commit()\\n-\\n-@click.command(name=\"rofi\")\\n-def rofi_launch():\\n-    session = get_session(get_db_path())\\n-    rm = RofiMarcador(session)\\n-    rm.launch()\\n-\\n-\\n-@click.group(invoke_without_command=True)\\n-@click.version_option(version)\\n-def cli():\\n-    db_path = get_db_path()\\n-    if not db_path.is_file():\\n-        print(db_path)\\n-        db_path.parent.mkdir(exist_ok=True)\\n-        db_path.touch()\\n-\\n-    return\\n-\\n-   \\n-cli.add_command(print_bookmarks)\\n-cli.add_command(print_bookmark)\\n-cli.add_command(print_tags)\\n-cli.add_command(print_tag)\\n-\\n-cli.add_command(open_bookmark_cmd)\\n-cli.add_command(add)\\n-cli.add_command(delete)\\n-cli.add_command(get_url)\\n-#cli.add_command(get_bookmark)\\n-cli.add_command(edit)\\n-cli.add_command(html)\\n-cli.add_command(gen_thumbnails)\\n-cli.add_command(rofi_launch)\\n-\\n-if __name__ == \"__main__\":\\n-    cli()', '@@ -1,10 +1,11 @@\\n from webbrowser import open\\n from .rofi import Rofi\\n-from .lib import Bookmark, Tag, BookmarkTag\\n+\\n+# from .lib import Bookmark, Tag, BookmarkTag\\n import clipboard\\n \\n \\n-class RofiMarcador():\\n+class RofiMarcador:\\n     def __init__(self, proxy):\\n         self.rofi = Rofi()\\n         self.proxy = proxy\\n@@ -37,7 +38,7 @@ def delete(self, index):\\n         self.proxy.delete(bookmark.url)\\n \\n     def edit(self, index):\\n-        i = self.bookmarks[index].split(\\',\\')[0]\\n+        i = self.bookmarks[index].split(\",\")[0]\\n         self.db.edit_bookmark(i)\\n         self.launch()\\n         return\\n@@ -54,10 +55,12 @@ def dispatch(self, index, key):\\n \\n     def launch(self):\\n         self.bookmarks = self.list()\\n-        ret = self.rofi.select(\"> \",\\n-                          self.bookmarks,\\n-                          key1=(\\'Alt+n\\', \"Add new bookmark\"),\\n-                          key2=(\\'Alt+d\\', \"Delete the selected bookmark\"),\\n-                          key3=(\\'Alt+e\\', \"Edit the selected bookmark\"))\\n+        ret = self.rofi.select(\\n+            \"> \",\\n+            self.bookmarks,\\n+            key1=(\"Alt+n\", \"Add new bookmark\"),\\n+            key2=(\"Alt+d\", \"Delete the selected bookmark\"),\\n+            key3=(\"Alt+e\", \"Edit the selected bookmark\"),\\n+        )\\n         index, key = ret\\n         self.dispatch(index, key)', '@@ -2,55 +2,50 @@\\n import json\\n import socket\\n \\n-from marcador.lib import get_session, get_db_path, Bookmark\\n+from marcador.lib import get_db_path\\n+from marcador.json_backend import JsonProxy\\n+\\n \\n def marcador_list(session, args):\\n-    bookmarks = session.query(Bookmark).all()\\n-    return [{\\n-        \\'url\\': bookmark.url,\\n-        \\'description\\': bookmark.description,\\n-        \\'count\\': bookmark.count,\\n-        \\'score\\': bookmark.score\\n-    } for bookmark in bookmarks]\\n+    bookmarks = session.list()\\n+    return [\\n+        {\\n+            \"url\": bookmark.url,\\n+            \"description\": bookmark.description,\\n+            \"tags\": bookmark.tags,\\n+        }\\n+        for bookmark in bookmarks\\n+    ]\\n+\\n \\n def marcador_add(session, args):\\n-    bookmark = Bookmark(url=args.get(\\'url\\'))\\n-    session.add(bookmark)\\n-    session.commit()\\n+    session.add(args.get(\"url\"), args.get(\"description\"), args.get(\"tags\"))\\n \\n-def marcador_tag(session, args):\\n-    tag = Tag(tag=args.get(\\'tag\\'))\\n-    self.session.add(tag)\\n-    book_tag = BookmarkTag(url=args.get(\\'url\\'), tag=args.get(\\'tag\\'))\\n-    session.add(book_tag)\\n-    session.commit()\\n \\n def marcador_delete(session, args):\\n-    session.query(Bookmark).filter(Bookmark.url == args.get(\\'url\\')).delete()\\n-    session.commit()\\n+    session.delete(arg.get(\"url\"))\\n+\\n \\n @click.command()\\n-@click.option(\\'--hostname\\', default=\\'127.0.0.1\\')\\n-@click.option(\\'--port\\', type=int, default=6003)\\n+@click.option(\"--hostname\", default=\"127.0.0.1\")\\n+@click.option(\"--port\", type=int, default=6003)\\n def server(hostname, port):\\n-    session = get_session(get_db_path())\\n+    session = JsonProxy(get_db_path())\\n \\n     sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\\n     sock.bind((hostname, port))\\n \\n     cmds = {\\n-        \\'list\\': marcador_list,\\n-        \\'add\\': marcador_add,\\n-        \\'tag\\': marcador_tag,\\n-        \\'delete\\': marcador_delete,\\n+        \"list\": marcador_list,\\n+        \"add\": marcador_add,\\n+        \"delete\": marcador_delete,\\n     }\\n \\n     while True:\\n         try:\\n             msg, addr = sock.recvfrom(1024)\\n             msg = json.loads(msg)\\n-            ret = cmds[msg[\\'cmd\\']](session, msg[\\'args\\'])\\n-            sock.sendto(bytes(json.dumps(ret), \\'utf-8\\'), addr)\\n+            ret = cmds[msg[\"cmd\"]](session, msg[\"args\"])\\n+            sock.sendto(bytes(json.dumps(ret), \"utf-8\"), addr)\\n         except Exception as e:\\n             continue\\n-', '@@ -3,12 +3,15 @@\\n import asyncio\\n from websockets import serve\\n \\n+\\n async def echo(websocket, path):\\n     async for message in websocket:\\n         await websocket.send(message)\\n \\n+\\n async def main():\\n     async with serve(echo, \"localhost\", 8765):\\n         await asyncio.Future()  # run forever\\n \\n+\\n asyncio.run(main())', '@@ -5,20 +5,21 @@\\n import os\\n from pprint import pprint\\n \\n+\\n class BookmarkHTMLParser(html.parser.HTMLParser):\\n     def __init__(self):\\n         self.parser = html.parser.HTMLParser.__init__(self)\\n-        self.bookmarks = {\\'None\\': []}\\n+        self.bookmarks = {\"None\": []}\\n         self.link = \"\"\\n         self.h3 = False\\n         self.tag = None\\n \\n     def handle_starttag(self, tag, attrs):\\n-        if tag == \\'a\\':\\n+        if tag == \"a\":\\n             for attr in attrs:\\n-                if attr[0] == \\'href\\':\\n+                if attr[0] == \"href\":\\n                     self.link = attr[1]\\n-        if tag == \\'h3\\':\\n+        if tag == \"h3\":\\n             self.h3 = True\\n \\n     def handle_data(self, data):\\n@@ -28,31 +29,34 @@ def handle_data(self, data):\\n             self.h3 = False\\n \\n     def handle_endtag(self, tag):\\n-        if tag == \\'a\\':\\n-            self.bookmarks[str(self.tag)].append(self.link) \\n+        if tag == \"a\":\\n+            self.bookmarks[str(self.tag)].append(self.link)\\n+\\n \\n def parse_args():\\n-    parser = argparse.ArgumentParser(description=\\'Import bookmarks from standard html file to rofi-bookmarks.\\')\\n-    parser.add_argument(\\'inputfile\\', metavar=\\'htmlfile\\' )\\n-    parser.add_argument(\\'outputfile\\', metavar=\\'bookmarkfile\\')\\n+    parser = argparse.ArgumentParser(\\n+        description=\"Import bookmarks from standard html file to rofi-bookmarks.\"\\n+    )\\n+    parser.add_argument(\"inputfile\", metavar=\"htmlfile\")\\n+    parser.add_argument(\"outputfile\", metavar=\"bookmarkfile\")\\n \\n     return parser.parse_args()\\n \\n \\n def main(args):\\n     if os.path.isfile(args.outputfile):\\n-        with open(args.outputfile,\\'r\\') as f:\\n-            last = int(f.readlines()[-1].split(\\'.\\')[0]) + 1\\n+        with open(args.outputfile, \"r\") as f:\\n+            last = int(f.readlines()[-1].split(\".\")[0]) + 1\\n     else:\\n-        last = 0 \\n+        last = 0\\n \\n-    inf= open(args.inputfile,\\'r\\')\\n-    outf = open(args.outputfile,\\'a\\')\\n+    inf = open(args.inputfile, \"r\")\\n+    outf = open(args.outputfile, \"a\")\\n \\n     p = BookmarkHTMLParser()\\n-    \\n+\\n     p.feed(inf.read())\\n-    \\n+\\n     for k, v in p.bookmarks.items():\\n         for link in v:\\n             outf.write(f\"{str(last)}. {link} {k}\\\\n\")', '@@ -33,9 +33,10 @@\\n \\n # Python < 3.2 doesn\\'t provide a context manager interface for Popen.\\n # Let\\'s make our own wrapper if needed.\\n-if hasattr(subprocess.Popen, \\'__exit__\\'):\\n+if hasattr(subprocess.Popen, \"__exit__\"):\\n     Popen = subprocess.Popen\\n else:\\n+\\n     class ContextManagedPopen(subprocess.Popen):\\n         def __enter__(self):\\n             return self\\n@@ -48,6 +49,7 @@ def __exit__(self, type, value, traceback):\\n             if self.stdin:\\n                 self.stdin.close()\\n             self.wait()\\n+\\n     Popen = ContextManagedPopen\\n \\n \\n@@ -71,9 +73,17 @@ class Rofi(object):\\n     for available markup.\\n \\n     \"\"\"\\n-    def __init__(self, lines=None, fixed_lines=None, width=None,\\n-                 fullscreen=None, location=None,\\n-                 exit_hotkeys=(\\'Alt+F4\\', \\'Control+q\\'), rofi_args=None):\\n+\\n+    def __init__(\\n+        self,\\n+        lines=None,\\n+        fixed_lines=None,\\n+        width=None,\\n+        fullscreen=None,\\n+        location=None,\\n+        exit_hotkeys=(\"Alt+F4\", \"Control+q\"),\\n+        rofi_args=None,\\n+    ):\\n         \"\"\"\\n         Parameters\\n         ----------\\n@@ -124,7 +134,6 @@ def __init__(self, lines=None, fixed_lines=None, width=None,\\n         # (e.g., an unhandled exception).\\n         atexit.register(self.close)\\n \\n-\\n     @classmethod\\n     def escape(self, string):\\n         \"\"\"Escape a string for Pango markup.\\n@@ -143,16 +152,9 @@ def escape(self, string):\\n         # dictionary, we can\\'t guarantee order of translations and so doing it\\n         # in one go would risk the ampersands in other translations being\\n         # escaped again.\\n-        return string.translate(\\n-            {38: \\'&amp;\\'}\\n-        ).translate({\\n-            34: \\'&quot;\\',\\n-            39: \\'&apos;\\',\\n-            60: \\'&lt;\\',\\n-            62: \\'&gt;\\'\\n-        })\\n-\\n-\\n+        return string.translate({38: \"&amp;\"}).translate(\\n+            {34: \"&quot;\", 39: \"&apos;\", 60: \"&lt;\", 62: \"&gt;\"}\\n+        )\\n \\n     def close(self):\\n         \"\"\"Close any open window.\\n@@ -167,7 +169,7 @@ def close(self):\\n             # If it doesn\\'t close itself promptly, be brutal.\\n             # Python 3.2+ added the timeout option to wait() and the\\n             # corresponding TimeoutExpired exception. If they exist, use them.\\n-            if hasattr(subprocess, \\'TimeoutExpired\\'):\\n+            if hasattr(subprocess, \"TimeoutExpired\"):\\n                 try:\\n                     self._process.wait(timeout=1)\\n                 except subprocess.TimeoutExpired:\\n@@ -189,7 +191,6 @@ def close(self):\\n             # Clean up.\\n             self._process = None\\n \\n-\\n     def _run_blocking(self, args, input=None):\\n         \"\"\"Internal API: run a blocking command with subprocess.\\n \\n@@ -214,17 +215,17 @@ def _run_blocking(self, args, input=None):\\n \\n         # Make sure we grab stdout as text (not bytes).\\n         kwargs = {}\\n-        kwargs[\\'stdout\\'] = subprocess.PIPE\\n-        kwargs[\\'universal_newlines\\'] = True\\n+        kwargs[\"stdout\"] = subprocess.PIPE\\n+        kwargs[\"universal_newlines\"] = True\\n \\n         # Use the run() method if available (Python 3.5+).\\n-        if hasattr(subprocess, \\'run\\'):\\n+        if hasattr(subprocess, \"run\"):\\n             result = subprocess.run(args, input=input, **kwargs)\\n             return result.returncode, result.stdout\\n \\n         # Have to do our own. If we need to feed stdin, we must open a pipe.\\n         if input is not None:\\n-            kwargs[\\'stdin\\'] = subprocess.PIPE\\n+            kwargs[\"stdin\"] = subprocess.PIPE\\n \\n         # Start the process.\\n         with Popen(args, **kwargs) as proc:\\n@@ -237,7 +238,6 @@ def _run_blocking(self, args, input=None):\\n             # Done.\\n             return returncode, stdout\\n \\n-\\n     def _run_nonblocking(self, args, input=None):\\n         \"\"\"Internal API: run a non-blocking command with subprocess.\\n \\n@@ -258,40 +258,38 @@ def _run_nonblocking(self, args, input=None):\\n         # Start the new one.\\n         self._process = subprocess.Popen(args, stdout=subprocess.PIPE)\\n \\n-\\n     def _common_args(self, allow_fullscreen=True, **kwargs):\\n         args = []\\n \\n         # Number of lines.\\n-        lines = kwargs.get(\\'lines\\', self.lines)\\n+        lines = kwargs.get(\"lines\", self.lines)\\n         if lines:\\n-            args.extend([\\'-lines\\', str(lines)])\\n-        fixed_lines = kwargs.get(\\'fixed_lines\\', self.fixed_lines)\\n+            args.extend([\"-lines\", str(lines)])\\n+        fixed_lines = kwargs.get(\"fixed_lines\", self.fixed_lines)\\n         if fixed_lines:\\n-            args.extend([\\'-fixed-num-lines\\', str(fixed_lines)])\\n+            args.extend([\"-fixed-num-lines\", str(fixed_lines)])\\n \\n         # Width.\\n-        width = kwargs.get(\\'width\\', self.width)\\n+        width = kwargs.get(\"width\", self.width)\\n         if width is not None:\\n-            args.extend([\\'-width\\', str(width)])\\n+            args.extend([\"-width\", str(width)])\\n \\n         # Fullscreen mode?\\n-        fullscreen = kwargs.get(\\'fullscreen\\', self.fullscreen)\\n+        fullscreen = kwargs.get(\"fullscreen\", self.fullscreen)\\n         if allow_fullscreen and fullscreen:\\n-            args.append(\\'-fullscreen\\')\\n+            args.append(\"-fullscreen\")\\n \\n         # Location on screen.\\n-        location = kwargs.get(\\'location\\', self.location)\\n+        location = kwargs.get(\"location\", self.location)\\n         if location is not None:\\n-            args.extend([\\'-location\\', str(location)])\\n+            args.extend([\"-location\", str(location)])\\n \\n         # Any other arguments\\n         args.extend(self.rofi_args)\\n \\n         # Done.\\n         return args\\n \\n-\\n     def error(self, message, rofi_args=None, **kwargs):\\n         \"\"\"Show an error window.\\n \\n@@ -308,14 +306,13 @@ def error(self, message, rofi_args=None, **kwargs):\\n         \"\"\"\\n         rofi_args = rofi_args or []\\n         # Generate arguments list.\\n-        args = [\\'rofi\\', \\'-e\\', message]\\n+        args = [\"rofi\", \"-e\", message]\\n         args.extend(self._common_args(allow_fullscreen=False, **kwargs))\\n         args.extend(rofi_args)\\n \\n         # Close any existing window and show the error.\\n         self._run_blocking(args)\\n \\n-\\n     def status(self, message, rofi_args=None, **kwargs):\\n         \"\"\"Show a status message.\\n \\n@@ -336,15 +333,16 @@ def status(self, message, rofi_args=None, **kwargs):\\n         \"\"\"\\n         rofi_args = rofi_args or []\\n         # Generate arguments list.\\n-        args = [\\'rofi\\', \\'-e\\', message]\\n+        args = [\"rofi\", \"-e\", message]\\n         args.extend(self._common_args(allow_fullscreen=False, **kwargs))\\n         args.extend(rofi_args)\\n \\n         # Update the status.\\n         self._run_nonblocking(args)\\n \\n-\\n-    def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwargs):\\n+    def select(\\n+        self, prompt, options, rofi_args=None, message=\"\", select=None, **kwargs\\n+    ):\\n         \"\"\"Show a list of options and return user selection.\\n \\n         This method blocks until the user makes their choice.\\n@@ -387,12 +385,12 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n         \"\"\"\\n         rofi_args = rofi_args or []\\n         # Replace newlines and turn the options into a single string.\\n-        optionstr = \\'\\\\n\\'.join(option.replace(\\'\\\\n\\', \\' \\') for option in options)\\n+        optionstr = \"\\\\n\".join(option.replace(\"\\\\n\", \" \") for option in options)\\n \\n         # Set up arguments.\\n-        args = [\\'rofi\\', \\'-dmenu\\', \\'-p\\', prompt, \\'-format\\', \\'i\\']\\n+        args = [\"rofi\", \"-dmenu\", \"-p\", prompt, \"-format\", \"i\"]\\n         if select is not None:\\n-            args.extend([\\'-selected-row\\', str(select)])\\n+            args.extend([\"-selected-row\", str(select)])\\n \\n         # Key bindings to display.\\n         display_bindings = []\\n@@ -401,7 +399,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n         user_keys = set()\\n         for k, v in kwargs.items():\\n             # See if the keyword name matches the needed format.\\n-            if not k.startswith(\\'key\\'):\\n+            if not k.startswith(\"key\"):\\n                 continue\\n             try:\\n                 keynum = int(k[3:])\\n@@ -411,7 +409,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n             # Add it to the set.\\n             key, action = v\\n             user_keys.add(keynum)\\n-            args.extend([\\'-kb-custom-{0:s}\\'.format(k[3:]), key])\\n+            args.extend([\"-kb-custom-{0:s}\".format(k[3:]), key])\\n             if action:\\n                 display_bindings.append(\"<b>{0:s}</b>: {1:s}\".format(key, action))\\n \\n@@ -422,7 +420,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n             while next_key in user_keys:\\n                 next_key += 1\\n             exit_keys.add(next_key)\\n-            args.extend([\\'-kb-custom-{0:d}\\'.format(next_key), key])\\n+            args.extend([\"-kb-custom-{0:d}\".format(next_key), key])\\n             next_key += 1\\n \\n         # Add any displayed key bindings to the message.\\n@@ -433,7 +431,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n \\n         # If we have a message, add it to the arguments.\\n         if message:\\n-            args.extend([\\'-mesg\\', message])\\n+            args.extend([\"-mesg\", message])\\n \\n         # Add in common arguments.\\n         args.extend(self._common_args(**kwargs))\\n@@ -456,13 +454,22 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n             if key in exit_keys:\\n                 raise SystemExit()\\n         else:\\n-            self.exit_with_error(\"Unexpected rofi returncode {0:d}.\".format(results.returncode))\\n+            self.exit_with_error(\\n+                \"Unexpected rofi returncode {0:d}.\".format(results.returncode)\\n+            )\\n \\n         # And return.\\n         return index, key\\n \\n-\\n-    def generic_entry(self, prompt, validator=None, message=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+    def generic_entry(\\n+        self,\\n+        prompt,\\n+        validator=None,\\n+        message=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"A generic entry box.\\n \\n         Parameters\\n@@ -502,17 +509,19 @@ def generic_entry(self, prompt, validator=None, message=None, rofi_args=None, st\\n \\n         # Keep going until we get something valid.\\n         while True:\\n-            args = [\\'rofi\\', \\'-dmenu\\', \\'-p\\', prompt, \\'-format\\', \\'s\\']\\n+            args = [\"rofi\", \"-dmenu\", \"-p\", prompt, \"-format\", \"s\"]\\n \\n             # Add any error to the given message.\\n             msg = message or \"\"\\n             if error:\\n-                msg = \\'<span color=\"#FF0000\" font_weight=\"bold\">{0:s}</span>\\\\n{1:s}\\'.format(error, msg)\\n-                msg = msg.rstrip(\\'\\\\n\\')\\n+                msg = \\'<span color=\"#FF0000\" font_weight=\"bold\">{0:s}</span>\\\\n{1:s}\\'.format(\\n+                    error, msg\\n+                )\\n+                msg = msg.rstrip(\"\\\\n\")\\n \\n             # If there is actually a message to show.\\n             if msg:\\n-                args.extend([\\'-mesg\\', msg])\\n+                args.extend([\"-mesg\", msg])\\n \\n             # Add in common arguments.\\n             args.extend(self._common_args(**kwargs))\\n@@ -526,17 +535,24 @@ def generic_entry(self, prompt, validator=None, message=None, rofi_args=None, st\\n                 return None\\n \\n             # Get rid of the trailing newline and check its validity.\\n-            text = stdout.rstrip(\\'\\\\n\\')\\n+            text = stdout.rstrip(\"\\\\n\")\\n             if validator:\\n                 value, error = validator(text)\\n                 if not error:\\n                     return value\\n             else:\\n                 return text\\n \\n-\\n-    def text_entry(self, prompt, message=None, allow_blank=False, strip=True,\\n-            rofi_args=None, stdin_str=\"\", **kwargs):\\n+    def text_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        allow_blank=False,\\n+        strip=True,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a piece of text.\\n \\n         Parameters\\n@@ -558,6 +574,7 @@ def text_entry(self, prompt, message=None, allow_blank=False, strip=True,\\n         string, or None if the dialog was cancelled.\\n \\n         \"\"\"\\n+\\n         def text_validator(text):\\n             if strip:\\n                 text = text.strip()\\n@@ -567,10 +584,20 @@ def text_validator(text):\\n \\n             return text, None\\n \\n-        return self.generic_entry(prompt, text_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def integer_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, text_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def integer_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        min=None,\\n+        max=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter an integer.\\n \\n         Parameters\\n@@ -608,10 +635,20 @@ def integer_validator(text):\\n \\n             return value, None\\n \\n-        return self.generic_entry(prompt, integer_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def float_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, integer_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def float_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        min=None,\\n+        max=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a floating point number.\\n \\n         Parameters\\n@@ -649,10 +686,20 @@ def float_validator(text):\\n \\n             return value, None\\n \\n-        return self.generic_entry(prompt, float_validator, message, rofi_args, stdin_str,stdin_str,  **kwargs)\\n-\\n-\\n-    def decimal_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, float_validator, message, rofi_args, stdin_str, stdin_str, **kwargs\\n+        )\\n+\\n+    def decimal_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        min=None,\\n+        max=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a decimal number.\\n \\n         Parameters\\n@@ -690,11 +737,20 @@ def decimal_validator(text):\\n \\n             return value, None\\n \\n-        return self.generic_entry(prompt, decimal_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def date_entry(self, prompt, message=None, formats=[\\'%x\\', \\'%d/%m/%Y\\'],\\n-            show_example=False, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, decimal_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def date_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        formats=[\"%x\", \"%d/%m/%Y\"],\\n+        show_example=False,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a date.\\n \\n         Parameters\\n@@ -719,6 +775,7 @@ def date_entry(self, prompt, message=None, formats=[\\'%x\\', \\'%d/%m/%Y\\'],\\n         datetime.date, or None if the dialog is cancelled.\\n \\n         \"\"\"\\n+\\n         def date_validator(text):\\n             # Try them in order.\\n             for format in formats:\\n@@ -731,18 +788,29 @@ def date_validator(text):\\n                     return (dt.date(), None)\\n \\n             # None of the formats worked.\\n-            return (None, \\'Please enter a valid date.\\')\\n+            return (None, \"Please enter a valid date.\")\\n \\n         # Add an example to the message?\\n         if show_example:\\n             message = message or \"\"\\n-            message += \"Today\\'s date in the correct format: \" + datetime.now().strftime(formats[0])\\n-\\n-        return self.generic_entry(prompt, date_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def time_entry(self, prompt, message=None, formats=[\\'%X\\', \\'%H:%M\\', \\'%I:%M\\', \\'%H.%M\\',\\n-        \\'%I.%M\\'], show_example=False, rofi_args=None, stdin_str=\"\", **kwargs):\\n+            message += \"Today\\'s date in the correct format: \" + datetime.now().strftime(\\n+                formats[0]\\n+            )\\n+\\n+        return self.generic_entry(\\n+            prompt, date_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def time_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        formats=[\"%X\", \"%H:%M\", \"%I:%M\", \"%H.%M\", \"%I.%M\"],\\n+        show_example=False,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a time.\\n \\n         Parameters\\n@@ -767,6 +835,7 @@ def time_entry(self, prompt, message=None, formats=[\\'%X\\', \\'%H:%M\\', \\'%I:%M\\', \\'%H.\\n         datetime.time, or None if the dialog is cancelled.\\n \\n         \"\"\"\\n+\\n         def time_validator(text):\\n             # Try them in order.\\n             for format in formats:\\n@@ -779,18 +848,34 @@ def time_validator(text):\\n                     return (dt.time(), None)\\n \\n             # None of the formats worked.\\n-            return (None, \\'Please enter a valid time.\\')\\n+            return (None, \"Please enter a valid time.\")\\n \\n         # Add an example to the message?\\n         if show_example:\\n             message = message or \"\"\\n-            message += \"Current time in the correct format: \" + datetime.now().strftime(formats[0])\\n-\\n-        return self.generic_entry(prompt, time_validator, message, rofi_args=None, stdin_str = stdin_str, **kwargs)\\n-\\n-\\n-    def datetime_entry(self, prompt, message=None, formats=[\\'%x %X\\'], show_example=False,\\n-            rofi_args=None, stdin_str=\"\", **kwargs):\\n+            message += \"Current time in the correct format: \" + datetime.now().strftime(\\n+                formats[0]\\n+            )\\n+\\n+        return self.generic_entry(\\n+            prompt,\\n+            time_validator,\\n+            message,\\n+            rofi_args=None,\\n+            stdin_str=stdin_str,\\n+            **kwargs\\n+        )\\n+\\n+    def datetime_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        formats=[\"%x %X\"],\\n+        show_example=False,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a date and time.\\n \\n         Parameters\\n@@ -815,6 +900,7 @@ def datetime_entry(self, prompt, message=None, formats=[\\'%x %X\\'], show_example=F\\n         datetime.datetime, or None if the dialog is cancelled.\\n \\n         \"\"\"\\n+\\n         def datetime_validator(text):\\n             # Try them in order.\\n             for format in formats:\\n@@ -827,15 +913,19 @@ def datetime_validator(text):\\n                     return (dt, None)\\n \\n             # None of the formats worked.\\n-            return (None, \\'Please enter a valid date and time.\\')\\n+            return (None, \"Please enter a valid date and time.\")\\n \\n         # Add an example to the message?\\n         if show_example:\\n             message = message or \"\"\\n-            message += \"Current date and time in the correct format: \" + datetime.now().strftime(formats[0])\\n-\\n-        return self.generic_entry(prompt, datetime_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n+            message += (\\n+                \"Current date and time in the correct format: \"\\n+                + datetime.now().strftime(formats[0])\\n+            )\\n+\\n+        return self.generic_entry(\\n+            prompt, datetime_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n \\n     def exit_with_error(self, error, **kwargs):\\n         \"\"\"Report an error and exit.', '@@ -1,11 +1,12 @@\\n-from marcador.lib import  Bookmark, Tag, BookmarkTag\\n import socket\\n import json\\n \\n from typing import *\\n \\n+\\n def cmd(name, args):\\n-    return bytes(json.dumps({\\'cmd\\': name, \\'args\\': args}), \\'utf-8\\')\\n+    return bytes(json.dumps({\"cmd\": name, \"args\": args}), \"utf-8\")\\n+\\n \\n class Bookmark(dict):\\n     def __init__(self, url, description, tags):\\n@@ -14,11 +15,11 @@ def __init__(self, url, description, tags):\\n         self.tags = tags\\n         dict.__init__(self, {\"url\": url, \"description\": description, \"tags\": tags})\\n \\n-\\n     def __repr__(self) -> str:\\n         return f\"{self.url}\"\\n \\n-class Proxy():\\n+\\n+class Proxy:\\n     def list(self) -> List[Bookmark]:\\n         pass\\n \\n@@ -28,25 +29,34 @@ def add(self, url: str, description: str, tags: List[str]):\\n     def add_tag(self, url: str, tag: str):\\n         pass\\n \\n-    def delete(self, url:str) -> Bookmark:\\n+    def delete(self, url: str) -> Bookmark:\\n         pass\\n \\n+\\n class RemoteProxy(Proxy):\\n     def __init__(self, addr):\\n         self.addr = addr\\n         self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\\n \\n     def list(self) -> List[Bookmark]:\\n-        self.sock.sendto(cmd(\\'list\\', {}), self.addr)\\n+        self.sock.sendto(cmd(\"list\", {}), self.addr)\\n         msg, addr = self.sock.recvfrom(1024)\\n-        for bookmark in json.loads(msg):\\n-            yield Bookmark.load(bookmark)\\n+\\n+        return [\\n+            Bookmark(\\n+                bookmark.get(\"url\"), bookmark.get(\"description\"), bookmark.get(\"tags\")\\n+            )\\n+            for bookmark in json.loads(msg)\\n+        ]\\n \\n     def add(self, url: str, description: str, tags: List[str]):\\n-        self.sock.sendto(cmd(\\'add\\', {\\'url\\': url, \\'description\\':description, \\'tags\\': tags}), self.addr)\\n+        self.sock.sendto(\\n+            cmd(\"add\", {\"url\": url, \"description\": description, \"tags\": tags}),\\n+            self.addr,\\n+        )\\n \\n     def add_tag(self, url: str, tag: str):\\n-        self.sock.sendto(cmd(\\'tag\\', {\\'url\\': url, \\'tag\\': tag}), self.addr)\\n+        self.sock.sendto(cmd(\"tag\", {\"url\": url, \"tag\": tag}), self.addr)\\n \\n     def delete(self, url: str) -> Bookmark:\\n-        self.sock.sendto(cmd(\\'delete\\', {\\'url\\': url}), self.addr)\\n+        self.sock.sendto(cmd(\"delete\", {\"url\": url}), self.addr)', '@@ -6,14 +6,17 @@\\n \\n from .proxy import Proxy, Bookmark\\n \\n+\\n class Tag(Model):\\n     tag: fields.Str()\\n \\n+\\n class JsonBookmark(Model):\\n     url: fields.Str()\\n     description: fields.Str()\\n     tags: fields.List(Tag)\\n \\n+\\n class JsonBook(Model):\\n     bookmarks: fields.List(JsonBookmark)\\n \\n@@ -38,7 +41,7 @@ def __init__(self, path: pathlib.Path):\\n         if not os.path.exists(path):\\n             self.book = JsonBook(bookmarks=[])\\n         else:\\n-            with open(path) as  f:\\n+            with open(path) as f:\\n                 self.book = JsonBook.from_json(f.read())\\n \\n     def save(self):\\n@@ -47,13 +50,17 @@ def save(self):\\n             f.write(json.dumps(self.book.to_dict()))\\n \\n     def list(self) -> List[Bookmark]:\\n-        return [Bookmark(bookmark.url, bookmark.description, [tag.tag for tag in bookmark.tags]) for bookmark in self.book.bookmarks]\\n+        return [\\n+            Bookmark(\\n+                bookmark.url, bookmark.description, [tag.tag for tag in bookmark.tags]\\n+            )\\n+            for bookmark in self.book.bookmarks\\n+        ]\\n \\n     def add(self, url: str, description: str, tags: List[str]):\\n         if self.book.get(url) is not None:\\n             return\\n \\n-\\n         tags = [Tag(tag) for tag in tags]\\n         self.book.bookmarks.append(JsonBookmark(url, description, tags))\\n         self.save()\\n@@ -75,10 +82,13 @@ def delete(self, url) -> Bookmark:\\n \\n \\n def main():\\n-    bookmark = Bookmark(url=\"www.google.com\", description=\"google\", tags=[Tag(\"search\")])\\n+    bookmark = Bookmark(\\n+        url=\"www.google.com\", description=\"google\", tags=[Tag(\"search\")]\\n+    )\\n     book = Book([bookmark])\\n     print(book.to_dict())\\n     print(json.dumps(bookmark.to_dict()))\\n \\n+\\n if __name__ == \"__main__\":\\n     main()', '@@ -11,284 +11,13 @@\\n \\n from appdirs import user_data_dir\\n \\n+\\n def get_user_data_dir():\\n     appauthor = \"joajfreitas\"\\n     appname = \"marcador\"\\n \\n     return user_data_dir(appname, appauthor)\\n \\n+\\n def get_db_path():\\n     return Path(get_user_data_dir()) / \"marcador.json\"\\n-\\n-Base = declarative_base()\\n-\\n-class Bookmark(Base):\\n-    __tablename__ = \\'bookmark\\'\\n-    #identifier = Column(Integer, primary_key=True)\\n-    url = Column(String, primary_key=True)\\n-    description = Column(String)\\n-    count = Column(Integer)\\n-    thumbnail = Column(String)\\n-    score = Column(Float)\\n-\\n-    def load(data):\\n-        return Bookmark(\\n-            url=data.get(\\'url\\') or \"\",\\n-            description=data.get(\\'description\\') or \"\",\\n-            count=data.get(\\'count\\') or 0,\\n-            thumbnail=data.get(\\'thumbnail\\') or \"\",\\n-            score=data.get(\\'score\\') or 1.0,\\n-        )\\n-\\n-    def __repr__(self):\\n-        return f\"Bookmark {{{self.url=}, {self.thumbnail=}}}\"\\n-\\n-class Tag(Base):\\n-    __tablename__ = \\'tag\\'\\n-\\n-    #identifier = Column(Integer, primary_key = True)\\n-    tag = Column(String, primary_key=True)\\n-\\n-    def __repr__(self):\\n-        return f\"Tag {{{self.tag=}}}\"\\n-\\n-class BookmarkTag(Base):\\n-    __tablename__ = \\'bookmark_tag\\'\\n-\\n-    url = Column(String, ForeignKey(\\'bookmark.url\\'), primary_key=True)\\n-    tag = Column(String, ForeignKey(\\'tag.tag\\'), primary_key=True)\\n-\\n-    def __repr__(self):\\n-        return f\"BookmarkTag {{bookmark={self.bookmark},tag={self.tag}}}\"\\n-\\n-\\n-def get_session(db_path: Path) -> Session:\\n-    if not os.path.isfile(db_path):\\n-        db_path.parent.mkdir(parents=True, exist_ok=True)\\n-        db_path.touch()\\n-\\n-    engine = create_engine(\"sqlite:///\"+str(db_path))\\n-    Base.metadata.create_all(engine)\\n-    Base.metadata.bind = engine\\n-    DBSession = sessionmaker(bind=engine)\\n-    session: Session = DBSession()\\n-\\n-    return session\\n-\\n-#class Database:\\n-#    def __init__(self, filename):\\n-#        self.filename = filename\\n-#        self.conn = self.open_database(self.filename)\\n-#        self.cursor = self.conn.cursor()\\n-#\\n-#    def open_db(self, filename):\\n-#        return sqlite3.connect(filename)\\n-#\\n-#    def set_default_db(self, filename):\\n-#        conn = self.open_db(filename)\\n-#        c = conn.cursor()\\n-#\\n-#        c.execute(\\n-#            \"\"\"CREATE TABLE bookmarks (\\n-#            identifier INTEGER PRIMARY KEY, \\n-#            url TEXT, \\n-#            description TEXT,\\n-#            count INTEGER,\\n-#            thumbnail TEXT,\\n-#            score REAL)\\n-#            \"\"\"\\n-#        )\\n-#\\n-#        c.execute(\\n-#            \"\"\"CREATE TABLE tags (\\n-#            identifier INTEGER PRIMARY KEY, \\n-#            tag TEXT)\\n-#            \"\"\"\\n-#        )\\n-#        c.execute(\\n-#            \"\"\"CREATE TABLE bookmarks_tags (\\n-#            bookmark REFERENCES bookmarks(identifier), \\n-#            tag REFERENCES tags(identifier))\\n-#            \"\"\"\\n-#        )\\n-#        conn.commit()\\n-#\\n-#        return conn\\n-#\\n-#    def open_database(self, filename):\\n-#        if not os.path.isfile(filename):\\n-#            return self.set_default_db(filename)\\n-#\\n-#        return self.open_db(filename)\\n-#\\n-#    def get_bookmarks(self, sorted=False):\\n-#        self.cursor.execute(\\n-#            \"\"\"select identifier, url, description, thumbnail, count from bookmarks\"\"\" + (\" order by score DESC\" if sorted else \"\")\\n-#        )\\n-#\\n-#        bookmarks = self.cursor.fetchall()\\n-#\\n-#        for id, url, desc, thumbnail, count in bookmarks:\\n-#            tags = self.get_bookmark_tags(id)\\n-#            tags = [tag for tag, id in tags]\\n-#\\n-#            yield id, url, thumbnail, tags\\n-#\\n-#    def open_bookmark(self, id):\\n-#        self.cursor.execute(f\"select url, count from bookmarks where identifier=\\'{id}\\'\")\\n-#\\n-#        url, count = self.cursor.fetchone()\\n-#\\n-#        self.hit_url(url)\\n-#\\n-#        import webbrowser\\n-#\\n-#        webbrowser.open(url)\\n-#\\n-#    def add_bookmark(self, url, tags):\\n-#        self.cursor.execute(f\\'insert into bookmarks (url,count,score) values (\"{url}\",0,1)\\')\\n-#        book_id = self.cursor.lastrowid\\n-#        for tag in tags:\\n-#            self.cursor.execute(f\\'insert into tags (tag) values (\"{tag}\")\\')\\n-#            tag_id = self.cursor.lastrowid\\n-#            self.cursor.execute(\\n-#                f\"insert into bookmarks_tags (bookmark, tag) values ({book_id}, {tag_id})\"\\n-#            )\\n-#\\n-#        self.conn.commit()\\n-#\\n-#    def rm_bookmark(self, id):\\n-#        self.cursor.execute(\\n-#            f\"delete from bookmarks_tags as bt where bt.bookmark = {id}\"\\n-#        )\\n-#        self.cursor.execute(f\"delete from bookmarks where identifier = {id}\")\\n-#        self.conn.commit()\\n-#\\n-#    def get_url(self, id):\\n-#        if id == 0:\\n-#            return None\\n-#\\n-#        self.cursor.execute(f\"select url from bookmarks where identifier={id}\")\\n-#        url = self.cursor.fetchone()\\n-#        return url\\n-#\\n-#    def get_bookmark(self, id):\\n-#        self.cursor.execute(\\n-#            f\"\"\"select identifier, url, description, thumbnail, count\\n-#                from bookmarks where identifier={id}\"\"\"\\n-#        )\\n-#\\n-#        id, url, desc, thumbnail, count = self.cursor.fetchone()\\n-#        return id, url, desc, thumbnail, count\\n-#\\n-#    def set_bookmark(self, id, url, tags):\\n-#        self.cursor.execute(f\"update bookmarks set url=\\'{url}\\' where identifier={id}\")\\n-#\\n-#        tag_set = self.bookmark_tag_list()\\n-#        _tags = [tag for tag in tags if tag not in tag_set]\\n-#        for tag in _tags:\\n-#            self.cursor.execute(f\"insert into tags (tag) values (\\'{tag}\\')\")\\n-#\\n-#        self.cursor.execute(f\"delete from bookmarks_tags as bt where bt.bookmark={id}\")\\n-#\\n-#        for tag in tags:\\n-#            tag_id = self.get_tag_id(tag)\\n-#            self.cursor.execute(\\n-#                f\"insert into bookmarks_tags as bt values ({id},{tag_id})\"\\n-#            )\\n-#\\n-#        self.conn.commit()\\n-#\\n-#    def set_thumbnail(self, id, thumbnail):\\n-#        self.cursor.execute(\\n-#            f\"update bookmarks set thumbnail=\\'{thumbnail}\\' where identifier={id}\"\\n-#        )\\n-#        self.conn.commit()\\n-#\\n-#    def edit_bookmark(self, id):\\n-#        id, url, desc, thumbnail, count = self.get_bookmark(id)\\n-#        tags = self.get_bookmark_tags(id)\\n-#\\n-#        tmp_file = \"/tmp/bookmarks.tmp\"\\n-#        with open(tmp_file, \"w\") as tmp:\\n-#            tmp.write(url + \"\\\\n\")\\n-#\\n-#            for tag, tag_id in tags:\\n-#                tmp.write(tag + \"\\\\n\")\\n-#\\n-#        term = os.path.expandvars(\"$TERM\")\\n-#        editor = os.path.expandvars(\"$EDITOR\")\\n-#        call([term, \"-e\", editor, tmp_file])\\n-#\\n-#        with open(tmp_file, \"r\") as tmp:\\n-#            lines = tmp.readlines()\\n-#\\n-#        lines = [l.strip(\"\\\\n\") for l in lines if l != \"\"]\\n-#\\n-#        url = lines[0]\\n-#        tags = [tag for tag in lines[1:]]\\n-#\\n-#        self.set_bookmark(id, url, tags)\\n-#\\n-#    def get_bookmark_tags(self, id):\\n-#        self.cursor.execute(\\n-#            f\"\"\"select tags.tag, tags.identifier from\\n-#            bookmarks_tags as bt, tags where bt.bookmark={id} and bt.tag = tags.identifier\"\"\"\\n-#        )\\n-#        return list(self.cursor.fetchall())\\n-#\\n-#    def bookmark_tag_search(self, tag):\\n-#        self.cursor.execute(f\"select identifier from tags where tag=\\'{tag}\\'\")\\n-#        r = self.cursor.fetchone()\\n-#        if r == None:\\n-#            return []\\n-#        id = r[0]\\n-#\\n-#        self.cursor.execute(\\n-#            f\"select bt.bookmark from bookmarks_tags as bt where bt.tag = {id}\"\\n-#        )\\n-#        bookmarks = self.cursor.fetchall()\\n-#\\n-#        for _book in bookmarks:\\n-#            book = _book[0]\\n-#            self.cursor.execute(\\n-#                    f\"\"\"select identifier, url, description, count\\n-#                    from bookmarks where identifier = {book}\"\"\")\\n-#\\n-#            id, url, desc, count = self.cursor.fetchone()\\n-#            yield id, url, desc, count\\n-#\\n-#    def bookmark_tag_list(self):\\n-#        self.cursor.execute(\"select tag from tags\")\\n-#        tags = self.cursor.fetchall()\\n-#\\n-#        for tag in tags:\\n-#            yield tag[0]\\n-#\\n-#    def get_tag_id(self, tag):\\n-#        self.cursor.execute(f\"select identifier from tags where tag=\\'{tag}\\'\")\\n-#        r = self.cursor.fetchone()\\n-#        return None if r == None else r[0]\\n-#\\n-#    def hit_url(self, url):\\n-#        self.cursor.execute(f\"select identifier, count, score from bookmarks where url=\\'{url}\\'\")\\n-#        id, count, score = self.cursor.fetchone()\\n-#        count = int(count)\\n-#        count += 1\\n-#        score += 1\\n-#\\n-#        self.cursor.execute(f\"update bookmarks set score = score*0.95 where identifier<>\\'{id}\\'\")\\n-#        self.cursor.execute(\\n-#            f\"update bookmarks set count = {count}, score = {score} where identifier=\\'{id}\\'\")\\n-#\\n-#        self.conn.commit()\\n-#\\n-#\\n-#def bookmark_to_str(bookmark):\\n-#    id, url, thumbnail, tags = bookmark\\n-#    output = f\"{id}, {url} \"\\n-#    for tag in tags:\\n-#        output += f\"{tag},\"\\n-#\\n-#    output = output[:-1] + \"\\\\n\"\\n-#    return output'], 'file': ['marcador/version.py', 'marcador/scripts/marcador.py', 'marcador/rofi_marcador.py', 'marcador/server.py', 'websockets/server.py', 'marcador/bookmarkimporter.py', 'marcador/rofi.py', 'marcador/proxy.py', 'marcador/json_backend.py', 'marcador/lib.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('957a7a89-5870-445e-ae72-c827ac86ac23'), UUID('3339894b-b2fe-4be8-9d6f-b6e897245add'), UUID('9ac0635f-9634-4139-bd7e-e3d1299636fd'), UUID('3b17e3d2-d851-4660-9699-53bbc81e0f29'), UUID('8bdc1d7d-9a3e-4b73-8245-7655d0c230c7'), UUID('62d4bd40-8916-4146-bed0-c6300524b02a'), UUID('857d88f6-9092-4157-b52d-cc711c4cdacb'), UUID('836f87e3-6fad-4b82-b476-219752f27806'), UUID('515f9999-75c3-4916-973a-64250941a05c'), UUID('9368b6f4-d06e-4c50-962d-ffa50b77e305')]}\n",
      "ERROR:root:Error in {'repo': 'joajfreitas/marcador', 'vulnerability_id': '2022-28470', 'commit': '84d84549c6c6d765abc9243ac7e85d810f32d6e7', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -1 +1 @@\\n-version = \\'0.16\\'\\n+version = \"0.16\"', '@@ -1,268 +0,0 @@\\n-#!/usr/bin/env python3\\n-\\n-from pathlib import Path\\n-from pprint import pprint\\n-\\n-import click\\n-import jinja2\\n-from appdirs import user_data_dir\\n-from selenium import webdriver\\n-\\n-from marcador import version\\n-from marcador.marcador_lib import (Bookmark, BookmarkTag, Database, Tag,\\n-                                   bookmark_to_str, get_session)\\n-from marcador.rofi_marcador import RofiMarcador\\n-\\n-\\n-from flask import Flask, jsonify, Response, send_from_directory, render_template, request, redirect\\n-from flask_cors import CORS\\n-\\n-\\n-def get_user_data_dir():\\n-    appauthor = \"joajfreitas\"\\n-    appname = \"marcador\"\\n-    \\n-    return user_data_dir(appname, appauthor)\\n-\\n-def get_db_path():\\n-    return Path(get_user_data_dir()) / Path(\"marcador.sqlite\")\\n-\\n-@click.command(name=\\'open\\')\\n-@click.argument(\\'url\\')\\n-def open_bookmark_cmd(url):\\n-    session = get_session(get_db_path())\\n-    bookmark = session.query(Bookmark).filter(Bookmark.url == url).one()\\n-    print(bookmark)\\n-\\n-    import webbrowser\\n-    webbrowser.open(bookmark.url)\\n-\\n-@click.command()\\n-@click.argument(\\'url\\')\\n-@click.argument(\\'tags\\', nargs=-1)\\n-def add(url, tags):\\n-    session = get_session(get_db_path())\\n-    \\n-    bookmark = Bookmark( \\n-            url = url, \\n-            description = \"\", \\n-            count = 0, \\n-            thumbnail = \"\", \\n-            score = 0)\\n-\\n-    session.add(bookmark)\\n-\\n-    for tag in tags:\\n-        tag = Tag(tag=tag)\\n-        session.add(tag)\\n-\\n-        bookmark_tag = BookmarkTag(url=url, tag=tag.tag)\\n-        session.add(bookmark_tag)\\n-    \\n-    session.commit()\\n-\\n-\\n-@click.command(name=\\'bookmarks\\')\\n-def print_bookmarks():\\n-    session = get_session(get_db_path())\\n-    \\n-    bookmarks = session.query(Bookmark).all()\\n-    pprint(bookmarks)\\n-\\n-@click.command(name=\\'bookmark\\')\\n-@click.argument(\\'url\\')\\n-def print_bookmark(url):\\n-    session = get_session(get_db_path())\\n-    \\n-    bookmark = session.query(Bookmark).filter(Bookmark.url == url).one()\\n-\\n-    pprint(bookmark)\\n-    pprint([bt.tag for bt in session.query(BookmarkTag).filter(BookmarkTag.url == url).all()])\\n-\\n-\\n-@click.command(name=\\'tags\\')\\n-def print_tags():\\n-    session = get_session(get_db_path())\\n-    \\n-    tags = session.query(Tag).all()\\n-    pprint(tags)\\n-\\n-\\n-@click.command(name=\\'tag\\')\\n-@click.argument(\\'tag\\')\\n-def print_tag(tag):\\n-    session = get_session(get_db_path())\\n-    \\n-    tag = session.query(Tag).filter(Tag.tag == tag).one()\\n-    pprint(tag)\\n-    \\n-    pprint([bt.url for bt in session.query(BookmarkTag).filter(BookmarkTag.tag == tag.tag).all()])\\n-\\n-@click.command()\\n-@click.argument(\\'url\\')\\n-def delete(url):\\n-    session = get_session(get_db_path())\\n-    session.query(Bookmark).filter(Bookmark.url == url).delete()\\n-    session.query(BookmarkTag).filter(BookmarkTag.url == url).delete()\\n-\\n-    session.commit()\\n-\\n-\\n-@click.command(name=\\'url\\')\\n-@click.argument(\\'id\\')\\n-def get_url(id):\\n-    session = get_session(get_db_path())\\n-    print(session.query(Bookmark).filter(Bookmark.identifier==id).one().url)\\n-\\n-\\n-@click.command(name=\\'bookmark\\')\\n-@click.argument(\\'id\\')\\n-def get_bookmark(id):\\n-    session = get_session(get_db_path())\\n-    print(session.query(Bookmark).filter(Bookmark.identifier == id).one())\\n-\\n-@click.command()\\n-@click.argument(\\'filename\\')\\n-@click.argument(\\'id\\')\\n-def edit(filename, id):\\n-    db = Database(filename)\\n-    db.edit_bookmark(id)\\n-\\n-@click.command(name=\\'html\\')\\n-@click.argument(\\'template\\')\\n-def html(template):\\n-    app = Flask(__name__)\\n-    app.config.from_object(__name__)\\n-\\n-    CORS(app, resources={r\\'/*\\': {\\'origins\\': \\'*\\'}})\\n-\\n-    with open(template) as t:\\n-        template = jinja2.Template(t.read())\\n-\\n-    @app.route(\\'/\\')\\n-    def index():\\n-        session = get_session(get_db_path())\\n-        bookmarks = session.query(Bookmark).order_by(Bookmark.score.desc()).all()\\n-\\n-        bookmarks = [(\"/redirect?url=\" + book.url, book.thumbnail) for book in bookmarks]\\n-\\n-        return template.render(bookmarks=bookmarks)\\n-\\n-    \\n-    @app.route(\\'/bookmarks\\')\\n-    def bookmarks():\\n-        session = get_session(get_db_path())\\n-        bookmarks = session.query(Bookmark).order_by(Bookmark.score.desc()).all()\\n-        \\n-        bookmarks_list = []\\n-        for bookmark in bookmarks:\\n-            bookmark = {\\n-                \"url\": bookmark.url,\\n-                \"thumb\": \"127.0.0.1:5000/\" + bookmark.thumbnail,\\n-                \"score\": bookmark.score,\\n-            }\\n-\\n-            bookmarks_list.append(bookmark)\\n-\\n-        return jsonify(bookmarks_list)\\n-\\n-\\n-    # sanity check route\\n-    @app.route(\\'/ping\\', methods=[\\'GET\\'])\\n-    def ping_pong():\\n-        return jsonify(\\'pong!\\')\\n-\\n-\\n-    @app.route(\\'/thumbnails/<filepath>\\', methods=[\\'GET\\'])\\n-    def thumbnails(filepath):  # pragma: no cover\\n-        return send_from_directory(str(get_user_data_dir() / Path(\".thumbnails\")), filepath)\\n-\\n-    @app.route(\\'/redirect\\')\\n-    def _redirect():\\n-        session = get_session(get_db_path())\\n-        url = request.args[\\'url\\']\\n-\\n-        bookmark = session.query(Bookmark).filter(Bookmark.url == url).one()\\n-\\n-        if bookmark.score == None:\\n-            bookmark.score = 0\\n-\\n-        if bookmark.count == None:\\n-            bookmark.count = 0\\n-\\n-        bookmark.score += 1\\n-        bookmark.count += 1\\n-\\n-        session.commit()\\n-\\n-        return redirect(url)\\n-\\n-    app.run()\\n-\\n-\\n-@click.command(name=\"gen_thumbnails\")\\n-def gen_thumbnails():\\n-    session = get_session(get_db_path())\\n-    user_data_dir = Path(get_user_data_dir())\\n-\\n-    bookmarks = session.query(Bookmark).all()\\n-\\n-    thumbnail_dir  = user_data_dir / Path(\".thumbnails/\")\\n-    thumbnail_dir.mkdir(parents=True, exist_ok=True)\\n-\\n-    thumbnails = thumbnail_dir.glob(\"*\")\\n-\\n-    driver = webdriver.Firefox()\\n-    for bookmark in bookmarks:\\n-        image_path = str(hash(bookmark.url)) + \".png\"\\n-        if image_path in thumbnails:\\n-            print(\"skiped\", bookmark.url)\\n-            continue\\n-        print(\"getting thumbnail for:\", bookmark.url)\\n-        try:\\n-            driver.get(bookmark.url)\\n-            thumbnail_path = image_path\\n-            driver.save_screenshot(str(thumbnail_dir / thumbnail_path))\\n-            bookmark.thumbnail = str(Path(\"thumbnails\") / thumbnail_path)\\n-        except Exception as e:\\n-            print(\"Error: \" + str(e))\\n-            continue\\n-    \\n-    session.commit()\\n-\\n-@click.command(name=\"rofi\")\\n-def rofi_launch():\\n-    session = get_session(get_db_path())\\n-    rm = RofiMarcador(session)\\n-    rm.launch()\\n-\\n-\\n-@click.group(invoke_without_command=True)\\n-@click.version_option(version)\\n-def cli():\\n-    db_path = get_db_path()\\n-    if not db_path.is_file():\\n-        print(db_path)\\n-        db_path.parent.mkdir(exist_ok=True)\\n-        db_path.touch()\\n-\\n-    return\\n-\\n-   \\n-cli.add_command(print_bookmarks)\\n-cli.add_command(print_bookmark)\\n-cli.add_command(print_tags)\\n-cli.add_command(print_tag)\\n-\\n-cli.add_command(open_bookmark_cmd)\\n-cli.add_command(add)\\n-cli.add_command(delete)\\n-cli.add_command(get_url)\\n-#cli.add_command(get_bookmark)\\n-cli.add_command(edit)\\n-cli.add_command(html)\\n-cli.add_command(gen_thumbnails)\\n-cli.add_command(rofi_launch)\\n-\\n-if __name__ == \"__main__\":\\n-    cli()', '@@ -1,10 +1,11 @@\\n from webbrowser import open\\n from .rofi import Rofi\\n-from .lib import Bookmark, Tag, BookmarkTag\\n+\\n+# from .lib import Bookmark, Tag, BookmarkTag\\n import clipboard\\n \\n \\n-class RofiMarcador():\\n+class RofiMarcador:\\n     def __init__(self, proxy):\\n         self.rofi = Rofi()\\n         self.proxy = proxy\\n@@ -37,7 +38,7 @@ def delete(self, index):\\n         self.proxy.delete(bookmark.url)\\n \\n     def edit(self, index):\\n-        i = self.bookmarks[index].split(\\',\\')[0]\\n+        i = self.bookmarks[index].split(\",\")[0]\\n         self.db.edit_bookmark(i)\\n         self.launch()\\n         return\\n@@ -54,10 +55,12 @@ def dispatch(self, index, key):\\n \\n     def launch(self):\\n         self.bookmarks = self.list()\\n-        ret = self.rofi.select(\"> \",\\n-                          self.bookmarks,\\n-                          key1=(\\'Alt+n\\', \"Add new bookmark\"),\\n-                          key2=(\\'Alt+d\\', \"Delete the selected bookmark\"),\\n-                          key3=(\\'Alt+e\\', \"Edit the selected bookmark\"))\\n+        ret = self.rofi.select(\\n+            \"> \",\\n+            self.bookmarks,\\n+            key1=(\"Alt+n\", \"Add new bookmark\"),\\n+            key2=(\"Alt+d\", \"Delete the selected bookmark\"),\\n+            key3=(\"Alt+e\", \"Edit the selected bookmark\"),\\n+        )\\n         index, key = ret\\n         self.dispatch(index, key)', '@@ -2,55 +2,50 @@\\n import json\\n import socket\\n \\n-from marcador.lib import get_session, get_db_path, Bookmark\\n+from marcador.lib import get_db_path\\n+from marcador.json_backend import JsonProxy\\n+\\n \\n def marcador_list(session, args):\\n-    bookmarks = session.query(Bookmark).all()\\n-    return [{\\n-        \\'url\\': bookmark.url,\\n-        \\'description\\': bookmark.description,\\n-        \\'count\\': bookmark.count,\\n-        \\'score\\': bookmark.score\\n-    } for bookmark in bookmarks]\\n+    bookmarks = session.list()\\n+    return [\\n+        {\\n+            \"url\": bookmark.url,\\n+            \"description\": bookmark.description,\\n+            \"tags\": bookmark.tags,\\n+        }\\n+        for bookmark in bookmarks\\n+    ]\\n+\\n \\n def marcador_add(session, args):\\n-    bookmark = Bookmark(url=args.get(\\'url\\'))\\n-    session.add(bookmark)\\n-    session.commit()\\n+    session.add(args.get(\"url\"), args.get(\"description\"), args.get(\"tags\"))\\n \\n-def marcador_tag(session, args):\\n-    tag = Tag(tag=args.get(\\'tag\\'))\\n-    self.session.add(tag)\\n-    book_tag = BookmarkTag(url=args.get(\\'url\\'), tag=args.get(\\'tag\\'))\\n-    session.add(book_tag)\\n-    session.commit()\\n \\n def marcador_delete(session, args):\\n-    session.query(Bookmark).filter(Bookmark.url == args.get(\\'url\\')).delete()\\n-    session.commit()\\n+    session.delete(arg.get(\"url\"))\\n+\\n \\n @click.command()\\n-@click.option(\\'--hostname\\', default=\\'127.0.0.1\\')\\n-@click.option(\\'--port\\', type=int, default=6003)\\n+@click.option(\"--hostname\", default=\"127.0.0.1\")\\n+@click.option(\"--port\", type=int, default=6003)\\n def server(hostname, port):\\n-    session = get_session(get_db_path())\\n+    session = JsonProxy(get_db_path())\\n \\n     sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\\n     sock.bind((hostname, port))\\n \\n     cmds = {\\n-        \\'list\\': marcador_list,\\n-        \\'add\\': marcador_add,\\n-        \\'tag\\': marcador_tag,\\n-        \\'delete\\': marcador_delete,\\n+        \"list\": marcador_list,\\n+        \"add\": marcador_add,\\n+        \"delete\": marcador_delete,\\n     }\\n \\n     while True:\\n         try:\\n             msg, addr = sock.recvfrom(1024)\\n             msg = json.loads(msg)\\n-            ret = cmds[msg[\\'cmd\\']](session, msg[\\'args\\'])\\n-            sock.sendto(bytes(json.dumps(ret), \\'utf-8\\'), addr)\\n+            ret = cmds[msg[\"cmd\"]](session, msg[\"args\"])\\n+            sock.sendto(bytes(json.dumps(ret), \"utf-8\"), addr)\\n         except Exception as e:\\n             continue\\n-', '@@ -3,12 +3,15 @@\\n import asyncio\\n from websockets import serve\\n \\n+\\n async def echo(websocket, path):\\n     async for message in websocket:\\n         await websocket.send(message)\\n \\n+\\n async def main():\\n     async with serve(echo, \"localhost\", 8765):\\n         await asyncio.Future()  # run forever\\n \\n+\\n asyncio.run(main())', '@@ -5,20 +5,21 @@\\n import os\\n from pprint import pprint\\n \\n+\\n class BookmarkHTMLParser(html.parser.HTMLParser):\\n     def __init__(self):\\n         self.parser = html.parser.HTMLParser.__init__(self)\\n-        self.bookmarks = {\\'None\\': []}\\n+        self.bookmarks = {\"None\": []}\\n         self.link = \"\"\\n         self.h3 = False\\n         self.tag = None\\n \\n     def handle_starttag(self, tag, attrs):\\n-        if tag == \\'a\\':\\n+        if tag == \"a\":\\n             for attr in attrs:\\n-                if attr[0] == \\'href\\':\\n+                if attr[0] == \"href\":\\n                     self.link = attr[1]\\n-        if tag == \\'h3\\':\\n+        if tag == \"h3\":\\n             self.h3 = True\\n \\n     def handle_data(self, data):\\n@@ -28,31 +29,34 @@ def handle_data(self, data):\\n             self.h3 = False\\n \\n     def handle_endtag(self, tag):\\n-        if tag == \\'a\\':\\n-            self.bookmarks[str(self.tag)].append(self.link) \\n+        if tag == \"a\":\\n+            self.bookmarks[str(self.tag)].append(self.link)\\n+\\n \\n def parse_args():\\n-    parser = argparse.ArgumentParser(description=\\'Import bookmarks from standard html file to rofi-bookmarks.\\')\\n-    parser.add_argument(\\'inputfile\\', metavar=\\'htmlfile\\' )\\n-    parser.add_argument(\\'outputfile\\', metavar=\\'bookmarkfile\\')\\n+    parser = argparse.ArgumentParser(\\n+        description=\"Import bookmarks from standard html file to rofi-bookmarks.\"\\n+    )\\n+    parser.add_argument(\"inputfile\", metavar=\"htmlfile\")\\n+    parser.add_argument(\"outputfile\", metavar=\"bookmarkfile\")\\n \\n     return parser.parse_args()\\n \\n \\n def main(args):\\n     if os.path.isfile(args.outputfile):\\n-        with open(args.outputfile,\\'r\\') as f:\\n-            last = int(f.readlines()[-1].split(\\'.\\')[0]) + 1\\n+        with open(args.outputfile, \"r\") as f:\\n+            last = int(f.readlines()[-1].split(\".\")[0]) + 1\\n     else:\\n-        last = 0 \\n+        last = 0\\n \\n-    inf= open(args.inputfile,\\'r\\')\\n-    outf = open(args.outputfile,\\'a\\')\\n+    inf = open(args.inputfile, \"r\")\\n+    outf = open(args.outputfile, \"a\")\\n \\n     p = BookmarkHTMLParser()\\n-    \\n+\\n     p.feed(inf.read())\\n-    \\n+\\n     for k, v in p.bookmarks.items():\\n         for link in v:\\n             outf.write(f\"{str(last)}. {link} {k}\\\\n\")', '@@ -33,9 +33,10 @@\\n \\n # Python < 3.2 doesn\\'t provide a context manager interface for Popen.\\n # Let\\'s make our own wrapper if needed.\\n-if hasattr(subprocess.Popen, \\'__exit__\\'):\\n+if hasattr(subprocess.Popen, \"__exit__\"):\\n     Popen = subprocess.Popen\\n else:\\n+\\n     class ContextManagedPopen(subprocess.Popen):\\n         def __enter__(self):\\n             return self\\n@@ -48,6 +49,7 @@ def __exit__(self, type, value, traceback):\\n             if self.stdin:\\n                 self.stdin.close()\\n             self.wait()\\n+\\n     Popen = ContextManagedPopen\\n \\n \\n@@ -71,9 +73,17 @@ class Rofi(object):\\n     for available markup.\\n \\n     \"\"\"\\n-    def __init__(self, lines=None, fixed_lines=None, width=None,\\n-                 fullscreen=None, location=None,\\n-                 exit_hotkeys=(\\'Alt+F4\\', \\'Control+q\\'), rofi_args=None):\\n+\\n+    def __init__(\\n+        self,\\n+        lines=None,\\n+        fixed_lines=None,\\n+        width=None,\\n+        fullscreen=None,\\n+        location=None,\\n+        exit_hotkeys=(\"Alt+F4\", \"Control+q\"),\\n+        rofi_args=None,\\n+    ):\\n         \"\"\"\\n         Parameters\\n         ----------\\n@@ -124,7 +134,6 @@ def __init__(self, lines=None, fixed_lines=None, width=None,\\n         # (e.g., an unhandled exception).\\n         atexit.register(self.close)\\n \\n-\\n     @classmethod\\n     def escape(self, string):\\n         \"\"\"Escape a string for Pango markup.\\n@@ -143,16 +152,9 @@ def escape(self, string):\\n         # dictionary, we can\\'t guarantee order of translations and so doing it\\n         # in one go would risk the ampersands in other translations being\\n         # escaped again.\\n-        return string.translate(\\n-            {38: \\'&amp;\\'}\\n-        ).translate({\\n-            34: \\'&quot;\\',\\n-            39: \\'&apos;\\',\\n-            60: \\'&lt;\\',\\n-            62: \\'&gt;\\'\\n-        })\\n-\\n-\\n+        return string.translate({38: \"&amp;\"}).translate(\\n+            {34: \"&quot;\", 39: \"&apos;\", 60: \"&lt;\", 62: \"&gt;\"}\\n+        )\\n \\n     def close(self):\\n         \"\"\"Close any open window.\\n@@ -167,7 +169,7 @@ def close(self):\\n             # If it doesn\\'t close itself promptly, be brutal.\\n             # Python 3.2+ added the timeout option to wait() and the\\n             # corresponding TimeoutExpired exception. If they exist, use them.\\n-            if hasattr(subprocess, \\'TimeoutExpired\\'):\\n+            if hasattr(subprocess, \"TimeoutExpired\"):\\n                 try:\\n                     self._process.wait(timeout=1)\\n                 except subprocess.TimeoutExpired:\\n@@ -189,7 +191,6 @@ def close(self):\\n             # Clean up.\\n             self._process = None\\n \\n-\\n     def _run_blocking(self, args, input=None):\\n         \"\"\"Internal API: run a blocking command with subprocess.\\n \\n@@ -214,17 +215,17 @@ def _run_blocking(self, args, input=None):\\n \\n         # Make sure we grab stdout as text (not bytes).\\n         kwargs = {}\\n-        kwargs[\\'stdout\\'] = subprocess.PIPE\\n-        kwargs[\\'universal_newlines\\'] = True\\n+        kwargs[\"stdout\"] = subprocess.PIPE\\n+        kwargs[\"universal_newlines\"] = True\\n \\n         # Use the run() method if available (Python 3.5+).\\n-        if hasattr(subprocess, \\'run\\'):\\n+        if hasattr(subprocess, \"run\"):\\n             result = subprocess.run(args, input=input, **kwargs)\\n             return result.returncode, result.stdout\\n \\n         # Have to do our own. If we need to feed stdin, we must open a pipe.\\n         if input is not None:\\n-            kwargs[\\'stdin\\'] = subprocess.PIPE\\n+            kwargs[\"stdin\"] = subprocess.PIPE\\n \\n         # Start the process.\\n         with Popen(args, **kwargs) as proc:\\n@@ -237,7 +238,6 @@ def _run_blocking(self, args, input=None):\\n             # Done.\\n             return returncode, stdout\\n \\n-\\n     def _run_nonblocking(self, args, input=None):\\n         \"\"\"Internal API: run a non-blocking command with subprocess.\\n \\n@@ -258,40 +258,38 @@ def _run_nonblocking(self, args, input=None):\\n         # Start the new one.\\n         self._process = subprocess.Popen(args, stdout=subprocess.PIPE)\\n \\n-\\n     def _common_args(self, allow_fullscreen=True, **kwargs):\\n         args = []\\n \\n         # Number of lines.\\n-        lines = kwargs.get(\\'lines\\', self.lines)\\n+        lines = kwargs.get(\"lines\", self.lines)\\n         if lines:\\n-            args.extend([\\'-lines\\', str(lines)])\\n-        fixed_lines = kwargs.get(\\'fixed_lines\\', self.fixed_lines)\\n+            args.extend([\"-lines\", str(lines)])\\n+        fixed_lines = kwargs.get(\"fixed_lines\", self.fixed_lines)\\n         if fixed_lines:\\n-            args.extend([\\'-fixed-num-lines\\', str(fixed_lines)])\\n+            args.extend([\"-fixed-num-lines\", str(fixed_lines)])\\n \\n         # Width.\\n-        width = kwargs.get(\\'width\\', self.width)\\n+        width = kwargs.get(\"width\", self.width)\\n         if width is not None:\\n-            args.extend([\\'-width\\', str(width)])\\n+            args.extend([\"-width\", str(width)])\\n \\n         # Fullscreen mode?\\n-        fullscreen = kwargs.get(\\'fullscreen\\', self.fullscreen)\\n+        fullscreen = kwargs.get(\"fullscreen\", self.fullscreen)\\n         if allow_fullscreen and fullscreen:\\n-            args.append(\\'-fullscreen\\')\\n+            args.append(\"-fullscreen\")\\n \\n         # Location on screen.\\n-        location = kwargs.get(\\'location\\', self.location)\\n+        location = kwargs.get(\"location\", self.location)\\n         if location is not None:\\n-            args.extend([\\'-location\\', str(location)])\\n+            args.extend([\"-location\", str(location)])\\n \\n         # Any other arguments\\n         args.extend(self.rofi_args)\\n \\n         # Done.\\n         return args\\n \\n-\\n     def error(self, message, rofi_args=None, **kwargs):\\n         \"\"\"Show an error window.\\n \\n@@ -308,14 +306,13 @@ def error(self, message, rofi_args=None, **kwargs):\\n         \"\"\"\\n         rofi_args = rofi_args or []\\n         # Generate arguments list.\\n-        args = [\\'rofi\\', \\'-e\\', message]\\n+        args = [\"rofi\", \"-e\", message]\\n         args.extend(self._common_args(allow_fullscreen=False, **kwargs))\\n         args.extend(rofi_args)\\n \\n         # Close any existing window and show the error.\\n         self._run_blocking(args)\\n \\n-\\n     def status(self, message, rofi_args=None, **kwargs):\\n         \"\"\"Show a status message.\\n \\n@@ -336,15 +333,16 @@ def status(self, message, rofi_args=None, **kwargs):\\n         \"\"\"\\n         rofi_args = rofi_args or []\\n         # Generate arguments list.\\n-        args = [\\'rofi\\', \\'-e\\', message]\\n+        args = [\"rofi\", \"-e\", message]\\n         args.extend(self._common_args(allow_fullscreen=False, **kwargs))\\n         args.extend(rofi_args)\\n \\n         # Update the status.\\n         self._run_nonblocking(args)\\n \\n-\\n-    def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwargs):\\n+    def select(\\n+        self, prompt, options, rofi_args=None, message=\"\", select=None, **kwargs\\n+    ):\\n         \"\"\"Show a list of options and return user selection.\\n \\n         This method blocks until the user makes their choice.\\n@@ -387,12 +385,12 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n         \"\"\"\\n         rofi_args = rofi_args or []\\n         # Replace newlines and turn the options into a single string.\\n-        optionstr = \\'\\\\n\\'.join(option.replace(\\'\\\\n\\', \\' \\') for option in options)\\n+        optionstr = \"\\\\n\".join(option.replace(\"\\\\n\", \" \") for option in options)\\n \\n         # Set up arguments.\\n-        args = [\\'rofi\\', \\'-dmenu\\', \\'-p\\', prompt, \\'-format\\', \\'i\\']\\n+        args = [\"rofi\", \"-dmenu\", \"-p\", prompt, \"-format\", \"i\"]\\n         if select is not None:\\n-            args.extend([\\'-selected-row\\', str(select)])\\n+            args.extend([\"-selected-row\", str(select)])\\n \\n         # Key bindings to display.\\n         display_bindings = []\\n@@ -401,7 +399,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n         user_keys = set()\\n         for k, v in kwargs.items():\\n             # See if the keyword name matches the needed format.\\n-            if not k.startswith(\\'key\\'):\\n+            if not k.startswith(\"key\"):\\n                 continue\\n             try:\\n                 keynum = int(k[3:])\\n@@ -411,7 +409,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n             # Add it to the set.\\n             key, action = v\\n             user_keys.add(keynum)\\n-            args.extend([\\'-kb-custom-{0:s}\\'.format(k[3:]), key])\\n+            args.extend([\"-kb-custom-{0:s}\".format(k[3:]), key])\\n             if action:\\n                 display_bindings.append(\"<b>{0:s}</b>: {1:s}\".format(key, action))\\n \\n@@ -422,7 +420,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n             while next_key in user_keys:\\n                 next_key += 1\\n             exit_keys.add(next_key)\\n-            args.extend([\\'-kb-custom-{0:d}\\'.format(next_key), key])\\n+            args.extend([\"-kb-custom-{0:d}\".format(next_key), key])\\n             next_key += 1\\n \\n         # Add any displayed key bindings to the message.\\n@@ -433,7 +431,7 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n \\n         # If we have a message, add it to the arguments.\\n         if message:\\n-            args.extend([\\'-mesg\\', message])\\n+            args.extend([\"-mesg\", message])\\n \\n         # Add in common arguments.\\n         args.extend(self._common_args(**kwargs))\\n@@ -456,13 +454,22 @@ def select(self, prompt, options, rofi_args=None, message=\"\", select=None, **kwa\\n             if key in exit_keys:\\n                 raise SystemExit()\\n         else:\\n-            self.exit_with_error(\"Unexpected rofi returncode {0:d}.\".format(results.returncode))\\n+            self.exit_with_error(\\n+                \"Unexpected rofi returncode {0:d}.\".format(results.returncode)\\n+            )\\n \\n         # And return.\\n         return index, key\\n \\n-\\n-    def generic_entry(self, prompt, validator=None, message=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+    def generic_entry(\\n+        self,\\n+        prompt,\\n+        validator=None,\\n+        message=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"A generic entry box.\\n \\n         Parameters\\n@@ -502,17 +509,19 @@ def generic_entry(self, prompt, validator=None, message=None, rofi_args=None, st\\n \\n         # Keep going until we get something valid.\\n         while True:\\n-            args = [\\'rofi\\', \\'-dmenu\\', \\'-p\\', prompt, \\'-format\\', \\'s\\']\\n+            args = [\"rofi\", \"-dmenu\", \"-p\", prompt, \"-format\", \"s\"]\\n \\n             # Add any error to the given message.\\n             msg = message or \"\"\\n             if error:\\n-                msg = \\'<span color=\"#FF0000\" font_weight=\"bold\">{0:s}</span>\\\\n{1:s}\\'.format(error, msg)\\n-                msg = msg.rstrip(\\'\\\\n\\')\\n+                msg = \\'<span color=\"#FF0000\" font_weight=\"bold\">{0:s}</span>\\\\n{1:s}\\'.format(\\n+                    error, msg\\n+                )\\n+                msg = msg.rstrip(\"\\\\n\")\\n \\n             # If there is actually a message to show.\\n             if msg:\\n-                args.extend([\\'-mesg\\', msg])\\n+                args.extend([\"-mesg\", msg])\\n \\n             # Add in common arguments.\\n             args.extend(self._common_args(**kwargs))\\n@@ -526,17 +535,24 @@ def generic_entry(self, prompt, validator=None, message=None, rofi_args=None, st\\n                 return None\\n \\n             # Get rid of the trailing newline and check its validity.\\n-            text = stdout.rstrip(\\'\\\\n\\')\\n+            text = stdout.rstrip(\"\\\\n\")\\n             if validator:\\n                 value, error = validator(text)\\n                 if not error:\\n                     return value\\n             else:\\n                 return text\\n \\n-\\n-    def text_entry(self, prompt, message=None, allow_blank=False, strip=True,\\n-            rofi_args=None, stdin_str=\"\", **kwargs):\\n+    def text_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        allow_blank=False,\\n+        strip=True,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a piece of text.\\n \\n         Parameters\\n@@ -558,6 +574,7 @@ def text_entry(self, prompt, message=None, allow_blank=False, strip=True,\\n         string, or None if the dialog was cancelled.\\n \\n         \"\"\"\\n+\\n         def text_validator(text):\\n             if strip:\\n                 text = text.strip()\\n@@ -567,10 +584,20 @@ def text_validator(text):\\n \\n             return text, None\\n \\n-        return self.generic_entry(prompt, text_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def integer_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, text_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def integer_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        min=None,\\n+        max=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter an integer.\\n \\n         Parameters\\n@@ -608,10 +635,20 @@ def integer_validator(text):\\n \\n             return value, None\\n \\n-        return self.generic_entry(prompt, integer_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def float_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, integer_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def float_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        min=None,\\n+        max=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a floating point number.\\n \\n         Parameters\\n@@ -649,10 +686,20 @@ def float_validator(text):\\n \\n             return value, None\\n \\n-        return self.generic_entry(prompt, float_validator, message, rofi_args, stdin_str,stdin_str,  **kwargs)\\n-\\n-\\n-    def decimal_entry(self, prompt, message=None, min=None, max=None, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, float_validator, message, rofi_args, stdin_str, stdin_str, **kwargs\\n+        )\\n+\\n+    def decimal_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        min=None,\\n+        max=None,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a decimal number.\\n \\n         Parameters\\n@@ -690,11 +737,20 @@ def decimal_validator(text):\\n \\n             return value, None\\n \\n-        return self.generic_entry(prompt, decimal_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def date_entry(self, prompt, message=None, formats=[\\'%x\\', \\'%d/%m/%Y\\'],\\n-            show_example=False, rofi_args=None, stdin_str=\"\", **kwargs):\\n+        return self.generic_entry(\\n+            prompt, decimal_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def date_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        formats=[\"%x\", \"%d/%m/%Y\"],\\n+        show_example=False,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a date.\\n \\n         Parameters\\n@@ -719,6 +775,7 @@ def date_entry(self, prompt, message=None, formats=[\\'%x\\', \\'%d/%m/%Y\\'],\\n         datetime.date, or None if the dialog is cancelled.\\n \\n         \"\"\"\\n+\\n         def date_validator(text):\\n             # Try them in order.\\n             for format in formats:\\n@@ -731,18 +788,29 @@ def date_validator(text):\\n                     return (dt.date(), None)\\n \\n             # None of the formats worked.\\n-            return (None, \\'Please enter a valid date.\\')\\n+            return (None, \"Please enter a valid date.\")\\n \\n         # Add an example to the message?\\n         if show_example:\\n             message = message or \"\"\\n-            message += \"Today\\'s date in the correct format: \" + datetime.now().strftime(formats[0])\\n-\\n-        return self.generic_entry(prompt, date_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n-\\n-    def time_entry(self, prompt, message=None, formats=[\\'%X\\', \\'%H:%M\\', \\'%I:%M\\', \\'%H.%M\\',\\n-        \\'%I.%M\\'], show_example=False, rofi_args=None, stdin_str=\"\", **kwargs):\\n+            message += \"Today\\'s date in the correct format: \" + datetime.now().strftime(\\n+                formats[0]\\n+            )\\n+\\n+        return self.generic_entry(\\n+            prompt, date_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n+\\n+    def time_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        formats=[\"%X\", \"%H:%M\", \"%I:%M\", \"%H.%M\", \"%I.%M\"],\\n+        show_example=False,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a time.\\n \\n         Parameters\\n@@ -767,6 +835,7 @@ def time_entry(self, prompt, message=None, formats=[\\'%X\\', \\'%H:%M\\', \\'%I:%M\\', \\'%H.\\n         datetime.time, or None if the dialog is cancelled.\\n \\n         \"\"\"\\n+\\n         def time_validator(text):\\n             # Try them in order.\\n             for format in formats:\\n@@ -779,18 +848,34 @@ def time_validator(text):\\n                     return (dt.time(), None)\\n \\n             # None of the formats worked.\\n-            return (None, \\'Please enter a valid time.\\')\\n+            return (None, \"Please enter a valid time.\")\\n \\n         # Add an example to the message?\\n         if show_example:\\n             message = message or \"\"\\n-            message += \"Current time in the correct format: \" + datetime.now().strftime(formats[0])\\n-\\n-        return self.generic_entry(prompt, time_validator, message, rofi_args=None, stdin_str = stdin_str, **kwargs)\\n-\\n-\\n-    def datetime_entry(self, prompt, message=None, formats=[\\'%x %X\\'], show_example=False,\\n-            rofi_args=None, stdin_str=\"\", **kwargs):\\n+            message += \"Current time in the correct format: \" + datetime.now().strftime(\\n+                formats[0]\\n+            )\\n+\\n+        return self.generic_entry(\\n+            prompt,\\n+            time_validator,\\n+            message,\\n+            rofi_args=None,\\n+            stdin_str=stdin_str,\\n+            **kwargs\\n+        )\\n+\\n+    def datetime_entry(\\n+        self,\\n+        prompt,\\n+        message=None,\\n+        formats=[\"%x %X\"],\\n+        show_example=False,\\n+        rofi_args=None,\\n+        stdin_str=\"\",\\n+        **kwargs\\n+    ):\\n         \"\"\"Prompt the user to enter a date and time.\\n \\n         Parameters\\n@@ -815,6 +900,7 @@ def datetime_entry(self, prompt, message=None, formats=[\\'%x %X\\'], show_example=F\\n         datetime.datetime, or None if the dialog is cancelled.\\n \\n         \"\"\"\\n+\\n         def datetime_validator(text):\\n             # Try them in order.\\n             for format in formats:\\n@@ -827,15 +913,19 @@ def datetime_validator(text):\\n                     return (dt, None)\\n \\n             # None of the formats worked.\\n-            return (None, \\'Please enter a valid date and time.\\')\\n+            return (None, \"Please enter a valid date and time.\")\\n \\n         # Add an example to the message?\\n         if show_example:\\n             message = message or \"\"\\n-            message += \"Current date and time in the correct format: \" + datetime.now().strftime(formats[0])\\n-\\n-        return self.generic_entry(prompt, datetime_validator, message, rofi_args, stdin_str, **kwargs)\\n-\\n+            message += (\\n+                \"Current date and time in the correct format: \"\\n+                + datetime.now().strftime(formats[0])\\n+            )\\n+\\n+        return self.generic_entry(\\n+            prompt, datetime_validator, message, rofi_args, stdin_str, **kwargs\\n+        )\\n \\n     def exit_with_error(self, error, **kwargs):\\n         \"\"\"Report an error and exit.', '@@ -1,11 +1,12 @@\\n-from marcador.lib import  Bookmark, Tag, BookmarkTag\\n import socket\\n import json\\n \\n from typing import *\\n \\n+\\n def cmd(name, args):\\n-    return bytes(json.dumps({\\'cmd\\': name, \\'args\\': args}), \\'utf-8\\')\\n+    return bytes(json.dumps({\"cmd\": name, \"args\": args}), \"utf-8\")\\n+\\n \\n class Bookmark(dict):\\n     def __init__(self, url, description, tags):\\n@@ -14,11 +15,11 @@ def __init__(self, url, description, tags):\\n         self.tags = tags\\n         dict.__init__(self, {\"url\": url, \"description\": description, \"tags\": tags})\\n \\n-\\n     def __repr__(self) -> str:\\n         return f\"{self.url}\"\\n \\n-class Proxy():\\n+\\n+class Proxy:\\n     def list(self) -> List[Bookmark]:\\n         pass\\n \\n@@ -28,25 +29,34 @@ def add(self, url: str, description: str, tags: List[str]):\\n     def add_tag(self, url: str, tag: str):\\n         pass\\n \\n-    def delete(self, url:str) -> Bookmark:\\n+    def delete(self, url: str) -> Bookmark:\\n         pass\\n \\n+\\n class RemoteProxy(Proxy):\\n     def __init__(self, addr):\\n         self.addr = addr\\n         self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\\n \\n     def list(self) -> List[Bookmark]:\\n-        self.sock.sendto(cmd(\\'list\\', {}), self.addr)\\n+        self.sock.sendto(cmd(\"list\", {}), self.addr)\\n         msg, addr = self.sock.recvfrom(1024)\\n-        for bookmark in json.loads(msg):\\n-            yield Bookmark.load(bookmark)\\n+\\n+        return [\\n+            Bookmark(\\n+                bookmark.get(\"url\"), bookmark.get(\"description\"), bookmark.get(\"tags\")\\n+            )\\n+            for bookmark in json.loads(msg)\\n+        ]\\n \\n     def add(self, url: str, description: str, tags: List[str]):\\n-        self.sock.sendto(cmd(\\'add\\', {\\'url\\': url, \\'description\\':description, \\'tags\\': tags}), self.addr)\\n+        self.sock.sendto(\\n+            cmd(\"add\", {\"url\": url, \"description\": description, \"tags\": tags}),\\n+            self.addr,\\n+        )\\n \\n     def add_tag(self, url: str, tag: str):\\n-        self.sock.sendto(cmd(\\'tag\\', {\\'url\\': url, \\'tag\\': tag}), self.addr)\\n+        self.sock.sendto(cmd(\"tag\", {\"url\": url, \"tag\": tag}), self.addr)\\n \\n     def delete(self, url: str) -> Bookmark:\\n-        self.sock.sendto(cmd(\\'delete\\', {\\'url\\': url}), self.addr)\\n+        self.sock.sendto(cmd(\"delete\", {\"url\": url}), self.addr)', '@@ -6,14 +6,17 @@\\n \\n from .proxy import Proxy, Bookmark\\n \\n+\\n class Tag(Model):\\n     tag: fields.Str()\\n \\n+\\n class JsonBookmark(Model):\\n     url: fields.Str()\\n     description: fields.Str()\\n     tags: fields.List(Tag)\\n \\n+\\n class JsonBook(Model):\\n     bookmarks: fields.List(JsonBookmark)\\n \\n@@ -38,7 +41,7 @@ def __init__(self, path: pathlib.Path):\\n         if not os.path.exists(path):\\n             self.book = JsonBook(bookmarks=[])\\n         else:\\n-            with open(path) as  f:\\n+            with open(path) as f:\\n                 self.book = JsonBook.from_json(f.read())\\n \\n     def save(self):\\n@@ -47,13 +50,17 @@ def save(self):\\n             f.write(json.dumps(self.book.to_dict()))\\n \\n     def list(self) -> List[Bookmark]:\\n-        return [Bookmark(bookmark.url, bookmark.description, [tag.tag for tag in bookmark.tags]) for bookmark in self.book.bookmarks]\\n+        return [\\n+            Bookmark(\\n+                bookmark.url, bookmark.description, [tag.tag for tag in bookmark.tags]\\n+            )\\n+            for bookmark in self.book.bookmarks\\n+        ]\\n \\n     def add(self, url: str, description: str, tags: List[str]):\\n         if self.book.get(url) is not None:\\n             return\\n \\n-\\n         tags = [Tag(tag) for tag in tags]\\n         self.book.bookmarks.append(JsonBookmark(url, description, tags))\\n         self.save()\\n@@ -75,10 +82,13 @@ def delete(self, url) -> Bookmark:\\n \\n \\n def main():\\n-    bookmark = Bookmark(url=\"www.google.com\", description=\"google\", tags=[Tag(\"search\")])\\n+    bookmark = Bookmark(\\n+        url=\"www.google.com\", description=\"google\", tags=[Tag(\"search\")]\\n+    )\\n     book = Book([bookmark])\\n     print(book.to_dict())\\n     print(json.dumps(bookmark.to_dict()))\\n \\n+\\n if __name__ == \"__main__\":\\n     main()', '@@ -11,284 +11,13 @@\\n \\n from appdirs import user_data_dir\\n \\n+\\n def get_user_data_dir():\\n     appauthor = \"joajfreitas\"\\n     appname = \"marcador\"\\n \\n     return user_data_dir(appname, appauthor)\\n \\n+\\n def get_db_path():\\n     return Path(get_user_data_dir()) / \"marcador.json\"\\n-\\n-Base = declarative_base()\\n-\\n-class Bookmark(Base):\\n-    __tablename__ = \\'bookmark\\'\\n-    #identifier = Column(Integer, primary_key=True)\\n-    url = Column(String, primary_key=True)\\n-    description = Column(String)\\n-    count = Column(Integer)\\n-    thumbnail = Column(String)\\n-    score = Column(Float)\\n-\\n-    def load(data):\\n-        return Bookmark(\\n-            url=data.get(\\'url\\') or \"\",\\n-            description=data.get(\\'description\\') or \"\",\\n-            count=data.get(\\'count\\') or 0,\\n-            thumbnail=data.get(\\'thumbnail\\') or \"\",\\n-            score=data.get(\\'score\\') or 1.0,\\n-        )\\n-\\n-    def __repr__(self):\\n-        return f\"Bookmark {{{self.url=}, {self.thumbnail=}}}\"\\n-\\n-class Tag(Base):\\n-    __tablename__ = \\'tag\\'\\n-\\n-    #identifier = Column(Integer, primary_key = True)\\n-    tag = Column(String, primary_key=True)\\n-\\n-    def __repr__(self):\\n-        return f\"Tag {{{self.tag=}}}\"\\n-\\n-class BookmarkTag(Base):\\n-    __tablename__ = \\'bookmark_tag\\'\\n-\\n-    url = Column(String, ForeignKey(\\'bookmark.url\\'), primary_key=True)\\n-    tag = Column(String, ForeignKey(\\'tag.tag\\'), primary_key=True)\\n-\\n-    def __repr__(self):\\n-        return f\"BookmarkTag {{bookmark={self.bookmark},tag={self.tag}}}\"\\n-\\n-\\n-def get_session(db_path: Path) -> Session:\\n-    if not os.path.isfile(db_path):\\n-        db_path.parent.mkdir(parents=True, exist_ok=True)\\n-        db_path.touch()\\n-\\n-    engine = create_engine(\"sqlite:///\"+str(db_path))\\n-    Base.metadata.create_all(engine)\\n-    Base.metadata.bind = engine\\n-    DBSession = sessionmaker(bind=engine)\\n-    session: Session = DBSession()\\n-\\n-    return session\\n-\\n-#class Database:\\n-#    def __init__(self, filename):\\n-#        self.filename = filename\\n-#        self.conn = self.open_database(self.filename)\\n-#        self.cursor = self.conn.cursor()\\n-#\\n-#    def open_db(self, filename):\\n-#        return sqlite3.connect(filename)\\n-#\\n-#    def set_default_db(self, filename):\\n-#        conn = self.open_db(filename)\\n-#        c = conn.cursor()\\n-#\\n-#        c.execute(\\n-#            \"\"\"CREATE TABLE bookmarks (\\n-#            identifier INTEGER PRIMARY KEY, \\n-#            url TEXT, \\n-#            description TEXT,\\n-#            count INTEGER,\\n-#            thumbnail TEXT,\\n-#            score REAL)\\n-#            \"\"\"\\n-#        )\\n-#\\n-#        c.execute(\\n-#            \"\"\"CREATE TABLE tags (\\n-#            identifier INTEGER PRIMARY KEY, \\n-#            tag TEXT)\\n-#            \"\"\"\\n-#        )\\n-#        c.execute(\\n-#            \"\"\"CREATE TABLE bookmarks_tags (\\n-#            bookmark REFERENCES bookmarks(identifier), \\n-#            tag REFERENCES tags(identifier))\\n-#            \"\"\"\\n-#        )\\n-#        conn.commit()\\n-#\\n-#        return conn\\n-#\\n-#    def open_database(self, filename):\\n-#        if not os.path.isfile(filename):\\n-#            return self.set_default_db(filename)\\n-#\\n-#        return self.open_db(filename)\\n-#\\n-#    def get_bookmarks(self, sorted=False):\\n-#        self.cursor.execute(\\n-#            \"\"\"select identifier, url, description, thumbnail, count from bookmarks\"\"\" + (\" order by score DESC\" if sorted else \"\")\\n-#        )\\n-#\\n-#        bookmarks = self.cursor.fetchall()\\n-#\\n-#        for id, url, desc, thumbnail, count in bookmarks:\\n-#            tags = self.get_bookmark_tags(id)\\n-#            tags = [tag for tag, id in tags]\\n-#\\n-#            yield id, url, thumbnail, tags\\n-#\\n-#    def open_bookmark(self, id):\\n-#        self.cursor.execute(f\"select url, count from bookmarks where identifier=\\'{id}\\'\")\\n-#\\n-#        url, count = self.cursor.fetchone()\\n-#\\n-#        self.hit_url(url)\\n-#\\n-#        import webbrowser\\n-#\\n-#        webbrowser.open(url)\\n-#\\n-#    def add_bookmark(self, url, tags):\\n-#        self.cursor.execute(f\\'insert into bookmarks (url,count,score) values (\"{url}\",0,1)\\')\\n-#        book_id = self.cursor.lastrowid\\n-#        for tag in tags:\\n-#            self.cursor.execute(f\\'insert into tags (tag) values (\"{tag}\")\\')\\n-#            tag_id = self.cursor.lastrowid\\n-#            self.cursor.execute(\\n-#                f\"insert into bookmarks_tags (bookmark, tag) values ({book_id}, {tag_id})\"\\n-#            )\\n-#\\n-#        self.conn.commit()\\n-#\\n-#    def rm_bookmark(self, id):\\n-#        self.cursor.execute(\\n-#            f\"delete from bookmarks_tags as bt where bt.bookmark = {id}\"\\n-#        )\\n-#        self.cursor.execute(f\"delete from bookmarks where identifier = {id}\")\\n-#        self.conn.commit()\\n-#\\n-#    def get_url(self, id):\\n-#        if id == 0:\\n-#            return None\\n-#\\n-#        self.cursor.execute(f\"select url from bookmarks where identifier={id}\")\\n-#        url = self.cursor.fetchone()\\n-#        return url\\n-#\\n-#    def get_bookmark(self, id):\\n-#        self.cursor.execute(\\n-#            f\"\"\"select identifier, url, description, thumbnail, count\\n-#                from bookmarks where identifier={id}\"\"\"\\n-#        )\\n-#\\n-#        id, url, desc, thumbnail, count = self.cursor.fetchone()\\n-#        return id, url, desc, thumbnail, count\\n-#\\n-#    def set_bookmark(self, id, url, tags):\\n-#        self.cursor.execute(f\"update bookmarks set url=\\'{url}\\' where identifier={id}\")\\n-#\\n-#        tag_set = self.bookmark_tag_list()\\n-#        _tags = [tag for tag in tags if tag not in tag_set]\\n-#        for tag in _tags:\\n-#            self.cursor.execute(f\"insert into tags (tag) values (\\'{tag}\\')\")\\n-#\\n-#        self.cursor.execute(f\"delete from bookmarks_tags as bt where bt.bookmark={id}\")\\n-#\\n-#        for tag in tags:\\n-#            tag_id = self.get_tag_id(tag)\\n-#            self.cursor.execute(\\n-#                f\"insert into bookmarks_tags as bt values ({id},{tag_id})\"\\n-#            )\\n-#\\n-#        self.conn.commit()\\n-#\\n-#    def set_thumbnail(self, id, thumbnail):\\n-#        self.cursor.execute(\\n-#            f\"update bookmarks set thumbnail=\\'{thumbnail}\\' where identifier={id}\"\\n-#        )\\n-#        self.conn.commit()\\n-#\\n-#    def edit_bookmark(self, id):\\n-#        id, url, desc, thumbnail, count = self.get_bookmark(id)\\n-#        tags = self.get_bookmark_tags(id)\\n-#\\n-#        tmp_file = \"/tmp/bookmarks.tmp\"\\n-#        with open(tmp_file, \"w\") as tmp:\\n-#            tmp.write(url + \"\\\\n\")\\n-#\\n-#            for tag, tag_id in tags:\\n-#                tmp.write(tag + \"\\\\n\")\\n-#\\n-#        term = os.path.expandvars(\"$TERM\")\\n-#        editor = os.path.expandvars(\"$EDITOR\")\\n-#        call([term, \"-e\", editor, tmp_file])\\n-#\\n-#        with open(tmp_file, \"r\") as tmp:\\n-#            lines = tmp.readlines()\\n-#\\n-#        lines = [l.strip(\"\\\\n\") for l in lines if l != \"\"]\\n-#\\n-#        url = lines[0]\\n-#        tags = [tag for tag in lines[1:]]\\n-#\\n-#        self.set_bookmark(id, url, tags)\\n-#\\n-#    def get_bookmark_tags(self, id):\\n-#        self.cursor.execute(\\n-#            f\"\"\"select tags.tag, tags.identifier from\\n-#            bookmarks_tags as bt, tags where bt.bookmark={id} and bt.tag = tags.identifier\"\"\"\\n-#        )\\n-#        return list(self.cursor.fetchall())\\n-#\\n-#    def bookmark_tag_search(self, tag):\\n-#        self.cursor.execute(f\"select identifier from tags where tag=\\'{tag}\\'\")\\n-#        r = self.cursor.fetchone()\\n-#        if r == None:\\n-#            return []\\n-#        id = r[0]\\n-#\\n-#        self.cursor.execute(\\n-#            f\"select bt.bookmark from bookmarks_tags as bt where bt.tag = {id}\"\\n-#        )\\n-#        bookmarks = self.cursor.fetchall()\\n-#\\n-#        for _book in bookmarks:\\n-#            book = _book[0]\\n-#            self.cursor.execute(\\n-#                    f\"\"\"select identifier, url, description, count\\n-#                    from bookmarks where identifier = {book}\"\"\")\\n-#\\n-#            id, url, desc, count = self.cursor.fetchone()\\n-#            yield id, url, desc, count\\n-#\\n-#    def bookmark_tag_list(self):\\n-#        self.cursor.execute(\"select tag from tags\")\\n-#        tags = self.cursor.fetchall()\\n-#\\n-#        for tag in tags:\\n-#            yield tag[0]\\n-#\\n-#    def get_tag_id(self, tag):\\n-#        self.cursor.execute(f\"select identifier from tags where tag=\\'{tag}\\'\")\\n-#        r = self.cursor.fetchone()\\n-#        return None if r == None else r[0]\\n-#\\n-#    def hit_url(self, url):\\n-#        self.cursor.execute(f\"select identifier, count, score from bookmarks where url=\\'{url}\\'\")\\n-#        id, count, score = self.cursor.fetchone()\\n-#        count = int(count)\\n-#        count += 1\\n-#        score += 1\\n-#\\n-#        self.cursor.execute(f\"update bookmarks set score = score*0.95 where identifier<>\\'{id}\\'\")\\n-#        self.cursor.execute(\\n-#            f\"update bookmarks set count = {count}, score = {score} where identifier=\\'{id}\\'\")\\n-#\\n-#        self.conn.commit()\\n-#\\n-#\\n-#def bookmark_to_str(bookmark):\\n-#    id, url, thumbnail, tags = bookmark\\n-#    output = f\"{id}, {url} \"\\n-#    for tag in tags:\\n-#        output += f\"{tag},\"\\n-#\\n-#    output = output[:-1] + \"\\\\n\"\\n-#    return output'], 'file': ['marcador/version.py', 'marcador/scripts/marcador.py', 'marcador/rofi_marcador.py', 'marcador/server.py', 'websockets/server.py', 'marcador/bookmarkimporter.py', 'marcador/rofi.py', 'marcador/proxy.py', 'marcador/json_backend.py', 'marcador/lib.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('957a7a89-5870-445e-ae72-c827ac86ac23'), UUID('3339894b-b2fe-4be8-9d6f-b6e897245add'), UUID('9ac0635f-9634-4139-bd7e-e3d1299636fd'), UUID('3b17e3d2-d851-4660-9699-53bbc81e0f29'), UUID('8bdc1d7d-9a3e-4b73-8245-7655d0c230c7'), UUID('62d4bd40-8916-4146-bed0-c6300524b02a'), UUID('857d88f6-9092-4157-b52d-cc711c4cdacb'), UUID('836f87e3-6fad-4b82-b476-219752f27806'), UUID('515f9999-75c3-4916-973a-64250941a05c'), UUID('9368b6f4-d06e-4c50-962d-ffa50b77e305')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: class Rofi(object):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 4:0: class Rofi(object):\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1297/1800 [16:28<24:00,  2.86s/it]ERROR:src.process_code_changes:Error processing commit 71b8741e3607cfda2833c7624d4ada87071aa8e5\n",
      "ERROR:src.process_code_changes:{'repo': 'scrapy/scrapy', 'vulnerability_id': '2024-3572', 'commit': '71b8741e3607cfda2833c7624d4ada87071aa8e5', 'commit_source': 'github', 'cwe_id': ['CWE-409'], 'patch': ['@@ -1,28 +1,75 @@\\n-import zlib\\n+import warnings\\n+from logging import getLogger\\n \\n-from scrapy.utils.gz import gunzip\\n+from scrapy import signals\\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\\n from scrapy.http import Response, TextResponse\\n from scrapy.responsetypes import responsetypes\\n-from scrapy.exceptions import NotConfigured\\n+from scrapy.utils._compression import (\\n+    _DecompressionMaxSizeExceeded,\\n+    _inflate,\\n+    _unbrotli,\\n+    _unzstd,\\n+)\\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\\n+from scrapy.utils.gz import gunzip\\n+\\n+logger = getLogger(__name__)\\n \\n+ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\\n \\n-ACCEPTED_ENCODINGS = [b\\'gzip\\', b\\'deflate\\']\\n+try:\\n+    import brotli  # noqa: F401\\n+except ImportError:\\n+    pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"br\")\\n \\n try:\\n-    import brotli\\n-    ACCEPTED_ENCODINGS.append(b\\'br\\')\\n+    import zstandard  # noqa: F401\\n except ImportError:\\n     pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\\n \\n \\n class HttpCompressionMiddleware(object):\\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\\n     sent/received from web sites\"\"\"\\n+\\n+    def __init__(self, crawler=None):\\n+        if not crawler:\\n+            self._max_size = 1073741824\\n+            self._warn_size = 33554432\\n+            return\\n+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        crawler.signals.connect(self.open_spider, signals.spider_opened)\\n+\\n     @classmethod\\n     def from_crawler(cls, crawler):\\n         if not crawler.settings.getbool(\\'COMPRESSION_ENABLED\\'):\\n             raise NotConfigured\\n-        return cls()\\n+        try:\\n+            return cls(crawler=crawler)\\n+        except TypeError:\\n+            warnings.warn(\\n+                \"HttpCompressionMiddleware subclasses must either modify \"\\n+                \"their \\'__init__\\' method to support a \\'crawler\\' parameter or \"\\n+                \"reimplement their \\'from_crawler\\' method.\",\\n+                ScrapyDeprecationWarning,\\n+            )\\n+            mw = cls()\\n+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+            crawler.signals.connect(mw.open_spider, signals.spider_opened)\\n+            return mw\\n+\\n+    def open_spider(self, spider):\\n+        if hasattr(spider, \"download_maxsize\"):\\n+            self._max_size = spider.download_maxsize\\n+        if hasattr(spider, \"download_warnsize\"):\\n+            self._warn_size = spider.download_warnsize\\n \\n     def process_request(self, request, spider):\\n         request.headers.setdefault(\\'Accept-Encoding\\',\\n@@ -36,9 +83,36 @@ def process_response(self, request, response, spider):\\n             content_encoding = response.headers.getlist(\\'Content-Encoding\\')\\n             if content_encoding:\\n                 encoding = content_encoding.pop()\\n-                decoded_body = self._decode(response.body, encoding.lower())\\n-                respcls = responsetypes.from_args(headers=response.headers, \\\\\\n-                    url=response.url, body=decoded_body)\\n+                max_size = request.meta.get(\"download_maxsize\", self._max_size)\\n+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\\n+                try:\\n+                    decoded_body = self._decode(\\n+                        response.body, encoding.lower(), max_size\\n+                    )\\n+                except _DecompressionMaxSizeExceeded:\\n+                    raise IgnoreRequest(\\n+                        \"Ignored response {response} because its body \"\\n+                        \"({body_size} B) exceeded DOWNLOAD_MAXSIZE \"\\n+                        \"({max_size} B) during decompression.\".format(\\n+                            response=response,\\n+                            body_size=len(response.body),\\n+                            max_size=max_size,\\n+                        )\\n+                    )\\n+                if len(response.body) < warn_size <= len(decoded_body):\\n+                    logger.warning(\\n+                        \"%(response)s body size after decompression \"\\n+                        \"(%(body_size)s B) is larger than the \"\\n+                        \"download warning size (%(warn_size)s B).\",\\n+                        {\\n+                            \"response\": response,\\n+                            \"body_size\": len(decoded_body),\\n+                            \"warn_size\": warn_size,\\n+                        },\\n+                    )\\n+                respcls = responsetypes.from_args(\\n+                    headers=response.headers, url=response.url, body=decoded_body\\n+                )\\n                 kwargs = dict(cls=respcls, body=decoded_body)\\n                 if issubclass(respcls, TextResponse):\\n                     # force recalculating the encoding until we make sure the\\n@@ -50,20 +124,13 @@ def process_response(self, request, response, spider):\\n \\n         return response\\n \\n-    def _decode(self, body, encoding):\\n-        if encoding == b\\'gzip\\' or encoding == b\\'x-gzip\\':\\n-            body = gunzip(body)\\n-\\n-        if encoding == b\\'deflate\\':\\n-            try:\\n-                body = zlib.decompress(body)\\n-            except zlib.error:\\n-                # ugly hack to work with raw deflate content that may\\n-                # be sent by microsoft servers. For more information, see:\\n-                # http://carsten.codimi.de/gzip.yaws/\\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n-                body = zlib.decompress(body, -15)\\n-        if encoding == b\\'br\\' and b\\'br\\' in ACCEPTED_ENCODINGS:\\n-            body = brotli.decompress(body)\\n+    def _decode(self, body, encoding, max_size):\\n+        if encoding == b\"gzip\" or encoding == b\"x-gzip\":\\n+            return gunzip(body, max_size=max_size)\\n+        if encoding == b\"deflate\":\\n+            return _inflate(body, max_size=max_size)\\n+        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\\n+            return _unbrotli(body, max_size=max_size)\\n+        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\\n+            return _unzstd(body, max_size=max_size)\\n         return body', '@@ -1,12 +1,13 @@\\n-import re\\n import logging\\n+import re\\n+\\n import six\\n \\n-from scrapy.spiders import Spider\\n-from scrapy.http import Request, XmlResponse\\n-from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\\n+from scrapy.http.response.xml import XmlResponse\\n+from scrapy.spiders import Request, Spider\\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\\n from scrapy.utils.gz import gunzip, gzip_magic_number\\n-\\n+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -18,6 +19,17 @@ class SitemapSpider(Spider):\\n     sitemap_follow = [\\'\\']\\n     sitemap_alternate_links = False\\n \\n+    @classmethod\\n+    def from_crawler(cls, crawler, *args, **kwargs):\\n+        spider = super(SitemapSpider, cls).from_crawler(crawler, *args, **kwargs)\\n+        spider._max_size = getattr(\\n+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        )\\n+        spider._warn_size = getattr(\\n+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        )\\n+        return spider\\n+\\n     def __init__(self, *a, **kw):\\n         super(SitemapSpider, self).__init__(*a, **kw)\\n         self._cbs = []\\n@@ -70,8 +82,25 @@ def _get_sitemap_body(self, response):\\n         \"\"\"\\n         if isinstance(response, XmlResponse):\\n             return response.body\\n-        elif gzip_magic_number(response):\\n-            return gunzip(response.body)\\n+        if gzip_magic_number(response):\\n+            uncompressed_size = len(response.body)\\n+            max_size = response.meta.get(\"download_maxsize\", self._max_size)\\n+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\\n+            try:\\n+                body = gunzip(response.body, max_size=max_size)\\n+            except _DecompressionMaxSizeExceeded:\\n+                return None\\n+            if uncompressed_size < warn_size <= len(body):\\n+                logger.warning(\\n+                    \"%(response)s body size after decompression (%(body_length)s B) \"\\n+                    \"is larger than the download warning size (%(warn_size)s B).\",\\n+                    {\\n+                        \"response\": response,\\n+                        \"body_length\": len(body),\\n+                        \"warn_size\": warn_size,\\n+                    },\\n+                )\\n+            return body\\n         # actual gzipped sitemap files are decompressed above ;\\n         # if we are here (response body is not gzipped)\\n         # and have a response for .xml.gz,\\n@@ -81,7 +110,7 @@ def _get_sitemap_body(self, response):\\n         # without actually being a .xml.gz file in the first place,\\n         # merely XML gzip-compressed on the fly,\\n         # in other word, here, we have plain XML\\n-        elif response.url.endswith(\\'.xml\\') or response.url.endswith(\\'.xml.gz\\'):\\n+        if response.url.endswith(\\'.xml\\') or response.url.endswith(\\'.xml.gz\\'):\\n             return response.body\\n \\n ', '@@ -0,0 +1,103 @@\\n+import zlib\\n+from io import BytesIO\\n+\\n+try:\\n+    import brotli\\n+except ImportError:\\n+    pass\\n+\\n+try:\\n+    import zstandard\\n+except ImportError:\\n+    pass\\n+\\n+\\n+_CHUNK_SIZE = 65536  # 64 KiB\\n+\\n+\\n+class _DecompressionMaxSizeExceeded(ValueError):\\n+    pass\\n+\\n+\\n+def _inflate(data, max_size=0):\\n+    decompressor = zlib.decompressobj()\\n+    raw_decompressor = zlib.decompressobj(-15)\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        try:\\n+            output_chunk = decompressor.decompress(input_chunk)\\n+        except zlib.error:\\n+            if decompressor != raw_decompressor:\\n+                # ugly hack to work with raw deflate content that may\\n+                # be sent by microsoft servers. For more information, see:\\n+                # http://carsten.codimi.de/gzip.yaws/\\n+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n+                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n+                decompressor = raw_decompressor\\n+                output_chunk = decompressor.decompress(input_chunk)\\n+            else:\\n+                raise\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unbrotli(data, max_size=0):\\n+    decompressor = brotli.Decompressor()\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        output_chunk = decompressor.decompress(input_chunk)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unzstd(data, max_size=0):\\n+    decompressor = zstandard.ZstdDecompressor()\\n+    stream_reader = decompressor.stream_reader(BytesIO(data))\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        output_chunk = stream_reader.read(_CHUNK_SIZE)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()', '@@ -4,13 +4,15 @@\\n     from cStringIO import StringIO as BytesIO\\n except ImportError:\\n     from io import BytesIO\\n+\\n+import re\\n from gzip import GzipFile\\n \\n import six\\n-import re\\n \\n from scrapy.utils.decorators import deprecated\\n \\n+from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\\n \\n # - Python>=3.5 GzipFile\\'s read() has issues returning leftover\\n #   uncompressed data when input is corrupted\\n@@ -27,31 +29,43 @@ def read1(gzf, size=-1):\\n         return gzf.read1(size)\\n \\n \\n-def gunzip(data):\\n+def gunzip(data, max_size=0):\\n     \"\"\"Gunzip the given data and return as much data as possible.\\n \\n     This is resilient to CRC checksum errors.\\n     \"\"\"\\n     f = GzipFile(fileobj=BytesIO(data))\\n-    output_list = []\\n-    chunk = b\\'.\\'\\n-    while chunk:\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n         try:\\n-            chunk = read1(f, 8196)\\n-            output_list.append(chunk)\\n+            output_chunk = read1(f, _CHUNK_SIZE)\\n         except (IOError, EOFError, struct.error):\\n             # complete only if there is some data, otherwise re-raise\\n             # see issue 87 about catching struct.error\\n             # some pages are quite small so output_list is empty and f.extrabuf\\n             # contains the whole page content\\n-            if output_list or getattr(f, \\'extrabuf\\', None):\\n+            if decompressed_size or getattr(f, \\'extrabuf\\', None):\\n                 try:\\n-                    output_list.append(f.extrabuf[-f.extrasize:])\\n+                    output_stream.write(f.extrabuf[-f.extrasize:])\\n                 finally:\\n                     break\\n             else:\\n                 raise\\n-    return b\\'\\'.join(output_list)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n \\n _is_gzipped = re.compile(br\\'^application/(x-)?gzip\\\\b\\', re.I).search\\n _is_octetstream = re.compile(br\\'^(application|binary)/octet-stream\\\\b\\', re.I).search', '@@ -8,6 +8,7 @@\\n import tarfile\\n import logging\\n from tempfile import mktemp\\n+from warnings import warn\\n \\n import six\\n \\n@@ -16,8 +17,18 @@\\n except ImportError:\\n     from io import BytesIO\\n \\n+from scrapy.exceptions import ScrapyDeprecationWarning\\n from scrapy.responsetypes import responsetypes\\n \\n+warn(\\n+    \"Use of the scrapy.downloadermiddlewares.decompression module is \"\\n+    \"discouraged, as it is susceptible to decompression bomb attacks. For \"\\n+    \"details, see \"\\n+    \"https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7\",\\n+    ScrapyDeprecationWarning,\\n+    stacklevel=2,\\n+)\\n+\\n logger = logging.getLogger(__name__)\\n \\n '], 'file': ['scrapy/downloadermiddlewares/httpcompression.py', 'scrapy/spiders/sitemap.py', 'scrapy/utils/_compression.py', 'scrapy/utils/gz.py', 'scrapy/downloadermiddlewares/decompression.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('ee7306a5-5424-4f75-804c-901f2e85dbf7'), UUID('182964e9-774d-47f2-b7ea-5a13998cc7e4'), UUID('d71f98f9-8810-431d-9f47-9fe442854127'), UUID('a98e6892-fb0b-493f-87f2-833471956281'), UUID('5357db98-d781-4860-a8cf-d6bce9fb01bc')]}\n",
      "ERROR:root:Error in {'repo': 'scrapy/scrapy', 'vulnerability_id': '2024-3572', 'commit': '71b8741e3607cfda2833c7624d4ada87071aa8e5', 'commit_source': 'github', 'cwe_id': ['CWE-409'], 'patch': ['@@ -1,28 +1,75 @@\\n-import zlib\\n+import warnings\\n+from logging import getLogger\\n \\n-from scrapy.utils.gz import gunzip\\n+from scrapy import signals\\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\\n from scrapy.http import Response, TextResponse\\n from scrapy.responsetypes import responsetypes\\n-from scrapy.exceptions import NotConfigured\\n+from scrapy.utils._compression import (\\n+    _DecompressionMaxSizeExceeded,\\n+    _inflate,\\n+    _unbrotli,\\n+    _unzstd,\\n+)\\n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\\n+from scrapy.utils.gz import gunzip\\n+\\n+logger = getLogger(__name__)\\n \\n+ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\\n \\n-ACCEPTED_ENCODINGS = [b\\'gzip\\', b\\'deflate\\']\\n+try:\\n+    import brotli  # noqa: F401\\n+except ImportError:\\n+    pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"br\")\\n \\n try:\\n-    import brotli\\n-    ACCEPTED_ENCODINGS.append(b\\'br\\')\\n+    import zstandard  # noqa: F401\\n except ImportError:\\n     pass\\n+else:\\n+    ACCEPTED_ENCODINGS.append(b\"zstd\")\\n \\n \\n class HttpCompressionMiddleware(object):\\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\\n     sent/received from web sites\"\"\"\\n+\\n+    def __init__(self, crawler=None):\\n+        if not crawler:\\n+            self._max_size = 1073741824\\n+            self._warn_size = 33554432\\n+            return\\n+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        crawler.signals.connect(self.open_spider, signals.spider_opened)\\n+\\n     @classmethod\\n     def from_crawler(cls, crawler):\\n         if not crawler.settings.getbool(\\'COMPRESSION_ENABLED\\'):\\n             raise NotConfigured\\n-        return cls()\\n+        try:\\n+            return cls(crawler=crawler)\\n+        except TypeError:\\n+            warnings.warn(\\n+                \"HttpCompressionMiddleware subclasses must either modify \"\\n+                \"their \\'__init__\\' method to support a \\'crawler\\' parameter or \"\\n+                \"reimplement their \\'from_crawler\\' method.\",\\n+                ScrapyDeprecationWarning,\\n+            )\\n+            mw = cls()\\n+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+            crawler.signals.connect(mw.open_spider, signals.spider_opened)\\n+            return mw\\n+\\n+    def open_spider(self, spider):\\n+        if hasattr(spider, \"download_maxsize\"):\\n+            self._max_size = spider.download_maxsize\\n+        if hasattr(spider, \"download_warnsize\"):\\n+            self._warn_size = spider.download_warnsize\\n \\n     def process_request(self, request, spider):\\n         request.headers.setdefault(\\'Accept-Encoding\\',\\n@@ -36,9 +83,36 @@ def process_response(self, request, response, spider):\\n             content_encoding = response.headers.getlist(\\'Content-Encoding\\')\\n             if content_encoding:\\n                 encoding = content_encoding.pop()\\n-                decoded_body = self._decode(response.body, encoding.lower())\\n-                respcls = responsetypes.from_args(headers=response.headers, \\\\\\n-                    url=response.url, body=decoded_body)\\n+                max_size = request.meta.get(\"download_maxsize\", self._max_size)\\n+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)\\n+                try:\\n+                    decoded_body = self._decode(\\n+                        response.body, encoding.lower(), max_size\\n+                    )\\n+                except _DecompressionMaxSizeExceeded:\\n+                    raise IgnoreRequest(\\n+                        \"Ignored response {response} because its body \"\\n+                        \"({body_size} B) exceeded DOWNLOAD_MAXSIZE \"\\n+                        \"({max_size} B) during decompression.\".format(\\n+                            response=response,\\n+                            body_size=len(response.body),\\n+                            max_size=max_size,\\n+                        )\\n+                    )\\n+                if len(response.body) < warn_size <= len(decoded_body):\\n+                    logger.warning(\\n+                        \"%(response)s body size after decompression \"\\n+                        \"(%(body_size)s B) is larger than the \"\\n+                        \"download warning size (%(warn_size)s B).\",\\n+                        {\\n+                            \"response\": response,\\n+                            \"body_size\": len(decoded_body),\\n+                            \"warn_size\": warn_size,\\n+                        },\\n+                    )\\n+                respcls = responsetypes.from_args(\\n+                    headers=response.headers, url=response.url, body=decoded_body\\n+                )\\n                 kwargs = dict(cls=respcls, body=decoded_body)\\n                 if issubclass(respcls, TextResponse):\\n                     # force recalculating the encoding until we make sure the\\n@@ -50,20 +124,13 @@ def process_response(self, request, response, spider):\\n \\n         return response\\n \\n-    def _decode(self, body, encoding):\\n-        if encoding == b\\'gzip\\' or encoding == b\\'x-gzip\\':\\n-            body = gunzip(body)\\n-\\n-        if encoding == b\\'deflate\\':\\n-            try:\\n-                body = zlib.decompress(body)\\n-            except zlib.error:\\n-                # ugly hack to work with raw deflate content that may\\n-                # be sent by microsoft servers. For more information, see:\\n-                # http://carsten.codimi.de/gzip.yaws/\\n-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n-                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n-                body = zlib.decompress(body, -15)\\n-        if encoding == b\\'br\\' and b\\'br\\' in ACCEPTED_ENCODINGS:\\n-            body = brotli.decompress(body)\\n+    def _decode(self, body, encoding, max_size):\\n+        if encoding == b\"gzip\" or encoding == b\"x-gzip\":\\n+            return gunzip(body, max_size=max_size)\\n+        if encoding == b\"deflate\":\\n+            return _inflate(body, max_size=max_size)\\n+        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:\\n+            return _unbrotli(body, max_size=max_size)\\n+        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:\\n+            return _unzstd(body, max_size=max_size)\\n         return body', '@@ -1,12 +1,13 @@\\n-import re\\n import logging\\n+import re\\n+\\n import six\\n \\n-from scrapy.spiders import Spider\\n-from scrapy.http import Request, XmlResponse\\n-from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\\n+from scrapy.http.response.xml import XmlResponse\\n+from scrapy.spiders import Request, Spider\\n+from scrapy.utils._compression import _DecompressionMaxSizeExceeded\\n from scrapy.utils.gz import gunzip, gzip_magic_number\\n-\\n+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\\n \\n logger = logging.getLogger(__name__)\\n \\n@@ -18,6 +19,17 @@ class SitemapSpider(Spider):\\n     sitemap_follow = [\\'\\']\\n     sitemap_alternate_links = False\\n \\n+    @classmethod\\n+    def from_crawler(cls, crawler, *args, **kwargs):\\n+        spider = super(SitemapSpider, cls).from_crawler(crawler, *args, **kwargs)\\n+        spider._max_size = getattr(\\n+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")\\n+        )\\n+        spider._warn_size = getattr(\\n+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")\\n+        )\\n+        return spider\\n+\\n     def __init__(self, *a, **kw):\\n         super(SitemapSpider, self).__init__(*a, **kw)\\n         self._cbs = []\\n@@ -70,8 +82,25 @@ def _get_sitemap_body(self, response):\\n         \"\"\"\\n         if isinstance(response, XmlResponse):\\n             return response.body\\n-        elif gzip_magic_number(response):\\n-            return gunzip(response.body)\\n+        if gzip_magic_number(response):\\n+            uncompressed_size = len(response.body)\\n+            max_size = response.meta.get(\"download_maxsize\", self._max_size)\\n+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)\\n+            try:\\n+                body = gunzip(response.body, max_size=max_size)\\n+            except _DecompressionMaxSizeExceeded:\\n+                return None\\n+            if uncompressed_size < warn_size <= len(body):\\n+                logger.warning(\\n+                    \"%(response)s body size after decompression (%(body_length)s B) \"\\n+                    \"is larger than the download warning size (%(warn_size)s B).\",\\n+                    {\\n+                        \"response\": response,\\n+                        \"body_length\": len(body),\\n+                        \"warn_size\": warn_size,\\n+                    },\\n+                )\\n+            return body\\n         # actual gzipped sitemap files are decompressed above ;\\n         # if we are here (response body is not gzipped)\\n         # and have a response for .xml.gz,\\n@@ -81,7 +110,7 @@ def _get_sitemap_body(self, response):\\n         # without actually being a .xml.gz file in the first place,\\n         # merely XML gzip-compressed on the fly,\\n         # in other word, here, we have plain XML\\n-        elif response.url.endswith(\\'.xml\\') or response.url.endswith(\\'.xml.gz\\'):\\n+        if response.url.endswith(\\'.xml\\') or response.url.endswith(\\'.xml.gz\\'):\\n             return response.body\\n \\n ', '@@ -0,0 +1,103 @@\\n+import zlib\\n+from io import BytesIO\\n+\\n+try:\\n+    import brotli\\n+except ImportError:\\n+    pass\\n+\\n+try:\\n+    import zstandard\\n+except ImportError:\\n+    pass\\n+\\n+\\n+_CHUNK_SIZE = 65536  # 64 KiB\\n+\\n+\\n+class _DecompressionMaxSizeExceeded(ValueError):\\n+    pass\\n+\\n+\\n+def _inflate(data, max_size=0):\\n+    decompressor = zlib.decompressobj()\\n+    raw_decompressor = zlib.decompressobj(-15)\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        try:\\n+            output_chunk = decompressor.decompress(input_chunk)\\n+        except zlib.error:\\n+            if decompressor != raw_decompressor:\\n+                # ugly hack to work with raw deflate content that may\\n+                # be sent by microsoft servers. For more information, see:\\n+                # http://carsten.codimi.de/gzip.yaws/\\n+                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx\\n+                # http://www.gzip.org/zlib/zlib_faq.html#faq38\\n+                decompressor = raw_decompressor\\n+                output_chunk = decompressor.decompress(input_chunk)\\n+            else:\\n+                raise\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unbrotli(data, max_size=0):\\n+    decompressor = brotli.Decompressor()\\n+    input_stream = BytesIO(data)\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\\n+        output_chunk = decompressor.decompress(input_chunk)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n+\\n+\\n+def _unzstd(data, max_size=0):\\n+    decompressor = zstandard.ZstdDecompressor()\\n+    stream_reader = decompressor.stream_reader(BytesIO(data))\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n+        output_chunk = stream_reader.read(_CHUNK_SIZE)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()', '@@ -4,13 +4,15 @@\\n     from cStringIO import StringIO as BytesIO\\n except ImportError:\\n     from io import BytesIO\\n+\\n+import re\\n from gzip import GzipFile\\n \\n import six\\n-import re\\n \\n from scrapy.utils.decorators import deprecated\\n \\n+from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\\n \\n # - Python>=3.5 GzipFile\\'s read() has issues returning leftover\\n #   uncompressed data when input is corrupted\\n@@ -27,31 +29,43 @@ def read1(gzf, size=-1):\\n         return gzf.read1(size)\\n \\n \\n-def gunzip(data):\\n+def gunzip(data, max_size=0):\\n     \"\"\"Gunzip the given data and return as much data as possible.\\n \\n     This is resilient to CRC checksum errors.\\n     \"\"\"\\n     f = GzipFile(fileobj=BytesIO(data))\\n-    output_list = []\\n-    chunk = b\\'.\\'\\n-    while chunk:\\n+    output_stream = BytesIO()\\n+    output_chunk = b\".\"\\n+    decompressed_size = 0\\n+    while output_chunk:\\n         try:\\n-            chunk = read1(f, 8196)\\n-            output_list.append(chunk)\\n+            output_chunk = read1(f, _CHUNK_SIZE)\\n         except (IOError, EOFError, struct.error):\\n             # complete only if there is some data, otherwise re-raise\\n             # see issue 87 about catching struct.error\\n             # some pages are quite small so output_list is empty and f.extrabuf\\n             # contains the whole page content\\n-            if output_list or getattr(f, \\'extrabuf\\', None):\\n+            if decompressed_size or getattr(f, \\'extrabuf\\', None):\\n                 try:\\n-                    output_list.append(f.extrabuf[-f.extrasize:])\\n+                    output_stream.write(f.extrabuf[-f.extrasize:])\\n                 finally:\\n                     break\\n             else:\\n                 raise\\n-    return b\\'\\'.join(output_list)\\n+        decompressed_size += len(output_chunk)\\n+        if max_size and decompressed_size > max_size:\\n+            raise _DecompressionMaxSizeExceeded(\\n+                \"The number of bytes decompressed so far \"\\n+                \"({decompressed_size} B) exceed the specified maximum \"\\n+                \"({max_size} B).\".format(\\n+                    decompressed_size=decompressed_size,\\n+                    max_size=max_size,\\n+                )\\n+            )\\n+        output_stream.write(output_chunk)\\n+    output_stream.seek(0)\\n+    return output_stream.read()\\n \\n _is_gzipped = re.compile(br\\'^application/(x-)?gzip\\\\b\\', re.I).search\\n _is_octetstream = re.compile(br\\'^(application|binary)/octet-stream\\\\b\\', re.I).search', '@@ -8,6 +8,7 @@\\n import tarfile\\n import logging\\n from tempfile import mktemp\\n+from warnings import warn\\n \\n import six\\n \\n@@ -16,8 +17,18 @@\\n except ImportError:\\n     from io import BytesIO\\n \\n+from scrapy.exceptions import ScrapyDeprecationWarning\\n from scrapy.responsetypes import responsetypes\\n \\n+warn(\\n+    \"Use of the scrapy.downloadermiddlewares.decompression module is \"\\n+    \"discouraged, as it is susceptible to decompression bomb attacks. For \"\\n+    \"details, see \"\\n+    \"https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7\",\\n+    ScrapyDeprecationWarning,\\n+    stacklevel=2,\\n+)\\n+\\n logger = logging.getLogger(__name__)\\n \\n '], 'file': ['scrapy/downloadermiddlewares/httpcompression.py', 'scrapy/spiders/sitemap.py', 'scrapy/utils/_compression.py', 'scrapy/utils/gz.py', 'scrapy/downloadermiddlewares/decompression.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('ee7306a5-5424-4f75-804c-901f2e85dbf7'), UUID('182964e9-774d-47f2-b7ea-5a13998cc7e4'), UUID('d71f98f9-8810-431d-9f47-9fe442854127'), UUID('a98e6892-fb0b-493f-87f2-833471956281'), UUID('5357db98-d781-4860-a8cf-d6bce9fb01bc')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0:     import brotli\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0:     import brotli\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1301/1800 [16:29<20:10,  2.43s/it]ERROR:src.process_code_changes:Error processing commit d2655d370586cb830e49acfb450f87598da60be8\n",
      "ERROR:src.process_code_changes:{'repo': 'latchset/jwcrypto', 'vulnerability_id': '2023-6681', 'commit': 'd2655d370586cb830e49acfb450f87598da60be8', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': [\"@@ -28,6 +28,8 @@\\n \\n # Implements RFC 7518 - JSON Web Algorithms (JWA)\\n \\n+default_max_pbkdf2_iterations = 16384\\n+\\n \\n class JWAAlgorithm(metaclass=ABCMeta):\\n \\n@@ -588,6 +590,9 @@ def __init__(self):\\n         self.aeskwmap = {128: _A128KW, 192: _A192KW, 256: _A256KW}\\n \\n     def _get_key(self, alg, key, p2s, p2c):\\n+        if p2c > default_max_pbkdf2_iterations:\\n+            raise ValueError('Invalid p2c value, too large')\\n+\\n         if not isinstance(key, JWK):\\n             # backwards compatibility for old interface\\n             if isinstance(key, bytes):\"], 'file': ['jwcrypto/jwa.py'], 'language': ['Python'], 'temp_id': [UUID('9777342d-c757-496e-98cd-350138831cde')]}\n",
      "ERROR:root:Error in {'repo': 'latchset/jwcrypto', 'vulnerability_id': '2023-6681', 'commit': 'd2655d370586cb830e49acfb450f87598da60be8', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': [\"@@ -28,6 +28,8 @@\\n \\n # Implements RFC 7518 - JSON Web Algorithms (JWA)\\n \\n+default_max_pbkdf2_iterations = 16384\\n+\\n \\n class JWAAlgorithm(metaclass=ABCMeta):\\n \\n@@ -588,6 +590,9 @@ def __init__(self):\\n         self.aeskwmap = {128: _A128KW, 192: _A192KW, 256: _A256KW}\\n \\n     def _get_key(self, alg, key, p2s, p2c):\\n+        if p2c > default_max_pbkdf2_iterations:\\n+            raise ValueError('Invalid p2c value, too large')\\n+\\n         if not isinstance(key, JWK):\\n             # backwards compatibility for old interface\\n             if isinstance(key, bytes):\"], 'file': ['jwcrypto/jwa.py'], 'language': ['Python'], 'temp_id': [UUID('9777342d-c757-496e-98cd-350138831cde')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 193, in get_changes\n",
      "    _get_changes_lines_units(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/get_changes_lines_units.py\", line 229, in _get_changes_lines_units\n",
      "    line_context = script.get_context(fix_line)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/helpers.py\", line 487, in wrapper\n",
      "    return func(self, line, column, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/__init__.py\", line 494, in get_context\n",
      "    context = module_context.create_context(leaf)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 288, in create_context\n",
      "    return from_scope_node(scope_node, is_nested=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 258, in from_scope_node\n",
      "    return self.create_value(scope_node).as_context()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 236, in create_value\n",
      "    func = value.FunctionValue.from_context(parent_context, node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/function.py\", line 145, in from_context\n",
      "    overloaded_funcs = list(_find_overload_functions(context, tree_node))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/function.py\", line 443, in _find_overload_functions\n",
      "    filter = ParserTreeFilter(\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 138, in __init__\n",
      "    super().__init__(parent_context, node_context)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 100, in __init__\n",
      "    self._parso_cache_node = get_parso_cache_node(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/parser_utils.py\", line 287, in get_parso_cache_node\n",
      "    return parser_cache[grammar._hashed][path]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: PosixPath('/Users/somen/repos/latchset/jwcrypto/jwcrypto/jwa.py')\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1303/1800 [16:32<19:11,  2.32s/it]ERROR:src.process_code_changes:Error processing commit b65b3b438c95894654fd9081139989c757bdc6c1\n",
      "ERROR:src.process_code_changes:{'repo': 'nonebot/nonebot2', 'vulnerability_id': '2024-21624', 'commit': 'b65b3b438c95894654fd9081139989c757bdc6c1', 'commit_source': 'github', 'cwe_id': ['CWE-200', 'CWE-200'], 'patch': ['@@ -20,9 +20,17 @@\\n     overload,\\n )\\n \\n+from _string import formatter_field_name_split  # type: ignore\\n+\\n if TYPE_CHECKING:\\n     from .message import Message, MessageSegment\\n \\n+    def formatter_field_name_split(  # noqa: F811\\n+        field_name: str,\\n+    ) -> Tuple[str, List[Tuple[bool, str]]]:\\n+        ...\\n+\\n+\\n TM = TypeVar(\"TM\", bound=\"Message\")\\n TF = TypeVar(\"TF\", str, \"Message\")\\n \\n@@ -36,26 +44,37 @@ class MessageTemplate(Formatter, Generic[TF]):\\n     å‚æ•°:\\n         template: æ¨¡æ¿\\n         factory: æ¶ˆæ¯ç±»å‹å·¥å‚ï¼Œé»˜è®¤ä¸º `str`\\n+        private_getattr: æ˜¯å¦å…è®¸åœ¨æ¨¡æ¿ä¸­è®¿é—®ç§æœ‰å±æ€§ï¼Œé»˜è®¤ä¸º `False`\\n     \"\"\"\\n \\n     @overload\\n     def __init__(\\n-        self: \"MessageTemplate[str]\", template: str, factory: Type[str] = str\\n+        self: \"MessageTemplate[str]\",\\n+        template: str,\\n+        factory: Type[str] = str,\\n+        private_getattr: bool = False,\\n     ) -> None:\\n         ...\\n \\n     @overload\\n     def __init__(\\n-        self: \"MessageTemplate[TM]\", template: Union[str, TM], factory: Type[TM]\\n+        self: \"MessageTemplate[TM]\",\\n+        template: Union[str, TM],\\n+        factory: Type[TM],\\n+        private_getattr: bool = False,\\n     ) -> None:\\n         ...\\n \\n     def __init__(\\n-        self, template: Union[str, TM], factory: Union[Type[str], Type[TM]] = str\\n+        self,\\n+        template: Union[str, TM],\\n+        factory: Union[Type[str], Type[TM]] = str,\\n+        private_getattr: bool = False,\\n     ) -> None:\\n         self.template: TF = template  # type: ignore\\n         self.factory: Type[TF] = factory  # type: ignore\\n         self.format_specs: Dict[str, FormatSpecFunc] = {}\\n+        self.private_getattr = private_getattr\\n \\n     def __repr__(self) -> str:\\n         return f\"MessageTemplate({self.template!r}, factory={self.factory!r})\"\\n@@ -167,6 +186,19 @@ def _vformat(\\n \\n         return functools.reduce(self._add, results), auto_arg_index\\n \\n+    def get_field(\\n+        self, field_name: str, args: Sequence[Any], kwargs: Mapping[str, Any]\\n+    ) -> Tuple[Any, Union[int, str]]:\\n+        first, rest = formatter_field_name_split(field_name)\\n+        obj = self.get_value(first, args, kwargs)\\n+\\n+        for is_attr, value in rest:\\n+            if not self.private_getattr and value.startswith(\"_\"):\\n+                raise ValueError(\"Cannot access private attribute\")\\n+            obj = getattr(obj, value) if is_attr else obj[value]\\n+\\n+        return obj, first\\n+\\n     def format_field(self, value: Any, format_spec: str) -> Any:\\n         formatter: Optional[FormatSpecFunc] = self.format_specs.get(format_spec)\\n         if formatter is None and not issubclass(self.factory, str):'], 'file': ['nonebot/internal/adapter/template.py'], 'language': ['Python'], 'temp_id': [UUID('6bda8a12-87c6-487c-a1d8-3f4d0c4af09f')]}\n",
      "ERROR:root:Error in {'repo': 'nonebot/nonebot2', 'vulnerability_id': '2024-21624', 'commit': 'b65b3b438c95894654fd9081139989c757bdc6c1', 'commit_source': 'github', 'cwe_id': ['CWE-200', 'CWE-200'], 'patch': ['@@ -20,9 +20,17 @@\\n     overload,\\n )\\n \\n+from _string import formatter_field_name_split  # type: ignore\\n+\\n if TYPE_CHECKING:\\n     from .message import Message, MessageSegment\\n \\n+    def formatter_field_name_split(  # noqa: F811\\n+        field_name: str,\\n+    ) -> Tuple[str, List[Tuple[bool, str]]]:\\n+        ...\\n+\\n+\\n TM = TypeVar(\"TM\", bound=\"Message\")\\n TF = TypeVar(\"TF\", str, \"Message\")\\n \\n@@ -36,26 +44,37 @@ class MessageTemplate(Formatter, Generic[TF]):\\n     å‚æ•°:\\n         template: æ¨¡æ¿\\n         factory: æ¶ˆæ¯ç±»å‹å·¥å‚ï¼Œé»˜è®¤ä¸º `str`\\n+        private_getattr: æ˜¯å¦å…è®¸åœ¨æ¨¡æ¿ä¸­è®¿é—®ç§æœ‰å±æ€§ï¼Œé»˜è®¤ä¸º `False`\\n     \"\"\"\\n \\n     @overload\\n     def __init__(\\n-        self: \"MessageTemplate[str]\", template: str, factory: Type[str] = str\\n+        self: \"MessageTemplate[str]\",\\n+        template: str,\\n+        factory: Type[str] = str,\\n+        private_getattr: bool = False,\\n     ) -> None:\\n         ...\\n \\n     @overload\\n     def __init__(\\n-        self: \"MessageTemplate[TM]\", template: Union[str, TM], factory: Type[TM]\\n+        self: \"MessageTemplate[TM]\",\\n+        template: Union[str, TM],\\n+        factory: Type[TM],\\n+        private_getattr: bool = False,\\n     ) -> None:\\n         ...\\n \\n     def __init__(\\n-        self, template: Union[str, TM], factory: Union[Type[str], Type[TM]] = str\\n+        self,\\n+        template: Union[str, TM],\\n+        factory: Union[Type[str], Type[TM]] = str,\\n+        private_getattr: bool = False,\\n     ) -> None:\\n         self.template: TF = template  # type: ignore\\n         self.factory: Type[TF] = factory  # type: ignore\\n         self.format_specs: Dict[str, FormatSpecFunc] = {}\\n+        self.private_getattr = private_getattr\\n \\n     def __repr__(self) -> str:\\n         return f\"MessageTemplate({self.template!r}, factory={self.factory!r})\"\\n@@ -167,6 +186,19 @@ def _vformat(\\n \\n         return functools.reduce(self._add, results), auto_arg_index\\n \\n+    def get_field(\\n+        self, field_name: str, args: Sequence[Any], kwargs: Mapping[str, Any]\\n+    ) -> Tuple[Any, Union[int, str]]:\\n+        first, rest = formatter_field_name_split(field_name)\\n+        obj = self.get_value(first, args, kwargs)\\n+\\n+        for is_attr, value in rest:\\n+            if not self.private_getattr and value.startswith(\"_\"):\\n+                raise ValueError(\"Cannot access private attribute\")\\n+            obj = getattr(obj, value) if is_attr else obj[value]\\n+\\n+        return obj, first\\n+\\n     def format_field(self, value: Any, format_spec: str) -> Any:\\n         formatter: Optional[FormatSpecFunc] = self.format_specs.get(format_spec)\\n         if formatter is None and not issubclass(self.factory, str):'], 'file': ['nonebot/internal/adapter/template.py'], 'language': ['Python'], 'temp_id': [UUID('6bda8a12-87c6-487c-a1d8-3f4d0c4af09f')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     def formatter_field_name_split(                \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     def formatter_field_name_split(  # noqa: F811\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1312/1800 [16:33<09:40,  1.19s/it]ERROR:src.process_code_changes:Error processing commit 58cb3d987372c91eb605853c35325701733337c2\n",
      "ERROR:src.process_code_changes:{'repo': 'zenml-io/zenml', 'vulnerability_id': '2024-2213', 'commit': '58cb3d987372c91eb605853c35325701733337c2', 'commit_source': 'github', 'cwe_id': ['CWE-287'], 'patch': ['@@ -201,6 +201,12 @@ class UserUpdate(UserBase, BaseZenModel):\\n     active: Optional[bool] = Field(\\n         default=None, title=\"Whether the account is active.\"\\n     )\\n+    old_password: Optional[str] = Field(\\n+        default=None,\\n+        title=\"The previous password for the user. Only relevant for user \"\\n+        \"accounts. Required when updating the password.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n \\n     @root_validator\\n     def user_email_updates(cls, values: Dict[str, Any]) -> Dict[str, Any]:', '@@ -21,9 +21,14 @@\\n from zenml.cli.cli import TagGroup, cli\\n from zenml.cli.utils import is_sorted_or_filtered, list_options\\n from zenml.client import Client\\n+from zenml.config.global_config import GlobalConfiguration\\n from zenml.console import console\\n from zenml.enums import CliCategories, StoreType\\n-from zenml.exceptions import EntityExistsError, IllegalOperationError\\n+from zenml.exceptions import (\\n+    AuthorizationException,\\n+    EntityExistsError,\\n+    IllegalOperationError,\\n+)\\n from zenml.models import UserFilter\\n \\n \\n@@ -156,6 +161,11 @@ def create_user(\\n                 default=\"\",\\n                 hide_input=True,\\n             )\\n+    else:\\n+        cli_utils.warning(\\n+            \"Supplying password values in the command line is not safe. \"\\n+            \"Please consider using the prompt option.\"\\n+        )\\n \\n     try:\\n         new_user = client.create_user(\\n@@ -204,14 +214,6 @@ def create_user(\\n     required=False,\\n     help=\"New user email.\",\\n )\\n-@click.option(\\n-    \"--password\",\\n-    \"-p\",\\n-    \"updated_password\",\\n-    type=str,\\n-    required=False,\\n-    help=\"New user password.\",\\n-)\\n @click.option(\\n     \"--admin\",\\n     \"-a\",\\n@@ -230,14 +232,22 @@ def create_user(\\n     default=None,\\n     help=\"Whether the user should be a regular user.\",\\n )\\n+@click.option(\\n+    \"--active\",\\n+    \"active\",\\n+    type=bool,\\n+    required=False,\\n+    default=None,\\n+    help=\"Use to activate or deactivate a user account.\",\\n+)\\n def update_user(\\n     user_name_or_id: str,\\n     updated_name: Optional[str] = None,\\n     updated_full_name: Optional[str] = None,\\n     updated_email: Optional[str] = None,\\n-    updated_password: Optional[str] = None,\\n     make_admin: Optional[bool] = None,\\n     make_user: Optional[bool] = None,\\n+    active: Optional[bool] = None,\\n ) -> None:\\n     \"\"\"Update an existing user.\\n \\n@@ -246,9 +256,9 @@ def update_user(\\n         updated_name: The name of the user to create.\\n         updated_full_name: The name of the user to create.\\n         updated_email: The name of the user to create.\\n-        updated_password: The name of the user to create.\\n         make_admin: Whether the user should be an admin.\\n         make_user: Whether the user should be a regular user.\\n+        active: Use to activate or deactivate a user account.\\n     \"\"\"\\n     if make_admin is not None and make_user is not None:\\n         cli_utils.error(\\n@@ -260,7 +270,8 @@ def update_user(\\n         )\\n         if current_user.is_admin and make_user:\\n             confirmation = cli_utils.confirmation(\\n-                f\"Currently user `{current_user.name}` is an admin. Are you sure you want to make them a regular user?\"\\n+                f\"Currently user `{current_user.name}` is an admin. Are you \"\\n+                \"sure you want to make them a regular user?\"\\n             )\\n             if not confirmation:\\n                 cli_utils.declare(\"User update canceled.\")\\n@@ -276,13 +287,137 @@ def update_user(\\n             updated_name=updated_name,\\n             updated_full_name=updated_full_name,\\n             updated_email=updated_email,\\n-            updated_password=updated_password,\\n             updated_is_admin=updated_is_admin,\\n+            active=active,\\n         )\\n     except (KeyError, IllegalOperationError) as err:\\n         cli_utils.error(str(err))\\n \\n \\n+@user.command(\\n+    \"change-password\",\\n+    help=\"Change the password for the current user account.\",\\n+)\\n+@click.option(\\n+    \"--password\",\\n+    help=(\\n+        \"The new user password. If omitted, a prompt will be shown to enter \"\\n+        \"the password.\"\\n+    ),\\n+    required=False,\\n+    type=str,\\n+)\\n+@click.option(\\n+    \"--old-password\",\\n+    help=(\\n+        \"The old user password. If omitted, a prompt will be shown to enter \"\\n+        \"the old password.\"\\n+    ),\\n+    required=False,\\n+    type=str,\\n+)\\n+def change_user_password(\\n+    password: Optional[str] = None, old_password: Optional[str] = None\\n+) -> None:\\n+    \"\"\"Change the password of the current user.\\n+\\n+    Args:\\n+        password: The new password for the current user.\\n+        old_password: The old password for the current user.\\n+    \"\"\"\\n+    active_user = Client().active_user\\n+\\n+    if old_password is not None or password is not None:\\n+        cli_utils.warning(\\n+            \"Supplying password values in the command line is not safe. \"\\n+            \"Please consider using the prompt option.\"\\n+        )\\n+\\n+    if old_password is None:\\n+        old_password = click.prompt(\\n+            f\"Current password for user {active_user.name}\",\\n+            hide_input=True,\\n+        )\\n+    if password is None:\\n+        password = click.prompt(\\n+            f\"New password for user {active_user.name}\",\\n+            hide_input=True,\\n+        )\\n+        password_again = click.prompt(\\n+            f\"Please re-enter the new password for user {active_user.name}\",\\n+            hide_input=True,\\n+        )\\n+        if password != password_again:\\n+            cli_utils.error(\"Passwords do not match.\")\\n+\\n+    try:\\n+        Client().update_user(\\n+            name_id_or_prefix=active_user.id,\\n+            old_password=old_password,\\n+            updated_password=password,\\n+        )\\n+    except (KeyError, IllegalOperationError, AuthorizationException) as err:\\n+        cli_utils.error(str(err))\\n+\\n+    cli_utils.declare(\\n+        f\"Successfully updated password for active user \\'{active_user.name}\\'.\"\\n+    )\\n+\\n+    store = GlobalConfiguration().store_configuration\\n+    if store.type == StoreType.REST:\\n+        from zenml.zen_stores.rest_zen_store import RestZenStoreConfiguration\\n+\\n+        assert isinstance(store, RestZenStoreConfiguration)\\n+\\n+        if store.password is not None:\\n+            cli_utils.declare(\\n+                \"You may need to log in again with your new password by \"\\n+                \"running `zenml connect`.\"\\n+            )\\n+\\n+\\n+@user.command(\\n+    \"deactivate\",\\n+    help=\"Generate an activation token to reset the password for a user account\",\\n+)\\n+@click.argument(\"user_name_or_id\", type=str, required=True)\\n+def deactivate_user(\\n+    user_name_or_id: str,\\n+) -> None:\\n+    \"\"\"Reset the password of a user.\\n+\\n+    Args:\\n+        user_name_or_id: The name or ID of the user to reset the password for.\\n+    \"\"\"\\n+    client = Client()\\n+\\n+    store = GlobalConfiguration().store_configuration\\n+    if store.type != StoreType.REST:\\n+        cli_utils.error(\\n+            \"Deactivating users is only supported when connected to a ZenML \"\\n+            \"server.\"\\n+        )\\n+\\n+    try:\\n+        if not client.active_user.is_admin:\\n+            cli_utils.error(\\n+                \"Only admins can reset the password of other users.\"\\n+            )\\n+\\n+        user = client.deactivate_user(\\n+            name_id_or_prefix=user_name_or_id,\\n+        )\\n+    except (KeyError, IllegalOperationError) as err:\\n+        cli_utils.error(str(err))\\n+\\n+    cli_utils.declare(\\n+        f\"Successfully deactivated user account \\'{user.name}\\'.\"\\n+        f\"To reactivate the account, please visit the dashboard at the \"\\n+        \"following URL:\\\\n\"\\n+        f\"{client.zen_store.url}/signup?user={str(user.id)}&username={user.name}&token={user.activation_token}\\\\n\"\\n+    )\\n+\\n+\\n @user.command(\"delete\")\\n @click.argument(\"user_name_or_id\", type=str, required=True)\\n def delete_user(user_name_or_id: str) -> None:', '@@ -206,6 +206,9 @@ def update_user(self, user_update: UserUpdate) -> \"UserSchema\":\\n             The updated `UserSchema`.\\n         \"\"\"\\n         for field, value in user_update.dict(exclude_unset=True).items():\\n+            if field == \"old_password\":\\n+                continue\\n+\\n             if field == \"password\":\\n                 setattr(self, field, user_update.create_hashed_password())\\n             elif field == \"activation_token\":', '@@ -32,7 +32,7 @@\\n from zenml.config.global_config import GlobalConfiguration\\n from zenml.console import console\\n from zenml.enums import ServerProviderType, StoreType\\n-from zenml.exceptions import IllegalOperationError\\n+from zenml.exceptions import AuthorizationException, IllegalOperationError\\n from zenml.logger import get_logger\\n from zenml.utils import terraform_utils, yaml_utils\\n from zenml.zen_server.utils import get_active_deployment\\n@@ -624,12 +624,6 @@ def status() -> None:\\n     required=False,\\n     type=str,\\n )\\n-@click.option(\\n-    \"--workspace\",\\n-    help=\"The workspace to use when connecting to the ZenML server.\",\\n-    required=False,\\n-    type=str,\\n-)\\n @click.option(\\n     \"--no-verify-ssl\",\\n     is_flag=True,\\n@@ -661,7 +655,6 @@ def connect(\\n     username: Optional[str] = None,\\n     password: Optional[str] = None,\\n     api_key: Optional[str] = None,\\n-    workspace: Optional[str] = None,\\n     no_verify_ssl: bool = False,\\n     ssl_ca_cert: Optional[str] = None,\\n     config: Optional[str] = None,\\n@@ -677,8 +670,6 @@ def connect(\\n             server.\\n         api_key: The API key that is used to authenticate with the ZenML\\n             server.\\n-        workspace: The active workspace that is used to connect to the ZenML\\n-            server.\\n         no_verify_ssl: Whether to verify the server\\'s TLS certificate.\\n         ssl_ca_cert: A path to a CA bundle to use to verify the server\\'s TLS\\n             certificate or the CA bundle value itself.\\n@@ -689,6 +680,12 @@ def connect(\\n     from zenml.config.store_config import StoreConfiguration\\n     from zenml.zen_stores.base_zen_store import BaseZenStore\\n \\n+    if password is not None:\\n+        cli_utils.warning(\\n+            \"Supplying password values in the command line is not safe. \"\\n+            \"Please consider using the prompt option.\"\\n+        )\\n+\\n     # Raise an error if a local server is running when trying to connect to\\n     # another server\\n     active_deployment = get_active_deployment(local=True)\\n@@ -767,6 +764,16 @@ def connect(\\n             username = click.prompt(\"Username\", type=str)\\n \\n     if username:\\n+        cli_utils.warning(\\n+            \"Connecting to a ZenML server using a username and password is \"\\n+            \"not recommended because the password is locally stored on your \"\\n+            \"filesystem. You should consider using the web login workflow by \"\\n+            \"omitting the `--username` and `--password` flags. An alternative \"\\n+            \"for non-interactive environments is to create and use a service \"\\n+            \"account API key (see https://docs.zenml.io/user-guide/advanced-guide/configuring-zenml/connecting-to-zenml#using-service-accounts-to-connect-to-a-deployed-zenml-server \"\\n+            \"for more information).\"\\n+        )\\n+\\n         store_dict[\"username\"] = username\\n \\n         if password is None:\\n@@ -790,16 +797,8 @@ def connect(\\n             f\"User \\'{username}\\' does not have sufficient permissions to \"\\n             f\"access the server at \\'{url}\\'.\"\\n         )\\n-\\n-    if workspace:\\n-        try:\\n-            Client().set_active_workspace(workspace_name_or_id=workspace)\\n-        except KeyError:\\n-            cli_utils.warning(\\n-                f\"The workspace {workspace} does not exist or is not accessible. \"\\n-                f\"Please set another workspace by running `zenml \"\\n-                f\"workspace set`.\"\\n-            )\\n+    except AuthorizationException as e:\\n+        cli_utils.warning(f\"Authorization error: {e}\")\\n \\n \\n @cli.command(\"disconnect\", help=\"Disconnect from a ZenML server.\")', '@@ -814,7 +814,9 @@ def update_user(\\n         updated_email_opt_in: Optional[bool] = None,\\n         updated_hub_token: Optional[str] = None,\\n         updated_password: Optional[str] = None,\\n+        old_password: Optional[str] = None,\\n         updated_is_admin: Optional[bool] = None,\\n+        active: Optional[bool] = None,\\n     ) -> UserResponse:\\n         \"\"\"Update a user.\\n \\n@@ -826,10 +828,17 @@ def update_user(\\n             updated_email_opt_in: The new email opt-in status of the user.\\n             updated_hub_token: Update the hub token\\n             updated_password: The new password of the user.\\n+            old_password: The old password of the user. Required for password\\n+                update.\\n             updated_is_admin: Whether the user should be an admin.\\n+            active: Use to activate or deactivate the user.\\n \\n         Returns:\\n             The updated user.\\n+\\n+        Raises:\\n+            ValidationError: If the old password is not provided when updating\\n+                the password.\\n         \"\"\"\\n         user = self.get_user(\\n             name_id_or_prefix=name_id_or_prefix, allow_name_prefix_match=False\\n@@ -848,13 +857,36 @@ def update_user(\\n             user_update.hub_token = updated_hub_token\\n         if updated_password is not None:\\n             user_update.password = updated_password\\n+            if old_password is None:\\n+                raise ValidationError(\\n+                    \"Old password is required to update the password.\"\\n+                )\\n+            user_update.old_password = old_password\\n         if updated_is_admin is not None:\\n             user_update.is_admin = updated_is_admin\\n+        if active is not None:\\n+            user_update.active = active\\n \\n         return self.zen_store.update_user(\\n             user_id=user.id, user_update=user_update\\n         )\\n \\n+    @_fail_for_sql_zen_store\\n+    def deactivate_user(self, name_id_or_prefix: str) -> \"UserResponse\":\\n+        \"\"\"Deactivate a user and generate an activation token.\\n+\\n+        Args:\\n+            name_id_or_prefix: The name or ID of the user to reset.\\n+\\n+        Returns:\\n+            The deactivated user.\\n+        \"\"\"\\n+        from zenml.zen_stores.rest_zen_store import RestZenStore\\n+\\n+        user = self.get_user(name_id_or_prefix, allow_name_prefix_match=False)\\n+        assert isinstance(self.zen_store, RestZenStore)\\n+        return self.zen_store.deactivate_user(user_name_or_id=user.name)\\n+\\n     def delete_user(self, name_id_or_prefix: str) -> None:\\n         \"\"\"Delete a user.\\n ', '@@ -32,6 +32,7 @@\\n from zenml.logger import get_logger\\n from zenml.models import (\\n     Page,\\n+    UserAuthModel,\\n     UserFilter,\\n     UserRequest,\\n     UserResponse,\\n@@ -253,7 +254,10 @@ def update_user(\\n \\n         Raises:\\n             IllegalOperationError: if the user tries change admin status,\\n-                while not an admin\\n+                while not an admin, if the user tries to change the password\\n+                of another user, or if the user tries to change their own\\n+                password without providing the old password or providing\\n+                an incorrect old password.\\n         \"\"\"\\n         user = zen_store().get_user(user_name_or_id)\\n         if user.id != auth_context.user.id:\\n@@ -264,6 +268,29 @@ def update_user(\\n                 user,\\n                 action=Action.UPDATE,\\n             )\\n+\\n+            if user_update.password is not None:\\n+                raise IllegalOperationError(\\n+                    \"Users cannot change the password of other users. Use the \"\\n+                    \"account deactivation and activation flow instead.\"\\n+                )\\n+\\n+        elif user_update.password is not None:\\n+            # If the user is updating their own password, we need to verify\\n+            # the old password\\n+            if user_update.old_password is None:\\n+                raise IllegalOperationError(\\n+                    \"The current password must be supplied when changing the \"\\n+                    \"password.\"\\n+                )\\n+            auth_user = zen_store().get_auth_user(user_name_or_id)\\n+            if not UserAuthModel.verify_password(\\n+                user_update.old_password, auth_user\\n+            ):\\n+                raise IllegalOperationError(\\n+                    \"The current password is incorrect.\"\\n+                )\\n+\\n         if (\\n             user_update.is_admin is not None\\n             and user.is_admin != user_update.is_admin\\n@@ -512,8 +539,27 @@ def update_myself(\\n \\n         Returns:\\n             The updated user.\\n+\\n+        Raises:\\n+            IllegalOperationError: if the current password is not supplied when\\n+                changing the password or if the current password is incorrect.\\n         \"\"\"\\n         current_user = zen_store().get_user(auth_context.user.id)\\n+\\n+        if user.password is not None:\\n+            # If the user is updating their password, we need to verify\\n+            # the old password\\n+            if user.old_password is None:\\n+                raise IllegalOperationError(\\n+                    \"The current password must be supplied when changing the \"\\n+                    \"password.\"\\n+                )\\n+            auth_user = zen_store().get_auth_user(auth_context.user.id)\\n+            if not UserAuthModel.verify_password(user.old_password, auth_user):\\n+                raise IllegalOperationError(\\n+                    \"The current password is incorrect.\"\\n+                )\\n+\\n         user.activation_token = current_user.activation_token\\n         user.active = current_user.active\\n         user.is_admin = current_user.is_admin', '@@ -50,6 +50,7 @@\\n     CODE_REFERENCES,\\n     CODE_REPOSITORIES,\\n     CURRENT_USER,\\n+    DEACTIVATE,\\n     DEFAULT_HTTP_TIMEOUT,\\n     DEVICES,\\n     DISABLE_CLIENT_SERVER_MISMATCH_WARNING,\\n@@ -2943,6 +2944,23 @@ def update_user(\\n             response_model=UserResponse,\\n         )\\n \\n+    def deactivate_user(\\n+        self, user_name_or_id: Union[str, UUID]\\n+    ) -> UserResponse:\\n+        \"\"\"Deactivates a user.\\n+\\n+        Args:\\n+            user_name_or_id: The name or ID of the user to delete.\\n+\\n+        Returns:\\n+            The deactivated user containing the activation token.\\n+        \"\"\"\\n+        response_body = self.put(\\n+            f\"{USERS}/{str(user_name_or_id)}{DEACTIVATE}\",\\n+        )\\n+\\n+        return UserResponse.parse_obj(response_body)\\n+\\n     def delete_user(self, user_name_or_id: Union[str, UUID]) -> None:\\n         \"\"\"Deletes a user.\\n '], 'file': ['src/zenml/models/v2/core/user.py', 'src/zenml/cli/user_management.py', 'src/zenml/zen_stores/schemas/user_schemas.py', 'src/zenml/cli/server.py', 'src/zenml/client.py', 'src/zenml/zen_server/routers/users_endpoints.py', 'src/zenml/zen_stores/rest_zen_store.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('94528472-1500-45e6-89c9-28c25feaaab2'), UUID('fa791959-6314-4722-a6f3-7c1a6eae12b5'), UUID('a3df4613-dc35-44fc-bc30-3ef06fff12b2'), UUID('8dbf4d6f-e3b7-4875-9fe1-492104178b09'), UUID('3a6d71e3-5ed9-4672-be03-166420d103d3'), UUID('a0bc8864-f424-457e-8524-586b3bc937fe'), UUID('7f1a6a8c-9652-4ccd-94b0-1894202ac154')]}\n",
      "ERROR:root:Error in {'repo': 'zenml-io/zenml', 'vulnerability_id': '2024-2213', 'commit': '58cb3d987372c91eb605853c35325701733337c2', 'commit_source': 'github', 'cwe_id': ['CWE-287'], 'patch': ['@@ -201,6 +201,12 @@ class UserUpdate(UserBase, BaseZenModel):\\n     active: Optional[bool] = Field(\\n         default=None, title=\"Whether the account is active.\"\\n     )\\n+    old_password: Optional[str] = Field(\\n+        default=None,\\n+        title=\"The previous password for the user. Only relevant for user \"\\n+        \"accounts. Required when updating the password.\",\\n+        max_length=STR_FIELD_MAX_LENGTH,\\n+    )\\n \\n     @root_validator\\n     def user_email_updates(cls, values: Dict[str, Any]) -> Dict[str, Any]:', '@@ -21,9 +21,14 @@\\n from zenml.cli.cli import TagGroup, cli\\n from zenml.cli.utils import is_sorted_or_filtered, list_options\\n from zenml.client import Client\\n+from zenml.config.global_config import GlobalConfiguration\\n from zenml.console import console\\n from zenml.enums import CliCategories, StoreType\\n-from zenml.exceptions import EntityExistsError, IllegalOperationError\\n+from zenml.exceptions import (\\n+    AuthorizationException,\\n+    EntityExistsError,\\n+    IllegalOperationError,\\n+)\\n from zenml.models import UserFilter\\n \\n \\n@@ -156,6 +161,11 @@ def create_user(\\n                 default=\"\",\\n                 hide_input=True,\\n             )\\n+    else:\\n+        cli_utils.warning(\\n+            \"Supplying password values in the command line is not safe. \"\\n+            \"Please consider using the prompt option.\"\\n+        )\\n \\n     try:\\n         new_user = client.create_user(\\n@@ -204,14 +214,6 @@ def create_user(\\n     required=False,\\n     help=\"New user email.\",\\n )\\n-@click.option(\\n-    \"--password\",\\n-    \"-p\",\\n-    \"updated_password\",\\n-    type=str,\\n-    required=False,\\n-    help=\"New user password.\",\\n-)\\n @click.option(\\n     \"--admin\",\\n     \"-a\",\\n@@ -230,14 +232,22 @@ def create_user(\\n     default=None,\\n     help=\"Whether the user should be a regular user.\",\\n )\\n+@click.option(\\n+    \"--active\",\\n+    \"active\",\\n+    type=bool,\\n+    required=False,\\n+    default=None,\\n+    help=\"Use to activate or deactivate a user account.\",\\n+)\\n def update_user(\\n     user_name_or_id: str,\\n     updated_name: Optional[str] = None,\\n     updated_full_name: Optional[str] = None,\\n     updated_email: Optional[str] = None,\\n-    updated_password: Optional[str] = None,\\n     make_admin: Optional[bool] = None,\\n     make_user: Optional[bool] = None,\\n+    active: Optional[bool] = None,\\n ) -> None:\\n     \"\"\"Update an existing user.\\n \\n@@ -246,9 +256,9 @@ def update_user(\\n         updated_name: The name of the user to create.\\n         updated_full_name: The name of the user to create.\\n         updated_email: The name of the user to create.\\n-        updated_password: The name of the user to create.\\n         make_admin: Whether the user should be an admin.\\n         make_user: Whether the user should be a regular user.\\n+        active: Use to activate or deactivate a user account.\\n     \"\"\"\\n     if make_admin is not None and make_user is not None:\\n         cli_utils.error(\\n@@ -260,7 +270,8 @@ def update_user(\\n         )\\n         if current_user.is_admin and make_user:\\n             confirmation = cli_utils.confirmation(\\n-                f\"Currently user `{current_user.name}` is an admin. Are you sure you want to make them a regular user?\"\\n+                f\"Currently user `{current_user.name}` is an admin. Are you \"\\n+                \"sure you want to make them a regular user?\"\\n             )\\n             if not confirmation:\\n                 cli_utils.declare(\"User update canceled.\")\\n@@ -276,13 +287,137 @@ def update_user(\\n             updated_name=updated_name,\\n             updated_full_name=updated_full_name,\\n             updated_email=updated_email,\\n-            updated_password=updated_password,\\n             updated_is_admin=updated_is_admin,\\n+            active=active,\\n         )\\n     except (KeyError, IllegalOperationError) as err:\\n         cli_utils.error(str(err))\\n \\n \\n+@user.command(\\n+    \"change-password\",\\n+    help=\"Change the password for the current user account.\",\\n+)\\n+@click.option(\\n+    \"--password\",\\n+    help=(\\n+        \"The new user password. If omitted, a prompt will be shown to enter \"\\n+        \"the password.\"\\n+    ),\\n+    required=False,\\n+    type=str,\\n+)\\n+@click.option(\\n+    \"--old-password\",\\n+    help=(\\n+        \"The old user password. If omitted, a prompt will be shown to enter \"\\n+        \"the old password.\"\\n+    ),\\n+    required=False,\\n+    type=str,\\n+)\\n+def change_user_password(\\n+    password: Optional[str] = None, old_password: Optional[str] = None\\n+) -> None:\\n+    \"\"\"Change the password of the current user.\\n+\\n+    Args:\\n+        password: The new password for the current user.\\n+        old_password: The old password for the current user.\\n+    \"\"\"\\n+    active_user = Client().active_user\\n+\\n+    if old_password is not None or password is not None:\\n+        cli_utils.warning(\\n+            \"Supplying password values in the command line is not safe. \"\\n+            \"Please consider using the prompt option.\"\\n+        )\\n+\\n+    if old_password is None:\\n+        old_password = click.prompt(\\n+            f\"Current password for user {active_user.name}\",\\n+            hide_input=True,\\n+        )\\n+    if password is None:\\n+        password = click.prompt(\\n+            f\"New password for user {active_user.name}\",\\n+            hide_input=True,\\n+        )\\n+        password_again = click.prompt(\\n+            f\"Please re-enter the new password for user {active_user.name}\",\\n+            hide_input=True,\\n+        )\\n+        if password != password_again:\\n+            cli_utils.error(\"Passwords do not match.\")\\n+\\n+    try:\\n+        Client().update_user(\\n+            name_id_or_prefix=active_user.id,\\n+            old_password=old_password,\\n+            updated_password=password,\\n+        )\\n+    except (KeyError, IllegalOperationError, AuthorizationException) as err:\\n+        cli_utils.error(str(err))\\n+\\n+    cli_utils.declare(\\n+        f\"Successfully updated password for active user \\'{active_user.name}\\'.\"\\n+    )\\n+\\n+    store = GlobalConfiguration().store_configuration\\n+    if store.type == StoreType.REST:\\n+        from zenml.zen_stores.rest_zen_store import RestZenStoreConfiguration\\n+\\n+        assert isinstance(store, RestZenStoreConfiguration)\\n+\\n+        if store.password is not None:\\n+            cli_utils.declare(\\n+                \"You may need to log in again with your new password by \"\\n+                \"running `zenml connect`.\"\\n+            )\\n+\\n+\\n+@user.command(\\n+    \"deactivate\",\\n+    help=\"Generate an activation token to reset the password for a user account\",\\n+)\\n+@click.argument(\"user_name_or_id\", type=str, required=True)\\n+def deactivate_user(\\n+    user_name_or_id: str,\\n+) -> None:\\n+    \"\"\"Reset the password of a user.\\n+\\n+    Args:\\n+        user_name_or_id: The name or ID of the user to reset the password for.\\n+    \"\"\"\\n+    client = Client()\\n+\\n+    store = GlobalConfiguration().store_configuration\\n+    if store.type != StoreType.REST:\\n+        cli_utils.error(\\n+            \"Deactivating users is only supported when connected to a ZenML \"\\n+            \"server.\"\\n+        )\\n+\\n+    try:\\n+        if not client.active_user.is_admin:\\n+            cli_utils.error(\\n+                \"Only admins can reset the password of other users.\"\\n+            )\\n+\\n+        user = client.deactivate_user(\\n+            name_id_or_prefix=user_name_or_id,\\n+        )\\n+    except (KeyError, IllegalOperationError) as err:\\n+        cli_utils.error(str(err))\\n+\\n+    cli_utils.declare(\\n+        f\"Successfully deactivated user account \\'{user.name}\\'.\"\\n+        f\"To reactivate the account, please visit the dashboard at the \"\\n+        \"following URL:\\\\n\"\\n+        f\"{client.zen_store.url}/signup?user={str(user.id)}&username={user.name}&token={user.activation_token}\\\\n\"\\n+    )\\n+\\n+\\n @user.command(\"delete\")\\n @click.argument(\"user_name_or_id\", type=str, required=True)\\n def delete_user(user_name_or_id: str) -> None:', '@@ -206,6 +206,9 @@ def update_user(self, user_update: UserUpdate) -> \"UserSchema\":\\n             The updated `UserSchema`.\\n         \"\"\"\\n         for field, value in user_update.dict(exclude_unset=True).items():\\n+            if field == \"old_password\":\\n+                continue\\n+\\n             if field == \"password\":\\n                 setattr(self, field, user_update.create_hashed_password())\\n             elif field == \"activation_token\":', '@@ -32,7 +32,7 @@\\n from zenml.config.global_config import GlobalConfiguration\\n from zenml.console import console\\n from zenml.enums import ServerProviderType, StoreType\\n-from zenml.exceptions import IllegalOperationError\\n+from zenml.exceptions import AuthorizationException, IllegalOperationError\\n from zenml.logger import get_logger\\n from zenml.utils import terraform_utils, yaml_utils\\n from zenml.zen_server.utils import get_active_deployment\\n@@ -624,12 +624,6 @@ def status() -> None:\\n     required=False,\\n     type=str,\\n )\\n-@click.option(\\n-    \"--workspace\",\\n-    help=\"The workspace to use when connecting to the ZenML server.\",\\n-    required=False,\\n-    type=str,\\n-)\\n @click.option(\\n     \"--no-verify-ssl\",\\n     is_flag=True,\\n@@ -661,7 +655,6 @@ def connect(\\n     username: Optional[str] = None,\\n     password: Optional[str] = None,\\n     api_key: Optional[str] = None,\\n-    workspace: Optional[str] = None,\\n     no_verify_ssl: bool = False,\\n     ssl_ca_cert: Optional[str] = None,\\n     config: Optional[str] = None,\\n@@ -677,8 +670,6 @@ def connect(\\n             server.\\n         api_key: The API key that is used to authenticate with the ZenML\\n             server.\\n-        workspace: The active workspace that is used to connect to the ZenML\\n-            server.\\n         no_verify_ssl: Whether to verify the server\\'s TLS certificate.\\n         ssl_ca_cert: A path to a CA bundle to use to verify the server\\'s TLS\\n             certificate or the CA bundle value itself.\\n@@ -689,6 +680,12 @@ def connect(\\n     from zenml.config.store_config import StoreConfiguration\\n     from zenml.zen_stores.base_zen_store import BaseZenStore\\n \\n+    if password is not None:\\n+        cli_utils.warning(\\n+            \"Supplying password values in the command line is not safe. \"\\n+            \"Please consider using the prompt option.\"\\n+        )\\n+\\n     # Raise an error if a local server is running when trying to connect to\\n     # another server\\n     active_deployment = get_active_deployment(local=True)\\n@@ -767,6 +764,16 @@ def connect(\\n             username = click.prompt(\"Username\", type=str)\\n \\n     if username:\\n+        cli_utils.warning(\\n+            \"Connecting to a ZenML server using a username and password is \"\\n+            \"not recommended because the password is locally stored on your \"\\n+            \"filesystem. You should consider using the web login workflow by \"\\n+            \"omitting the `--username` and `--password` flags. An alternative \"\\n+            \"for non-interactive environments is to create and use a service \"\\n+            \"account API key (see https://docs.zenml.io/user-guide/advanced-guide/configuring-zenml/connecting-to-zenml#using-service-accounts-to-connect-to-a-deployed-zenml-server \"\\n+            \"for more information).\"\\n+        )\\n+\\n         store_dict[\"username\"] = username\\n \\n         if password is None:\\n@@ -790,16 +797,8 @@ def connect(\\n             f\"User \\'{username}\\' does not have sufficient permissions to \"\\n             f\"access the server at \\'{url}\\'.\"\\n         )\\n-\\n-    if workspace:\\n-        try:\\n-            Client().set_active_workspace(workspace_name_or_id=workspace)\\n-        except KeyError:\\n-            cli_utils.warning(\\n-                f\"The workspace {workspace} does not exist or is not accessible. \"\\n-                f\"Please set another workspace by running `zenml \"\\n-                f\"workspace set`.\"\\n-            )\\n+    except AuthorizationException as e:\\n+        cli_utils.warning(f\"Authorization error: {e}\")\\n \\n \\n @cli.command(\"disconnect\", help=\"Disconnect from a ZenML server.\")', '@@ -814,7 +814,9 @@ def update_user(\\n         updated_email_opt_in: Optional[bool] = None,\\n         updated_hub_token: Optional[str] = None,\\n         updated_password: Optional[str] = None,\\n+        old_password: Optional[str] = None,\\n         updated_is_admin: Optional[bool] = None,\\n+        active: Optional[bool] = None,\\n     ) -> UserResponse:\\n         \"\"\"Update a user.\\n \\n@@ -826,10 +828,17 @@ def update_user(\\n             updated_email_opt_in: The new email opt-in status of the user.\\n             updated_hub_token: Update the hub token\\n             updated_password: The new password of the user.\\n+            old_password: The old password of the user. Required for password\\n+                update.\\n             updated_is_admin: Whether the user should be an admin.\\n+            active: Use to activate or deactivate the user.\\n \\n         Returns:\\n             The updated user.\\n+\\n+        Raises:\\n+            ValidationError: If the old password is not provided when updating\\n+                the password.\\n         \"\"\"\\n         user = self.get_user(\\n             name_id_or_prefix=name_id_or_prefix, allow_name_prefix_match=False\\n@@ -848,13 +857,36 @@ def update_user(\\n             user_update.hub_token = updated_hub_token\\n         if updated_password is not None:\\n             user_update.password = updated_password\\n+            if old_password is None:\\n+                raise ValidationError(\\n+                    \"Old password is required to update the password.\"\\n+                )\\n+            user_update.old_password = old_password\\n         if updated_is_admin is not None:\\n             user_update.is_admin = updated_is_admin\\n+        if active is not None:\\n+            user_update.active = active\\n \\n         return self.zen_store.update_user(\\n             user_id=user.id, user_update=user_update\\n         )\\n \\n+    @_fail_for_sql_zen_store\\n+    def deactivate_user(self, name_id_or_prefix: str) -> \"UserResponse\":\\n+        \"\"\"Deactivate a user and generate an activation token.\\n+\\n+        Args:\\n+            name_id_or_prefix: The name or ID of the user to reset.\\n+\\n+        Returns:\\n+            The deactivated user.\\n+        \"\"\"\\n+        from zenml.zen_stores.rest_zen_store import RestZenStore\\n+\\n+        user = self.get_user(name_id_or_prefix, allow_name_prefix_match=False)\\n+        assert isinstance(self.zen_store, RestZenStore)\\n+        return self.zen_store.deactivate_user(user_name_or_id=user.name)\\n+\\n     def delete_user(self, name_id_or_prefix: str) -> None:\\n         \"\"\"Delete a user.\\n ', '@@ -32,6 +32,7 @@\\n from zenml.logger import get_logger\\n from zenml.models import (\\n     Page,\\n+    UserAuthModel,\\n     UserFilter,\\n     UserRequest,\\n     UserResponse,\\n@@ -253,7 +254,10 @@ def update_user(\\n \\n         Raises:\\n             IllegalOperationError: if the user tries change admin status,\\n-                while not an admin\\n+                while not an admin, if the user tries to change the password\\n+                of another user, or if the user tries to change their own\\n+                password without providing the old password or providing\\n+                an incorrect old password.\\n         \"\"\"\\n         user = zen_store().get_user(user_name_or_id)\\n         if user.id != auth_context.user.id:\\n@@ -264,6 +268,29 @@ def update_user(\\n                 user,\\n                 action=Action.UPDATE,\\n             )\\n+\\n+            if user_update.password is not None:\\n+                raise IllegalOperationError(\\n+                    \"Users cannot change the password of other users. Use the \"\\n+                    \"account deactivation and activation flow instead.\"\\n+                )\\n+\\n+        elif user_update.password is not None:\\n+            # If the user is updating their own password, we need to verify\\n+            # the old password\\n+            if user_update.old_password is None:\\n+                raise IllegalOperationError(\\n+                    \"The current password must be supplied when changing the \"\\n+                    \"password.\"\\n+                )\\n+            auth_user = zen_store().get_auth_user(user_name_or_id)\\n+            if not UserAuthModel.verify_password(\\n+                user_update.old_password, auth_user\\n+            ):\\n+                raise IllegalOperationError(\\n+                    \"The current password is incorrect.\"\\n+                )\\n+\\n         if (\\n             user_update.is_admin is not None\\n             and user.is_admin != user_update.is_admin\\n@@ -512,8 +539,27 @@ def update_myself(\\n \\n         Returns:\\n             The updated user.\\n+\\n+        Raises:\\n+            IllegalOperationError: if the current password is not supplied when\\n+                changing the password or if the current password is incorrect.\\n         \"\"\"\\n         current_user = zen_store().get_user(auth_context.user.id)\\n+\\n+        if user.password is not None:\\n+            # If the user is updating their password, we need to verify\\n+            # the old password\\n+            if user.old_password is None:\\n+                raise IllegalOperationError(\\n+                    \"The current password must be supplied when changing the \"\\n+                    \"password.\"\\n+                )\\n+            auth_user = zen_store().get_auth_user(auth_context.user.id)\\n+            if not UserAuthModel.verify_password(user.old_password, auth_user):\\n+                raise IllegalOperationError(\\n+                    \"The current password is incorrect.\"\\n+                )\\n+\\n         user.activation_token = current_user.activation_token\\n         user.active = current_user.active\\n         user.is_admin = current_user.is_admin', '@@ -50,6 +50,7 @@\\n     CODE_REFERENCES,\\n     CODE_REPOSITORIES,\\n     CURRENT_USER,\\n+    DEACTIVATE,\\n     DEFAULT_HTTP_TIMEOUT,\\n     DEVICES,\\n     DISABLE_CLIENT_SERVER_MISMATCH_WARNING,\\n@@ -2943,6 +2944,23 @@ def update_user(\\n             response_model=UserResponse,\\n         )\\n \\n+    def deactivate_user(\\n+        self, user_name_or_id: Union[str, UUID]\\n+    ) -> UserResponse:\\n+        \"\"\"Deactivates a user.\\n+\\n+        Args:\\n+            user_name_or_id: The name or ID of the user to delete.\\n+\\n+        Returns:\\n+            The deactivated user containing the activation token.\\n+        \"\"\"\\n+        response_body = self.put(\\n+            f\"{USERS}/{str(user_name_or_id)}{DEACTIVATE}\",\\n+        )\\n+\\n+        return UserResponse.parse_obj(response_body)\\n+\\n     def delete_user(self, user_name_or_id: Union[str, UUID]) -> None:\\n         \"\"\"Deletes a user.\\n '], 'file': ['src/zenml/models/v2/core/user.py', 'src/zenml/cli/user_management.py', 'src/zenml/zen_stores/schemas/user_schemas.py', 'src/zenml/cli/server.py', 'src/zenml/client.py', 'src/zenml/zen_server/routers/users_endpoints.py', 'src/zenml/zen_stores/rest_zen_store.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('94528472-1500-45e6-89c9-28c25feaaab2'), UUID('fa791959-6314-4722-a6f3-7c1a6eae12b5'), UUID('a3df4613-dc35-44fc-bc30-3ef06fff12b2'), UUID('8dbf4d6f-e3b7-4875-9fe1-492104178b09'), UUID('3a6d71e3-5ed9-4672-be03-166420d103d3'), UUID('a0bc8864-f424-457e-8524-586b3bc937fe'), UUID('7f1a6a8c-9652-4ccd-94b0-1894202ac154')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 9:0:     def update_user(\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 9:0:     def update_user(\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1318/1800 [16:35<07:10,  1.12it/s]ERROR:src.process_code_changes:Error processing commit 7b68405c81db889f83c32846462b238ccae5be80\n",
      "ERROR:src.process_code_changes:{'repo': 'yaml/pyyaml', 'vulnerability_id': '2017-18342', 'commit': '7b68405c81db889f83c32846462b238ccae5be80', 'commit_source': 'github', 'cwe_id': ['CWE-502'], 'patch': [\"@@ -1,6 +1,6 @@\\n \\n-__all__ = ['CBaseLoader', 'CSafeLoader', 'CLoader',\\n-        'CBaseDumper', 'CSafeDumper', 'CDumper']\\n+__all__ = ['CBaseLoader', 'CSafeLoader', 'CLoader', 'CDangerLoader',\\n+        'CBaseDumper', 'CSafeDumper', 'CDumper', 'CDangerDumper']\\n \\n from _yaml import CParser, CEmitter\\n \\n@@ -18,14 +18,15 @@ def __init__(self, stream):\\n         BaseConstructor.__init__(self)\\n         BaseResolver.__init__(self)\\n \\n-class CSafeLoader(CParser, SafeConstructor, Resolver):\\n+class CLoader(CParser, SafeConstructor, Resolver):\\n \\n     def __init__(self, stream):\\n         CParser.__init__(self, stream)\\n         SafeConstructor.__init__(self)\\n         Resolver.__init__(self)\\n+CSafeLoader = CLoader\\n \\n-class CLoader(CParser, Constructor, Resolver):\\n+class CDangerLoader(CParser, Constructor, Resolver):\\n \\n     def __init__(self, stream):\\n         CParser.__init__(self, stream)\\n@@ -49,7 +50,7 @@ def __init__(self, stream,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n \\n-class CSafeDumper(CEmitter, SafeRepresenter, Resolver):\\n+class CDumper(CEmitter, SafeRepresenter, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -65,8 +66,9 @@ def __init__(self, stream,\\n         SafeRepresenter.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n+CSafeDumper = CDumper\\n \\n-class CDumper(CEmitter, Serializer, Representer, Resolver):\\n+class CDangerDumper(CEmitter, Serializer, Representer, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -82,4 +84,3 @@ def __init__(self, stream,\\n         Representer.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n-\", \"@@ -1,5 +1,5 @@\\n \\n-__all__ = ['BaseLoader', 'SafeLoader', 'Loader']\\n+__all__ = ['BaseLoader', 'SafeLoader', 'Loader', 'DangerLoader']\\n \\n from reader import *\\n from scanner import *\\n@@ -18,7 +18,7 @@ def __init__(self, stream):\\n         BaseConstructor.__init__(self)\\n         BaseResolver.__init__(self)\\n \\n-class SafeLoader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):\\n+class Loader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):\\n \\n     def __init__(self, stream):\\n         Reader.__init__(self, stream)\\n@@ -27,8 +27,9 @@ def __init__(self, stream):\\n         Composer.__init__(self)\\n         SafeConstructor.__init__(self)\\n         Resolver.__init__(self)\\n+SafeLoader = Loader\\n \\n-class Loader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\\n+class DangerLoader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\\n \\n     def __init__(self, stream):\\n         Reader.__init__(self, stream)\\n@@ -37,4 +38,3 @@ def __init__(self, stream):\\n         Composer.__init__(self)\\n         Constructor.__init__(self)\\n         Resolver.__init__(self)\\n-\", \"@@ -1,5 +1,5 @@\\n \\n-__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']\\n+__all__ = ['BaseDumper', 'SafeDumper', 'Dumper', 'DangerDumper']\\n \\n from emitter import *\\n from serializer import *\\n@@ -24,7 +24,7 @@ def __init__(self, stream,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n \\n-class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):\\n+class Dumper(Emitter, Serializer, SafeRepresenter, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -41,8 +41,9 @@ def __init__(self, stream,\\n         SafeRepresenter.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n+SafeDumper = Dump\\n \\n-class Dumper(Emitter, Serializer, Representer, Resolver):\\n+class DangerDumper(Emitter, Serializer, Representer, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -59,4 +60,3 @@ def __init__(self, stream,\\n         Representer.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n-\"], 'file': ['lib/yaml/cyaml.py', 'lib/yaml/loader.py', 'lib/yaml/dumper.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('cbccb25d-7059-4d47-8b71-e8bb6bf24d1c'), UUID('32934232-6288-4d9d-9c16-c68a13407800'), UUID('4a04f892-027d-4095-b31b-0d38aab89fb1')]}\n",
      "ERROR:root:Error in {'repo': 'yaml/pyyaml', 'vulnerability_id': '2017-18342', 'commit': '7b68405c81db889f83c32846462b238ccae5be80', 'commit_source': 'github', 'cwe_id': ['CWE-502'], 'patch': [\"@@ -1,6 +1,6 @@\\n \\n-__all__ = ['CBaseLoader', 'CSafeLoader', 'CLoader',\\n-        'CBaseDumper', 'CSafeDumper', 'CDumper']\\n+__all__ = ['CBaseLoader', 'CSafeLoader', 'CLoader', 'CDangerLoader',\\n+        'CBaseDumper', 'CSafeDumper', 'CDumper', 'CDangerDumper']\\n \\n from _yaml import CParser, CEmitter\\n \\n@@ -18,14 +18,15 @@ def __init__(self, stream):\\n         BaseConstructor.__init__(self)\\n         BaseResolver.__init__(self)\\n \\n-class CSafeLoader(CParser, SafeConstructor, Resolver):\\n+class CLoader(CParser, SafeConstructor, Resolver):\\n \\n     def __init__(self, stream):\\n         CParser.__init__(self, stream)\\n         SafeConstructor.__init__(self)\\n         Resolver.__init__(self)\\n+CSafeLoader = CLoader\\n \\n-class CLoader(CParser, Constructor, Resolver):\\n+class CDangerLoader(CParser, Constructor, Resolver):\\n \\n     def __init__(self, stream):\\n         CParser.__init__(self, stream)\\n@@ -49,7 +50,7 @@ def __init__(self, stream,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n \\n-class CSafeDumper(CEmitter, SafeRepresenter, Resolver):\\n+class CDumper(CEmitter, SafeRepresenter, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -65,8 +66,9 @@ def __init__(self, stream,\\n         SafeRepresenter.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n+CSafeDumper = CDumper\\n \\n-class CDumper(CEmitter, Serializer, Representer, Resolver):\\n+class CDangerDumper(CEmitter, Serializer, Representer, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -82,4 +84,3 @@ def __init__(self, stream,\\n         Representer.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n-\", \"@@ -1,5 +1,5 @@\\n \\n-__all__ = ['BaseLoader', 'SafeLoader', 'Loader']\\n+__all__ = ['BaseLoader', 'SafeLoader', 'Loader', 'DangerLoader']\\n \\n from reader import *\\n from scanner import *\\n@@ -18,7 +18,7 @@ def __init__(self, stream):\\n         BaseConstructor.__init__(self)\\n         BaseResolver.__init__(self)\\n \\n-class SafeLoader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):\\n+class Loader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):\\n \\n     def __init__(self, stream):\\n         Reader.__init__(self, stream)\\n@@ -27,8 +27,9 @@ def __init__(self, stream):\\n         Composer.__init__(self)\\n         SafeConstructor.__init__(self)\\n         Resolver.__init__(self)\\n+SafeLoader = Loader\\n \\n-class Loader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\\n+class DangerLoader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\\n \\n     def __init__(self, stream):\\n         Reader.__init__(self, stream)\\n@@ -37,4 +38,3 @@ def __init__(self, stream):\\n         Composer.__init__(self)\\n         Constructor.__init__(self)\\n         Resolver.__init__(self)\\n-\", \"@@ -1,5 +1,5 @@\\n \\n-__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']\\n+__all__ = ['BaseDumper', 'SafeDumper', 'Dumper', 'DangerDumper']\\n \\n from emitter import *\\n from serializer import *\\n@@ -24,7 +24,7 @@ def __init__(self, stream,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n \\n-class SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):\\n+class Dumper(Emitter, Serializer, SafeRepresenter, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -41,8 +41,9 @@ def __init__(self, stream,\\n         SafeRepresenter.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n+SafeDumper = Dump\\n \\n-class Dumper(Emitter, Serializer, Representer, Resolver):\\n+class DangerDumper(Emitter, Serializer, Representer, Resolver):\\n \\n     def __init__(self, stream,\\n             default_style=None, default_flow_style=None,\\n@@ -59,4 +60,3 @@ def __init__(self, stream,\\n         Representer.__init__(self, default_style=default_style,\\n                 default_flow_style=default_flow_style)\\n         Resolver.__init__(self)\\n-\"], 'file': ['lib/yaml/cyaml.py', 'lib/yaml/loader.py', 'lib/yaml/dumper.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('cbccb25d-7059-4d47-8b71-e8bb6bf24d1c'), UUID('32934232-6288-4d9d-9c16-c68a13407800'), UUID('4a04f892-027d-4095-b31b-0d38aab89fb1')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0: CSafeLoader = CLoader\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 5:0: CSafeLoader = CLoader\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1352/1800 [16:35<01:57,  3.83it/s]ERROR:src.process_code_changes:Error processing commit 0aa1ee22ab6e204e9d3d0e9dd63ea648ed691ef1\n",
      "ERROR:src.process_code_changes:{'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': '0aa1ee22ab6e204e9d3d0e9dd63ea648ed691ef1', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -2152,7 +2152,7 @@ def _parents(path):\\n def _ancestry(path):\\n     \"\"\"\\n     Given a path with elements separated by\\n-    posixpath.sep, generate all elements of that path\\n+    posixpath.sep, generate all elements of that path.\\n \\n     >>> list(_ancestry(\\'b/d\\'))\\n     [\\'b/d\\', \\'b\\']\\n@@ -2164,9 +2164,14 @@ def _ancestry(path):\\n     [\\'b\\']\\n     >>> list(_ancestry(\\'\\'))\\n     []\\n+\\n+    Multiple separators are treated like a single.\\n+\\n+    >>> list(_ancestry(\\'//b//d///f//\\'))\\n+    [\\'//b//d///f\\', \\'//b//d\\', \\'//b\\']\\n     \"\"\"\\n     path = path.rstrip(posixpath.sep)\\n-    while path and path != posixpath.sep:\\n+    while path.rstrip(posixpath.sep):\\n         yield path\\n         path, tail = posixpath.split(path)\\n \\n@@ -2183,65 +2188,7 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class SanitizedNames:\\n-    \"\"\"\\n-    ZipFile mix-in to ensure names are sanitized.\\n-    \"\"\"\\n-\\n-    def namelist(self):\\n-        return list(map(self._sanitize, super().namelist()))\\n-\\n-    @staticmethod\\n-    def _sanitize(name):\\n-        r\"\"\"\\n-        Ensure a relative path with posix separators and no dot names.\\n-        Modeled after\\n-        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n-        but provides consistent cross-platform behavior.\\n-        >>> san = SanitizedNames._sanitize\\n-        >>> san(\\'/foo/bar\\')\\n-        \\'foo/bar\\'\\n-        >>> san(\\'//foo.txt\\')\\n-        \\'foo.txt\\'\\n-        >>> san(\\'foo/.././bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'foo../.bar.txt\\')\\n-        \\'foo../.bar.txt\\'\\n-        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n-        \\'D/foo.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n-        \\'server/share/file.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n-        \\'?/GLOBALROOT/Volume3\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n-        \\'PhysicalDrive1/root\\'\\n-        Retain any trailing slash.\\n-        >>> san(\\'abc/\\')\\n-        \\'abc/\\'\\n-        Raises a ValueError if the result is empty.\\n-        >>> san(\\'../..\\')\\n-        Traceback (most recent call last):\\n-        ...\\n-        ValueError: Empty filename\\n-        \"\"\"\\n-\\n-        def allowed(part):\\n-            return part and part not in {\\'..\\', \\'.\\'}\\n-\\n-        # Remove the drive letter.\\n-        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n-        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n-        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n-        parts = clean.split(\\'/\\')\\n-        joined = \\'/\\'.join(filter(allowed, parts))\\n-        if not joined:\\n-            raise ValueError(\"Empty filename\")\\n-        return joined + \\'/\\' * name.endswith(\\'/\\')\\n-\\n-\\n-class CompleteDirs(SanitizedNames, ZipFile):\\n+class CompleteDirs(ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('ea0a3a86-577a-480f-a309-35ab16c4255f')]}\n",
      "ERROR:root:Error in {'repo': 'python/cpython', 'vulnerability_id': '2024-8088', 'commit': '0aa1ee22ab6e204e9d3d0e9dd63ea648ed691ef1', 'commit_source': 'github', 'cwe_id': ['CWE-835'], 'patch': ['@@ -2152,7 +2152,7 @@ def _parents(path):\\n def _ancestry(path):\\n     \"\"\"\\n     Given a path with elements separated by\\n-    posixpath.sep, generate all elements of that path\\n+    posixpath.sep, generate all elements of that path.\\n \\n     >>> list(_ancestry(\\'b/d\\'))\\n     [\\'b/d\\', \\'b\\']\\n@@ -2164,9 +2164,14 @@ def _ancestry(path):\\n     [\\'b\\']\\n     >>> list(_ancestry(\\'\\'))\\n     []\\n+\\n+    Multiple separators are treated like a single.\\n+\\n+    >>> list(_ancestry(\\'//b//d///f//\\'))\\n+    [\\'//b//d///f\\', \\'//b//d\\', \\'//b\\']\\n     \"\"\"\\n     path = path.rstrip(posixpath.sep)\\n-    while path and path != posixpath.sep:\\n+    while path.rstrip(posixpath.sep):\\n         yield path\\n         path, tail = posixpath.split(path)\\n \\n@@ -2183,65 +2188,7 @@ def _difference(minuend, subtrahend):\\n     return itertools.filterfalse(set(subtrahend).__contains__, minuend)\\n \\n \\n-class SanitizedNames:\\n-    \"\"\"\\n-    ZipFile mix-in to ensure names are sanitized.\\n-    \"\"\"\\n-\\n-    def namelist(self):\\n-        return list(map(self._sanitize, super().namelist()))\\n-\\n-    @staticmethod\\n-    def _sanitize(name):\\n-        r\"\"\"\\n-        Ensure a relative path with posix separators and no dot names.\\n-        Modeled after\\n-        https://github.com/python/cpython/blob/bcc1be39cb1d04ad9fc0bd1b9193d3972835a57c/Lib/zipfile/__init__.py#L1799-L1813\\n-        but provides consistent cross-platform behavior.\\n-        >>> san = SanitizedNames._sanitize\\n-        >>> san(\\'/foo/bar\\')\\n-        \\'foo/bar\\'\\n-        >>> san(\\'//foo.txt\\')\\n-        \\'foo.txt\\'\\n-        >>> san(\\'foo/.././bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'foo../.bar.txt\\')\\n-        \\'foo../.bar.txt\\'\\n-        >>> san(\\'\\\\\\\\foo\\\\\\\\bar.txt\\')\\n-        \\'foo/bar.txt\\'\\n-        >>> san(\\'D:\\\\\\\\foo.txt\\')\\n-        \\'D/foo.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\server\\\\\\\\share\\\\\\\\file.txt\\')\\n-        \\'server/share/file.txt\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\?\\\\\\\\GLOBALROOT\\\\\\\\Volume3\\')\\n-        \\'?/GLOBALROOT/Volume3\\'\\n-        >>> san(\\'\\\\\\\\\\\\\\\\.\\\\\\\\PhysicalDrive1\\\\\\\\root\\')\\n-        \\'PhysicalDrive1/root\\'\\n-        Retain any trailing slash.\\n-        >>> san(\\'abc/\\')\\n-        \\'abc/\\'\\n-        Raises a ValueError if the result is empty.\\n-        >>> san(\\'../..\\')\\n-        Traceback (most recent call last):\\n-        ...\\n-        ValueError: Empty filename\\n-        \"\"\"\\n-\\n-        def allowed(part):\\n-            return part and part not in {\\'..\\', \\'.\\'}\\n-\\n-        # Remove the drive letter.\\n-        # Don\\'t use ntpath.splitdrive, because that also strips UNC paths\\n-        bare = re.sub(\\'^([A-Z]):\\', r\\'\\\\1\\', name, flags=re.IGNORECASE)\\n-        clean = bare.replace(\\'\\\\\\\\\\', \\'/\\')\\n-        parts = clean.split(\\'/\\')\\n-        joined = \\'/\\'.join(filter(allowed, parts))\\n-        if not joined:\\n-            raise ValueError(\"Empty filename\")\\n-        return joined + \\'/\\' * name.endswith(\\'/\\')\\n-\\n-\\n-class CompleteDirs(SanitizedNames, ZipFile):\\n+class CompleteDirs(ZipFile):\\n     \"\"\"\\n     A ZipFile subclass that ensures that implied directories\\n     are always included in the namelist.'], 'file': ['Lib/zipfile.py'], 'language': ['Python'], 'temp_id': [UUID('ea0a3a86-577a-480f-a309-35ab16c4255f')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 11:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 33:0: <line number missing in source>\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1359/1800 [16:37<01:49,  4.02it/s]ERROR:src.process_code_changes:Error processing commit c57355dc2033ad90b7050d681b2c3ba548ff0004\n",
      "ERROR:src.process_code_changes:{'repo': 'iterative/PyDrive2', 'vulnerability_id': '2023-49297', 'commit': 'c57355dc2033ad90b7050d681b2c3ba548ff0004', 'commit_source': 'github', 'cwe_id': ['CWE-502'], 'patch': ['@@ -2,9 +2,9 @@\\n from yaml import YAMLError\\n \\n try:\\n-    from yaml import CLoader as Loader\\n+    from yaml import CSafeLoader as SafeLoader\\n except ImportError:\\n-    from yaml import Loader\\n+    from yaml import SafeLoader\\n \\n SETTINGS_FILE = \"settings.yaml\"\\n SETTINGS_STRUCT = {\\n@@ -114,7 +114,7 @@ def LoadSettingsFile(filename=SETTINGS_FILE):\\n     \"\"\"\\n     try:\\n         with open(filename) as stream:\\n-            data = load(stream, Loader=Loader)\\n+            data = load(stream, Loader=SafeLoader)\\n     except (YAMLError, OSError) as e:\\n         raise SettingsError(e)\\n     return data'], 'file': ['pydrive2/settings.py'], 'language': ['Python'], 'temp_id': [UUID('8ffc7eff-fdd9-4e12-8638-a4ecbebddca9')]}\n",
      "ERROR:root:Error in {'repo': 'iterative/PyDrive2', 'vulnerability_id': '2023-49297', 'commit': 'c57355dc2033ad90b7050d681b2c3ba548ff0004', 'commit_source': 'github', 'cwe_id': ['CWE-502'], 'patch': ['@@ -2,9 +2,9 @@\\n from yaml import YAMLError\\n \\n try:\\n-    from yaml import CLoader as Loader\\n+    from yaml import CSafeLoader as SafeLoader\\n except ImportError:\\n-    from yaml import Loader\\n+    from yaml import SafeLoader\\n \\n SETTINGS_FILE = \"settings.yaml\"\\n SETTINGS_STRUCT = {\\n@@ -114,7 +114,7 @@ def LoadSettingsFile(filename=SETTINGS_FILE):\\n     \"\"\"\\n     try:\\n         with open(filename) as stream:\\n-            data = load(stream, Loader=Loader)\\n+            data = load(stream, Loader=SafeLoader)\\n     except (YAMLError, OSError) as e:\\n         raise SettingsError(e)\\n     return data'], 'file': ['pydrive2/settings.py'], 'language': ['Python'], 'temp_id': [UUID('8ffc7eff-fdd9-4e12-8638-a4ecbebddca9')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     from yaml import SafeLoader\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     from yaml import SafeLoader\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1405/1800 [16:43<00:58,  6.72it/s]ERROR:src.process_code_changes:Error processing commit 06f89b43469aae70e8833e55192721523f86c5a2\n",
      "ERROR:src.process_code_changes:{'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2023-5289', 'commit': '06f89b43469aae70e8833e55192721523f86c5a2', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': [\"@@ -119,6 +119,7 @@ def populate_obj(self, userobj):\\n \\n class PagePrefTokens(Controller):\\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def default(self, **kwargs):\\n         form = TokenForm()\\n         delete_form = DeleteTokenForm()\", \"@@ -115,6 +115,7 @@ def populate_obj(self, userobj):\\n \\n class PagePrefSshKeys(Controller):\\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def default(self, **kwargs):\\n         # Handle action\\n         add_form = SshForm()\", '@@ -265,7 +265,8 @@ def index(self):\\n             ldap_enabled=self.app.cfg.ldap_uri,\\n         )\\n \\n-    @cherrypy.expose()\\n+    @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=[\\'POST\\'])\\n     def new(self, **kwargs):\\n         form = UserForm()\\n         if form.is_submitted():\\n@@ -282,7 +283,7 @@ def new(self, **kwargs):\\n                 flash(form.error_message, level=\\'error\\')\\n         return self._compile_template(\"admin_user_new.html\", form=form)\\n \\n-    @cherrypy.expose()\\n+    @cherrypy.expose\\n     def edit(self, username_vpath, **kwargs):\\n         user = UserObject.get_user(username_vpath)\\n         if not user:\\n@@ -297,7 +298,7 @@ def edit(self, username_vpath, **kwargs):\\n                 flash(form.error_message, level=\\'error\\')\\n         return self._compile_template(\"admin_user_edit.html\", form=form)\\n \\n-    @cherrypy.expose()\\n+    @cherrypy.expose\\n     def delete(self, username=None, **kwargs):\\n         # Validate form method.\\n         form = DeleteUserForm()\\n@@ -320,6 +321,3 @@ def delete(self, username=None, **kwargs):\\n         else:\\n             flash(form.error_message, level=\\'error\\')\\n         raise cherrypy.HTTPRedirect(url_for(\\'admin\\', \\'users\\'))\\n-\\n-\\n-# TODO Allow configuration of notification settigns'], 'file': ['rdiffweb/controller/page_pref_tokens.py', 'rdiffweb/controller/page_pref_sshkeys.py', 'rdiffweb/controller/page_admin_users.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('467efd2a-4b8a-4874-9f1f-9e70c292d8b8'), UUID('fd33ea53-51ae-463e-8b59-3fecc4629b1e'), UUID('71bb84bf-e454-4cb7-8ec5-62f96cc2823e')]}\n",
      "ERROR:root:Error in {'repo': 'ikus060/rdiffweb', 'vulnerability_id': '2023-5289', 'commit': '06f89b43469aae70e8833e55192721523f86c5a2', 'commit_source': 'github', 'cwe_id': ['CWE-770'], 'patch': [\"@@ -119,6 +119,7 @@ def populate_obj(self, userobj):\\n \\n class PagePrefTokens(Controller):\\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def default(self, **kwargs):\\n         form = TokenForm()\\n         delete_form = DeleteTokenForm()\", \"@@ -115,6 +115,7 @@ def populate_obj(self, userobj):\\n \\n class PagePrefSshKeys(Controller):\\n     @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=['POST'])\\n     def default(self, **kwargs):\\n         # Handle action\\n         add_form = SshForm()\", '@@ -265,7 +265,8 @@ def index(self):\\n             ldap_enabled=self.app.cfg.ldap_uri,\\n         )\\n \\n-    @cherrypy.expose()\\n+    @cherrypy.expose\\n+    @cherrypy.tools.ratelimit(methods=[\\'POST\\'])\\n     def new(self, **kwargs):\\n         form = UserForm()\\n         if form.is_submitted():\\n@@ -282,7 +283,7 @@ def new(self, **kwargs):\\n                 flash(form.error_message, level=\\'error\\')\\n         return self._compile_template(\"admin_user_new.html\", form=form)\\n \\n-    @cherrypy.expose()\\n+    @cherrypy.expose\\n     def edit(self, username_vpath, **kwargs):\\n         user = UserObject.get_user(username_vpath)\\n         if not user:\\n@@ -297,7 +298,7 @@ def edit(self, username_vpath, **kwargs):\\n                 flash(form.error_message, level=\\'error\\')\\n         return self._compile_template(\"admin_user_edit.html\", form=form)\\n \\n-    @cherrypy.expose()\\n+    @cherrypy.expose\\n     def delete(self, username=None, **kwargs):\\n         # Validate form method.\\n         form = DeleteUserForm()\\n@@ -320,6 +321,3 @@ def delete(self, username=None, **kwargs):\\n         else:\\n             flash(form.error_message, level=\\'error\\')\\n         raise cherrypy.HTTPRedirect(url_for(\\'admin\\', \\'users\\'))\\n-\\n-\\n-# TODO Allow configuration of notification settigns'], 'file': ['rdiffweb/controller/page_pref_tokens.py', 'rdiffweb/controller/page_pref_sshkeys.py', 'rdiffweb/controller/page_admin_users.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('467efd2a-4b8a-4874-9f1f-9e70c292d8b8'), UUID('fd33ea53-51ae-463e-8b59-3fecc4629b1e'), UUID('71bb84bf-e454-4cb7-8ec5-62f96cc2823e')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1510/1800 [19:35<04:11,  1.15it/s]ERROR:src.process_code_changes:Error processing commit ee1e2942e0a1ae84a08a05464e41c8108a03fa9c\n",
      "ERROR:src.process_code_changes:{'repo': 'gradio-app/gradio', 'vulnerability_id': '2024-4941', 'commit': 'ee1e2942e0a1ae84a08a05464e41c8108a03fa9c', 'commit_source': 'github', 'cwe_id': ['CWE-20', 'CWE-20'], 'patch': ['@@ -20,7 +20,8 @@\\n from PIL import Image, ImageOps, PngImagePlugin\\n \\n from gradio import utils, wasm_utils\\n-from gradio.data_classes import FileData, GradioModel, GradioRootModel\\n+from gradio.data_classes import FileData, GradioModel, GradioRootModel, JsonData\\n+from gradio.exceptions import Error\\n from gradio.utils import abspath, get_upload_folder, is_in_or_equal\\n \\n with warnings.catch_warnings():\\n@@ -341,6 +342,20 @@ def move_resource_to_block_cache(\\n     return block.move_resource_to_block_cache(url_or_file_path)\\n \\n \\n+def check_all_files_in_cache(data: JsonData):\\n+    def _in_cache(d: dict):\\n+        if (\\n+            (path := d.get(\"path\", \"\"))\\n+            and not client_utils.is_http_url_like(path)\\n+            and not is_in_or_equal(path, get_upload_folder())\\n+        ):\\n+            raise Error(\\n+                f\"File {path} is not in the cache folder and cannot be accessed.\"\\n+            )\\n+\\n+    client_utils.traverse(data, _in_cache, client_utils.is_file_obj)\\n+\\n+\\n def move_files_to_cache(\\n     data: Any,\\n     block: Block,\\n@@ -475,7 +490,6 @@ async def _move_to_cache(d: dict):\\n \\n     if isinstance(data, (GradioRootModel, GradioModel)):\\n         data = data.model_dump()\\n-\\n     return await client_utils.async_traverse(\\n         data, _move_to_cache, client_utils.is_file_obj\\n     )', '@@ -19,7 +19,7 @@\\n from gradio import utils\\n from gradio.blocks import Block, BlockContext\\n from gradio.component_meta import ComponentMeta\\n-from gradio.data_classes import GradioDataModel\\n+from gradio.data_classes import GradioDataModel, JsonData\\n from gradio.events import EventListener\\n from gradio.layouts import Form\\n from gradio.processing_utils import move_files_to_cache\\n@@ -298,6 +298,8 @@ def flag(self, payload: Any, flag_dir: str | Path = \"\") -> str:\\n             payload = self.data_model.from_json(payload)\\n             Path(flag_dir).mkdir(exist_ok=True)\\n             payload = payload.copy_to_dir(flag_dir).model_dump()\\n+        if isinstance(payload, JsonData):\\n+            payload = payload.model_dump()\\n         if not isinstance(payload, str):\\n             payload = json.dumps(payload)\\n         return payload', '@@ -933,6 +933,7 @@ def special_args(\\n         ):\\n             event_data_index = i\\n             if inputs is not None and event_data is not None:\\n+                processing_utils.check_all_files_in_cache(event_data._data)\\n                 inputs.insert(i, type_hint(event_data.target, event_data._data))\\n         elif (\\n             param.default is not param.empty and inputs is not None and len(inputs) <= i', '@@ -9,6 +9,7 @@\\n from gradio_client.documentation import document\\n \\n from gradio.components.base import Component\\n+from gradio.data_classes import JsonData\\n from gradio.events import Events\\n \\n \\n@@ -77,7 +78,7 @@ def preprocess(self, payload: dict | list | None) -> dict | list | None:\\n         \"\"\"\\n         return payload\\n \\n-    def postprocess(self, value: dict | list | str | None) -> dict | list | None:\\n+    def postprocess(self, value: dict | list | str | None) -> JsonData | None:\\n         \"\"\"\\n         Parameters:\\n             value: Expects a valid JSON `str` -- or a `list` or `dict` that can be serialized to a JSON string. The `list` or `dict` value can contain numpy arrays.\\n@@ -87,16 +88,19 @@ def postprocess(self, value: dict | list | str | None) -> dict | list | None:\\n         if value is None:\\n             return None\\n         if isinstance(value, str):\\n-            return orjson.loads(value)\\n+            return JsonData(orjson.loads(value))\\n         else:\\n             # Use orjson to convert NumPy arrays and datetime objects to JSON.\\n             # This ensures a backward compatibility with the previous behavior.\\n             # See https://github.com/gradio-app/gradio/pull/8041\\n-            return orjson.loads(\\n-                orjson.dumps(\\n-                    value,\\n-                    option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,\\n-                    default=str,\\n+            return JsonData(\\n+                orjson.loads(\\n+                    orjson.dumps(\\n+                        value,\\n+                        option=orjson.OPT_SERIALIZE_NUMPY\\n+                        | orjson.OPT_PASSTHROUGH_DATETIME,\\n+                        default=str,\\n+                    )\\n                 )\\n             )\\n ', '@@ -16,7 +16,7 @@\\n from . import wasm_utils\\n \\n if not wasm_utils.IS_WASM or TYPE_CHECKING:\\n-    from pydantic import BaseModel, RootModel, ValidationError\\n+    from pydantic import BaseModel, JsonValue, RootModel, ValidationError\\n else:\\n     # XXX: Currently Pyodide V2 is not available on Pyodide,\\n     # so we install V1 for the Wasm version.\\n@@ -25,6 +25,8 @@\\n     from pydantic import BaseModel as BaseModelV1\\n     from pydantic import ValidationError, schema_of\\n \\n+    JsonValue = Any\\n+\\n     # Map V2 method calls to V1 implementations.\\n     # Ref: https://docs.pydantic.dev/latest/migration/#changes-to-pydanticbasemodel\\n     class BaseModelMeta(type(BaseModelV1)):\\n@@ -161,6 +163,12 @@ def from_json(cls, x) -> GradioDataModel:\\n         pass\\n \\n \\n+class JsonData(RootModel):\\n+    \"\"\"JSON data returned from a component that should not be modified further.\"\"\"\\n+\\n+    root: JsonValue\\n+\\n+\\n class GradioModel(GradioBaseModel, BaseModel):\\n     @classmethod\\n     def from_json(cls, x) -> GradioModel:'], 'file': ['gradio/processing_utils.py', 'gradio/components/base.py', 'gradio/helpers.py', 'gradio/components/json_component.py', 'gradio/data_classes.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('aefe2a10-6c67-487d-90f9-cd92e496b665'), UUID('95b74eca-38aa-4cfb-b541-0c56a7688b6e'), UUID('e6b492ff-8d95-4fa1-868e-a929ade6c385'), UUID('0b23fa4b-ddb8-45e1-9b11-fb136c4d2af7'), UUID('6bed1c91-5041-41cb-ae3f-2315d3a7e684')]}\n",
      "ERROR:root:Error in {'repo': 'gradio-app/gradio', 'vulnerability_id': '2024-4941', 'commit': 'ee1e2942e0a1ae84a08a05464e41c8108a03fa9c', 'commit_source': 'github', 'cwe_id': ['CWE-20', 'CWE-20'], 'patch': ['@@ -20,7 +20,8 @@\\n from PIL import Image, ImageOps, PngImagePlugin\\n \\n from gradio import utils, wasm_utils\\n-from gradio.data_classes import FileData, GradioModel, GradioRootModel\\n+from gradio.data_classes import FileData, GradioModel, GradioRootModel, JsonData\\n+from gradio.exceptions import Error\\n from gradio.utils import abspath, get_upload_folder, is_in_or_equal\\n \\n with warnings.catch_warnings():\\n@@ -341,6 +342,20 @@ def move_resource_to_block_cache(\\n     return block.move_resource_to_block_cache(url_or_file_path)\\n \\n \\n+def check_all_files_in_cache(data: JsonData):\\n+    def _in_cache(d: dict):\\n+        if (\\n+            (path := d.get(\"path\", \"\"))\\n+            and not client_utils.is_http_url_like(path)\\n+            and not is_in_or_equal(path, get_upload_folder())\\n+        ):\\n+            raise Error(\\n+                f\"File {path} is not in the cache folder and cannot be accessed.\"\\n+            )\\n+\\n+    client_utils.traverse(data, _in_cache, client_utils.is_file_obj)\\n+\\n+\\n def move_files_to_cache(\\n     data: Any,\\n     block: Block,\\n@@ -475,7 +490,6 @@ async def _move_to_cache(d: dict):\\n \\n     if isinstance(data, (GradioRootModel, GradioModel)):\\n         data = data.model_dump()\\n-\\n     return await client_utils.async_traverse(\\n         data, _move_to_cache, client_utils.is_file_obj\\n     )', '@@ -19,7 +19,7 @@\\n from gradio import utils\\n from gradio.blocks import Block, BlockContext\\n from gradio.component_meta import ComponentMeta\\n-from gradio.data_classes import GradioDataModel\\n+from gradio.data_classes import GradioDataModel, JsonData\\n from gradio.events import EventListener\\n from gradio.layouts import Form\\n from gradio.processing_utils import move_files_to_cache\\n@@ -298,6 +298,8 @@ def flag(self, payload: Any, flag_dir: str | Path = \"\") -> str:\\n             payload = self.data_model.from_json(payload)\\n             Path(flag_dir).mkdir(exist_ok=True)\\n             payload = payload.copy_to_dir(flag_dir).model_dump()\\n+        if isinstance(payload, JsonData):\\n+            payload = payload.model_dump()\\n         if not isinstance(payload, str):\\n             payload = json.dumps(payload)\\n         return payload', '@@ -933,6 +933,7 @@ def special_args(\\n         ):\\n             event_data_index = i\\n             if inputs is not None and event_data is not None:\\n+                processing_utils.check_all_files_in_cache(event_data._data)\\n                 inputs.insert(i, type_hint(event_data.target, event_data._data))\\n         elif (\\n             param.default is not param.empty and inputs is not None and len(inputs) <= i', '@@ -9,6 +9,7 @@\\n from gradio_client.documentation import document\\n \\n from gradio.components.base import Component\\n+from gradio.data_classes import JsonData\\n from gradio.events import Events\\n \\n \\n@@ -77,7 +78,7 @@ def preprocess(self, payload: dict | list | None) -> dict | list | None:\\n         \"\"\"\\n         return payload\\n \\n-    def postprocess(self, value: dict | list | str | None) -> dict | list | None:\\n+    def postprocess(self, value: dict | list | str | None) -> JsonData | None:\\n         \"\"\"\\n         Parameters:\\n             value: Expects a valid JSON `str` -- or a `list` or `dict` that can be serialized to a JSON string. The `list` or `dict` value can contain numpy arrays.\\n@@ -87,16 +88,19 @@ def postprocess(self, value: dict | list | str | None) -> dict | list | None:\\n         if value is None:\\n             return None\\n         if isinstance(value, str):\\n-            return orjson.loads(value)\\n+            return JsonData(orjson.loads(value))\\n         else:\\n             # Use orjson to convert NumPy arrays and datetime objects to JSON.\\n             # This ensures a backward compatibility with the previous behavior.\\n             # See https://github.com/gradio-app/gradio/pull/8041\\n-            return orjson.loads(\\n-                orjson.dumps(\\n-                    value,\\n-                    option=orjson.OPT_SERIALIZE_NUMPY | orjson.OPT_PASSTHROUGH_DATETIME,\\n-                    default=str,\\n+            return JsonData(\\n+                orjson.loads(\\n+                    orjson.dumps(\\n+                        value,\\n+                        option=orjson.OPT_SERIALIZE_NUMPY\\n+                        | orjson.OPT_PASSTHROUGH_DATETIME,\\n+                        default=str,\\n+                    )\\n                 )\\n             )\\n ', '@@ -16,7 +16,7 @@\\n from . import wasm_utils\\n \\n if not wasm_utils.IS_WASM or TYPE_CHECKING:\\n-    from pydantic import BaseModel, RootModel, ValidationError\\n+    from pydantic import BaseModel, JsonValue, RootModel, ValidationError\\n else:\\n     # XXX: Currently Pyodide V2 is not available on Pyodide,\\n     # so we install V1 for the Wasm version.\\n@@ -25,6 +25,8 @@\\n     from pydantic import BaseModel as BaseModelV1\\n     from pydantic import ValidationError, schema_of\\n \\n+    JsonValue = Any\\n+\\n     # Map V2 method calls to V1 implementations.\\n     # Ref: https://docs.pydantic.dev/latest/migration/#changes-to-pydanticbasemodel\\n     class BaseModelMeta(type(BaseModelV1)):\\n@@ -161,6 +163,12 @@ def from_json(cls, x) -> GradioDataModel:\\n         pass\\n \\n \\n+class JsonData(RootModel):\\n+    \"\"\"JSON data returned from a component that should not be modified further.\"\"\"\\n+\\n+    root: JsonValue\\n+\\n+\\n class GradioModel(GradioBaseModel, BaseModel):\\n     @classmethod\\n     def from_json(cls, x) -> GradioModel:'], 'file': ['gradio/processing_utils.py', 'gradio/components/base.py', 'gradio/helpers.py', 'gradio/components/json_component.py', 'gradio/data_classes.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('aefe2a10-6c67-487d-90f9-cd92e496b665'), UUID('95b74eca-38aa-4cfb-b541-0c56a7688b6e'), UUID('e6b492ff-8d95-4fa1-868e-a929ade6c385'), UUID('0b23fa4b-ddb8-45e1-9b11-fb136c4d2af7'), UUID('6bed1c91-5041-41cb-ae3f-2315d3a7e684')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     JsonValue = Any\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     JsonValue = Any\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1525/1800 [19:46<03:32,  1.29it/s]ERROR:src.process_code_changes:Error processing commit 7f0573e917227686d2cc127be1364e2908740807\n",
      "ERROR:src.process_code_changes:{'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-0406', 'commit': '7f0573e917227686d2cc127be1364e2908740807', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -11,6 +11,7 @@\\n from django.utils.translation import ugettext as _, ungettext\\n from django.views import generic\\n from django.views.decorators.csrf import ensure_csrf_cookie\\n+from django.views.decorators.http import require_http_methods\\n \\n from modoboa.core.models import User\\n from modoboa.lib.exceptions import BadRequest, PermDeniedException\\n@@ -185,6 +186,7 @@ def editaccount(request, pk):\\n \\n @login_required\\n @permission_required(\"core.delete_user\")\\n+@require_http_methods([\"POST\"])\\n def delaccount(request, pk):\\n     User.objects.get(pk=pk).delete()\\n     return render_to_json_response('], 'file': ['modoboa/admin/views/identity.py'], 'language': ['Python'], 'temp_id': [UUID('41b6b1b7-4329-4ea7-97a4-46db29b19715')]}\n",
      "ERROR:root:Error in {'repo': 'modoboa/modoboa', 'vulnerability_id': '2023-0406', 'commit': '7f0573e917227686d2cc127be1364e2908740807', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -11,6 +11,7 @@\\n from django.utils.translation import ugettext as _, ungettext\\n from django.views import generic\\n from django.views.decorators.csrf import ensure_csrf_cookie\\n+from django.views.decorators.http import require_http_methods\\n \\n from modoboa.core.models import User\\n from modoboa.lib.exceptions import BadRequest, PermDeniedException\\n@@ -185,6 +186,7 @@ def editaccount(request, pk):\\n \\n @login_required\\n @permission_required(\"core.delete_user\")\\n+@require_http_methods([\"POST\"])\\n def delaccount(request, pk):\\n     User.objects.get(pk=pk).delete()\\n     return render_to_json_response('], 'file': ['modoboa/admin/views/identity.py'], 'language': ['Python'], 'temp_id': [UUID('41b6b1b7-4329-4ea7-97a4-46db29b19715')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: <line number missing in source>\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1528/1800 [19:47<03:07,  1.45it/s]ERROR:src.process_code_changes:Error processing commit 979e0bd15e794ceb00cc63737fcd5fd9addc4a99\n",
      "ERROR:src.process_code_changes:{'repo': 'jupyter/notebook', 'vulnerability_id': '2019-10856', 'commit': '979e0bd15e794ceb00cc63737fcd5fd9addc4a99', 'commit_source': 'github', 'cwe_id': ['CWE-601'], 'patch': ['@@ -7,9 +7,9 @@\\n import os\\n \\n try:\\n-    from urllib.parse import urlparse # Py 3\\n+    from urllib.parse import urlparse, urlunparse  # Py 3\\n except ImportError:\\n-    from urlparse import urlparse # Py 2\\n+    from urlparse import urlparse, urlunparse  # Py 2\\n import uuid\\n \\n from tornado.escape import url_escape\\n@@ -44,15 +44,18 @@ def _redirect_safe(self, url, default=None):\\n         # instead of %5C, causing `\\\\\\\\` to behave as `//`\\n         url = url.replace(\"\\\\\\\\\", \"%5C\")\\n         parsed = urlparse(url)\\n-        if parsed.netloc or not (parsed.path + \\'/\\').startswith(self.base_url):\\n+        path_only = urlunparse(parsed._replace(netloc=\\'\\', scheme=\\'\\'))\\n+        if url != path_only or not (parsed.path + \\'/\\').startswith(self.base_url):\\n             # require that next_url be absolute path within our path\\n             allow = False\\n             # OR pass our cross-origin check\\n-            if parsed.netloc:\\n+            if url != path_only:\\n                 # if full URL, run our cross-origin check:\\n                 origin = \\'%s://%s\\' % (parsed.scheme, parsed.netloc)\\n                 origin = origin.lower()\\n-                if self.allow_origin:\\n+                if origin == \\'%s://%s\\' % (self.request.protocol, self.request.host):\\n+                    allow = True\\n+                elif self.allow_origin:\\n                     allow = self.allow_origin == origin\\n                 elif self.allow_origin_pat:\\n                     allow = bool(self.allow_origin_pat.match(origin))'], 'file': ['notebook/auth/login.py'], 'language': ['Python'], 'temp_id': [UUID('c5960e49-fe39-49b2-afce-a24b059fc828')]}\n",
      "ERROR:root:Error in {'repo': 'jupyter/notebook', 'vulnerability_id': '2019-10856', 'commit': '979e0bd15e794ceb00cc63737fcd5fd9addc4a99', 'commit_source': 'github', 'cwe_id': ['CWE-601'], 'patch': ['@@ -7,9 +7,9 @@\\n import os\\n \\n try:\\n-    from urllib.parse import urlparse # Py 3\\n+    from urllib.parse import urlparse, urlunparse  # Py 3\\n except ImportError:\\n-    from urlparse import urlparse # Py 2\\n+    from urlparse import urlparse, urlunparse  # Py 2\\n import uuid\\n \\n from tornado.escape import url_escape\\n@@ -44,15 +44,18 @@ def _redirect_safe(self, url, default=None):\\n         # instead of %5C, causing `\\\\\\\\` to behave as `//`\\n         url = url.replace(\"\\\\\\\\\", \"%5C\")\\n         parsed = urlparse(url)\\n-        if parsed.netloc or not (parsed.path + \\'/\\').startswith(self.base_url):\\n+        path_only = urlunparse(parsed._replace(netloc=\\'\\', scheme=\\'\\'))\\n+        if url != path_only or not (parsed.path + \\'/\\').startswith(self.base_url):\\n             # require that next_url be absolute path within our path\\n             allow = False\\n             # OR pass our cross-origin check\\n-            if parsed.netloc:\\n+            if url != path_only:\\n                 # if full URL, run our cross-origin check:\\n                 origin = \\'%s://%s\\' % (parsed.scheme, parsed.netloc)\\n                 origin = origin.lower()\\n-                if self.allow_origin:\\n+                if origin == \\'%s://%s\\' % (self.request.protocol, self.request.host):\\n+                    allow = True\\n+                elif self.allow_origin:\\n                     allow = self.allow_origin == origin\\n                 elif self.allow_origin_pat:\\n                     allow = bool(self.allow_origin_pat.match(origin))'], 'file': ['notebook/auth/login.py'], 'language': ['Python'], 'temp_id': [UUID('c5960e49-fe39-49b2-afce-a24b059fc828')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     from urlparse import urlparse, urlunparse          \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     from urlparse import urlparse, urlunparse  # Py 2\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1549/1800 [19:49<01:18,  3.20it/s]ERROR:src.process_code_changes:Error processing commit 37265b230e511480a9ceace492f9f6a484be1387\n",
      "ERROR:src.process_code_changes:{'repo': 'strawberry-graphql/strawberry', 'vulnerability_id': '2024-47082', 'commit': '37265b230e511480a9ceace492f9f6a484be1387', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -102,11 +102,13 @@ def __init__(\\n         allow_queries_via_get: bool = True,\\n         json_encoder: Optional[Type[json.JSONEncoder]] = None,\\n         json_dumps_params: Optional[Dict[str, Any]] = None,\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n         self.json_encoder = json_encoder\\n         self.json_dumps_params = json_dumps_params\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if self.json_encoder is not None:  # pragma: no cover\\n             warnings.warn(', '@@ -61,9 +61,11 @@ def __init__(\\n         graphiql: Optional[bool] = None,\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -410,6 +410,7 @@ def make_graphql_controller(\\n         GRAPHQL_WS_PROTOCOL,\\n     ),\\n     connection_init_wait_timeout: timedelta = timedelta(minutes=1),\\n+    multipart_uploads_enabled: bool = False,\\n ) -> Type[GraphQLController]:  # sourcery skip: move-assign\\n     if context_getter is None:\\n         custom_context_getter_ = _none_custom_context_getter\\n@@ -456,6 +457,7 @@ class _GraphQLController(GraphQLController):\\n     _GraphQLController.schema = schema_\\n     _GraphQLController.allow_queries_via_get = allow_queries_via_get_\\n     _GraphQLController.graphql_ide = graphql_ide_\\n+    _GraphQLController.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n     return _GraphQLController\\n ', '@@ -156,6 +156,7 @@ def __init__(\\n         generate_unique_id_function: Callable[[APIRoute], str] = Default(\\n             generate_unique_id\\n         ),\\n+        multipart_uploads_enabled: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         super().__init__(\\n@@ -190,6 +191,7 @@ def __init__(\\n         )\\n         self.protocols = subscription_protocols\\n         self.connection_init_wait_timeout = connection_init_wait_timeout\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -111,6 +111,7 @@ def __init__(\\n             GRAPHQL_WS_PROTOCOL,\\n         ),\\n         connection_init_wait_timeout: timedelta = timedelta(minutes=1),\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n@@ -119,6 +120,7 @@ def __init__(\\n         self.debug = debug\\n         self.subscription_protocols = subscription_protocols\\n         self.connection_init_wait_timeout = connection_init_wait_timeout\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -168,12 +168,14 @@ def __init__(\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n         subscriptions_enabled: bool = True,\\n+        multipart_uploads_enabled: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n         self.subscriptions_enabled = subscriptions_enabled\\n         self._ide_subscriptions_enabled = subscriptions_enabled\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -71,10 +71,12 @@ def __init__(\\n         graphiql: Optional[bool] = None,\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.graphiql = graphiql\\n         self.allow_queries_via_get = allow_queries_via_get\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -333,7 +333,7 @@ async def parse_http_body(\\n             data = self.parse_query_params(request.query_params)\\n         elif \"application/json\" in content_type:\\n             data = self.parse_json(await request.get_body())\\n-        elif content_type == \"multipart/form-data\":\\n+        elif self.multipart_uploads_enabled and content_type == \"multipart/form-data\":\\n             data = await self.parse_multipart(request)\\n         else:\\n             raise HTTPException(400, \"Unsupported content type\")', '@@ -143,7 +143,7 @@ def parse_http_body(self, request: SyncHTTPRequestAdapter) -> GraphQLRequestData\\n         elif \"application/json\" in content_type:\\n             data = self.parse_json(request.body)\\n         # TODO: multipart via get?\\n-        elif content_type == \"multipart/form-data\":\\n+        elif self.multipart_uploads_enabled and content_type == \"multipart/form-data\":\\n             data = self.parse_multipart(request)\\n         elif self._is_multipart_subscriptions(content_type, params):\\n             raise HTTPException(', '@@ -28,8 +28,7 @@\\n from django.template.exceptions import TemplateDoesNotExist\\n from django.template.loader import render_to_string\\n from django.template.response import TemplateResponse\\n-from django.utils.decorators import classonlymethod, method_decorator\\n-from django.views.decorators.csrf import csrf_exempt\\n+from django.utils.decorators import classonlymethod\\n from django.views.generic import View\\n \\n from strawberry.http.async_base_view import AsyncBaseHTTPView, AsyncHTTPRequestAdapter\\n@@ -147,11 +146,13 @@ def __init__(\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n         subscriptions_enabled: bool = False,\\n+        multipart_uploads_enabled: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n         self.subscriptions_enabled = subscriptions_enabled\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(\\n@@ -229,7 +230,6 @@ def get_context(self, request: HttpRequest, response: HttpResponse) -> Any:\\n     def get_sub_response(self, request: HttpRequest) -> TemporalHttpResponse:\\n         return TemporalHttpResponse()\\n \\n-    @method_decorator(csrf_exempt)\\n     def dispatch(\\n         self, request: HttpRequest, *args: Any, **kwargs: Any\\n     ) -> Union[HttpResponseNotAllowed, TemplateResponse, HttpResponseBase]:\\n@@ -288,7 +288,6 @@ async def get_context(self, request: HttpRequest, response: HttpResponse) -> Any\\n     async def get_sub_response(self, request: HttpRequest) -> TemporalHttpResponse:\\n         return TemporalHttpResponse()\\n \\n-    @method_decorator(csrf_exempt)\\n     async def dispatch(  # pyright: ignore\\n         self, request: HttpRequest, *args: Any, **kwargs: Any\\n     ) -> Union[HttpResponseNotAllowed, TemplateResponse, HttpResponseBase]:', '@@ -23,6 +23,7 @@ def headers(self) -> Mapping[str, str]: ...\\n \\n class BaseView(Generic[Request]):\\n     graphql_ide: Optional[GraphQL_IDE]\\n+    multipart_uploads_enabled: bool = False\\n \\n     # TODO: we might remove this in future :)\\n     _ide_replace_variables: bool = True'], 'file': ['strawberry/sanic/views.py', 'strawberry/quart/views.py', 'strawberry/litestar/controller.py', 'strawberry/fastapi/router.py', 'strawberry/aiohttp/views.py', 'strawberry/channels/handlers/http_handler.py', 'strawberry/flask/views.py', 'strawberry/http/async_base_view.py', 'strawberry/http/sync_base_view.py', 'strawberry/django/views.py', 'strawberry/http/base.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('4350977a-6e5e-4841-9652-2d5cdc7fcdd0'), UUID('4cf559ce-0156-46b3-98c5-e901ebf47d8c'), UUID('8fdcdff4-a7ea-4080-a161-00ab0cbff0fd'), UUID('b47edae4-50da-46b9-95ea-3b58adcf08a4'), UUID('1ccc7437-a9e5-4529-83dc-5d3fd528cf38'), UUID('a790fea7-1802-4412-b131-5eda609d627f'), UUID('2e6e3a37-2ae4-44a4-b9b4-6146dbb77041'), UUID('b20448f1-d1ad-41bd-9159-65955650e0a8'), UUID('0cc407d7-1153-4cb1-abdb-ba6e778bb341'), UUID('1dae3051-b617-4f0d-8bbb-d291c8916005'), UUID('969fa50e-df4a-483a-868e-fd1f0ee8ad31')]}\n",
      "ERROR:root:Error in {'repo': 'strawberry-graphql/strawberry', 'vulnerability_id': '2024-47082', 'commit': '37265b230e511480a9ceace492f9f6a484be1387', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -102,11 +102,13 @@ def __init__(\\n         allow_queries_via_get: bool = True,\\n         json_encoder: Optional[Type[json.JSONEncoder]] = None,\\n         json_dumps_params: Optional[Dict[str, Any]] = None,\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n         self.json_encoder = json_encoder\\n         self.json_dumps_params = json_dumps_params\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if self.json_encoder is not None:  # pragma: no cover\\n             warnings.warn(', '@@ -61,9 +61,11 @@ def __init__(\\n         graphiql: Optional[bool] = None,\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -410,6 +410,7 @@ def make_graphql_controller(\\n         GRAPHQL_WS_PROTOCOL,\\n     ),\\n     connection_init_wait_timeout: timedelta = timedelta(minutes=1),\\n+    multipart_uploads_enabled: bool = False,\\n ) -> Type[GraphQLController]:  # sourcery skip: move-assign\\n     if context_getter is None:\\n         custom_context_getter_ = _none_custom_context_getter\\n@@ -456,6 +457,7 @@ class _GraphQLController(GraphQLController):\\n     _GraphQLController.schema = schema_\\n     _GraphQLController.allow_queries_via_get = allow_queries_via_get_\\n     _GraphQLController.graphql_ide = graphql_ide_\\n+    _GraphQLController.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n     return _GraphQLController\\n ', '@@ -156,6 +156,7 @@ def __init__(\\n         generate_unique_id_function: Callable[[APIRoute], str] = Default(\\n             generate_unique_id\\n         ),\\n+        multipart_uploads_enabled: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         super().__init__(\\n@@ -190,6 +191,7 @@ def __init__(\\n         )\\n         self.protocols = subscription_protocols\\n         self.connection_init_wait_timeout = connection_init_wait_timeout\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -111,6 +111,7 @@ def __init__(\\n             GRAPHQL_WS_PROTOCOL,\\n         ),\\n         connection_init_wait_timeout: timedelta = timedelta(minutes=1),\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n@@ -119,6 +120,7 @@ def __init__(\\n         self.debug = debug\\n         self.subscription_protocols = subscription_protocols\\n         self.connection_init_wait_timeout = connection_init_wait_timeout\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -168,12 +168,14 @@ def __init__(\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n         subscriptions_enabled: bool = True,\\n+        multipart_uploads_enabled: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n         self.subscriptions_enabled = subscriptions_enabled\\n         self._ide_subscriptions_enabled = subscriptions_enabled\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -71,10 +71,12 @@ def __init__(\\n         graphiql: Optional[bool] = None,\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n+        multipart_uploads_enabled: bool = False,\\n     ) -> None:\\n         self.schema = schema\\n         self.graphiql = graphiql\\n         self.allow_queries_via_get = allow_queries_via_get\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(', '@@ -333,7 +333,7 @@ async def parse_http_body(\\n             data = self.parse_query_params(request.query_params)\\n         elif \"application/json\" in content_type:\\n             data = self.parse_json(await request.get_body())\\n-        elif content_type == \"multipart/form-data\":\\n+        elif self.multipart_uploads_enabled and content_type == \"multipart/form-data\":\\n             data = await self.parse_multipart(request)\\n         else:\\n             raise HTTPException(400, \"Unsupported content type\")', '@@ -143,7 +143,7 @@ def parse_http_body(self, request: SyncHTTPRequestAdapter) -> GraphQLRequestData\\n         elif \"application/json\" in content_type:\\n             data = self.parse_json(request.body)\\n         # TODO: multipart via get?\\n-        elif content_type == \"multipart/form-data\":\\n+        elif self.multipart_uploads_enabled and content_type == \"multipart/form-data\":\\n             data = self.parse_multipart(request)\\n         elif self._is_multipart_subscriptions(content_type, params):\\n             raise HTTPException(', '@@ -28,8 +28,7 @@\\n from django.template.exceptions import TemplateDoesNotExist\\n from django.template.loader import render_to_string\\n from django.template.response import TemplateResponse\\n-from django.utils.decorators import classonlymethod, method_decorator\\n-from django.views.decorators.csrf import csrf_exempt\\n+from django.utils.decorators import classonlymethod\\n from django.views.generic import View\\n \\n from strawberry.http.async_base_view import AsyncBaseHTTPView, AsyncHTTPRequestAdapter\\n@@ -147,11 +146,13 @@ def __init__(\\n         graphql_ide: Optional[GraphQL_IDE] = \"graphiql\",\\n         allow_queries_via_get: bool = True,\\n         subscriptions_enabled: bool = False,\\n+        multipart_uploads_enabled: bool = False,\\n         **kwargs: Any,\\n     ) -> None:\\n         self.schema = schema\\n         self.allow_queries_via_get = allow_queries_via_get\\n         self.subscriptions_enabled = subscriptions_enabled\\n+        self.multipart_uploads_enabled = multipart_uploads_enabled\\n \\n         if graphiql is not None:\\n             warnings.warn(\\n@@ -229,7 +230,6 @@ def get_context(self, request: HttpRequest, response: HttpResponse) -> Any:\\n     def get_sub_response(self, request: HttpRequest) -> TemporalHttpResponse:\\n         return TemporalHttpResponse()\\n \\n-    @method_decorator(csrf_exempt)\\n     def dispatch(\\n         self, request: HttpRequest, *args: Any, **kwargs: Any\\n     ) -> Union[HttpResponseNotAllowed, TemplateResponse, HttpResponseBase]:\\n@@ -288,7 +288,6 @@ async def get_context(self, request: HttpRequest, response: HttpResponse) -> Any\\n     async def get_sub_response(self, request: HttpRequest) -> TemporalHttpResponse:\\n         return TemporalHttpResponse()\\n \\n-    @method_decorator(csrf_exempt)\\n     async def dispatch(  # pyright: ignore\\n         self, request: HttpRequest, *args: Any, **kwargs: Any\\n     ) -> Union[HttpResponseNotAllowed, TemplateResponse, HttpResponseBase]:', '@@ -23,6 +23,7 @@ def headers(self) -> Mapping[str, str]: ...\\n \\n class BaseView(Generic[Request]):\\n     graphql_ide: Optional[GraphQL_IDE]\\n+    multipart_uploads_enabled: bool = False\\n \\n     # TODO: we might remove this in future :)\\n     _ide_replace_variables: bool = True'], 'file': ['strawberry/sanic/views.py', 'strawberry/quart/views.py', 'strawberry/litestar/controller.py', 'strawberry/fastapi/router.py', 'strawberry/aiohttp/views.py', 'strawberry/channels/handlers/http_handler.py', 'strawberry/flask/views.py', 'strawberry/http/async_base_view.py', 'strawberry/http/sync_base_view.py', 'strawberry/django/views.py', 'strawberry/http/base.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('4350977a-6e5e-4841-9652-2d5cdc7fcdd0'), UUID('4cf559ce-0156-46b3-98c5-e901ebf47d8c'), UUID('8fdcdff4-a7ea-4080-a161-00ab0cbff0fd'), UUID('b47edae4-50da-46b9-95ea-3b58adcf08a4'), UUID('1ccc7437-a9e5-4529-83dc-5d3fd528cf38'), UUID('a790fea7-1802-4412-b131-5eda609d627f'), UUID('2e6e3a37-2ae4-44a4-b9b4-6146dbb77041'), UUID('b20448f1-d1ad-41bd-9159-65955650e0a8'), UUID('0cc407d7-1153-4cb1-abdb-ba6e778bb341'), UUID('1dae3051-b617-4f0d-8bbb-d291c8916005'), UUID('969fa50e-df4a-483a-868e-fd1f0ee8ad31')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0:     @method_decorator(csrf_exempt)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0:     @method_decorator(csrf_exempt)\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1554/1800 [19:54<01:56,  2.12it/s]ERROR:src.process_code_changes:Error processing commit 42dd0c8e5fc84e17e1d3578d18aaea169eece474\n",
      "ERROR:src.process_code_changes:{'repo': 'IncludeSecurity/safeurl-python', 'vulnerability_id': '2023-24622', 'commit': '42dd0c8e5fc84e17e1d3578d18aaea169eece474', 'commit_source': 'github', 'cwe_id': ['CWE-918'], 'patch': ['@@ -6,7 +6,7 @@\\n     sc = safeurl.SafeURL()\\n     res = sc.execute(\"https://fin1te.net\")\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n # options\\n try:\\n@@ -20,13 +20,13 @@\\n     sc.setOptions(opt)\\n     res = sc.execute(\"http://www.youtube.com\")\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n # url\\n try:\\n     url = safeurl.Url.validateUrl(\"http://google.com\", safeurl.Options())\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n # redirects\\n try:\\n@@ -38,7 +38,7 @@\\n \\n     res = sc.execute(\"http://fin1te.net\")\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n \\n # forbidden host\\n@@ -51,4 +51,20 @@\\n \\n     res = sc.execute(\"http://localhost\")\\n except:\\n-    print \"Error:\", sys.exc_info()\\n+    print(\"Error:\", sys.exc_info())\\n+\\n+\\n+# regex bug\\n+try:\\n+    sc = safeurl.SafeURL()\\n+\\n+    opt = safeurl.Options()\\n+    opt.setList(\"whitelist\", [\"exam.le\"], \"domain\")\\n+    sc.setOptions(opt)\\n+\\n+    res = sc.execute(\"https://example.com/\")\\n+\\n+except:\\n+    print(\"Error:\", sys.exc_info())\\n+\\n+', '@@ -1,41 +0,0 @@\\n-import safeurl\\n-import sys\\n-\\n-# Default\\n-try:\\n-    sc = safeurl.SafeURL()\\n-    res = sc.execute(\"https://fin1te.net\")\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n-\\n-# options\\n-try:\\n-    sc = safeurl.SafeURL()\\n-\\n-    opt = safeurl.Options()\\n-    opt.clearList(\"whitelist\")\\n-    opt.clearList(\"blacklist\")\\n-    opt.setList(\"whitelist\", [\"google.com\", \"youtube.com\"], \"domain\")\\n-\\n-    sc.setOptions(opt)\\n-    res = sc.execute(\"http://www.youtube.com\")\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n-\\n-# url\\n-try:\\n-    url = safeurl.Url.validateUrl(\"http://google.com\", safeurl.Options())\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n-\\n-# redirects\\n-try:\\n-    sc = safeurl.SafeURL()\\n-\\n-    opt = safeurl.Options()\\n-    opt.enableFollowLocation().setFollowLocationLimit(10)\\n-    sc.setOptions(opt)\\n-\\n-    res = sc.execute(\"http://fin1te.net\")\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()', '@@ -12,21 +12,12 @@\\n from numbers import Number\\n from socket import gethostbyname_ex\\n \\n-import re\\n import netaddr\\n import pycurl\\n import socket\\n-import StringIO\\n-\\n-# Python 2.7/3 urlparse\\n-try:\\n-    # Python 2.7\\n-    from urlparse import urlparse\\n-    from urllib import quote\\n-except:\\n-    # Python 3\\n-    from urllib.parse import urlparse\\n-    from urllib.parse import quote\\n+import io\\n+from urllib.parse import urlparse\\n+from urllib.parse import quote\\n \\n class ObsoletePyCurlException(Exception): pass\\n class InvalidOptionException(Exception): pass\\n@@ -204,10 +195,9 @@ def isInList(self, lst, type_, value):\\n             else:\\n                 return False\\n \\n-        # For domains, a regex match is needed\\n         if type_ == \"domain\":\\n             for domain in dst:\\n-                if re.match(\"(?i)^%s\" % domain, value) is not None:\\n+                if domain.lower() == value.lower():\\n                     return True\\n             return False\\n         else:\\n@@ -661,7 +651,7 @@ def execute(self, url):\\n             self._handle.setopt(pycurl.URL, url[\"cleanUrl\"])\\n \\n             # Execute the cURL request\\n-            response = StringIO.StringIO()\\n+            response = io.BytesIO()\\n             self._handle.setopt(pycurl.OPENSOCKETFUNCTION, self._openSocketCallback)\\n             self._handle.setopt(pycurl.WRITEFUNCTION, response.write)\\n             self._handle.perform()'], 'file': ['safeurl/safeurl_tests.py', 'safeurl/safeurl_examples.py', 'safeurl/safeurl.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('0f34fd68-ea6d-4564-b50f-c954ead7648a'), UUID('0b922b12-eaed-4887-848b-85313d2b87b1'), UUID('e90da163-c92a-4889-b495-3e6e1f4dd3c6')]}\n",
      "ERROR:root:Error in {'repo': 'IncludeSecurity/safeurl-python', 'vulnerability_id': '2023-24622', 'commit': '42dd0c8e5fc84e17e1d3578d18aaea169eece474', 'commit_source': 'github', 'cwe_id': ['CWE-918'], 'patch': ['@@ -6,7 +6,7 @@\\n     sc = safeurl.SafeURL()\\n     res = sc.execute(\"https://fin1te.net\")\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n # options\\n try:\\n@@ -20,13 +20,13 @@\\n     sc.setOptions(opt)\\n     res = sc.execute(\"http://www.youtube.com\")\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n # url\\n try:\\n     url = safeurl.Url.validateUrl(\"http://google.com\", safeurl.Options())\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n # redirects\\n try:\\n@@ -38,7 +38,7 @@\\n \\n     res = sc.execute(\"http://fin1te.net\")\\n except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n+    print(\"Unexpected error:\", sys.exc_info())\\n \\n \\n # forbidden host\\n@@ -51,4 +51,20 @@\\n \\n     res = sc.execute(\"http://localhost\")\\n except:\\n-    print \"Error:\", sys.exc_info()\\n+    print(\"Error:\", sys.exc_info())\\n+\\n+\\n+# regex bug\\n+try:\\n+    sc = safeurl.SafeURL()\\n+\\n+    opt = safeurl.Options()\\n+    opt.setList(\"whitelist\", [\"exam.le\"], \"domain\")\\n+    sc.setOptions(opt)\\n+\\n+    res = sc.execute(\"https://example.com/\")\\n+\\n+except:\\n+    print(\"Error:\", sys.exc_info())\\n+\\n+', '@@ -1,41 +0,0 @@\\n-import safeurl\\n-import sys\\n-\\n-# Default\\n-try:\\n-    sc = safeurl.SafeURL()\\n-    res = sc.execute(\"https://fin1te.net\")\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n-\\n-# options\\n-try:\\n-    sc = safeurl.SafeURL()\\n-\\n-    opt = safeurl.Options()\\n-    opt.clearList(\"whitelist\")\\n-    opt.clearList(\"blacklist\")\\n-    opt.setList(\"whitelist\", [\"google.com\", \"youtube.com\"], \"domain\")\\n-\\n-    sc.setOptions(opt)\\n-    res = sc.execute(\"http://www.youtube.com\")\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n-\\n-# url\\n-try:\\n-    url = safeurl.Url.validateUrl(\"http://google.com\", safeurl.Options())\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()\\n-\\n-# redirects\\n-try:\\n-    sc = safeurl.SafeURL()\\n-\\n-    opt = safeurl.Options()\\n-    opt.enableFollowLocation().setFollowLocationLimit(10)\\n-    sc.setOptions(opt)\\n-\\n-    res = sc.execute(\"http://fin1te.net\")\\n-except:\\n-    print \"Unexpected error:\", sys.exc_info()', '@@ -12,21 +12,12 @@\\n from numbers import Number\\n from socket import gethostbyname_ex\\n \\n-import re\\n import netaddr\\n import pycurl\\n import socket\\n-import StringIO\\n-\\n-# Python 2.7/3 urlparse\\n-try:\\n-    # Python 2.7\\n-    from urlparse import urlparse\\n-    from urllib import quote\\n-except:\\n-    # Python 3\\n-    from urllib.parse import urlparse\\n-    from urllib.parse import quote\\n+import io\\n+from urllib.parse import urlparse\\n+from urllib.parse import quote\\n \\n class ObsoletePyCurlException(Exception): pass\\n class InvalidOptionException(Exception): pass\\n@@ -204,10 +195,9 @@ def isInList(self, lst, type_, value):\\n             else:\\n                 return False\\n \\n-        # For domains, a regex match is needed\\n         if type_ == \"domain\":\\n             for domain in dst:\\n-                if re.match(\"(?i)^%s\" % domain, value) is not None:\\n+                if domain.lower() == value.lower():\\n                     return True\\n             return False\\n         else:\\n@@ -661,7 +651,7 @@ def execute(self, url):\\n             self._handle.setopt(pycurl.URL, url[\"cleanUrl\"])\\n \\n             # Execute the cURL request\\n-            response = StringIO.StringIO()\\n+            response = io.BytesIO()\\n             self._handle.setopt(pycurl.OPENSOCKETFUNCTION, self._openSocketCallback)\\n             self._handle.setopt(pycurl.WRITEFUNCTION, response.write)\\n             self._handle.perform()'], 'file': ['safeurl/safeurl_tests.py', 'safeurl/safeurl_examples.py', 'safeurl/safeurl.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('0f34fd68-ea6d-4564-b50f-c954ead7648a'), UUID('0b922b12-eaed-4887-848b-85313d2b87b1'), UUID('e90da163-c92a-4889-b495-3e6e1f4dd3c6')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:0: except:\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 1:0: except:\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1623/1800 [20:04<00:30,  5.75it/s]ERROR:src.process_code_changes:Error processing commit fc2c1ea1b8d795094abb15ac73cab90830534e04\n",
      "ERROR:src.process_code_changes:{'repo': 'nivit/redports', 'vulnerability_id': '2014-125082', 'commit': 'fc2c1ea1b8d795094abb15ac73cab90830534e04', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -757,7 +757,7 @@ def __init__(self, env):\\n     def filter(self, owner=None, queueid=None, revision=None, uniqueports=False):\\n         self.owner = owner\\n         self.queueid = queueid\\n-\\tself.revision = int(revision)\\n+\\tself.revision = revision\\n         self.uniqueports = uniqueports\\n \\n     def count(self):\\n@@ -772,13 +772,13 @@ def _get_filter(self):\\n         filter = \\'\\'\\n \\n         if self.queueid:\\n-            filter += \"AND buildqueue.id = \\'%s\\'\" % (self.queueid)\\n+            filter += \"AND buildqueue.id = \\'%s\\'\" % (re.sub(\"[\\\\\"\\']\", \"\", self.queueid))\\n \\n         if self.owner:\\n-            filter += \"AND buildqueue.owner = \\'%s\\'\" % (self.owner)\\n+            filter += \"AND buildqueue.owner = \\'%s\\'\" % (re.sub(\"[\\\\\"\\']\", \"\", self.owner))\\n \\n \\tif self.revision:\\n-            filter += \"AND buildqueue.revision = \\'%s\\'\" % (self.revision)\\n+            filter += \"AND buildqueue.revision = %s\" % (int(self.revision))\\n \\n         return filter\\n '], 'file': ['redports-trac/redports/model.py'], 'language': ['Python'], 'temp_id': [UUID('3d0260cd-8374-4d8b-b230-a78988d51042')]}\n",
      "ERROR:root:Error in {'repo': 'nivit/redports', 'vulnerability_id': '2014-125082', 'commit': 'fc2c1ea1b8d795094abb15ac73cab90830534e04', 'commit_source': 'github', 'cwe_id': ['CWE-89'], 'patch': ['@@ -757,7 +757,7 @@ def __init__(self, env):\\n     def filter(self, owner=None, queueid=None, revision=None, uniqueports=False):\\n         self.owner = owner\\n         self.queueid = queueid\\n-\\tself.revision = int(revision)\\n+\\tself.revision = revision\\n         self.uniqueports = uniqueports\\n \\n     def count(self):\\n@@ -772,13 +772,13 @@ def _get_filter(self):\\n         filter = \\'\\'\\n \\n         if self.queueid:\\n-            filter += \"AND buildqueue.id = \\'%s\\'\" % (self.queueid)\\n+            filter += \"AND buildqueue.id = \\'%s\\'\" % (re.sub(\"[\\\\\"\\']\", \"\", self.queueid))\\n \\n         if self.owner:\\n-            filter += \"AND buildqueue.owner = \\'%s\\'\" % (self.owner)\\n+            filter += \"AND buildqueue.owner = \\'%s\\'\" % (re.sub(\"[\\\\\"\\']\", \"\", self.owner))\\n \\n \\tif self.revision:\\n-            filter += \"AND buildqueue.revision = \\'%s\\'\" % (self.revision)\\n+            filter += \"AND buildqueue.revision = %s\" % (int(self.revision))\\n \\n         return filter\\n '], 'file': ['redports-trac/redports/model.py'], 'language': ['Python'], 'temp_id': [UUID('3d0260cd-8374-4d8b-b230-a78988d51042')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 101, in clear_file_content\n",
      "    new_content = remove_comments(content)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 71, in remove_comments\n",
      "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 541, in _generate_tokens_from_c_tokenizer\n",
      "    raise e from None\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 537, in _generate_tokens_from_c_tokenizer\n",
      "    for info in it:\n",
      "  File \"<string>\", line 3\n",
      "    def _get_filter(self):\n",
      "                          ^\n",
      "IndentationError: unindent does not match any outer indentation level\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1631/1800 [20:05<00:25,  6.58it/s]ERROR:src.process_code_changes:Error processing commit 22b74fa09d7ccbc8c52270d648a0da7f3f0fa2bc\n",
      "ERROR:src.process_code_changes:{'repo': 'django/django', 'vulnerability_id': '2013-1443', 'commit': '22b74fa09d7ccbc8c52270d648a0da7f3f0fa2bc', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -1,6 +1,7 @@\\n from __future__ import unicode_literals\\n \\n import base64\\n+import functools\\n import hashlib\\n \\n from django.dispatch import receiver\\n@@ -16,6 +17,7 @@\\n \\n \\n UNUSABLE_PASSWORD = \\'!\\'  # This will never be a valid encoded hash\\n+MAXIMUM_PASSWORD_LENGTH = 4096  # The maximum length a password can be to prevent DoS\\n HASHERS = None  # lazily loaded from PASSWORD_HASHERS\\n PREFERRED_HASHER = None  # defaults to first item in PASSWORD_HASHERS\\n \\n@@ -27,6 +29,18 @@ def reset_hashers(**kwargs):\\n         PREFERRED_HASHER = None\\n \\n \\n+def password_max_length(max_length):\\n+    def inner(fn):\\n+        @functools.wraps(fn)\\n+        def wrapper(self, password, *args, **kwargs):\\n+            if len(password) > max_length:\\n+                raise ValueError(\"Invalid password; Must be less than or equal\"\\n+                                 \" to %d bytes\" % max_length)\\n+            return fn(self, password, *args, **kwargs)\\n+        return wrapper\\n+    return inner\\n+\\n+\\n def is_password_usable(encoded):\\n     if encoded is None or encoded == UNUSABLE_PASSWORD:\\n         return False\\n@@ -225,6 +239,7 @@ class PBKDF2PasswordHasher(BasePasswordHasher):\\n     iterations = 10000\\n     digest = hashlib.sha256\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt, iterations=None):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n@@ -234,6 +249,7 @@ def encode(self, password, salt, iterations=None):\\n         hash = base64.b64encode(hash).decode(\\'ascii\\').strip()\\n         return \"%s$%d$%s$%s\" % (self.algorithm, iterations, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, iterations, salt, hash = encoded.split(\\'$\\', 3)\\n         assert algorithm == self.algorithm\\n@@ -279,13 +295,15 @@ def salt(self):\\n         bcrypt = self._load_library()\\n         return bcrypt.gensalt(self.rounds)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         bcrypt = self._load_library()\\n         # Need to reevaluate the force_bytes call once bcrypt is supported on\\n         # Python 3\\n         data = bcrypt.hashpw(force_bytes(password), salt)\\n         return \"%s$%s\" % (self.algorithm, data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, data = encoded.split(\\'$\\', 1)\\n         assert algorithm == self.algorithm\\n@@ -310,12 +328,14 @@ class SHA1PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"sha1\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.sha1(force_bytes(salt + password)).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -338,12 +358,14 @@ class MD5PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"md5\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.md5(force_bytes(salt + password)).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -374,11 +396,13 @@ class UnsaltedSHA1PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         hash = hashlib.sha1(force_bytes(password)).hexdigest()\\n         return \\'sha1$$%s\\' % hash\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         encoded_2 = self.encode(password, \\'\\')\\n         return constant_time_compare(encoded, encoded_2)\\n@@ -408,10 +432,12 @@ class UnsaltedMD5PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         return hashlib.md5(force_bytes(password)).hexdigest()\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         if len(encoded) == 37 and encoded.startswith(\\'md5$$\\'):\\n             encoded = encoded[5:]\\n@@ -437,13 +463,15 @@ class CryptPasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return get_random_string(2)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         crypt = self._load_library()\\n         assert len(salt) == 2\\n         data = crypt.crypt(force_str(password), salt)\\n         # we don\\'t need to store the salt, but Django used to do this\\n         return \"%s$%s$%s\" % (self.algorithm, \\'\\', data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         crypt = self._load_library()\\n         algorithm, salt, data = encoded.split(\\'$\\', 2)\\n@@ -458,4 +486,3 @@ def safe_summary(self, encoded):\\n             (_(\\'salt\\'), salt),\\n             (_(\\'hash\\'), mask_hash(data, show=3)),\\n         ])\\n-', '@@ -12,7 +12,9 @@\\n \\n from django.contrib.auth import authenticate, get_user_model\\n from django.contrib.auth.models import User\\n-from django.contrib.auth.hashers import UNUSABLE_PASSWORD, identify_hasher\\n+from django.contrib.auth.hashers import (\\n+    MAXIMUM_PASSWORD_LENGTH, UNUSABLE_PASSWORD, identify_hasher,\\n+)\\n from django.contrib.auth.tokens import default_token_generator\\n from django.contrib.sites.models import get_current_site\\n \\n@@ -75,9 +77,10 @@ class UserCreationForm(forms.ModelForm):\\n             \\'invalid\\': _(\"This value may contain only letters, numbers and \"\\n                          \"@/./+/-/_ characters.\")})\\n     password1 = forms.CharField(label=_(\"Password\"),\\n-        widget=forms.PasswordInput)\\n+        widget=forms.PasswordInput, max_length=MAXIMUM_PASSWORD_LENGTH)\\n     password2 = forms.CharField(label=_(\"Password confirmation\"),\\n         widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n         help_text=_(\"Enter the same password as above, for verification.\"))\\n \\n     class Meta:\\n@@ -145,7 +148,11 @@ class AuthenticationForm(forms.Form):\\n     username/password logins.\\n     \"\"\"\\n     username = forms.CharField(max_length=254)\\n-    password = forms.CharField(label=_(\"Password\"), widget=forms.PasswordInput)\\n+    password = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     error_messages = {\\n         \\'invalid_login\\': _(\"Please enter a correct %(username)s and password. \"\\n@@ -269,10 +276,16 @@ class SetPasswordForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    new_password1 = forms.CharField(label=_(\"New password\"),\\n-                                    widget=forms.PasswordInput)\\n-    new_password2 = forms.CharField(label=_(\"New password confirmation\"),\\n-                                    widget=forms.PasswordInput)\\n+    new_password1 = forms.CharField(\\n+        label=_(\"New password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    new_password2 = forms.CharField(\\n+        label=_(\"New password confirmation\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user\\n@@ -303,8 +316,11 @@ class PasswordChangeForm(SetPasswordForm):\\n         \\'password_incorrect\\': _(\"Your old password was entered incorrectly. \"\\n                                 \"Please enter it again.\"),\\n     })\\n-    old_password = forms.CharField(label=_(\"Old password\"),\\n-                                   widget=forms.PasswordInput)\\n+    old_password = forms.CharField(\\n+        label=_(\"Old password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def clean_old_password(self):\\n         \"\"\"\\n@@ -329,10 +345,16 @@ class AdminPasswordChangeForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    password1 = forms.CharField(label=_(\"Password\"),\\n-                                widget=forms.PasswordInput)\\n-    password2 = forms.CharField(label=_(\"Password (again)\"),\\n-                                widget=forms.PasswordInput)\\n+    password1 = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    password2 = forms.CharField(\\n+        label=_(\"Password (again)\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user'], 'file': ['django/contrib/auth/hashers.py', 'django/contrib/auth/forms.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('8dcf5dd0-51fc-46b5-baac-fda0446b34c6'), UUID('4451c60b-98d7-4365-b92f-2f443b166a02')]}\n",
      "ERROR:root:Error in {'repo': 'django/django', 'vulnerability_id': '2013-1443', 'commit': '22b74fa09d7ccbc8c52270d648a0da7f3f0fa2bc', 'commit_source': 'github', 'cwe_id': ['CWE-400'], 'patch': ['@@ -1,6 +1,7 @@\\n from __future__ import unicode_literals\\n \\n import base64\\n+import functools\\n import hashlib\\n \\n from django.dispatch import receiver\\n@@ -16,6 +17,7 @@\\n \\n \\n UNUSABLE_PASSWORD = \\'!\\'  # This will never be a valid encoded hash\\n+MAXIMUM_PASSWORD_LENGTH = 4096  # The maximum length a password can be to prevent DoS\\n HASHERS = None  # lazily loaded from PASSWORD_HASHERS\\n PREFERRED_HASHER = None  # defaults to first item in PASSWORD_HASHERS\\n \\n@@ -27,6 +29,18 @@ def reset_hashers(**kwargs):\\n         PREFERRED_HASHER = None\\n \\n \\n+def password_max_length(max_length):\\n+    def inner(fn):\\n+        @functools.wraps(fn)\\n+        def wrapper(self, password, *args, **kwargs):\\n+            if len(password) > max_length:\\n+                raise ValueError(\"Invalid password; Must be less than or equal\"\\n+                                 \" to %d bytes\" % max_length)\\n+            return fn(self, password, *args, **kwargs)\\n+        return wrapper\\n+    return inner\\n+\\n+\\n def is_password_usable(encoded):\\n     if encoded is None or encoded == UNUSABLE_PASSWORD:\\n         return False\\n@@ -225,6 +239,7 @@ class PBKDF2PasswordHasher(BasePasswordHasher):\\n     iterations = 10000\\n     digest = hashlib.sha256\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt, iterations=None):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n@@ -234,6 +249,7 @@ def encode(self, password, salt, iterations=None):\\n         hash = base64.b64encode(hash).decode(\\'ascii\\').strip()\\n         return \"%s$%d$%s$%s\" % (self.algorithm, iterations, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, iterations, salt, hash = encoded.split(\\'$\\', 3)\\n         assert algorithm == self.algorithm\\n@@ -279,13 +295,15 @@ def salt(self):\\n         bcrypt = self._load_library()\\n         return bcrypt.gensalt(self.rounds)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         bcrypt = self._load_library()\\n         # Need to reevaluate the force_bytes call once bcrypt is supported on\\n         # Python 3\\n         data = bcrypt.hashpw(force_bytes(password), salt)\\n         return \"%s$%s\" % (self.algorithm, data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, data = encoded.split(\\'$\\', 1)\\n         assert algorithm == self.algorithm\\n@@ -310,12 +328,14 @@ class SHA1PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"sha1\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.sha1(force_bytes(salt + password)).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -338,12 +358,14 @@ class MD5PasswordHasher(BasePasswordHasher):\\n     \"\"\"\\n     algorithm = \"md5\"\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert password\\n         assert salt and \\'$\\' not in salt\\n         hash = hashlib.md5(force_bytes(salt + password)).hexdigest()\\n         return \"%s$%s$%s\" % (self.algorithm, salt, hash)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         algorithm, salt, hash = encoded.split(\\'$\\', 2)\\n         assert algorithm == self.algorithm\\n@@ -374,11 +396,13 @@ class UnsaltedSHA1PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         hash = hashlib.sha1(force_bytes(password)).hexdigest()\\n         return \\'sha1$$%s\\' % hash\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         encoded_2 = self.encode(password, \\'\\')\\n         return constant_time_compare(encoded, encoded_2)\\n@@ -408,10 +432,12 @@ class UnsaltedMD5PasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return \\'\\'\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         assert salt == \\'\\'\\n         return hashlib.md5(force_bytes(password)).hexdigest()\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         if len(encoded) == 37 and encoded.startswith(\\'md5$$\\'):\\n             encoded = encoded[5:]\\n@@ -437,13 +463,15 @@ class CryptPasswordHasher(BasePasswordHasher):\\n     def salt(self):\\n         return get_random_string(2)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def encode(self, password, salt):\\n         crypt = self._load_library()\\n         assert len(salt) == 2\\n         data = crypt.crypt(force_str(password), salt)\\n         # we don\\'t need to store the salt, but Django used to do this\\n         return \"%s$%s$%s\" % (self.algorithm, \\'\\', data)\\n \\n+    @password_max_length(MAXIMUM_PASSWORD_LENGTH)\\n     def verify(self, password, encoded):\\n         crypt = self._load_library()\\n         algorithm, salt, data = encoded.split(\\'$\\', 2)\\n@@ -458,4 +486,3 @@ def safe_summary(self, encoded):\\n             (_(\\'salt\\'), salt),\\n             (_(\\'hash\\'), mask_hash(data, show=3)),\\n         ])\\n-', '@@ -12,7 +12,9 @@\\n \\n from django.contrib.auth import authenticate, get_user_model\\n from django.contrib.auth.models import User\\n-from django.contrib.auth.hashers import UNUSABLE_PASSWORD, identify_hasher\\n+from django.contrib.auth.hashers import (\\n+    MAXIMUM_PASSWORD_LENGTH, UNUSABLE_PASSWORD, identify_hasher,\\n+)\\n from django.contrib.auth.tokens import default_token_generator\\n from django.contrib.sites.models import get_current_site\\n \\n@@ -75,9 +77,10 @@ class UserCreationForm(forms.ModelForm):\\n             \\'invalid\\': _(\"This value may contain only letters, numbers and \"\\n                          \"@/./+/-/_ characters.\")})\\n     password1 = forms.CharField(label=_(\"Password\"),\\n-        widget=forms.PasswordInput)\\n+        widget=forms.PasswordInput, max_length=MAXIMUM_PASSWORD_LENGTH)\\n     password2 = forms.CharField(label=_(\"Password confirmation\"),\\n         widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n         help_text=_(\"Enter the same password as above, for verification.\"))\\n \\n     class Meta:\\n@@ -145,7 +148,11 @@ class AuthenticationForm(forms.Form):\\n     username/password logins.\\n     \"\"\"\\n     username = forms.CharField(max_length=254)\\n-    password = forms.CharField(label=_(\"Password\"), widget=forms.PasswordInput)\\n+    password = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     error_messages = {\\n         \\'invalid_login\\': _(\"Please enter a correct %(username)s and password. \"\\n@@ -269,10 +276,16 @@ class SetPasswordForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    new_password1 = forms.CharField(label=_(\"New password\"),\\n-                                    widget=forms.PasswordInput)\\n-    new_password2 = forms.CharField(label=_(\"New password confirmation\"),\\n-                                    widget=forms.PasswordInput)\\n+    new_password1 = forms.CharField(\\n+        label=_(\"New password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    new_password2 = forms.CharField(\\n+        label=_(\"New password confirmation\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user\\n@@ -303,8 +316,11 @@ class PasswordChangeForm(SetPasswordForm):\\n         \\'password_incorrect\\': _(\"Your old password was entered incorrectly. \"\\n                                 \"Please enter it again.\"),\\n     })\\n-    old_password = forms.CharField(label=_(\"Old password\"),\\n-                                   widget=forms.PasswordInput)\\n+    old_password = forms.CharField(\\n+        label=_(\"Old password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def clean_old_password(self):\\n         \"\"\"\\n@@ -329,10 +345,16 @@ class AdminPasswordChangeForm(forms.Form):\\n     error_messages = {\\n         \\'password_mismatch\\': _(\"The two password fields didn\\'t match.\"),\\n     }\\n-    password1 = forms.CharField(label=_(\"Password\"),\\n-                                widget=forms.PasswordInput)\\n-    password2 = forms.CharField(label=_(\"Password (again)\"),\\n-                                widget=forms.PasswordInput)\\n+    password1 = forms.CharField(\\n+        label=_(\"Password\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n+    password2 = forms.CharField(\\n+        label=_(\"Password (again)\"),\\n+        widget=forms.PasswordInput,\\n+        max_length=MAXIMUM_PASSWORD_LENGTH,\\n+    )\\n \\n     def __init__(self, user, *args, **kwargs):\\n         self.user = user'], 'file': ['django/contrib/auth/hashers.py', 'django/contrib/auth/forms.py'], 'language': ['Python', 'Python'], 'temp_id': [UUID('8dcf5dd0-51fc-46b5-baac-fda0446b34c6'), UUID('4451c60b-98d7-4365-b92f-2f443b166a02')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 193, in get_changes\n",
      "    _get_changes_lines_units(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/get_changes_lines_units.py\", line 250, in _get_changes_lines_units\n",
      "    <= name.get_definition_end_position()[0]\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/classes.py\", line 260, in get_definition_end_position\n",
      "    if self.type in (\"function\", \"class\"):\n",
      "       ^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/api/classes.py\", line 190, in type\n",
      "    for value in self._name.infer():\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 281, in infer\n",
      "    return tree_name_to_values(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/__init__.py\", line 21, in wrapper\n",
      "    return built_functions[public_name](*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/stdlib.py\", line 878, in wrapper\n",
      "    return func(inference_state, context, tree_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/django.py\", line 177, in wrapper\n",
      "    result = func(inference_state, context, tree_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 771, in tree_name_to_values\n",
      "    types = imports.infer_import(context, tree_name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/cache.py\", line 44, in wrapper\n",
      "    rv = function(obj, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/imports.py\", line 57, in infer_import\n",
      "    values = values.py__getattribute__(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in py__getattribute__\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in <genexpr>\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 84, in py__getattribute__\n",
      "    values = ValueSet.from_sets(name.infer() for name in names)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 84, in <genexpr>\n",
      "    values = ValueSet.from_sets(name.infer() for name in names)\n",
      "                                ^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 657, in infer\n",
      "    inferred = super().infer()\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 281, in infer\n",
      "    return tree_name_to_values(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/__init__.py\", line 21, in wrapper\n",
      "    return built_functions[public_name](*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/stdlib.py\", line 878, in wrapper\n",
      "    return func(inference_state, context, tree_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/django.py\", line 177, in wrapper\n",
      "    result = func(inference_state, context, tree_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 771, in tree_name_to_values\n",
      "    types = imports.infer_import(context, tree_name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/cache.py\", line 44, in wrapper\n",
      "    rv = function(obj, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/imports.py\", line 57, in infer_import\n",
      "    values = values.py__getattribute__(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in py__getattribute__\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in <genexpr>\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 84, in py__getattribute__\n",
      "    values = ValueSet.from_sets(name.infer() for name in names)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 84, in <genexpr>\n",
      "    values = ValueSet.from_sets(name.infer() for name in names)\n",
      "                                ^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 657, in infer\n",
      "    inferred = super().infer()\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 281, in infer\n",
      "    return tree_name_to_values(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/__init__.py\", line 21, in wrapper\n",
      "    return built_functions[public_name](*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/stdlib.py\", line 878, in wrapper\n",
      "    return func(inference_state, context, tree_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/django.py\", line 177, in wrapper\n",
      "    result = func(inference_state, context, tree_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 709, in tree_name_to_values\n",
      "    value_set |= annotation.infer_annotation(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/gradual/annotation.py\", line 34, in infer_annotation\n",
      "    value_set = context.infer_node(annotation)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 224, in infer_node\n",
      "    return infer_node(self, node)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 157, in infer_node\n",
      "    return _infer_node_if_inferred(context, element)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 170, in _infer_node_if_inferred\n",
      "    return _infer_node_cached(context, element)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/cache.py\", line 44, in wrapper\n",
      "    rv = function(obj, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 175, in _infer_node_cached\n",
      "    return _infer_node(context, element)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/debug.py\", line 81, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 83, in wrapper\n",
      "    return func(context, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 185, in _infer_node\n",
      "    return infer_atom(context, element)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 305, in infer_atom\n",
      "    return context.py__getattribute__(atom, position=position)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 77, in py__getattribute__\n",
      "    values = ValueSet.from_sets(name.infer() for name in names)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/context.py\", line 77, in <genexpr>\n",
      "    values = ValueSet.from_sets(name.infer() for name in names)\n",
      "                                ^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/names.py\", line 281, in infer\n",
      "    return tree_name_to_values(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/__init__.py\", line 21, in wrapper\n",
      "    return built_functions[public_name](*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/stdlib.py\", line 878, in wrapper\n",
      "    return func(inference_state, context, tree_name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/plugins/django.py\", line 177, in wrapper\n",
      "    result = func(inference_state, context, tree_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/syntax_tree.py\", line 771, in tree_name_to_values\n",
      "    types = imports.infer_import(context, tree_name)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/cache.py\", line 44, in wrapper\n",
      "    rv = function(obj, *args, **kwargs)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/imports.py\", line 57, in infer_import\n",
      "    values = values.py__getattribute__(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in py__getattribute__\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 430, in from_sets\n",
      "    for set_ in sets:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 496, in <genexpr>\n",
      "    return ValueSet.from_sets(c.py__getattribute__(*args, **kwargs) for c in self._set)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 83, in py__getattribute__\n",
      "    names = self.goto(name_or_str, name_context, analysis_errors)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 72, in goto\n",
      "    names = finder.filter_name(filters, name_or_str)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/finder.py\", line 35, in filter_name\n",
      "    for filter in filters:\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/base_value.py\", line 62, in _get_value_filters\n",
      "    yield from self.get_filters(origin_scope=origin_scope)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/gradual/stub_value.py\", line 44, in get_filters\n",
      "    next(filters, None)  # Ignore the first filter and replace it with our own\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/value/module.py\", line 63, in get_filters\n",
      "    ParserTreeFilter(\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 138, in __init__\n",
      "    super().__init__(parent_context, node_context)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/inference/filters.py\", line 100, in __init__\n",
      "    self._parso_cache_node = get_parso_cache_node(\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/parser_utils.py\", line 287, in get_parso_cache_node\n",
      "    return parser_cache[grammar._hashed][path]\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "KeyError: PosixPath('/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/.venv/lib/python3.12/site-packages/jedi/third_party/django-stubs/django-stubs/dispatch/__init__.pyi')\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1636/1800 [20:07<00:31,  5.28it/s]ERROR:src.process_code_changes:Error processing commit 87ac4deb00cc9fe334706e42a365903a1d581624\n",
      "ERROR:src.process_code_changes:{'repo': 'dbt-labs/dbt-core', 'vulnerability_id': '2024-40637', 'commit': '87ac4deb00cc9fe334706e42a365903a1d581624', 'commit_source': 'github', 'cwe_id': ['CWE-74', 'CWE-74', 'CWE-74'], 'patch': ['@@ -39,23 +39,24 @@ def get_flags():\\n     return GLOBAL_FLAGS\\n \\n \\n-def set_from_args(args: Namespace, user_config):\\n+def set_from_args(args: Namespace, project_flags):\\n     global GLOBAL_FLAGS\\n     from dbt.cli.main import cli\\n     from dbt.cli.flags import Flags, convert_config\\n \\n-    # we set attributes of args after initialize the flags, but user_config\\n+    # we set attributes of args after initialize the flags, but project_flags\\n     # is being read in the Flags constructor, so we need to read it here and pass in\\n-    # to make sure we use the correct user_config\\n-    if (hasattr(args, \"PROFILES_DIR\") or hasattr(args, \"profiles_dir\")) and not user_config:\\n-        from dbt.config.profile import read_user_config\\n+    # to make sure we use the correct project_flags\\n+    profiles_dir = getattr(args, \"PROFILES_DIR\", None) or getattr(args, \"profiles_dir\", None)\\n+    project_dir = getattr(args, \"PROJECT_DIR\", None) or getattr(args, \"project_dir\", None)\\n+    if profiles_dir and project_dir:\\n+        from dbt.config.project import read_project_flags\\n \\n-        profiles_dir = getattr(args, \"PROFILES_DIR\", None) or getattr(args, \"profiles_dir\")\\n-        user_config = read_user_config(profiles_dir)\\n+        project_flags = read_project_flags(project_dir, profiles_dir)\\n \\n     # make a dummy context to get the flags, totally arbitrary\\n     ctx = cli.make_context(\"run\", [\"run\"])\\n-    flags = Flags(ctx, user_config)\\n+    flags = Flags(ctx, project_flags)\\n     for arg_name, args_param_value in vars(args).items():\\n         args_param_value = convert_config(arg_name, args_param_value)\\n         object.__setattr__(flags, arg_name.upper(), args_param_value)', '@@ -20,7 +20,7 @@\\n from dbt.config.project import load_raw_project\\n from dbt.contracts.connection import AdapterRequiredConfig, Credentials, HasCredentials\\n from dbt.contracts.graph.manifest import ManifestMetadata\\n-from dbt.contracts.project import Configuration, UserConfig\\n+from dbt.contracts.project import Configuration\\n from dbt.contracts.relation import ComponentName\\n from dbt.dataclass_schema import ValidationError\\n from dbt.events.functions import warn_or_error\\n@@ -178,7 +178,6 @@ def from_parts(\\n             profile_env_vars=profile.profile_env_vars,\\n             profile_name=profile.profile_name,\\n             target_name=profile.target_name,\\n-            user_config=profile.user_config,\\n             threads=profile.threads,\\n             credentials=profile.credentials,\\n             args=args,\\n@@ -432,7 +431,6 @@ def _connection_keys(self):\\n class UnsetProfile(Profile):\\n     def __init__(self):\\n         self.credentials = UnsetCredentials()\\n-        self.user_config = UserConfig()  # This will be read in _get_rendered_profile\\n         self.profile_name = \"\"\\n         self.target_name = \"\"\\n         self.threads = -1', '@@ -631,7 +631,7 @@ def _connection_exception_retry(fn, max_attempts: int, attempt: int = 0):\\n def args_to_dict(args):\\n     var_args = vars(args).copy()\\n     # update the args with the flags, which could also come from environment\\n-    # variables or user_config\\n+    # variables or project_flags\\n     flag_dict = flags.get_flag_dict()\\n     var_args.update(flag_dict)\\n     dict_args = {}', '@@ -471,7 +471,6 @@ def process(self, record):\\n \\n \\n def initialize_from_flags(send_anonymous_usage_stats, profiles_dir):\\n-    # Setting these used to be in UserConfig, but had to be moved here\\n     global active_user\\n     if send_anonymous_usage_stats:\\n         active_user = User(profiles_dir)', '@@ -3,6 +3,7 @@\\n from dataclasses import dataclass\\n from importlib import import_module\\n from multiprocessing import get_context\\n+from pathlib import Path\\n from pprint import pformat as pf\\n from typing import Any, Callable, Dict, List, Optional, Set, Union\\n \\n@@ -11,8 +12,8 @@\\n from dbt.cli.exceptions import DbtUsageException\\n from dbt.cli.resolvers import default_log_path, default_project_dir\\n from dbt.cli.types import Command as CliCommand\\n-from dbt.config.profile import read_user_config\\n-from dbt.contracts.project import UserConfig\\n+from dbt.config.project import read_project_flags\\n+from dbt.contracts.project import ProjectFlags\\n from dbt.exceptions import DbtInternalError\\n from dbt.deprecations import renamed_env_var\\n from dbt.helper_types import WarnErrorOptions\\n@@ -25,7 +26,7 @@\\n     \"INDIRECT_SELECTION\": \"eager\",\\n     \"TARGET_PATH\": None,\\n     \"WARN_ERROR\": None,\\n-    # Cli args without user_config or env var option.\\n+    # Cli args without project_flags or env var option.\\n     \"FULL_REFRESH\": False,\\n     \"STRICT_MODE\": False,\\n     \"STORE_FAILURES\": False,\\n@@ -77,7 +78,7 @@ class Flags:\\n     \"\"\"Primary configuration artifact for running dbt\"\"\"\\n \\n     def __init__(\\n-        self, ctx: Optional[Context] = None, user_config: Optional[UserConfig] = None\\n+        self, ctx: Optional[Context] = None, project_flags: Optional[ProjectFlags] = None\\n     ) -> None:\\n         # Set the default flags.\\n         for key, value in FLAGS_DEFAULTS.items():\\n@@ -200,27 +201,40 @@ def _assign_params(\\n                 invoked_subcommand_ctx, params_assigned_from_default, deprecated_env_vars\\n             )\\n \\n-        if not user_config:\\n+        if not project_flags:\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             profiles_dir = getattr(self, \"PROFILES_DIR\", None)\\n-            user_config = read_user_config(profiles_dir) if profiles_dir else None\\n+            if profiles_dir and project_dir:\\n+                project_flags = read_project_flags(project_dir, profiles_dir)\\n+            else:\\n+                project_flags = None\\n \\n         # Add entire invocation command to flags\\n         object.__setattr__(self, \"INVOCATION_COMMAND\", \"dbt \" + \" \".join(sys.argv[1:]))\\n \\n-        # Overwrite default assignments with user config if available.\\n-        if user_config:\\n+        if project_flags:\\n+            # Overwrite default assignments with project flags if available.\\n             param_assigned_from_default_copy = params_assigned_from_default.copy()\\n             for param_assigned_from_default in params_assigned_from_default:\\n-                user_config_param_value = getattr(user_config, param_assigned_from_default, None)\\n-                if user_config_param_value is not None:\\n+                project_flags_param_value = getattr(\\n+                    project_flags, param_assigned_from_default, None\\n+                )\\n+                if project_flags_param_value is not None:\\n                     object.__setattr__(\\n                         self,\\n                         param_assigned_from_default.upper(),\\n-                        convert_config(param_assigned_from_default, user_config_param_value),\\n+                        convert_config(param_assigned_from_default, project_flags_param_value),\\n                     )\\n                     param_assigned_from_default_copy.remove(param_assigned_from_default)\\n             params_assigned_from_default = param_assigned_from_default_copy\\n \\n+            # Add project-level flags that are not available as CLI options / env vars\\n+            for (\\n+                project_level_flag_name,\\n+                project_level_flag_value,\\n+            ) in project_flags.project_only_flags.items():\\n+                object.__setattr__(self, project_level_flag_name.upper(), project_level_flag_value)\\n+\\n         # Set hard coded flags.\\n         object.__setattr__(self, \"WHICH\", invoked_subcommand_name or ctx.info_name)\\n         object.__setattr__(self, \"MP_CONTEXT\", get_context(\"spawn\"))\\n@@ -234,9 +248,11 @@ def _assign_params(\\n         # Starting in v1.5, if `log-path` is set in `dbt_project.yml`, it will raise a deprecation warning,\\n         # with the possibility of removing it in a future release.\\n         if getattr(self, \"LOG_PATH\", None) is None:\\n-            project_dir = getattr(self, \"PROJECT_DIR\", default_project_dir())\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             version_check = getattr(self, \"VERSION_CHECK\", True)\\n-            object.__setattr__(self, \"LOG_PATH\", default_log_path(project_dir, version_check))\\n+            object.__setattr__(\\n+                self, \"LOG_PATH\", default_log_path(Path(project_dir), version_check)\\n+            )\\n \\n         # Support console DO NOT TRACK initiative.\\n         if os.getenv(\"DO_NOT_TRACK\", \"\").lower() in (\"1\", \"t\", \"true\", \"y\", \"yes\"):', '@@ -178,17 +178,9 @@ def __post_serialize__(self, dct):\\n         return dct\\n \\n \\n-class UserConfigContract(Protocol):\\n-    send_anonymous_usage_stats: bool\\n-    use_colors: Optional[bool] = None\\n-    partial_parse: Optional[bool] = None\\n-    printer_width: Optional[int] = None\\n-\\n-\\n class HasCredentials(Protocol):\\n     credentials: Credentials\\n     profile_name: str\\n-    user_config: UserConfigContract\\n     target_name: str\\n     threads: int\\n ', '@@ -1,5 +1,5 @@\\n from dbt.contracts.util import Replaceable, Mergeable, list_str, Identifier\\n-from dbt.contracts.connection import QueryComment, UserConfigContract\\n+from dbt.contracts.connection import QueryComment\\n from dbt.helper_types import NoValue\\n from dbt.dataclass_schema import (\\n     dbtClassMixin,\\n@@ -283,7 +283,7 @@ def validate(cls, data):\\n \\n \\n @dataclass\\n-class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n+class ProjectFlags(ExtensibleDbtClassMixin, Replaceable):\\n     cache_selected_only: Optional[bool] = None\\n     debug: Optional[bool] = None\\n     fail_fast: Optional[bool] = None\\n@@ -295,6 +295,7 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     partial_parse: Optional[bool] = None\\n     populate_cache: Optional[bool] = None\\n     printer_width: Optional[int] = None\\n+    require_explicit_package_overrides_for_builtin_materializations: bool = False\\n     send_anonymous_usage_stats: bool = DEFAULT_SEND_ANONYMOUS_USAGE_STATS\\n     static_parser: Optional[bool] = None\\n     use_colors: Optional[bool] = None\\n@@ -305,12 +306,17 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     warn_error_options: Optional[Dict[str, Union[str, List[str]]]] = None\\n     write_json: Optional[bool] = None\\n \\n+    @property\\n+    def project_only_flags(self) -> Dict[str, Any]:\\n+        return {\\n+            \"require_explicit_package_overrides_for_builtin_materializations\": self.require_explicit_package_overrides_for_builtin_materializations,\\n+        }\\n+\\n \\n @dataclass\\n class ProfileConfig(dbtClassMixin, Replaceable):\\n     profile_name: str\\n     target_name: str\\n-    user_config: UserConfig\\n     threads: int\\n     # TODO: make this a dynamic union of some kind?\\n     credentials: Optional[Dict[str, Any]]', '@@ -8,7 +8,7 @@\\n from dbt.clients.system import load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n from dbt.contracts.connection import Credentials, HasCredentials\\n-from dbt.contracts.project import ProfileConfig, UserConfig\\n+from dbt.contracts.project import ProfileConfig\\n from dbt.exceptions import (\\n     CompilationError,\\n     DbtProfileError,\\n@@ -19,7 +19,6 @@\\n )\\n from dbt.events.types import MissingProfileTarget\\n from dbt.events.functions import fire_event\\n-from dbt.utils import coerce_dict_str\\n \\n from .renderer import ProfileRenderer\\n \\n@@ -51,27 +50,13 @@ def read_profile(profiles_dir: str) -> Dict[str, Any]:\\n     return {}\\n \\n \\n-def read_user_config(directory: str) -> UserConfig:\\n-    try:\\n-        profile = read_profile(directory)\\n-        if profile:\\n-            user_config = coerce_dict_str(profile.get(\"config\", {}))\\n-            if user_config is not None:\\n-                UserConfig.validate(user_config)\\n-                return UserConfig.from_dict(user_config)\\n-    except (DbtRuntimeError, ValidationError):\\n-        pass\\n-    return UserConfig()\\n-\\n-\\n # The Profile class is included in RuntimeConfig, so any attribute\\n # additions must also be set where the RuntimeConfig class is created\\n # `init=False` is a workaround for https://bugs.python.org/issue45081\\n @dataclass(init=False)\\n class Profile(HasCredentials):\\n     profile_name: str\\n     target_name: str\\n-    user_config: UserConfig\\n     threads: int\\n     credentials: Credentials\\n     profile_env_vars: Dict[str, Any]\\n@@ -80,7 +65,6 @@ def __init__(\\n         self,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: UserConfig,\\n         threads: int,\\n         credentials: Credentials,\\n     ) -> None:\\n@@ -89,7 +73,6 @@ def __init__(\\n         \"\"\"\\n         self.profile_name = profile_name\\n         self.target_name = target_name\\n-        self.user_config = user_config\\n         self.threads = threads\\n         self.credentials = credentials\\n         self.profile_env_vars = {}  # never available on init\\n@@ -106,12 +89,10 @@ def to_profile_info(self, serialize_credentials: bool = False) -> Dict[str, Any]\\n         result = {\\n             \"profile_name\": self.profile_name,\\n             \"target_name\": self.target_name,\\n-            \"user_config\": self.user_config,\\n             \"threads\": self.threads,\\n             \"credentials\": self.credentials,\\n         }\\n         if serialize_credentials:\\n-            result[\"user_config\"] = self.user_config.to_dict(omit_none=True)\\n             result[\"credentials\"] = self.credentials.to_dict(omit_none=True)\\n         return result\\n \\n@@ -124,7 +105,6 @@ def to_target_dict(self) -> Dict[str, Any]:\\n                 \"name\": self.target_name,\\n                 \"target_name\": self.target_name,\\n                 \"profile_name\": self.profile_name,\\n-                \"config\": self.user_config.to_dict(omit_none=True),\\n             }\\n         )\\n         return target\\n@@ -246,7 +226,6 @@ def from_credentials(\\n         threads: int,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n     ) -> \"Profile\":\\n         \"\"\"Create a profile from an existing set of Credentials and the\\n         remaining information.\\n@@ -255,20 +234,13 @@ def from_credentials(\\n         :param threads: The number of threads to use for connections.\\n         :param profile_name: The profile name used for this profile.\\n         :param target_name: The target name used for this profile.\\n-        :param user_config: The user-level config block from the\\n-            raw profiles, if specified.\\n         :raises DbtProfileError: If the profile is invalid.\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        if user_config is None:\\n-            user_config = {}\\n-        UserConfig.validate(user_config)\\n-        user_config_obj: UserConfig = UserConfig.from_dict(user_config)\\n \\n         profile = cls(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n-            user_config=user_config_obj,\\n             threads=threads,\\n             credentials=credentials,\\n         )\\n@@ -316,7 +288,6 @@ def from_raw_profile_info(\\n         raw_profile: Dict[str, Any],\\n         profile_name: str,\\n         renderer: ProfileRenderer,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n         target_override: Optional[str] = None,\\n         threads_override: Optional[int] = None,\\n     ) -> \"Profile\":\\n@@ -328,8 +299,6 @@ def from_raw_profile_info(\\n             disk as yaml and its values rendered with jinja.\\n         :param profile_name: The profile name used.\\n         :param renderer: The config renderer.\\n-        :param user_config: The global config for the user, if it\\n-            was present.\\n         :param target_override: The target to use, if provided on\\n             the command line.\\n         :param threads_override: The thread count to use, if\\n@@ -338,9 +307,6 @@ def from_raw_profile_info(\\n             target could not be found\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        # user_config is not rendered.\\n-        if user_config is None:\\n-            user_config = raw_profile.get(\"config\")\\n         # TODO: should it be, and the values coerced to bool?\\n         target_name, profile_data = cls.render_profile(\\n             raw_profile, profile_name, target_override, renderer\\n@@ -361,7 +327,6 @@ def from_raw_profile_info(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n             threads=threads,\\n-            user_config=user_config,\\n         )\\n \\n     @classmethod\\n@@ -396,13 +361,11 @@ def from_raw_profiles(\\n         if not raw_profile:\\n             msg = f\"Profile {profile_name} in profiles.yml is empty\"\\n             raise DbtProfileError(INVALID_PROFILE_MESSAGE.format(error_string=msg))\\n-        user_config = raw_profiles.get(\"config\")\\n \\n         return cls.from_raw_profile_info(\\n             raw_profile=raw_profile,\\n             profile_name=profile_name,\\n             renderer=renderer,\\n-            user_config=user_config,\\n             target_override=target_override,\\n             threads_override=threads_override,\\n         )', '@@ -25,6 +25,7 @@\\n from typing_extensions import Protocol\\n from uuid import UUID\\n \\n+\\n from dbt.contracts.graph.nodes import (\\n     BaseNode,\\n     Documentation,\\n@@ -67,7 +68,7 @@\\n from dbt.events.contextvars import get_node_info\\n from dbt.node_types import NodeType, AccessType\\n from dbt.flags import get_flags, MP_CONTEXT\\n-from dbt import tracking\\n+from dbt import tracking, deprecations\\n import dbt.utils\\n \\n \\n@@ -616,11 +617,29 @@ def __lt__(self, other: object) -> bool:\\n \\n \\n class CandidateList(List[M]):\\n-    def last(self) -> Optional[Macro]:\\n+    def last_candidate(\\n+        self, valid_localities: Optional[List[Locality]] = None\\n+    ) -> Optional[MacroCandidate]:\\n+        \"\"\"\\n+        Obtain the last (highest precedence) MacroCandidate from the CandidateList of any locality in valid_localities.\\n+        If valid_localities is not specified, return the last MacroCandidate of any locality.\\n+        \"\"\"\\n         if not self:\\n             return None\\n         self.sort()\\n-        return self[-1].macro\\n+\\n+        if valid_localities is None:\\n+            return self[-1]\\n+\\n+        for candidate in reversed(self):\\n+            if candidate.locality in valid_localities:\\n+                return candidate\\n+\\n+        return None\\n+\\n+    def last(self) -> Optional[Macro]:\\n+        last_candidate = self.last_candidate()\\n+        return last_candidate.macro if last_candidate is not None else None\\n \\n \\n def _get_locality(macro: Macro, root_project_name: str, internal_packages: Set[str]) -> Locality:\\n@@ -914,7 +933,33 @@ def find_materialization_macro_by_name(\\n                 for specificity, atype in enumerate(self._get_parent_adapter_types(adapter_type))\\n             )\\n         )\\n-        return candidates.last()\\n+        core_candidates = [\\n+            candidate for candidate in candidates if candidate.locality == Locality.Core\\n+        ]\\n+\\n+        materialization_candidate = candidates.last_candidate()\\n+        # If an imported materialization macro was found that also had a core candidate, fire a deprecation\\n+        if (\\n+            materialization_candidate is not None\\n+            and materialization_candidate.locality == Locality.Imported\\n+            and core_candidates\\n+        ):\\n+            # preserve legacy behaviour - allow materialization override\\n+            if (\\n+                get_flags().require_explicit_package_overrides_for_builtin_materializations\\n+                is False\\n+            ):\\n+                deprecations.warn(\\n+                    \"package-materialization-override\",\\n+                    package_name=materialization_candidate.macro.package_name,\\n+                    materialization_name=materialization_name,\\n+                )\\n+            else:\\n+                materialization_candidate = candidates.last_candidate(\\n+                    valid_localities=[Locality.Core, Locality.Root]\\n+                )\\n+\\n+        return materialization_candidate.macro if materialization_candidate else None\\n \\n     def get_resource_fqns(self) -> Mapping[str, PathSet]:\\n         resource_fqns: Dict[str, Set[Tuple[str, ...]]] = {}', '@@ -20,6 +20,7 @@\\n     DEPENDENCIES_FILE_NAME,\\n     PACKAGES_FILE_NAME,\\n     PACKAGE_LOCK_HASH_KEY,\\n+    DBT_PROJECT_FILE_NAME,\\n )\\n from dbt.clients.system import path_exists, load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n@@ -35,12 +36,13 @@\\n from dbt.helper_types import NoValue\\n from dbt.semver import VersionSpecifier, versions_compatible\\n from dbt.version import get_installed_version\\n-from dbt.utils import MultiDict, md5\\n+from dbt.utils import MultiDict, md5, coerce_dict_str\\n from dbt.node_types import NodeType\\n from dbt.config.selectors import SelectorDict\\n from dbt.contracts.project import (\\n     Project as ProjectContract,\\n     SemverString,\\n+    ProjectFlags,\\n )\\n from dbt.contracts.project import PackageConfig, ProjectPackageMetadata\\n from dbt.dataclass_schema import ValidationError\\n@@ -81,8 +83,8 @@\\n \"\"\"\\n \\n MISSING_DBT_PROJECT_ERROR = \"\"\"\\\\\\n-No dbt_project.yml found at expected path {path}\\n-Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml\\n+No {DBT_PROJECT_FILE_NAME} found at expected path {path}\\n+Verify that each entry within packages.yml (and their transitive dependencies) contains a file named {DBT_PROJECT_FILE_NAME}\\n \"\"\"\\n \\n \\n@@ -199,16 +201,20 @@ def value_or(value: Optional[T], default: T) -> T:\\n def load_raw_project(project_root: str) -> Dict[str, Any]:\\n \\n     project_root = os.path.normpath(project_root)\\n-    project_yaml_filepath = os.path.join(project_root, \"dbt_project.yml\")\\n+    project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n \\n     # get the project.yml contents\\n     if not path_exists(project_yaml_filepath):\\n-        raise DbtProjectError(MISSING_DBT_PROJECT_ERROR.format(path=project_yaml_filepath))\\n+        raise DbtProjectError(\\n+            MISSING_DBT_PROJECT_ERROR.format(\\n+                path=project_yaml_filepath, DBT_PROJECT_FILE_NAME=DBT_PROJECT_FILE_NAME\\n+            )\\n+        )\\n \\n     project_dict = _load_yaml(project_yaml_filepath)\\n \\n     if not isinstance(project_dict, dict):\\n-        raise DbtProjectError(\"dbt_project.yml does not parse to a dictionary\")\\n+        raise DbtProjectError(f\"{DBT_PROJECT_FILE_NAME} does not parse to a dictionary\")\\n \\n     return project_dict\\n \\n@@ -323,21 +329,21 @@ def get_rendered(\\n             selectors_dict=rendered_selectors,\\n         )\\n \\n-    # Called by Project.from_project_root (not PartialProject.from_project_root!)\\n+    # Called by Project.from_project_root which first calls PartialProject.from_project_root\\n     def render(self, renderer: DbtProjectYamlRenderer) -> \"Project\":\\n         try:\\n             rendered = self.get_rendered(renderer)\\n             return self.create_project(rendered)\\n         except DbtProjectError as exc:\\n             if exc.path is None:\\n-                exc.path = os.path.join(self.project_root, \"dbt_project.yml\")\\n+                exc.path = os.path.join(self.project_root, DBT_PROJECT_FILE_NAME)\\n             raise\\n \\n     def render_package_metadata(self, renderer: PackageRenderer) -> ProjectPackageMetadata:\\n         packages_data = renderer.render_data(self.packages_dict)\\n         packages_config = package_config_from_data(packages_data, self.packages_dict)\\n         if not self.project_name:\\n-            raise DbtProjectError(\"Package dbt_project.yml must have a name!\")\\n+            raise DbtProjectError(f\"Package defined in {DBT_PROJECT_FILE_NAME} must have a name!\")\\n         return ProjectPackageMetadata(self.project_name, packages_config.packages)\\n \\n     def check_config_path(\\n@@ -348,7 +354,7 @@ def check_config_path(\\n                 msg = (\\n                     \"{deprecated_path} and {expected_path} cannot both be defined. The \"\\n                     \"`{deprecated_path}` config has been deprecated in favor of `{expected_path}`. \"\\n-                    \"Please update your `dbt_project.yml` configuration to reflect this \"\\n+                    f\"Please update your `{DBT_PROJECT_FILE_NAME}` configuration to reflect this \"\\n                     \"change.\"\\n                 )\\n                 raise DbtProjectError(\\n@@ -420,11 +426,11 @@ def create_project(self, rendered: RenderComponents) -> \"Project\":\\n \\n         docs_paths: List[str] = value_or(cfg.docs_paths, all_source_paths)\\n         asset_paths: List[str] = value_or(cfg.asset_paths, [])\\n-        flags = get_flags()\\n+        global_flags = get_flags()\\n \\n-        flag_target_path = str(flags.TARGET_PATH) if flags.TARGET_PATH else None\\n+        flag_target_path = str(global_flags.TARGET_PATH) if global_flags.TARGET_PATH else None\\n         target_path: str = flag_or(flag_target_path, cfg.target_path, \"target\")\\n-        log_path: str = str(flags.LOG_PATH)\\n+        log_path: str = str(global_flags.LOG_PATH)\\n \\n         clean_targets: List[str] = value_or(cfg.clean_targets, [target_path])\\n         packages_install_path: str = value_or(cfg.packages_install_path, \"dbt_packages\")\\n@@ -569,6 +575,11 @@ def from_project_root(\\n         ) = package_and_project_data_from_root(project_root)\\n         selectors_dict = selector_data_from_root(project_root)\\n \\n+        if \"flags\" in project_dict:\\n+            # We don\\'t want to include \"flags\" in the Project,\\n+            # it goes in ProjectFlags\\n+            project_dict.pop(\"flags\")\\n+\\n         return cls.from_dicts(\\n             project_root=project_root,\\n             project_dict=project_dict,\\n@@ -709,7 +720,6 @@ def to_project_config(self, with_packages=False):\\n                 \"exposures\": self.exposures,\\n                 \"vars\": self.vars.to_dict(),\\n                 \"require-dbt-version\": [v.to_version_string() for v in self.dbt_version],\\n-                \"config-version\": self.config_version,\\n                 \"restrict-access\": self.restrict_access,\\n                 \"dbt-cloud\": self.dbt_cloud,\\n             }\\n@@ -773,3 +783,52 @@ def get_macro_search_order(self, macro_namespace: str):\\n     def project_target_path(self):\\n         # If target_path is absolute, project_root will not be included\\n         return os.path.join(self.project_root, self.target_path)\\n+\\n+\\n+def read_project_flags(project_dir: str, profiles_dir: str) -> ProjectFlags:\\n+    try:\\n+        project_flags: Dict[str, Any] = {}\\n+        # Read project_flags from dbt_project.yml first\\n+        # Flags are instantiated before the project, so we don\\'t\\n+        # want to throw an error for non-existence of dbt_project.yml here\\n+        # because it breaks things.\\n+        project_root = os.path.normpath(project_dir)\\n+        project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n+        if path_exists(project_yaml_filepath):\\n+            try:\\n+                project_dict = load_raw_project(project_root)\\n+                if \"flags\" in project_dict:\\n+                    project_flags = project_dict.pop(\"flags\")\\n+            except Exception:\\n+                # This is probably a yaml load error.The error will be reported\\n+                # later, when the project loads.\\n+                pass\\n+\\n+        from dbt.config.profile import read_profile\\n+\\n+        profile = read_profile(profiles_dir)\\n+        profile_project_flags: Optional[Dict[str, Any]] = {}\\n+        if profile:\\n+            profile_project_flags = coerce_dict_str(profile.get(\"config\", {}))\\n+\\n+        if project_flags and profile_project_flags:\\n+            raise DbtProjectError(\\n+                f\"Do not specify both \\'config\\' in profiles.yml and \\'flags\\' in {DBT_PROJECT_FILE_NAME}. \"\\n+                \"Using \\'config\\' in profiles.yml is deprecated.\"\\n+            )\\n+\\n+        if profile_project_flags:\\n+            # This can\\'t use WARN_ERROR or WARN_ERROR_OPTIONS because they\\'re in\\n+            # the config that we\\'re loading. Uses special \"warn\" method.\\n+            deprecations.warn(\"project-flags-moved\")\\n+            project_flags = profile_project_flags\\n+\\n+        if project_flags is not None:\\n+            ProjectFlags.validate(project_flags)\\n+            return ProjectFlags.from_dict(project_flags)\\n+    except (DbtProjectError) as exc:\\n+        # We don\\'t want to eat the DbtProjectError for UserConfig to ProjectFlags\\n+        raise exc\\n+    except (DbtRuntimeError, ValidationError):\\n+        pass\\n+    return ProjectFlags()', '@@ -796,6 +796,29 @@ def message(self) -> str:\\n         return line_wrap_message(warning_tag(msg))\\n \\n \\n+class ProjectFlagsMovedDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D013\"\\n+\\n+    def message(self) -> str:\\n+        description = (\\n+            \"User config should be moved from the \\'config\\' key in profiles.yml to the \\'flags\\' \"\\n+            \"key in dbt_project.yml.\"\\n+        )\\n+        # Can\\'t use line_wrap_message here because flags.printer_width isn\\'t available yet\\n+        return warning_tag(f\"Deprecated functionality\\\\n\\\\n{description}\")\\n+\\n+\\n+class PackageMaterializationOverrideDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D016\"\\n+\\n+    def message(self) -> str:\\n+        description = f\"Installed package \\'{self.package_name}\\' is overriding the built-in materialization \\'{self.materialization_name}\\'. Overrides of built-in materializations from installed packages will be deprecated in future versions of dbt. Please refer to https://docs.getdbt.com/reference/global-configs/legacy-behaviors#require_explicit_package_overrides_for_builtin_materializations for detailed documentation and suggested workarounds.\"\\n+\\n+        return line_wrap_message(warning_tag(description))\\n+\\n+\\n # =======================================================\\n # I - Project parsing\\n # ======================================================='], 'file': ['core/dbt/flags.py', 'core/dbt/config/runtime.py', 'core/dbt/utils.py', 'core/dbt/tracking.py', 'core/dbt/cli/flags.py', 'core/dbt/contracts/connection.py', 'core/dbt/contracts/project.py', 'core/dbt/config/profile.py', 'core/dbt/contracts/graph/manifest.py', 'core/dbt/config/project.py', 'core/dbt/events/types.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('4d883184-922a-4611-91be-3f6a2ab3e6db'), UUID('50e94f8d-65eb-4f29-8a68-63b8f52a805f'), UUID('5fc20d4b-1316-43a1-8eb2-531d6676b583'), UUID('8620fd42-7ec2-4155-aed1-37a4889fffb9'), UUID('d5670d70-edc6-42b1-885d-7d5193894e52'), UUID('76970b65-79c5-434d-9afa-da57a8604e55'), UUID('c724d46a-6d81-42be-82ca-d6878313a9ed'), UUID('b6ee5e4a-fa22-48c8-9db7-255b899ced16'), UUID('5a73fd2e-d443-4674-89ea-32e1d5aa09f0'), UUID('380f5b03-eb4e-46a3-ba37-652b0824af6a'), UUID('30aca7db-b0b0-4166-b151-b690f629621a')]}\n",
      "ERROR:root:Error in {'repo': 'dbt-labs/dbt-core', 'vulnerability_id': '2024-40637', 'commit': '87ac4deb00cc9fe334706e42a365903a1d581624', 'commit_source': 'github', 'cwe_id': ['CWE-74', 'CWE-74', 'CWE-74'], 'patch': ['@@ -39,23 +39,24 @@ def get_flags():\\n     return GLOBAL_FLAGS\\n \\n \\n-def set_from_args(args: Namespace, user_config):\\n+def set_from_args(args: Namespace, project_flags):\\n     global GLOBAL_FLAGS\\n     from dbt.cli.main import cli\\n     from dbt.cli.flags import Flags, convert_config\\n \\n-    # we set attributes of args after initialize the flags, but user_config\\n+    # we set attributes of args after initialize the flags, but project_flags\\n     # is being read in the Flags constructor, so we need to read it here and pass in\\n-    # to make sure we use the correct user_config\\n-    if (hasattr(args, \"PROFILES_DIR\") or hasattr(args, \"profiles_dir\")) and not user_config:\\n-        from dbt.config.profile import read_user_config\\n+    # to make sure we use the correct project_flags\\n+    profiles_dir = getattr(args, \"PROFILES_DIR\", None) or getattr(args, \"profiles_dir\", None)\\n+    project_dir = getattr(args, \"PROJECT_DIR\", None) or getattr(args, \"project_dir\", None)\\n+    if profiles_dir and project_dir:\\n+        from dbt.config.project import read_project_flags\\n \\n-        profiles_dir = getattr(args, \"PROFILES_DIR\", None) or getattr(args, \"profiles_dir\")\\n-        user_config = read_user_config(profiles_dir)\\n+        project_flags = read_project_flags(project_dir, profiles_dir)\\n \\n     # make a dummy context to get the flags, totally arbitrary\\n     ctx = cli.make_context(\"run\", [\"run\"])\\n-    flags = Flags(ctx, user_config)\\n+    flags = Flags(ctx, project_flags)\\n     for arg_name, args_param_value in vars(args).items():\\n         args_param_value = convert_config(arg_name, args_param_value)\\n         object.__setattr__(flags, arg_name.upper(), args_param_value)', '@@ -20,7 +20,7 @@\\n from dbt.config.project import load_raw_project\\n from dbt.contracts.connection import AdapterRequiredConfig, Credentials, HasCredentials\\n from dbt.contracts.graph.manifest import ManifestMetadata\\n-from dbt.contracts.project import Configuration, UserConfig\\n+from dbt.contracts.project import Configuration\\n from dbt.contracts.relation import ComponentName\\n from dbt.dataclass_schema import ValidationError\\n from dbt.events.functions import warn_or_error\\n@@ -178,7 +178,6 @@ def from_parts(\\n             profile_env_vars=profile.profile_env_vars,\\n             profile_name=profile.profile_name,\\n             target_name=profile.target_name,\\n-            user_config=profile.user_config,\\n             threads=profile.threads,\\n             credentials=profile.credentials,\\n             args=args,\\n@@ -432,7 +431,6 @@ def _connection_keys(self):\\n class UnsetProfile(Profile):\\n     def __init__(self):\\n         self.credentials = UnsetCredentials()\\n-        self.user_config = UserConfig()  # This will be read in _get_rendered_profile\\n         self.profile_name = \"\"\\n         self.target_name = \"\"\\n         self.threads = -1', '@@ -631,7 +631,7 @@ def _connection_exception_retry(fn, max_attempts: int, attempt: int = 0):\\n def args_to_dict(args):\\n     var_args = vars(args).copy()\\n     # update the args with the flags, which could also come from environment\\n-    # variables or user_config\\n+    # variables or project_flags\\n     flag_dict = flags.get_flag_dict()\\n     var_args.update(flag_dict)\\n     dict_args = {}', '@@ -471,7 +471,6 @@ def process(self, record):\\n \\n \\n def initialize_from_flags(send_anonymous_usage_stats, profiles_dir):\\n-    # Setting these used to be in UserConfig, but had to be moved here\\n     global active_user\\n     if send_anonymous_usage_stats:\\n         active_user = User(profiles_dir)', '@@ -3,6 +3,7 @@\\n from dataclasses import dataclass\\n from importlib import import_module\\n from multiprocessing import get_context\\n+from pathlib import Path\\n from pprint import pformat as pf\\n from typing import Any, Callable, Dict, List, Optional, Set, Union\\n \\n@@ -11,8 +12,8 @@\\n from dbt.cli.exceptions import DbtUsageException\\n from dbt.cli.resolvers import default_log_path, default_project_dir\\n from dbt.cli.types import Command as CliCommand\\n-from dbt.config.profile import read_user_config\\n-from dbt.contracts.project import UserConfig\\n+from dbt.config.project import read_project_flags\\n+from dbt.contracts.project import ProjectFlags\\n from dbt.exceptions import DbtInternalError\\n from dbt.deprecations import renamed_env_var\\n from dbt.helper_types import WarnErrorOptions\\n@@ -25,7 +26,7 @@\\n     \"INDIRECT_SELECTION\": \"eager\",\\n     \"TARGET_PATH\": None,\\n     \"WARN_ERROR\": None,\\n-    # Cli args without user_config or env var option.\\n+    # Cli args without project_flags or env var option.\\n     \"FULL_REFRESH\": False,\\n     \"STRICT_MODE\": False,\\n     \"STORE_FAILURES\": False,\\n@@ -77,7 +78,7 @@ class Flags:\\n     \"\"\"Primary configuration artifact for running dbt\"\"\"\\n \\n     def __init__(\\n-        self, ctx: Optional[Context] = None, user_config: Optional[UserConfig] = None\\n+        self, ctx: Optional[Context] = None, project_flags: Optional[ProjectFlags] = None\\n     ) -> None:\\n         # Set the default flags.\\n         for key, value in FLAGS_DEFAULTS.items():\\n@@ -200,27 +201,40 @@ def _assign_params(\\n                 invoked_subcommand_ctx, params_assigned_from_default, deprecated_env_vars\\n             )\\n \\n-        if not user_config:\\n+        if not project_flags:\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             profiles_dir = getattr(self, \"PROFILES_DIR\", None)\\n-            user_config = read_user_config(profiles_dir) if profiles_dir else None\\n+            if profiles_dir and project_dir:\\n+                project_flags = read_project_flags(project_dir, profiles_dir)\\n+            else:\\n+                project_flags = None\\n \\n         # Add entire invocation command to flags\\n         object.__setattr__(self, \"INVOCATION_COMMAND\", \"dbt \" + \" \".join(sys.argv[1:]))\\n \\n-        # Overwrite default assignments with user config if available.\\n-        if user_config:\\n+        if project_flags:\\n+            # Overwrite default assignments with project flags if available.\\n             param_assigned_from_default_copy = params_assigned_from_default.copy()\\n             for param_assigned_from_default in params_assigned_from_default:\\n-                user_config_param_value = getattr(user_config, param_assigned_from_default, None)\\n-                if user_config_param_value is not None:\\n+                project_flags_param_value = getattr(\\n+                    project_flags, param_assigned_from_default, None\\n+                )\\n+                if project_flags_param_value is not None:\\n                     object.__setattr__(\\n                         self,\\n                         param_assigned_from_default.upper(),\\n-                        convert_config(param_assigned_from_default, user_config_param_value),\\n+                        convert_config(param_assigned_from_default, project_flags_param_value),\\n                     )\\n                     param_assigned_from_default_copy.remove(param_assigned_from_default)\\n             params_assigned_from_default = param_assigned_from_default_copy\\n \\n+            # Add project-level flags that are not available as CLI options / env vars\\n+            for (\\n+                project_level_flag_name,\\n+                project_level_flag_value,\\n+            ) in project_flags.project_only_flags.items():\\n+                object.__setattr__(self, project_level_flag_name.upper(), project_level_flag_value)\\n+\\n         # Set hard coded flags.\\n         object.__setattr__(self, \"WHICH\", invoked_subcommand_name or ctx.info_name)\\n         object.__setattr__(self, \"MP_CONTEXT\", get_context(\"spawn\"))\\n@@ -234,9 +248,11 @@ def _assign_params(\\n         # Starting in v1.5, if `log-path` is set in `dbt_project.yml`, it will raise a deprecation warning,\\n         # with the possibility of removing it in a future release.\\n         if getattr(self, \"LOG_PATH\", None) is None:\\n-            project_dir = getattr(self, \"PROJECT_DIR\", default_project_dir())\\n+            project_dir = getattr(self, \"PROJECT_DIR\", str(default_project_dir()))\\n             version_check = getattr(self, \"VERSION_CHECK\", True)\\n-            object.__setattr__(self, \"LOG_PATH\", default_log_path(project_dir, version_check))\\n+            object.__setattr__(\\n+                self, \"LOG_PATH\", default_log_path(Path(project_dir), version_check)\\n+            )\\n \\n         # Support console DO NOT TRACK initiative.\\n         if os.getenv(\"DO_NOT_TRACK\", \"\").lower() in (\"1\", \"t\", \"true\", \"y\", \"yes\"):', '@@ -178,17 +178,9 @@ def __post_serialize__(self, dct):\\n         return dct\\n \\n \\n-class UserConfigContract(Protocol):\\n-    send_anonymous_usage_stats: bool\\n-    use_colors: Optional[bool] = None\\n-    partial_parse: Optional[bool] = None\\n-    printer_width: Optional[int] = None\\n-\\n-\\n class HasCredentials(Protocol):\\n     credentials: Credentials\\n     profile_name: str\\n-    user_config: UserConfigContract\\n     target_name: str\\n     threads: int\\n ', '@@ -1,5 +1,5 @@\\n from dbt.contracts.util import Replaceable, Mergeable, list_str, Identifier\\n-from dbt.contracts.connection import QueryComment, UserConfigContract\\n+from dbt.contracts.connection import QueryComment\\n from dbt.helper_types import NoValue\\n from dbt.dataclass_schema import (\\n     dbtClassMixin,\\n@@ -283,7 +283,7 @@ def validate(cls, data):\\n \\n \\n @dataclass\\n-class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n+class ProjectFlags(ExtensibleDbtClassMixin, Replaceable):\\n     cache_selected_only: Optional[bool] = None\\n     debug: Optional[bool] = None\\n     fail_fast: Optional[bool] = None\\n@@ -295,6 +295,7 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     partial_parse: Optional[bool] = None\\n     populate_cache: Optional[bool] = None\\n     printer_width: Optional[int] = None\\n+    require_explicit_package_overrides_for_builtin_materializations: bool = False\\n     send_anonymous_usage_stats: bool = DEFAULT_SEND_ANONYMOUS_USAGE_STATS\\n     static_parser: Optional[bool] = None\\n     use_colors: Optional[bool] = None\\n@@ -305,12 +306,17 @@ class UserConfig(ExtensibleDbtClassMixin, Replaceable, UserConfigContract):\\n     warn_error_options: Optional[Dict[str, Union[str, List[str]]]] = None\\n     write_json: Optional[bool] = None\\n \\n+    @property\\n+    def project_only_flags(self) -> Dict[str, Any]:\\n+        return {\\n+            \"require_explicit_package_overrides_for_builtin_materializations\": self.require_explicit_package_overrides_for_builtin_materializations,\\n+        }\\n+\\n \\n @dataclass\\n class ProfileConfig(dbtClassMixin, Replaceable):\\n     profile_name: str\\n     target_name: str\\n-    user_config: UserConfig\\n     threads: int\\n     # TODO: make this a dynamic union of some kind?\\n     credentials: Optional[Dict[str, Any]]', '@@ -8,7 +8,7 @@\\n from dbt.clients.system import load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n from dbt.contracts.connection import Credentials, HasCredentials\\n-from dbt.contracts.project import ProfileConfig, UserConfig\\n+from dbt.contracts.project import ProfileConfig\\n from dbt.exceptions import (\\n     CompilationError,\\n     DbtProfileError,\\n@@ -19,7 +19,6 @@\\n )\\n from dbt.events.types import MissingProfileTarget\\n from dbt.events.functions import fire_event\\n-from dbt.utils import coerce_dict_str\\n \\n from .renderer import ProfileRenderer\\n \\n@@ -51,27 +50,13 @@ def read_profile(profiles_dir: str) -> Dict[str, Any]:\\n     return {}\\n \\n \\n-def read_user_config(directory: str) -> UserConfig:\\n-    try:\\n-        profile = read_profile(directory)\\n-        if profile:\\n-            user_config = coerce_dict_str(profile.get(\"config\", {}))\\n-            if user_config is not None:\\n-                UserConfig.validate(user_config)\\n-                return UserConfig.from_dict(user_config)\\n-    except (DbtRuntimeError, ValidationError):\\n-        pass\\n-    return UserConfig()\\n-\\n-\\n # The Profile class is included in RuntimeConfig, so any attribute\\n # additions must also be set where the RuntimeConfig class is created\\n # `init=False` is a workaround for https://bugs.python.org/issue45081\\n @dataclass(init=False)\\n class Profile(HasCredentials):\\n     profile_name: str\\n     target_name: str\\n-    user_config: UserConfig\\n     threads: int\\n     credentials: Credentials\\n     profile_env_vars: Dict[str, Any]\\n@@ -80,7 +65,6 @@ def __init__(\\n         self,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: UserConfig,\\n         threads: int,\\n         credentials: Credentials,\\n     ) -> None:\\n@@ -89,7 +73,6 @@ def __init__(\\n         \"\"\"\\n         self.profile_name = profile_name\\n         self.target_name = target_name\\n-        self.user_config = user_config\\n         self.threads = threads\\n         self.credentials = credentials\\n         self.profile_env_vars = {}  # never available on init\\n@@ -106,12 +89,10 @@ def to_profile_info(self, serialize_credentials: bool = False) -> Dict[str, Any]\\n         result = {\\n             \"profile_name\": self.profile_name,\\n             \"target_name\": self.target_name,\\n-            \"user_config\": self.user_config,\\n             \"threads\": self.threads,\\n             \"credentials\": self.credentials,\\n         }\\n         if serialize_credentials:\\n-            result[\"user_config\"] = self.user_config.to_dict(omit_none=True)\\n             result[\"credentials\"] = self.credentials.to_dict(omit_none=True)\\n         return result\\n \\n@@ -124,7 +105,6 @@ def to_target_dict(self) -> Dict[str, Any]:\\n                 \"name\": self.target_name,\\n                 \"target_name\": self.target_name,\\n                 \"profile_name\": self.profile_name,\\n-                \"config\": self.user_config.to_dict(omit_none=True),\\n             }\\n         )\\n         return target\\n@@ -246,7 +226,6 @@ def from_credentials(\\n         threads: int,\\n         profile_name: str,\\n         target_name: str,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n     ) -> \"Profile\":\\n         \"\"\"Create a profile from an existing set of Credentials and the\\n         remaining information.\\n@@ -255,20 +234,13 @@ def from_credentials(\\n         :param threads: The number of threads to use for connections.\\n         :param profile_name: The profile name used for this profile.\\n         :param target_name: The target name used for this profile.\\n-        :param user_config: The user-level config block from the\\n-            raw profiles, if specified.\\n         :raises DbtProfileError: If the profile is invalid.\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        if user_config is None:\\n-            user_config = {}\\n-        UserConfig.validate(user_config)\\n-        user_config_obj: UserConfig = UserConfig.from_dict(user_config)\\n \\n         profile = cls(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n-            user_config=user_config_obj,\\n             threads=threads,\\n             credentials=credentials,\\n         )\\n@@ -316,7 +288,6 @@ def from_raw_profile_info(\\n         raw_profile: Dict[str, Any],\\n         profile_name: str,\\n         renderer: ProfileRenderer,\\n-        user_config: Optional[Dict[str, Any]] = None,\\n         target_override: Optional[str] = None,\\n         threads_override: Optional[int] = None,\\n     ) -> \"Profile\":\\n@@ -328,8 +299,6 @@ def from_raw_profile_info(\\n             disk as yaml and its values rendered with jinja.\\n         :param profile_name: The profile name used.\\n         :param renderer: The config renderer.\\n-        :param user_config: The global config for the user, if it\\n-            was present.\\n         :param target_override: The target to use, if provided on\\n             the command line.\\n         :param threads_override: The thread count to use, if\\n@@ -338,9 +307,6 @@ def from_raw_profile_info(\\n             target could not be found\\n         :returns: The new Profile object.\\n         \"\"\"\\n-        # user_config is not rendered.\\n-        if user_config is None:\\n-            user_config = raw_profile.get(\"config\")\\n         # TODO: should it be, and the values coerced to bool?\\n         target_name, profile_data = cls.render_profile(\\n             raw_profile, profile_name, target_override, renderer\\n@@ -361,7 +327,6 @@ def from_raw_profile_info(\\n             profile_name=profile_name,\\n             target_name=target_name,\\n             threads=threads,\\n-            user_config=user_config,\\n         )\\n \\n     @classmethod\\n@@ -396,13 +361,11 @@ def from_raw_profiles(\\n         if not raw_profile:\\n             msg = f\"Profile {profile_name} in profiles.yml is empty\"\\n             raise DbtProfileError(INVALID_PROFILE_MESSAGE.format(error_string=msg))\\n-        user_config = raw_profiles.get(\"config\")\\n \\n         return cls.from_raw_profile_info(\\n             raw_profile=raw_profile,\\n             profile_name=profile_name,\\n             renderer=renderer,\\n-            user_config=user_config,\\n             target_override=target_override,\\n             threads_override=threads_override,\\n         )', '@@ -25,6 +25,7 @@\\n from typing_extensions import Protocol\\n from uuid import UUID\\n \\n+\\n from dbt.contracts.graph.nodes import (\\n     BaseNode,\\n     Documentation,\\n@@ -67,7 +68,7 @@\\n from dbt.events.contextvars import get_node_info\\n from dbt.node_types import NodeType, AccessType\\n from dbt.flags import get_flags, MP_CONTEXT\\n-from dbt import tracking\\n+from dbt import tracking, deprecations\\n import dbt.utils\\n \\n \\n@@ -616,11 +617,29 @@ def __lt__(self, other: object) -> bool:\\n \\n \\n class CandidateList(List[M]):\\n-    def last(self) -> Optional[Macro]:\\n+    def last_candidate(\\n+        self, valid_localities: Optional[List[Locality]] = None\\n+    ) -> Optional[MacroCandidate]:\\n+        \"\"\"\\n+        Obtain the last (highest precedence) MacroCandidate from the CandidateList of any locality in valid_localities.\\n+        If valid_localities is not specified, return the last MacroCandidate of any locality.\\n+        \"\"\"\\n         if not self:\\n             return None\\n         self.sort()\\n-        return self[-1].macro\\n+\\n+        if valid_localities is None:\\n+            return self[-1]\\n+\\n+        for candidate in reversed(self):\\n+            if candidate.locality in valid_localities:\\n+                return candidate\\n+\\n+        return None\\n+\\n+    def last(self) -> Optional[Macro]:\\n+        last_candidate = self.last_candidate()\\n+        return last_candidate.macro if last_candidate is not None else None\\n \\n \\n def _get_locality(macro: Macro, root_project_name: str, internal_packages: Set[str]) -> Locality:\\n@@ -914,7 +933,33 @@ def find_materialization_macro_by_name(\\n                 for specificity, atype in enumerate(self._get_parent_adapter_types(adapter_type))\\n             )\\n         )\\n-        return candidates.last()\\n+        core_candidates = [\\n+            candidate for candidate in candidates if candidate.locality == Locality.Core\\n+        ]\\n+\\n+        materialization_candidate = candidates.last_candidate()\\n+        # If an imported materialization macro was found that also had a core candidate, fire a deprecation\\n+        if (\\n+            materialization_candidate is not None\\n+            and materialization_candidate.locality == Locality.Imported\\n+            and core_candidates\\n+        ):\\n+            # preserve legacy behaviour - allow materialization override\\n+            if (\\n+                get_flags().require_explicit_package_overrides_for_builtin_materializations\\n+                is False\\n+            ):\\n+                deprecations.warn(\\n+                    \"package-materialization-override\",\\n+                    package_name=materialization_candidate.macro.package_name,\\n+                    materialization_name=materialization_name,\\n+                )\\n+            else:\\n+                materialization_candidate = candidates.last_candidate(\\n+                    valid_localities=[Locality.Core, Locality.Root]\\n+                )\\n+\\n+        return materialization_candidate.macro if materialization_candidate else None\\n \\n     def get_resource_fqns(self) -> Mapping[str, PathSet]:\\n         resource_fqns: Dict[str, Set[Tuple[str, ...]]] = {}', '@@ -20,6 +20,7 @@\\n     DEPENDENCIES_FILE_NAME,\\n     PACKAGES_FILE_NAME,\\n     PACKAGE_LOCK_HASH_KEY,\\n+    DBT_PROJECT_FILE_NAME,\\n )\\n from dbt.clients.system import path_exists, load_file_contents\\n from dbt.clients.yaml_helper import load_yaml_text\\n@@ -35,12 +36,13 @@\\n from dbt.helper_types import NoValue\\n from dbt.semver import VersionSpecifier, versions_compatible\\n from dbt.version import get_installed_version\\n-from dbt.utils import MultiDict, md5\\n+from dbt.utils import MultiDict, md5, coerce_dict_str\\n from dbt.node_types import NodeType\\n from dbt.config.selectors import SelectorDict\\n from dbt.contracts.project import (\\n     Project as ProjectContract,\\n     SemverString,\\n+    ProjectFlags,\\n )\\n from dbt.contracts.project import PackageConfig, ProjectPackageMetadata\\n from dbt.dataclass_schema import ValidationError\\n@@ -81,8 +83,8 @@\\n \"\"\"\\n \\n MISSING_DBT_PROJECT_ERROR = \"\"\"\\\\\\n-No dbt_project.yml found at expected path {path}\\n-Verify that each entry within packages.yml (and their transitive dependencies) contains a file named dbt_project.yml\\n+No {DBT_PROJECT_FILE_NAME} found at expected path {path}\\n+Verify that each entry within packages.yml (and their transitive dependencies) contains a file named {DBT_PROJECT_FILE_NAME}\\n \"\"\"\\n \\n \\n@@ -199,16 +201,20 @@ def value_or(value: Optional[T], default: T) -> T:\\n def load_raw_project(project_root: str) -> Dict[str, Any]:\\n \\n     project_root = os.path.normpath(project_root)\\n-    project_yaml_filepath = os.path.join(project_root, \"dbt_project.yml\")\\n+    project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n \\n     # get the project.yml contents\\n     if not path_exists(project_yaml_filepath):\\n-        raise DbtProjectError(MISSING_DBT_PROJECT_ERROR.format(path=project_yaml_filepath))\\n+        raise DbtProjectError(\\n+            MISSING_DBT_PROJECT_ERROR.format(\\n+                path=project_yaml_filepath, DBT_PROJECT_FILE_NAME=DBT_PROJECT_FILE_NAME\\n+            )\\n+        )\\n \\n     project_dict = _load_yaml(project_yaml_filepath)\\n \\n     if not isinstance(project_dict, dict):\\n-        raise DbtProjectError(\"dbt_project.yml does not parse to a dictionary\")\\n+        raise DbtProjectError(f\"{DBT_PROJECT_FILE_NAME} does not parse to a dictionary\")\\n \\n     return project_dict\\n \\n@@ -323,21 +329,21 @@ def get_rendered(\\n             selectors_dict=rendered_selectors,\\n         )\\n \\n-    # Called by Project.from_project_root (not PartialProject.from_project_root!)\\n+    # Called by Project.from_project_root which first calls PartialProject.from_project_root\\n     def render(self, renderer: DbtProjectYamlRenderer) -> \"Project\":\\n         try:\\n             rendered = self.get_rendered(renderer)\\n             return self.create_project(rendered)\\n         except DbtProjectError as exc:\\n             if exc.path is None:\\n-                exc.path = os.path.join(self.project_root, \"dbt_project.yml\")\\n+                exc.path = os.path.join(self.project_root, DBT_PROJECT_FILE_NAME)\\n             raise\\n \\n     def render_package_metadata(self, renderer: PackageRenderer) -> ProjectPackageMetadata:\\n         packages_data = renderer.render_data(self.packages_dict)\\n         packages_config = package_config_from_data(packages_data, self.packages_dict)\\n         if not self.project_name:\\n-            raise DbtProjectError(\"Package dbt_project.yml must have a name!\")\\n+            raise DbtProjectError(f\"Package defined in {DBT_PROJECT_FILE_NAME} must have a name!\")\\n         return ProjectPackageMetadata(self.project_name, packages_config.packages)\\n \\n     def check_config_path(\\n@@ -348,7 +354,7 @@ def check_config_path(\\n                 msg = (\\n                     \"{deprecated_path} and {expected_path} cannot both be defined. The \"\\n                     \"`{deprecated_path}` config has been deprecated in favor of `{expected_path}`. \"\\n-                    \"Please update your `dbt_project.yml` configuration to reflect this \"\\n+                    f\"Please update your `{DBT_PROJECT_FILE_NAME}` configuration to reflect this \"\\n                     \"change.\"\\n                 )\\n                 raise DbtProjectError(\\n@@ -420,11 +426,11 @@ def create_project(self, rendered: RenderComponents) -> \"Project\":\\n \\n         docs_paths: List[str] = value_or(cfg.docs_paths, all_source_paths)\\n         asset_paths: List[str] = value_or(cfg.asset_paths, [])\\n-        flags = get_flags()\\n+        global_flags = get_flags()\\n \\n-        flag_target_path = str(flags.TARGET_PATH) if flags.TARGET_PATH else None\\n+        flag_target_path = str(global_flags.TARGET_PATH) if global_flags.TARGET_PATH else None\\n         target_path: str = flag_or(flag_target_path, cfg.target_path, \"target\")\\n-        log_path: str = str(flags.LOG_PATH)\\n+        log_path: str = str(global_flags.LOG_PATH)\\n \\n         clean_targets: List[str] = value_or(cfg.clean_targets, [target_path])\\n         packages_install_path: str = value_or(cfg.packages_install_path, \"dbt_packages\")\\n@@ -569,6 +575,11 @@ def from_project_root(\\n         ) = package_and_project_data_from_root(project_root)\\n         selectors_dict = selector_data_from_root(project_root)\\n \\n+        if \"flags\" in project_dict:\\n+            # We don\\'t want to include \"flags\" in the Project,\\n+            # it goes in ProjectFlags\\n+            project_dict.pop(\"flags\")\\n+\\n         return cls.from_dicts(\\n             project_root=project_root,\\n             project_dict=project_dict,\\n@@ -709,7 +720,6 @@ def to_project_config(self, with_packages=False):\\n                 \"exposures\": self.exposures,\\n                 \"vars\": self.vars.to_dict(),\\n                 \"require-dbt-version\": [v.to_version_string() for v in self.dbt_version],\\n-                \"config-version\": self.config_version,\\n                 \"restrict-access\": self.restrict_access,\\n                 \"dbt-cloud\": self.dbt_cloud,\\n             }\\n@@ -773,3 +783,52 @@ def get_macro_search_order(self, macro_namespace: str):\\n     def project_target_path(self):\\n         # If target_path is absolute, project_root will not be included\\n         return os.path.join(self.project_root, self.target_path)\\n+\\n+\\n+def read_project_flags(project_dir: str, profiles_dir: str) -> ProjectFlags:\\n+    try:\\n+        project_flags: Dict[str, Any] = {}\\n+        # Read project_flags from dbt_project.yml first\\n+        # Flags are instantiated before the project, so we don\\'t\\n+        # want to throw an error for non-existence of dbt_project.yml here\\n+        # because it breaks things.\\n+        project_root = os.path.normpath(project_dir)\\n+        project_yaml_filepath = os.path.join(project_root, DBT_PROJECT_FILE_NAME)\\n+        if path_exists(project_yaml_filepath):\\n+            try:\\n+                project_dict = load_raw_project(project_root)\\n+                if \"flags\" in project_dict:\\n+                    project_flags = project_dict.pop(\"flags\")\\n+            except Exception:\\n+                # This is probably a yaml load error.The error will be reported\\n+                # later, when the project loads.\\n+                pass\\n+\\n+        from dbt.config.profile import read_profile\\n+\\n+        profile = read_profile(profiles_dir)\\n+        profile_project_flags: Optional[Dict[str, Any]] = {}\\n+        if profile:\\n+            profile_project_flags = coerce_dict_str(profile.get(\"config\", {}))\\n+\\n+        if project_flags and profile_project_flags:\\n+            raise DbtProjectError(\\n+                f\"Do not specify both \\'config\\' in profiles.yml and \\'flags\\' in {DBT_PROJECT_FILE_NAME}. \"\\n+                \"Using \\'config\\' in profiles.yml is deprecated.\"\\n+            )\\n+\\n+        if profile_project_flags:\\n+            # This can\\'t use WARN_ERROR or WARN_ERROR_OPTIONS because they\\'re in\\n+            # the config that we\\'re loading. Uses special \"warn\" method.\\n+            deprecations.warn(\"project-flags-moved\")\\n+            project_flags = profile_project_flags\\n+\\n+        if project_flags is not None:\\n+            ProjectFlags.validate(project_flags)\\n+            return ProjectFlags.from_dict(project_flags)\\n+    except (DbtProjectError) as exc:\\n+        # We don\\'t want to eat the DbtProjectError for UserConfig to ProjectFlags\\n+        raise exc\\n+    except (DbtRuntimeError, ValidationError):\\n+        pass\\n+    return ProjectFlags()', '@@ -796,6 +796,29 @@ def message(self) -> str:\\n         return line_wrap_message(warning_tag(msg))\\n \\n \\n+class ProjectFlagsMovedDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D013\"\\n+\\n+    def message(self) -> str:\\n+        description = (\\n+            \"User config should be moved from the \\'config\\' key in profiles.yml to the \\'flags\\' \"\\n+            \"key in dbt_project.yml.\"\\n+        )\\n+        # Can\\'t use line_wrap_message here because flags.printer_width isn\\'t available yet\\n+        return warning_tag(f\"Deprecated functionality\\\\n\\\\n{description}\")\\n+\\n+\\n+class PackageMaterializationOverrideDeprecation(WarnLevel):\\n+    def code(self) -> str:\\n+        return \"D016\"\\n+\\n+    def message(self) -> str:\\n+        description = f\"Installed package \\'{self.package_name}\\' is overriding the built-in materialization \\'{self.materialization_name}\\'. Overrides of built-in materializations from installed packages will be deprecated in future versions of dbt. Please refer to https://docs.getdbt.com/reference/global-configs/legacy-behaviors#require_explicit_package_overrides_for_builtin_materializations for detailed documentation and suggested workarounds.\"\\n+\\n+        return line_wrap_message(warning_tag(description))\\n+\\n+\\n # =======================================================\\n # I - Project parsing\\n # ======================================================='], 'file': ['core/dbt/flags.py', 'core/dbt/config/runtime.py', 'core/dbt/utils.py', 'core/dbt/tracking.py', 'core/dbt/cli/flags.py', 'core/dbt/contracts/connection.py', 'core/dbt/contracts/project.py', 'core/dbt/config/profile.py', 'core/dbt/contracts/graph/manifest.py', 'core/dbt/config/project.py', 'core/dbt/events/types.py'], 'language': ['Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('4d883184-922a-4611-91be-3f6a2ab3e6db'), UUID('50e94f8d-65eb-4f29-8a68-63b8f52a805f'), UUID('5fc20d4b-1316-43a1-8eb2-531d6676b583'), UUID('8620fd42-7ec2-4155-aed1-37a4889fffb9'), UUID('d5670d70-edc6-42b1-885d-7d5193894e52'), UUID('76970b65-79c5-434d-9afa-da57a8604e55'), UUID('c724d46a-6d81-42be-82ca-d6878313a9ed'), UUID('b6ee5e4a-fa22-48c8-9db7-255b899ced16'), UUID('5a73fd2e-d443-4674-89ea-32e1d5aa09f0'), UUID('380f5b03-eb4e-46a3-ba37-652b0824af6a'), UUID('30aca7db-b0b0-4166-b151-b690f629621a')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: class ProfileConfig(dbtClassMixin, Replaceable):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:0: class ProfileConfig(dbtClassMixin, Replaceable):\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1675/1800 [20:14<00:24,  5.04it/s]ERROR:src.process_code_changes:Error processing commit 796c3ae318eea183fc88c87ec5a27355b0f6a99d\n",
      "ERROR:src.process_code_changes:{'repo': 'archivy/archivy', 'vulnerability_id': '2021-4162', 'commit': '796c3ae318eea183fc88c87ec5a27355b0f6a99d', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -137,10 +137,8 @@ def _get_download_link(field_info):\\n \\r\\n class RequestToCommandArgs:\\r\\n     def __init__(self):\\r\\n-        field_infos = [\\r\\n-            FieldInfo.factory(key)\\r\\n-            for key in list(request.form.keys()) + list(request.files.keys())\\r\\n-        ]\\r\\n+        keys = [key for key in list(request.form.keys()) + list(request.files.keys())]\\r\\n+        field_infos = [FieldInfo.factory(key) for key in keys if key != \"csrf_token\"]\\r\\n         # important to sort them so they will be in expected order on command line\\r\\n         self.field_infos = list(sorted(field_infos))\\r\\n \\r', '@@ -29,6 +29,7 @@ <h3 class=\"command-title\">{{ command.name|title }}</h3>\\n             </div>\\r\\n         {% endfor %}\\r\\n     {% endfor %}\\r\\n+\\t<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\"/>\\r\\n     <button type=\"submit\" id=\"submit_btn\" class=\"btn btn-primary m-2\">Run</button>\\r\\n </form>\\r\\n \\r', '@@ -71,7 +71,7 @@ <h2 id=\"post-title\">\\n             <svg class=\"octicon\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"><path fill-rule=\"evenodd\" d=\"M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z\"></path></svg>\\n             <span>Edit</span>\\n           </button>\\n-          <form action=\"/dataobj/delete/{{ dataobj[\\'id\\'] }}\" method=\"delete\" onsubmit=\"return confirm(\\'Delete this item permanently?\\')\" novalidate>\\n+          <form action=\"/dataobj/delete/{{ dataobj[\\'id\\'] }}\" method=\"POST\" onsubmit=\"return confirm(\\'Delete this item permanently?\\')\" novalidate>\\n             {{ form.hidden_tag() }}\\n             <button class=\"btn btn-delete\">\\n               <svg class=\"octicon\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"><path fill-rule=\"evenodd\" d=\"M6.5 1.75a.25.25 0 01.25-.25h2.5a.25.25 0 01.25.25V3h-3V1.75zm4.5 0V3h2.25a.75.75 0 010 1.5H2.75a.75.75 0 010-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75zM4.496 6.675a.75.75 0 10-1.492.15l.66 6.6A1.75 1.75 0 005.405 15h5.19c.9 0 1.652-.681 1.741-1.576l.66-6.6a.75.75 0 00-1.492-.149l-.66 6.6a.25.25 0 01-.249.225h-5.19a.25.25 0 01-.249-.225l-.66-6.6z\"></path></svg>', '@@ -232,7 +232,7 @@ def move_item(dataobj_id):\\n         return redirect(f\"/dataobj/{dataobj_id}\")\\n \\n \\n-@app.route(\"/dataobj/delete/<int:dataobj_id>\", methods=[\"DELETE\", \"GET\"])\\n+@app.route(\"/dataobj/delete/<int:dataobj_id>\", methods=[\"POST\"])\\n def delete_data(dataobj_id):\\n     try:\\n         data.delete_item(dataobj_id)'], 'file': ['archivy/click_web/resources/cmd_exec.py', 'archivy/templates/click_web/command_form.html', 'archivy/templates/dataobjs/show.html', 'archivy/routes.py'], 'language': ['Python', 'HTML', 'HTML', 'Python'], 'temp_id': [UUID('877b8187-ee97-4295-a4be-7bc857401b4d'), UUID('d96b53bc-c168-4b02-af27-88b56d42b5d7'), UUID('27fad76f-04ab-4f2c-9671-81756f3218ee'), UUID('41904a30-5b7c-44b1-93e0-ea58a497e46e')]}\n",
      "ERROR:root:Error in {'repo': 'archivy/archivy', 'vulnerability_id': '2021-4162', 'commit': '796c3ae318eea183fc88c87ec5a27355b0f6a99d', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -137,10 +137,8 @@ def _get_download_link(field_info):\\n \\r\\n class RequestToCommandArgs:\\r\\n     def __init__(self):\\r\\n-        field_infos = [\\r\\n-            FieldInfo.factory(key)\\r\\n-            for key in list(request.form.keys()) + list(request.files.keys())\\r\\n-        ]\\r\\n+        keys = [key for key in list(request.form.keys()) + list(request.files.keys())]\\r\\n+        field_infos = [FieldInfo.factory(key) for key in keys if key != \"csrf_token\"]\\r\\n         # important to sort them so they will be in expected order on command line\\r\\n         self.field_infos = list(sorted(field_infos))\\r\\n \\r', '@@ -29,6 +29,7 @@ <h3 class=\"command-title\">{{ command.name|title }}</h3>\\n             </div>\\r\\n         {% endfor %}\\r\\n     {% endfor %}\\r\\n+\\t<input type=\"hidden\" name=\"csrf_token\" value=\"{{ csrf_token() }}\"/>\\r\\n     <button type=\"submit\" id=\"submit_btn\" class=\"btn btn-primary m-2\">Run</button>\\r\\n </form>\\r\\n \\r', '@@ -71,7 +71,7 @@ <h2 id=\"post-title\">\\n             <svg class=\"octicon\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"><path fill-rule=\"evenodd\" d=\"M11.013 1.427a1.75 1.75 0 012.474 0l1.086 1.086a1.75 1.75 0 010 2.474l-8.61 8.61c-.21.21-.47.364-.756.445l-3.251.93a.75.75 0 01-.927-.928l.929-3.25a1.75 1.75 0 01.445-.758l8.61-8.61zm1.414 1.06a.25.25 0 00-.354 0L10.811 3.75l1.439 1.44 1.263-1.263a.25.25 0 000-.354l-1.086-1.086zM11.189 6.25L9.75 4.81l-6.286 6.287a.25.25 0 00-.064.108l-.558 1.953 1.953-.558a.249.249 0 00.108-.064l6.286-6.286z\"></path></svg>\\n             <span>Edit</span>\\n           </button>\\n-          <form action=\"/dataobj/delete/{{ dataobj[\\'id\\'] }}\" method=\"delete\" onsubmit=\"return confirm(\\'Delete this item permanently?\\')\" novalidate>\\n+          <form action=\"/dataobj/delete/{{ dataobj[\\'id\\'] }}\" method=\"POST\" onsubmit=\"return confirm(\\'Delete this item permanently?\\')\" novalidate>\\n             {{ form.hidden_tag() }}\\n             <button class=\"btn btn-delete\">\\n               <svg class=\"octicon\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 16 16\" width=\"16\" height=\"16\"><path fill-rule=\"evenodd\" d=\"M6.5 1.75a.25.25 0 01.25-.25h2.5a.25.25 0 01.25.25V3h-3V1.75zm4.5 0V3h2.25a.75.75 0 010 1.5H2.75a.75.75 0 010-1.5H5V1.75C5 .784 5.784 0 6.75 0h2.5C10.216 0 11 .784 11 1.75zM4.496 6.675a.75.75 0 10-1.492.15l.66 6.6A1.75 1.75 0 005.405 15h5.19c.9 0 1.652-.681 1.741-1.576l.66-6.6a.75.75 0 00-1.492-.149l-.66 6.6a.25.25 0 01-.249.225h-5.19a.25.25 0 01-.249-.225l-.66-6.6z\"></path></svg>', '@@ -232,7 +232,7 @@ def move_item(dataobj_id):\\n         return redirect(f\"/dataobj/{dataobj_id}\")\\n \\n \\n-@app.route(\"/dataobj/delete/<int:dataobj_id>\", methods=[\"DELETE\", \"GET\"])\\n+@app.route(\"/dataobj/delete/<int:dataobj_id>\", methods=[\"POST\"])\\n def delete_data(dataobj_id):\\n     try:\\n         data.delete_item(dataobj_id)'], 'file': ['archivy/click_web/resources/cmd_exec.py', 'archivy/templates/click_web/command_form.html', 'archivy/templates/dataobjs/show.html', 'archivy/routes.py'], 'language': ['Python', 'HTML', 'HTML', 'Python'], 'temp_id': [UUID('877b8187-ee97-4295-a4be-7bc857401b4d'), UUID('d96b53bc-c168-4b02-af27-88b56d42b5d7'), UUID('27fad76f-04ab-4f2c-9671-81756f3218ee'), UUID('41904a30-5b7c-44b1-93e0-ea58a497e46e')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0: <line number missing in source>\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1693/1800 [20:14<00:13,  7.79it/s]ERROR:src.process_code_changes:Error processing commit 68ccd2c621a40eb66fdd6af2be9d5fcc9c373318\n",
      "ERROR:src.process_code_changes:{'repo': 'codingjoe/django-s3file', 'vulnerability_id': '2022-24840', 'commit': '68ccd2c621a40eb66fdd6af2be9d5fcc9c373318', 'commit_source': 'github', 'cwe_id': ['CWE-22', 'CWE-22'], 'patch': ['@@ -2,6 +2,7 @@\\n import hashlib\\n import hmac\\n import logging\\n+from pathlib import Path\\n \\n from django import http\\n from django.conf import settings', \"@@ -94,6 +94,12 @@\\n         hiddenFileInput.name = name\\n         hiddenFileInput.value = parseURL(result)\\n         form.appendChild(hiddenFileInput)\\n+        var hiddenSignatureInput = document.createElement('input')\\n+        hiddenSignatureInput.type = 'hidden'\\n+        hiddenSignatureInput.name = name + '-s3f-signature'\\n+        console.log(fileInput.dataset.s3fSignature)\\n+        hiddenSignatureInput.value = fileInput.dataset.s3fSignature\\n+        form.appendChild(hiddenSignatureInput)\\n       })\\n       fileInput.name = ''\\n       window.uploading -= 1\", '@@ -1,9 +1,13 @@\\n import logging\\n import pathlib\\n \\n-from s3file.storages import local_dev, storage\\n+from django.core import signing\\n+from django.core.exceptions import PermissionDenied, SuspiciousFileOperation\\n+from django.utils.crypto import constant_time_compare\\n \\n from . import views\\n+from .forms import S3FileInputMixin\\n+from .storages import local_dev, storage\\n \\n logger = logging.getLogger(\"s3file\")\\n \\n@@ -15,25 +19,50 @@ def __init__(self, get_response):\\n     def __call__(self, request):\\n         file_fields = request.POST.getlist(\"s3file\")\\n         for field_name in file_fields:\\n+\\n             paths = request.POST.getlist(field_name)\\n-            request.FILES.setlist(field_name, list(self.get_files_from_storage(paths)))\\n+            if paths:\\n+                try:\\n+                    signature = request.POST[f\"{field_name}-s3f-signature\"]\\n+                except KeyError:\\n+                    raise PermissionDenied(\"No signature provided.\")\\n+                try:\\n+                    request.FILES.setlist(\\n+                        field_name, list(self.get_files_from_storage(paths, signature))\\n+                    )\\n+                except SuspiciousFileOperation as e:\\n+                    raise PermissionDenied(\"Illegal file name!\") from e\\n \\n         if local_dev and request.path == \"/__s3_mock__/\":\\n             return views.S3MockView.as_view()(request)\\n \\n         return self.get_response(request)\\n \\n     @staticmethod\\n-    def get_files_from_storage(paths):\\n+    def get_files_from_storage(paths, signature):\\n         \"\"\"Return S3 file where the name does not include the path.\"\"\"\\n+        try:\\n+            location = storage.aws_location\\n+        except AttributeError:\\n+            location = storage.location\\n+        signer = signing.Signer(\\n+            salt=f\"{S3FileInputMixin.__module__}.{S3FileInputMixin.__name__}\"\\n+        )\\n         for path in paths:\\n             path = pathlib.PurePosixPath(path)\\n+            print(path)\\n+            print(signer.signature(path.parent), signature)\\n+            if not constant_time_compare(signer.signature(path.parent), signature):\\n+                raise PermissionDenied(\"Illegal signature!\")\\n             try:\\n-                location = storage.aws_location\\n-            except AttributeError:\\n-                location = storage.location\\n+                relative_path = str(path.relative_to(location))\\n+            except ValueError as e:\\n+                raise SuspiciousFileOperation(\\n+                    f\"Path is not inside the designated upload location: {path}\"\\n+                ) from e\\n+\\n             try:\\n-                f = storage.open(str(path.relative_to(location)))\\n+                f = storage.open(relative_path)\\n                 f.name = path.name\\n                 yield f\\n             except (OSError, ValueError):', '@@ -4,6 +4,7 @@\\n import uuid\\n \\n from django.conf import settings\\n+from django.core import signing\\n from django.utils.functional import cached_property\\n from storages.utils import safe_join\\n \\n@@ -16,10 +17,14 @@ class S3FileInputMixin:\\n     \"\"\"FileInput that uses JavaScript to directly upload to Amazon S3.\"\"\"\\n \\n     needs_multipart_form = False\\n-    upload_path = str(\\n-        getattr(settings, \"S3FILE_UPLOAD_PATH\", pathlib.PurePosixPath(\"tmp\", \"s3file\"))\\n+    upload_path = safe_join(\\n+        str(storage.aws_location),\\n+        str(\\n+            getattr(\\n+                settings, \"S3FILE_UPLOAD_PATH\", pathlib.PurePosixPath(\"tmp\", \"s3file\")\\n+            )\\n+        ),\\n     )\\n-    upload_path = safe_join(str(storage.location), upload_path)\\n     expires = settings.SESSION_COOKIE_AGE\\n \\n     @property\\n@@ -45,6 +50,11 @@ def build_attrs(self, *args, **kwargs):\\n             \"data-fields-%s\" % key: value for key, value in response[\"fields\"].items()\\n         }\\n         defaults[\"data-url\"] = response[\"url\"]\\n+        signer = signing.Signer(\\n+            salt=f\"{S3FileInputMixin.__module__}.{S3FileInputMixin.__name__}\"\\n+        )\\n+        print(self.upload_folder)\\n+        defaults[\"data-s3f-signature\"] = signer.signature(self.upload_folder)\\n         defaults.update(attrs)\\n \\n         try:'], 'file': ['s3file/views.py', 's3file/static/s3file/js/s3file.js', 's3file/middleware.py', 's3file/forms.py'], 'language': ['Python', 'JavaScript/TypeScript', 'Python', 'Python'], 'temp_id': [UUID('44d70ab1-73f6-4acc-be28-044c784b92ac'), UUID('67ac97e6-1a04-4b06-908a-d739ae2b28c5'), UUID('50fa3e89-e51a-43c3-9c85-9b6f1406b3ec'), UUID('259ebb71-6d56-4457-ad34-6489de64234d')]}\n",
      "ERROR:root:Error in {'repo': 'codingjoe/django-s3file', 'vulnerability_id': '2022-24840', 'commit': '68ccd2c621a40eb66fdd6af2be9d5fcc9c373318', 'commit_source': 'github', 'cwe_id': ['CWE-22', 'CWE-22'], 'patch': ['@@ -2,6 +2,7 @@\\n import hashlib\\n import hmac\\n import logging\\n+from pathlib import Path\\n \\n from django import http\\n from django.conf import settings', \"@@ -94,6 +94,12 @@\\n         hiddenFileInput.name = name\\n         hiddenFileInput.value = parseURL(result)\\n         form.appendChild(hiddenFileInput)\\n+        var hiddenSignatureInput = document.createElement('input')\\n+        hiddenSignatureInput.type = 'hidden'\\n+        hiddenSignatureInput.name = name + '-s3f-signature'\\n+        console.log(fileInput.dataset.s3fSignature)\\n+        hiddenSignatureInput.value = fileInput.dataset.s3fSignature\\n+        form.appendChild(hiddenSignatureInput)\\n       })\\n       fileInput.name = ''\\n       window.uploading -= 1\", '@@ -1,9 +1,13 @@\\n import logging\\n import pathlib\\n \\n-from s3file.storages import local_dev, storage\\n+from django.core import signing\\n+from django.core.exceptions import PermissionDenied, SuspiciousFileOperation\\n+from django.utils.crypto import constant_time_compare\\n \\n from . import views\\n+from .forms import S3FileInputMixin\\n+from .storages import local_dev, storage\\n \\n logger = logging.getLogger(\"s3file\")\\n \\n@@ -15,25 +19,50 @@ def __init__(self, get_response):\\n     def __call__(self, request):\\n         file_fields = request.POST.getlist(\"s3file\")\\n         for field_name in file_fields:\\n+\\n             paths = request.POST.getlist(field_name)\\n-            request.FILES.setlist(field_name, list(self.get_files_from_storage(paths)))\\n+            if paths:\\n+                try:\\n+                    signature = request.POST[f\"{field_name}-s3f-signature\"]\\n+                except KeyError:\\n+                    raise PermissionDenied(\"No signature provided.\")\\n+                try:\\n+                    request.FILES.setlist(\\n+                        field_name, list(self.get_files_from_storage(paths, signature))\\n+                    )\\n+                except SuspiciousFileOperation as e:\\n+                    raise PermissionDenied(\"Illegal file name!\") from e\\n \\n         if local_dev and request.path == \"/__s3_mock__/\":\\n             return views.S3MockView.as_view()(request)\\n \\n         return self.get_response(request)\\n \\n     @staticmethod\\n-    def get_files_from_storage(paths):\\n+    def get_files_from_storage(paths, signature):\\n         \"\"\"Return S3 file where the name does not include the path.\"\"\"\\n+        try:\\n+            location = storage.aws_location\\n+        except AttributeError:\\n+            location = storage.location\\n+        signer = signing.Signer(\\n+            salt=f\"{S3FileInputMixin.__module__}.{S3FileInputMixin.__name__}\"\\n+        )\\n         for path in paths:\\n             path = pathlib.PurePosixPath(path)\\n+            print(path)\\n+            print(signer.signature(path.parent), signature)\\n+            if not constant_time_compare(signer.signature(path.parent), signature):\\n+                raise PermissionDenied(\"Illegal signature!\")\\n             try:\\n-                location = storage.aws_location\\n-            except AttributeError:\\n-                location = storage.location\\n+                relative_path = str(path.relative_to(location))\\n+            except ValueError as e:\\n+                raise SuspiciousFileOperation(\\n+                    f\"Path is not inside the designated upload location: {path}\"\\n+                ) from e\\n+\\n             try:\\n-                f = storage.open(str(path.relative_to(location)))\\n+                f = storage.open(relative_path)\\n                 f.name = path.name\\n                 yield f\\n             except (OSError, ValueError):', '@@ -4,6 +4,7 @@\\n import uuid\\n \\n from django.conf import settings\\n+from django.core import signing\\n from django.utils.functional import cached_property\\n from storages.utils import safe_join\\n \\n@@ -16,10 +17,14 @@ class S3FileInputMixin:\\n     \"\"\"FileInput that uses JavaScript to directly upload to Amazon S3.\"\"\"\\n \\n     needs_multipart_form = False\\n-    upload_path = str(\\n-        getattr(settings, \"S3FILE_UPLOAD_PATH\", pathlib.PurePosixPath(\"tmp\", \"s3file\"))\\n+    upload_path = safe_join(\\n+        str(storage.aws_location),\\n+        str(\\n+            getattr(\\n+                settings, \"S3FILE_UPLOAD_PATH\", pathlib.PurePosixPath(\"tmp\", \"s3file\")\\n+            )\\n+        ),\\n     )\\n-    upload_path = safe_join(str(storage.location), upload_path)\\n     expires = settings.SESSION_COOKIE_AGE\\n \\n     @property\\n@@ -45,6 +50,11 @@ def build_attrs(self, *args, **kwargs):\\n             \"data-fields-%s\" % key: value for key, value in response[\"fields\"].items()\\n         }\\n         defaults[\"data-url\"] = response[\"url\"]\\n+        signer = signing.Signer(\\n+            salt=f\"{S3FileInputMixin.__module__}.{S3FileInputMixin.__name__}\"\\n+        )\\n+        print(self.upload_folder)\\n+        defaults[\"data-s3f-signature\"] = signer.signature(self.upload_folder)\\n         defaults.update(attrs)\\n \\n         try:'], 'file': ['s3file/views.py', 's3file/static/s3file/js/s3file.js', 's3file/middleware.py', 's3file/forms.py'], 'language': ['Python', 'JavaScript/TypeScript', 'Python', 'Python'], 'temp_id': [UUID('44d70ab1-73f6-4acc-be28-044c784b92ac'), UUID('67ac97e6-1a04-4b06-908a-d739ae2b28c5'), UUID('50fa3e89-e51a-43c3-9c85-9b6f1406b3ec'), UUID('259ebb71-6d56-4457-ad34-6489de64234d')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 269, in get_changes\n",
      "    code_unit_before_fix = clear_file_content(code_unit_before_fix)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 101, in clear_file_content\n",
      "    new_content = remove_comments(content)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 71, in remove_comments\n",
      "    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:\n",
      "  File \"/Users/somen/.pyenv/versions/3.12.2/lib/python3.12/tokenize.py\", line 543, in _generate_tokens_from_c_tokenizer\n",
      "    raise TokenError(msg, (e.lineno, e.offset)) from None\n",
      "tokenize.TokenError: ('unexpected EOF in multi-line statement', (6, 0))\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1695/1800 [20:16<00:16,  6.25it/s]ERROR:src.process_code_changes:Error processing commit 6aae0a9f145f536515e268dd6b25aa740a5edfe7\n",
      "ERROR:src.process_code_changes:{'repo': 'ibus/ibus-anthy', 'vulnerability_id': '2013-4509', 'commit': '6aae0a9f145f536515e268dd6b25aa740a5edfe7', 'commit_source': 'github', 'cwe_id': [None], 'patch': [\"@@ -126,6 +126,10 @@ def __init__(self, bus, object_path):\\n         # init state\\n         self.__idle_id = 0\\n         self.__prop_dict = {}\\n+        self.__input_purpose = 0\\n+        self.__has_input_purpose = False\\n+        if hasattr(IBus, 'InputPurpose'):\\n+            self.__has_input_purpose = True\\n         try:\\n             self.__is_utf8 = (getpreferredencoding().lower() == 'utf-8')\\n         except:\\n@@ -963,11 +967,17 @@ def do_focus_in(self):\\n             self.__lookup_table.set_page_size(size)\\n \\n     def do_focus_out(self):\\n+        if self.__has_input_purpose:\\n+            self.__input_purpose = 0\\n         mode = self.__prefs.get_value('common', 'behavior_on_focus_out')\\n         if mode == 0 or mode == 1:\\n             self.__reset()\\n             self.__invalidate()\\n \\n+    def do_set_content_type(self, purpose, hints):\\n+        if self.__has_input_purpose:\\n+            self.__input_purpose = purpose\\n+\\n     def do_disable(self):\\n         self.__reset()\\n         self.__invalidate()\\n@@ -1807,6 +1817,10 @@ def T2():\\n         return True\\n \\n     def __process_key_event_internal2(self, keyval, keycode, state):\\n+        if self.__has_input_purpose and \\\\\\n+           self.__input_purpose == IBus.InputPurpose.PASSWORD:\\n+            return False\\n+\\n         if Engine.__typing_mode == jastring.TYPING_MODE_THUMB_SHIFT and \\\\\\n            Engine.__input_mode not in [INPUT_MODE_LATIN, INPUT_MODE_WIDE_LATIN]:\\n             return self.process_key_event_thumb(keyval, keycode, state)\"], 'file': ['engine/engine.py'], 'language': ['Python'], 'temp_id': [UUID('cfa01fd4-28c6-4c17-af52-dabccffc96f6')]}\n",
      "ERROR:root:Error in {'repo': 'ibus/ibus-anthy', 'vulnerability_id': '2013-4509', 'commit': '6aae0a9f145f536515e268dd6b25aa740a5edfe7', 'commit_source': 'github', 'cwe_id': [None], 'patch': [\"@@ -126,6 +126,10 @@ def __init__(self, bus, object_path):\\n         # init state\\n         self.__idle_id = 0\\n         self.__prop_dict = {}\\n+        self.__input_purpose = 0\\n+        self.__has_input_purpose = False\\n+        if hasattr(IBus, 'InputPurpose'):\\n+            self.__has_input_purpose = True\\n         try:\\n             self.__is_utf8 = (getpreferredencoding().lower() == 'utf-8')\\n         except:\\n@@ -963,11 +967,17 @@ def do_focus_in(self):\\n             self.__lookup_table.set_page_size(size)\\n \\n     def do_focus_out(self):\\n+        if self.__has_input_purpose:\\n+            self.__input_purpose = 0\\n         mode = self.__prefs.get_value('common', 'behavior_on_focus_out')\\n         if mode == 0 or mode == 1:\\n             self.__reset()\\n             self.__invalidate()\\n \\n+    def do_set_content_type(self, purpose, hints):\\n+        if self.__has_input_purpose:\\n+            self.__input_purpose = purpose\\n+\\n     def do_disable(self):\\n         self.__reset()\\n         self.__invalidate()\\n@@ -1807,6 +1817,10 @@ def T2():\\n         return True\\n \\n     def __process_key_event_internal2(self, keyval, keycode, state):\\n+        if self.__has_input_purpose and \\\\\\n+           self.__input_purpose == IBus.InputPurpose.PASSWORD:\\n+            return False\\n+\\n         if Engine.__typing_mode == jastring.TYPING_MODE_THUMB_SHIFT and \\\\\\n            Engine.__input_mode not in [INPUT_MODE_LATIN, INPUT_MODE_WIDE_LATIN]:\\n             return self.process_key_event_thumb(keyval, keycode, state)\"], 'file': ['engine/engine.py'], 'language': ['Python'], 'temp_id': [UUID('cfa01fd4-28c6-4c17-af52-dabccffc96f6')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 76:22:                 print 'cmd =', cmd\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 78:22:                 print 'cmd =', cmd\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1735/1800 [20:20<00:07,  8.21it/s]ERROR:src.process_code_changes:Error processing commit b9cdd1f52bdf127cf33bb1be369e374a2855f8e6\n",
      "ERROR:src.process_code_changes:{'repo': 'MobSF/Mobile-Security-Framework-MobSF', 'vulnerability_id': '2022-41547', 'commit': 'b9cdd1f52bdf127cf33bb1be369e374a2855f8e6', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -38,7 +38,7 @@ def DynamicAnalyzer(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG) or re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",LNCH):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 # Delete ScreenCast Cache\\n                 SCREEN_FILE=os.path.join(settings.SCREEN_DIR, \\'screen.png\\')\\n@@ -82,7 +82,7 @@ def GetEnv(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG) or re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",LNCH):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 DIR=settings.BASE_DIR\\n                 APP_DIR=os.path.join(settings.UPLD_DIR, MD5+\\'/\\') #APP DIRECTORY\\n@@ -113,7 +113,7 @@ def TakeScreenShot(request):\\n     try:\\n         if request.method == \\'POST\\':\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 data = {}\\n                 r=random.randint(1, 1000000)\\n@@ -263,7 +263,7 @@ def FinalTest(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PACKAGE):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 #Stop ScreenCast Client if it is running\\n                 tcp_server_mode = \"off\"\\n@@ -307,7 +307,7 @@ def DumpData(request):\\n             data = {}\\n             PACKAGE=request.POST[\\'pkg\\']\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PACKAGE):\\n                     print \"[ATTACK] Possible RCE\"\\n@@ -353,7 +353,7 @@ def ExportedActivityTester(request):\\n     try:\\n         MD5=request.POST[\\'md5\\']\\n         PKG=request.POST[\\'pkg\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m:\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG):\\n                 print \"[ATTACK] Possible RCE\"\\n@@ -410,7 +410,7 @@ def ActivityTester(request):\\n     try:\\n         MD5=request.POST[\\'md5\\']\\n         PKG=request.POST[\\'pkg\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m:\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG):\\n                 print \"[ATTACK] Possible RCE\"\\n@@ -473,7 +473,7 @@ def Report(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 DIR=settings.BASE_DIR\\n                 APP_DIR=os.path.join(settings.UPLD_DIR, MD5+\\'/\\') #APP DIRECTORY\\n@@ -891,7 +891,7 @@ def View(request):\\n         fil=\\'\\'\\n         rtyp=\\'\\'\\n         dat=\\'\\'\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         if m:\\n             fil=request.GET[\\'file\\']\\n             MD5=request.GET[\\'md5\\']', '@@ -48,7 +48,7 @@ def APIFuzzer(request):\\n     try:\\n         if request.method == \\'GET\\':\\n             MD5=request.GET[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 URLS = getListOfURLS(MD5,False)\\n                 if (len(URLS)) == 0:\\n@@ -71,7 +71,7 @@ def APIFuzzer(request):\\n                 return HttpResponseRedirect(\\'/error/\\')\\n         elif request.method ==\"POST\":\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 SCOPE_URLS = [] #All DOMAINS that needs to be tested\\n                 SCOPE_TESTS = [] #All TESTS that needs to be executed\\n@@ -128,7 +128,7 @@ def StartScan(request):\\n     try:\\n         if request.method ==\"POST\":\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 #Scan Mode\\n                 SCAN_MODE=request.POST[\\'scanmode\\']', '@@ -34,7 +34,7 @@ def PDF(request):\\n     try:\\n         MD5=request.GET[\\'md5\\']\\n         TYP=request.GET[\\'type\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m:\\n             if (TYP==\\'APK\\' or TYP==\\'ANDZIP\\'):\\n                 DB=StaticAnalyzerAndroid.objects.filter(MD5=MD5)\\n@@ -159,7 +159,7 @@ def PDF(request):\\n         pass\\n def Java(request):\\n     try:\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         typ=request.GET[\\'type\\']\\n         if m:\\n             MD5=request.GET[\\'md5\\']\\n@@ -198,7 +198,7 @@ def Java(request):\\n         return HttpResponseRedirect(\\'/error/\\')\\n def Smali(request):\\n     try:\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         if m:\\n             MD5=request.GET[\\'md5\\']\\n             SRC=os.path.join(settings.UPLD_DIR, MD5+\\'/smali_source/\\')\\n@@ -224,7 +224,7 @@ def Smali(request):\\n         return HttpResponseRedirect(\\'/error/\\')\\n def Find(request):\\n     try:\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.POST[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.POST[\\'md5\\'])\\n         if m:\\n             MD5=request.POST[\\'md5\\']\\n             q=request.POST[\\'q\\']\\n@@ -265,7 +265,7 @@ def Find(request):\\n def ViewSource(request):\\n     try:\\n         fil=\\'\\'\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         if m and (request.GET[\\'file\\'].endswith(\\'.java\\') or request.GET[\\'file\\'].endswith(\\'.smali\\')):\\n             fil=request.GET[\\'file\\']\\n             MD5=request.GET[\\'md5\\']\\n@@ -304,7 +304,7 @@ def ManifestView(request):\\n         MD5=request.GET[\\'md5\\']  #MD5\\n         TYP=request.GET[\\'type\\'] #APK or SOURCE\\n         BIN=request.GET[\\'bin\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m and (TYP==\\'eclipse\\' or TYP==\\'studio\\' or TYP==\\'apk\\') and (BIN==\\'1\\' or BIN==\\'0\\'):\\n             APP_DIR=os.path.join(settings.UPLD_DIR, MD5+\\'/\\') #APP DIRECTORY\\n             TOOLS_DIR=os.path.join(DIR, \\'StaticAnalyzer/tools/\\')  #TOOLS DIR\\n@@ -326,7 +326,7 @@ def StaticAnalyzer(request):\\n     try:\\n         #Input validation\\n         TYP=request.GET[\\'type\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'checksum\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'checksum\\'])\\n         if ((m) and (request.GET[\\'name\\'].lower().endswith(\\'.apk\\') or request.GET[\\'name\\'].lower().endswith(\\'.zip\\')) and ((TYP==\\'zip\\') or (TYP==\\'apk\\'))):\\n             DIR=settings.BASE_DIR        #BASE DIR\\n             APP_NAME=request.GET[\\'name\\'] #APP ORGINAL NAME\\n@@ -1754,7 +1754,7 @@ def StaticAnalyzer_iOS(request):\\n         print \"[INFO] iOS Static Analysis Started\"\\n         TYP=request.GET[\\'type\\']\\n         RESCAN= str(request.GET.get(\\'rescan\\', 0))\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'checksum\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'checksum\\'])\\n         if ((m) and (request.GET[\\'name\\'].lower().endswith(\\'.ipa\\') or request.GET[\\'name\\'].lower().endswith(\\'.zip\\')) and ((TYP==\\'ipa\\') or (TYP==\\'ios\\'))):\\n             DIR=settings.BASE_DIR        #BASE DIR\\n             APP_NAME=request.GET[\\'name\\'] #APP ORGINAL NAME\\n@@ -1963,7 +1963,7 @@ def ViewFile(request):\\n         typ=request.GET[\\'type\\']\\n         MD5=request.GET[\\'md5\\']\\n         mode=request.GET[\\'mode\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         ext=fil.split(\\'.\\')[-1]\\n         f=re.search(\"plist|db|sqlitedb|sqlite|txt|m\",ext)\\n         if m and f and re.findall(\\'xml|db|txt|m\\',typ) and re.findall(\\'ios|ipa\\',mode):'], 'file': ['DynamicAnalyzer/views.py', 'APITester/views.py', 'StaticAnalyzer/views.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('8ecc9df8-2da5-47db-b901-dbcf52b05320'), UUID('848cd7e8-1eb2-41de-b232-da670646d1f9'), UUID('0ad9e252-03bc-4172-9951-b9b37b91a723')]}\n",
      "ERROR:root:Error in {'repo': 'MobSF/Mobile-Security-Framework-MobSF', 'vulnerability_id': '2022-41547', 'commit': 'b9cdd1f52bdf127cf33bb1be369e374a2855f8e6', 'commit_source': 'github', 'cwe_id': [None], 'patch': ['@@ -38,7 +38,7 @@ def DynamicAnalyzer(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG) or re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",LNCH):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 # Delete ScreenCast Cache\\n                 SCREEN_FILE=os.path.join(settings.SCREEN_DIR, \\'screen.png\\')\\n@@ -82,7 +82,7 @@ def GetEnv(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG) or re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",LNCH):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 DIR=settings.BASE_DIR\\n                 APP_DIR=os.path.join(settings.UPLD_DIR, MD5+\\'/\\') #APP DIRECTORY\\n@@ -113,7 +113,7 @@ def TakeScreenShot(request):\\n     try:\\n         if request.method == \\'POST\\':\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 data = {}\\n                 r=random.randint(1, 1000000)\\n@@ -263,7 +263,7 @@ def FinalTest(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PACKAGE):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 #Stop ScreenCast Client if it is running\\n                 tcp_server_mode = \"off\"\\n@@ -307,7 +307,7 @@ def DumpData(request):\\n             data = {}\\n             PACKAGE=request.POST[\\'pkg\\']\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PACKAGE):\\n                     print \"[ATTACK] Possible RCE\"\\n@@ -353,7 +353,7 @@ def ExportedActivityTester(request):\\n     try:\\n         MD5=request.POST[\\'md5\\']\\n         PKG=request.POST[\\'pkg\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m:\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG):\\n                 print \"[ATTACK] Possible RCE\"\\n@@ -410,7 +410,7 @@ def ActivityTester(request):\\n     try:\\n         MD5=request.POST[\\'md5\\']\\n         PKG=request.POST[\\'pkg\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m:\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG):\\n                 print \"[ATTACK] Possible RCE\"\\n@@ -473,7 +473,7 @@ def Report(request):\\n             if re.findall(\";|\\\\$\\\\(|\\\\|\\\\||&&\",PKG):\\n                 print \"[ATTACK] Possible RCE\"\\n                 return HttpResponseRedirect(\\'/error/\\') \\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 DIR=settings.BASE_DIR\\n                 APP_DIR=os.path.join(settings.UPLD_DIR, MD5+\\'/\\') #APP DIRECTORY\\n@@ -891,7 +891,7 @@ def View(request):\\n         fil=\\'\\'\\n         rtyp=\\'\\'\\n         dat=\\'\\'\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         if m:\\n             fil=request.GET[\\'file\\']\\n             MD5=request.GET[\\'md5\\']', '@@ -48,7 +48,7 @@ def APIFuzzer(request):\\n     try:\\n         if request.method == \\'GET\\':\\n             MD5=request.GET[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 URLS = getListOfURLS(MD5,False)\\n                 if (len(URLS)) == 0:\\n@@ -71,7 +71,7 @@ def APIFuzzer(request):\\n                 return HttpResponseRedirect(\\'/error/\\')\\n         elif request.method ==\"POST\":\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 SCOPE_URLS = [] #All DOMAINS that needs to be tested\\n                 SCOPE_TESTS = [] #All TESTS that needs to be executed\\n@@ -128,7 +128,7 @@ def StartScan(request):\\n     try:\\n         if request.method ==\"POST\":\\n             MD5=request.POST[\\'md5\\']\\n-            m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+            m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n             if m:\\n                 #Scan Mode\\n                 SCAN_MODE=request.POST[\\'scanmode\\']', '@@ -34,7 +34,7 @@ def PDF(request):\\n     try:\\n         MD5=request.GET[\\'md5\\']\\n         TYP=request.GET[\\'type\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m:\\n             if (TYP==\\'APK\\' or TYP==\\'ANDZIP\\'):\\n                 DB=StaticAnalyzerAndroid.objects.filter(MD5=MD5)\\n@@ -159,7 +159,7 @@ def PDF(request):\\n         pass\\n def Java(request):\\n     try:\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         typ=request.GET[\\'type\\']\\n         if m:\\n             MD5=request.GET[\\'md5\\']\\n@@ -198,7 +198,7 @@ def Java(request):\\n         return HttpResponseRedirect(\\'/error/\\')\\n def Smali(request):\\n     try:\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         if m:\\n             MD5=request.GET[\\'md5\\']\\n             SRC=os.path.join(settings.UPLD_DIR, MD5+\\'/smali_source/\\')\\n@@ -224,7 +224,7 @@ def Smali(request):\\n         return HttpResponseRedirect(\\'/error/\\')\\n def Find(request):\\n     try:\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.POST[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.POST[\\'md5\\'])\\n         if m:\\n             MD5=request.POST[\\'md5\\']\\n             q=request.POST[\\'q\\']\\n@@ -265,7 +265,7 @@ def Find(request):\\n def ViewSource(request):\\n     try:\\n         fil=\\'\\'\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'md5\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'md5\\'])\\n         if m and (request.GET[\\'file\\'].endswith(\\'.java\\') or request.GET[\\'file\\'].endswith(\\'.smali\\')):\\n             fil=request.GET[\\'file\\']\\n             MD5=request.GET[\\'md5\\']\\n@@ -304,7 +304,7 @@ def ManifestView(request):\\n         MD5=request.GET[\\'md5\\']  #MD5\\n         TYP=request.GET[\\'type\\'] #APK or SOURCE\\n         BIN=request.GET[\\'bin\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         if m and (TYP==\\'eclipse\\' or TYP==\\'studio\\' or TYP==\\'apk\\') and (BIN==\\'1\\' or BIN==\\'0\\'):\\n             APP_DIR=os.path.join(settings.UPLD_DIR, MD5+\\'/\\') #APP DIRECTORY\\n             TOOLS_DIR=os.path.join(DIR, \\'StaticAnalyzer/tools/\\')  #TOOLS DIR\\n@@ -326,7 +326,7 @@ def StaticAnalyzer(request):\\n     try:\\n         #Input validation\\n         TYP=request.GET[\\'type\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'checksum\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'checksum\\'])\\n         if ((m) and (request.GET[\\'name\\'].lower().endswith(\\'.apk\\') or request.GET[\\'name\\'].lower().endswith(\\'.zip\\')) and ((TYP==\\'zip\\') or (TYP==\\'apk\\'))):\\n             DIR=settings.BASE_DIR        #BASE DIR\\n             APP_NAME=request.GET[\\'name\\'] #APP ORGINAL NAME\\n@@ -1754,7 +1754,7 @@ def StaticAnalyzer_iOS(request):\\n         print \"[INFO] iOS Static Analysis Started\"\\n         TYP=request.GET[\\'type\\']\\n         RESCAN= str(request.GET.get(\\'rescan\\', 0))\\n-        m=re.match(\\'[0-9a-f]{32}\\',request.GET[\\'checksum\\'])\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',request.GET[\\'checksum\\'])\\n         if ((m) and (request.GET[\\'name\\'].lower().endswith(\\'.ipa\\') or request.GET[\\'name\\'].lower().endswith(\\'.zip\\')) and ((TYP==\\'ipa\\') or (TYP==\\'ios\\'))):\\n             DIR=settings.BASE_DIR        #BASE DIR\\n             APP_NAME=request.GET[\\'name\\'] #APP ORGINAL NAME\\n@@ -1963,7 +1963,7 @@ def ViewFile(request):\\n         typ=request.GET[\\'type\\']\\n         MD5=request.GET[\\'md5\\']\\n         mode=request.GET[\\'mode\\']\\n-        m=re.match(\\'[0-9a-f]{32}\\',MD5)\\n+        m=re.match(\\'^[0-9a-f]{32}$\\',MD5)\\n         ext=fil.split(\\'.\\')[-1]\\n         f=re.search(\"plist|db|sqlitedb|sqlite|txt|m\",ext)\\n         if m and f and re.findall(\\'xml|db|txt|m\\',typ) and re.findall(\\'ios|ipa\\',mode):'], 'file': ['DynamicAnalyzer/views.py', 'APITester/views.py', 'StaticAnalyzer/views.py'], 'language': ['Python', 'Python', 'Python'], 'temp_id': [UUID('8ecc9df8-2da5-47db-b901-dbcf52b05320'), UUID('848cd7e8-1eb2-41de-b232-da670646d1f9'), UUID('0ad9e252-03bc-4172-9951-b9b37b91a723')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:10:     print \"\\n[INFO] Dynamic Analysis Started\"\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 3:10:     print \"\\n[INFO] Dynamic Analysis Started\"\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1736/1800 [20:21<00:08,  7.18it/s]ERROR:src.process_code_changes:Error processing commit 61bbb5aa7a23a93f2f93710005f71bc972826099\n",
      "ERROR:src.process_code_changes:{'repo': 'getredash/redash', 'vulnerability_id': '2021-43780', 'commit': '61bbb5aa7a23a93f2f93710005f71bc972826099', 'commit_source': 'github', 'cwe_id': ['CWE-918'], 'patch': ['@@ -2,7 +2,9 @@\\n import yaml\\n import datetime\\n from funcy import compact, project\\n-from redash import settings\\n+\\n+from redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\\n+\\n from redash.utils import json_dumps\\n from redash.query_runner import (\\n     BaseHTTPQueryRunner,\\n@@ -12,7 +14,6 @@\\n     TYPE_FLOAT,\\n     TYPE_INTEGER,\\n     TYPE_STRING,\\n-    is_private_address,\\n )\\n \\n \\n@@ -163,8 +164,6 @@ def run_query(self, query, user):\\n         if \"url\" not in query:\\n             raise QueryParseError(\"Query must include \\'url\\' option.\")\\n \\n-        if is_private_address(query[\"url\"]) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n-            raise Exception(\"Can\\'t query private addresses.\")\\n \\n         method = query.get(\"method\", \"get\")\\n         request_options = project(query, (\"params\", \"headers\", \"data\", \"auth\", \"json\"))', '@@ -1,9 +1,9 @@\\n import logging\\n import yaml\\n-import requests\\n import io\\n \\n-from redash import settings\\n+from redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\\n+\\n from redash.query_runner import *\\n from redash.utils import json_dumps\\n \\n@@ -52,14 +52,11 @@ def run_query(self, query, user):\\n             args.pop(\\'url\\', None)\\n             ua = args[\\'user-agent\\']\\n             args.pop(\\'user-agent\\', None)\\n-\\n-            if is_private_address(path) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n-                raise Exception(\"Can\\'t query private addresses.\")\\n         except:\\n             pass\\n \\n         try:\\n-            response = requests.get(url=path, headers={\"User-agent\": ua})\\n+            response = requests_or_advocate.get(url=path, headers={\"User-agent\": ua})\\n             workbook = pd.read_csv(io.BytesIO(response.content),sep=\",\", **args)\\n \\n             df = workbook.copy()\\n@@ -88,6 +85,9 @@ def run_query(self, query, user):\\n         except KeyboardInterrupt:\\n             error = \"Query cancelled by user.\"\\n             json_data = None\\n+        except UnacceptableAddressException:\\n+            error = \"Can\\'t query private addresses.\"\\n+            json_data = None\\n         except Exception as e:\\n             error = \"Error reading {0}. {1}\".format(path, str(e))\\n             json_data = None', '@@ -1,8 +1,8 @@\\n import logging\\n import yaml\\n-import requests\\n \\n-from redash import settings\\n+from redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\\n+\\n from redash.query_runner import *\\n from redash.utils import json_dumps\\n \\n@@ -49,13 +49,11 @@ def run_query(self, query, user):\\n             ua = args[\\'user-agent\\']\\n             args.pop(\\'user-agent\\', None)\\n \\n-            if is_private_address(path) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n-                raise Exception(\"Can\\'t query private addresses.\")\\n         except:\\n             pass\\n \\n         try:\\n-            response = requests.get(url=path, headers={\"User-agent\": ua})\\n+            response = requests_or_advocate.get(url=path, headers={\"User-agent\": ua})\\n             workbook = pd.read_excel(response.content, **args)\\n \\n             df = workbook.copy()\\n@@ -84,6 +82,9 @@ def run_query(self, query, user):\\n         except KeyboardInterrupt:\\n             error = \"Query cancelled by user.\"\\n             json_data = None\\n+        except UnacceptableAddressException:\\n+            error = \"Can\\'t query private addresses.\"\\n+            json_data = None\\n         except Exception as e:\\n             error = \"Error reading {0}. {1}\".format(path, str(e))\\n             json_data = None', '@@ -1,8 +1,14 @@\\n-import requests\\n from redash import settings\\n \\n+from advocate.exceptions import UnacceptableAddressException\\n+if settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n+    import advocate as requests_or_advocate\\n+else:\\n+    import requests as requests_or_advocate\\n \\n-class ConfiguredSession(requests.Session):\\n+\\n+\\n+class ConfiguredSession(requests_or_advocate.Session):\\n     def request(self, *args, **kwargs):\\n         if not settings.REQUESTS_ALLOW_REDIRECTS:\\n             kwargs.update({\"allow_redirects\": False})'], 'file': ['redash/query_runner/json_ds.py', 'redash/query_runner/csv.py', 'redash/query_runner/excel.py', 'redash/utils/requests_session.py'], 'language': ['Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('1afe99c6-bce3-420a-a189-6eee9fbe1459'), UUID('6d74cb4b-b737-43a5-b3c9-495d01f2293b'), UUID('a6538ad6-76fc-4447-bbb7-973a2b0fea62'), UUID('d7630651-907a-42ab-952b-f4deb1d9b1f5')]}\n",
      "ERROR:root:Error in {'repo': 'getredash/redash', 'vulnerability_id': '2021-43780', 'commit': '61bbb5aa7a23a93f2f93710005f71bc972826099', 'commit_source': 'github', 'cwe_id': ['CWE-918'], 'patch': ['@@ -2,7 +2,9 @@\\n import yaml\\n import datetime\\n from funcy import compact, project\\n-from redash import settings\\n+\\n+from redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\\n+\\n from redash.utils import json_dumps\\n from redash.query_runner import (\\n     BaseHTTPQueryRunner,\\n@@ -12,7 +14,6 @@\\n     TYPE_FLOAT,\\n     TYPE_INTEGER,\\n     TYPE_STRING,\\n-    is_private_address,\\n )\\n \\n \\n@@ -163,8 +164,6 @@ def run_query(self, query, user):\\n         if \"url\" not in query:\\n             raise QueryParseError(\"Query must include \\'url\\' option.\")\\n \\n-        if is_private_address(query[\"url\"]) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n-            raise Exception(\"Can\\'t query private addresses.\")\\n \\n         method = query.get(\"method\", \"get\")\\n         request_options = project(query, (\"params\", \"headers\", \"data\", \"auth\", \"json\"))', '@@ -1,9 +1,9 @@\\n import logging\\n import yaml\\n-import requests\\n import io\\n \\n-from redash import settings\\n+from redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\\n+\\n from redash.query_runner import *\\n from redash.utils import json_dumps\\n \\n@@ -52,14 +52,11 @@ def run_query(self, query, user):\\n             args.pop(\\'url\\', None)\\n             ua = args[\\'user-agent\\']\\n             args.pop(\\'user-agent\\', None)\\n-\\n-            if is_private_address(path) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n-                raise Exception(\"Can\\'t query private addresses.\")\\n         except:\\n             pass\\n \\n         try:\\n-            response = requests.get(url=path, headers={\"User-agent\": ua})\\n+            response = requests_or_advocate.get(url=path, headers={\"User-agent\": ua})\\n             workbook = pd.read_csv(io.BytesIO(response.content),sep=\",\", **args)\\n \\n             df = workbook.copy()\\n@@ -88,6 +85,9 @@ def run_query(self, query, user):\\n         except KeyboardInterrupt:\\n             error = \"Query cancelled by user.\"\\n             json_data = None\\n+        except UnacceptableAddressException:\\n+            error = \"Can\\'t query private addresses.\"\\n+            json_data = None\\n         except Exception as e:\\n             error = \"Error reading {0}. {1}\".format(path, str(e))\\n             json_data = None', '@@ -1,8 +1,8 @@\\n import logging\\n import yaml\\n-import requests\\n \\n-from redash import settings\\n+from redash.utils.requests_session import requests_or_advocate, UnacceptableAddressException\\n+\\n from redash.query_runner import *\\n from redash.utils import json_dumps\\n \\n@@ -49,13 +49,11 @@ def run_query(self, query, user):\\n             ua = args[\\'user-agent\\']\\n             args.pop(\\'user-agent\\', None)\\n \\n-            if is_private_address(path) and settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n-                raise Exception(\"Can\\'t query private addresses.\")\\n         except:\\n             pass\\n \\n         try:\\n-            response = requests.get(url=path, headers={\"User-agent\": ua})\\n+            response = requests_or_advocate.get(url=path, headers={\"User-agent\": ua})\\n             workbook = pd.read_excel(response.content, **args)\\n \\n             df = workbook.copy()\\n@@ -84,6 +82,9 @@ def run_query(self, query, user):\\n         except KeyboardInterrupt:\\n             error = \"Query cancelled by user.\"\\n             json_data = None\\n+        except UnacceptableAddressException:\\n+            error = \"Can\\'t query private addresses.\"\\n+            json_data = None\\n         except Exception as e:\\n             error = \"Error reading {0}. {1}\".format(path, str(e))\\n             json_data = None', '@@ -1,8 +1,14 @@\\n-import requests\\n from redash import settings\\n \\n+from advocate.exceptions import UnacceptableAddressException\\n+if settings.ENFORCE_PRIVATE_ADDRESS_BLOCK:\\n+    import advocate as requests_or_advocate\\n+else:\\n+    import requests as requests_or_advocate\\n \\n-class ConfiguredSession(requests.Session):\\n+\\n+\\n+class ConfiguredSession(requests_or_advocate.Session):\\n     def request(self, *args, **kwargs):\\n         if not settings.REQUESTS_ALLOW_REDIRECTS:\\n             kwargs.update({\"allow_redirects\": False})'], 'file': ['redash/query_runner/json_ds.py', 'redash/query_runner/csv.py', 'redash/query_runner/excel.py', 'redash/utils/requests_session.py'], 'language': ['Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('1afe99c6-bce3-420a-a189-6eee9fbe1459'), UUID('6d74cb4b-b737-43a5-b3c9-495d01f2293b'), UUID('a6538ad6-76fc-4447-bbb7-973a2b0fea62'), UUID('d7630651-907a-42ab-952b-f4deb1d9b1f5')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 7:0: <line number missing in source>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 7:0: <line number missing in source>\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1747/1800 [20:22<00:06,  8.54it/s]ERROR:src.process_code_changes:Error processing commit f77cbc607d6e2a62e63287d37ad320109a2cc78a\n",
      "ERROR:src.process_code_changes:{'repo': 'divio/django-cms', 'vulnerability_id': '2015-5081', 'commit': 'f77cbc607d6e2a62e63287d37ad320109a2cc78a', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -928,6 +928,7 @@ def change_template(self, request, object_id):\\n             helpers.make_revision_with_plugins(page, request.user, message)\\n         return HttpResponse(force_unicode(_(\"The template was successfully changed\")))\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def move_page(self, request, page_id, extra_context=None):\\n         \"\"\"\\n@@ -1013,6 +1014,7 @@ def copy_language(self, request, page_id):\\n                 helpers.make_revision_with_plugins(page, request.user, message)\\n             return HttpResponse(\"ok\")\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def copy_page(self, request, page_id, extra_context=None):\\n         \"\"\"\\n@@ -1046,6 +1048,7 @@ def copy_page(self, request, page_id, extra_context=None):\\n         context.update(extra_context or {})\\n         return HttpResponseRedirect(\\'../../\\')\\n \\n+    @require_POST\\n     @wrap_transaction\\n     @create_revision()\\n     def publish_page(self, request, page_id, language):\\n@@ -1146,6 +1149,7 @@ def cleanup_history(self, page, publish=False):\\n                         revision.delete()\\n                         deleted.append(revision.pk)\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def unpublish(self, request, page_id, language):\\n         \"\"\"\\n@@ -1181,6 +1185,7 @@ def unpublish(self, request, page_id, language):\\n             path = \"%s?language=%s&page_id=%s\" % (path, request.GET.get(\\'redirect_language\\'), request.GET.get(\\'redirect_page_id\\'))\\n         return HttpResponseRedirect(path)\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def revert_page(self, request, page_id, language):\\n         page = get_object_or_404(Page, id=page_id)\\n@@ -1316,6 +1321,7 @@ def preview_page(self, request, object_id, language):\\n             page.site.domain, url)\\n         return HttpResponseRedirect(url)\\n \\n+    @require_POST\\n     def change_innavigation(self, request, page_id):\\n         \"\"\"\\n         Switch the in_navigation of a page', '@@ -30,8 +30,8 @@\\n \\t\\t\\t\\t{% if lang in page.languages %}\\n \\t\\t\\t\\t<div class=\"language-tooltip\" hidden=\"hidden\">\\n \\t\\t\\t\\t\\t{% trans \"Pick an action:\" %}\\n-\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/unpublish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\">{% trans \"Unpublish\" %}</a>\\n-\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/publish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\">{% trans \"Publish\" %}</a>\\n+\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/unpublish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\" class=\"js-ajax-submit\">{% trans \"Unpublish\" %}</a>\\n+\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/publish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\" class=\"js-ajax-submit\">{% trans \"Publish\" %}</a>\\n \\t\\t\\t\\t</div>\\n \\t\\t\\t\\t{% endif %}\\n \\t\\t\\t{% else %}', \"@@ -213,7 +213,27 @@ $(document).ready(function () {\\n \\n \\t\\t\\t\\t// in case of the publish button\\n \\t\\t\\t\\tbtn.find('.cms_publish-page').bind(that.click, function (e) {\\n-\\t\\t\\t\\t\\tif(!confirm(that.config.lang.publish)) e.preventDefault();\\n+\\t\\t\\t\\t\\tif(!confirm(that.config.lang.publish)) {\\n+\\t\\t\\t\\t\\t\\te.preventDefault();\\n+\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t});\\n+\\n+\\t\\t\\t\\tbtn.find('.cms_btn-publish').bind(that.click, function (e) {\\n+\\t\\t\\t\\t\\te.preventDefault();\\n+\\t\\t\\t\\t\\t// send post request to prevent xss attacks\\n+\\t\\t\\t\\t\\t$.ajax({\\n+\\t\\t\\t\\t\\t\\t'type': 'post',\\n+\\t\\t\\t\\t\\t\\t'url': $(this).prop('href'),\\n+\\t\\t\\t\\t\\t\\t'data': {\\n+\\t\\t\\t\\t\\t\\t\\t'csrfmiddlewaretoken': CMS.config.csrf\\n+\\t\\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t\\t'success': function () {\\n+\\t\\t\\t\\t\\t\\t\\tCMS.API.Helpers.reloadBrowser();\\n+\\t\\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t\\t'error': function (request) {\\n+\\t\\t\\t\\t\\t\\t\\tthrow new Error(request);\\n+\\t\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t\\t});\\n \\t\\t\\t\\t});\\n \\t\\t\\t});\\n \\t\\t},\", \"@@ -180,8 +180,17 @@ $(document).ready(function () {\\n \\t\\t\\t\\t// cancel if not confirmed\\n \\t\\t\\t\\tif(!confirm(that.options.lang.publish.replace('Â§', $(this).text().toLowerCase()))) return false;\\n \\n-\\t\\t\\t\\t// publish page and update\\n-\\t\\t\\t\\twindow.location.href = $(this).attr('href');\\n+\\t\\t\\t\\t// send post request to prevent xss attacks\\n+\\t\\t\\t\\t$.ajax({\\n+\\t\\t\\t\\t\\t'type': 'post',\\n+\\t\\t\\t\\t\\t'url': $(this).prop('href'),\\n+\\t\\t\\t\\t\\t'success': function () {\\n+\\t\\t\\t\\t\\t\\tCMS.API.Helpers.reloadBrowser();\\n+\\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t'error': function (request) {\\n+\\t\\t\\t\\t\\t\\tthrow new Error(request);\\n+\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t});\\n \\t\\t\\t});\\n \\t\\t},\\n \"], 'file': ['cms/admin/pageadmin.py', 'cms/templates/admin/cms/page/tree/menu_item.html', 'cms/static/cms/js/modules/cms.toolbar.js', 'cms/static/cms/js/modules/cms.changelist.js'], 'language': ['Python', 'HTML', 'JavaScript/TypeScript', 'JavaScript/TypeScript'], 'temp_id': [UUID('190dd255-b121-47d7-9bf0-5c7f05e2367c'), UUID('5208f651-e9ec-4e62-970e-9efe2a709378'), UUID('52f8aceb-c77a-4889-9687-9f65c4fcf804'), UUID('c652b829-1569-4085-b8bf-93d3f8e565a5')]}\n",
      "ERROR:root:Error in {'repo': 'divio/django-cms', 'vulnerability_id': '2015-5081', 'commit': 'f77cbc607d6e2a62e63287d37ad320109a2cc78a', 'commit_source': 'github', 'cwe_id': ['CWE-352'], 'patch': ['@@ -928,6 +928,7 @@ def change_template(self, request, object_id):\\n             helpers.make_revision_with_plugins(page, request.user, message)\\n         return HttpResponse(force_unicode(_(\"The template was successfully changed\")))\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def move_page(self, request, page_id, extra_context=None):\\n         \"\"\"\\n@@ -1013,6 +1014,7 @@ def copy_language(self, request, page_id):\\n                 helpers.make_revision_with_plugins(page, request.user, message)\\n             return HttpResponse(\"ok\")\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def copy_page(self, request, page_id, extra_context=None):\\n         \"\"\"\\n@@ -1046,6 +1048,7 @@ def copy_page(self, request, page_id, extra_context=None):\\n         context.update(extra_context or {})\\n         return HttpResponseRedirect(\\'../../\\')\\n \\n+    @require_POST\\n     @wrap_transaction\\n     @create_revision()\\n     def publish_page(self, request, page_id, language):\\n@@ -1146,6 +1149,7 @@ def cleanup_history(self, page, publish=False):\\n                         revision.delete()\\n                         deleted.append(revision.pk)\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def unpublish(self, request, page_id, language):\\n         \"\"\"\\n@@ -1181,6 +1185,7 @@ def unpublish(self, request, page_id, language):\\n             path = \"%s?language=%s&page_id=%s\" % (path, request.GET.get(\\'redirect_language\\'), request.GET.get(\\'redirect_page_id\\'))\\n         return HttpResponseRedirect(path)\\n \\n+    @require_POST\\n     @wrap_transaction\\n     def revert_page(self, request, page_id, language):\\n         page = get_object_or_404(Page, id=page_id)\\n@@ -1316,6 +1321,7 @@ def preview_page(self, request, object_id, language):\\n             page.site.domain, url)\\n         return HttpResponseRedirect(url)\\n \\n+    @require_POST\\n     def change_innavigation(self, request, page_id):\\n         \"\"\"\\n         Switch the in_navigation of a page', '@@ -30,8 +30,8 @@\\n \\t\\t\\t\\t{% if lang in page.languages %}\\n \\t\\t\\t\\t<div class=\"language-tooltip\" hidden=\"hidden\">\\n \\t\\t\\t\\t\\t{% trans \"Pick an action:\" %}\\n-\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/unpublish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\">{% trans \"Unpublish\" %}</a>\\n-\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/publish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\">{% trans \"Publish\" %}</a>\\n+\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/unpublish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\" class=\"js-ajax-submit\">{% trans \"Unpublish\" %}</a>\\n+\\t\\t\\t\\t\\t<a href=\"./{{ page.id }}/{{ lang }}/publish/?redirect_language={{ preview_language }}&redirect_page_id={{ request.GET.page_id }}\" class=\"js-ajax-submit\">{% trans \"Publish\" %}</a>\\n \\t\\t\\t\\t</div>\\n \\t\\t\\t\\t{% endif %}\\n \\t\\t\\t{% else %}', \"@@ -213,7 +213,27 @@ $(document).ready(function () {\\n \\n \\t\\t\\t\\t// in case of the publish button\\n \\t\\t\\t\\tbtn.find('.cms_publish-page').bind(that.click, function (e) {\\n-\\t\\t\\t\\t\\tif(!confirm(that.config.lang.publish)) e.preventDefault();\\n+\\t\\t\\t\\t\\tif(!confirm(that.config.lang.publish)) {\\n+\\t\\t\\t\\t\\t\\te.preventDefault();\\n+\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t});\\n+\\n+\\t\\t\\t\\tbtn.find('.cms_btn-publish').bind(that.click, function (e) {\\n+\\t\\t\\t\\t\\te.preventDefault();\\n+\\t\\t\\t\\t\\t// send post request to prevent xss attacks\\n+\\t\\t\\t\\t\\t$.ajax({\\n+\\t\\t\\t\\t\\t\\t'type': 'post',\\n+\\t\\t\\t\\t\\t\\t'url': $(this).prop('href'),\\n+\\t\\t\\t\\t\\t\\t'data': {\\n+\\t\\t\\t\\t\\t\\t\\t'csrfmiddlewaretoken': CMS.config.csrf\\n+\\t\\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t\\t'success': function () {\\n+\\t\\t\\t\\t\\t\\t\\tCMS.API.Helpers.reloadBrowser();\\n+\\t\\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t\\t'error': function (request) {\\n+\\t\\t\\t\\t\\t\\t\\tthrow new Error(request);\\n+\\t\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t\\t});\\n \\t\\t\\t\\t});\\n \\t\\t\\t});\\n \\t\\t},\", \"@@ -180,8 +180,17 @@ $(document).ready(function () {\\n \\t\\t\\t\\t// cancel if not confirmed\\n \\t\\t\\t\\tif(!confirm(that.options.lang.publish.replace('Â§', $(this).text().toLowerCase()))) return false;\\n \\n-\\t\\t\\t\\t// publish page and update\\n-\\t\\t\\t\\twindow.location.href = $(this).attr('href');\\n+\\t\\t\\t\\t// send post request to prevent xss attacks\\n+\\t\\t\\t\\t$.ajax({\\n+\\t\\t\\t\\t\\t'type': 'post',\\n+\\t\\t\\t\\t\\t'url': $(this).prop('href'),\\n+\\t\\t\\t\\t\\t'success': function () {\\n+\\t\\t\\t\\t\\t\\tCMS.API.Helpers.reloadBrowser();\\n+\\t\\t\\t\\t\\t},\\n+\\t\\t\\t\\t\\t'error': function (request) {\\n+\\t\\t\\t\\t\\t\\tthrow new Error(request);\\n+\\t\\t\\t\\t\\t}\\n+\\t\\t\\t\\t});\\n \\t\\t\\t});\\n \\t\\t},\\n \"], 'file': ['cms/admin/pageadmin.py', 'cms/templates/admin/cms/page/tree/menu_item.html', 'cms/static/cms/js/modules/cms.toolbar.js', 'cms/static/cms/js/modules/cms.changelist.js'], 'language': ['Python', 'HTML', 'JavaScript/TypeScript', 'JavaScript/TypeScript'], 'temp_id': [UUID('190dd255-b121-47d7-9bf0-5c7f05e2367c'), UUID('5208f651-e9ec-4e62-970e-9efe2a709378'), UUID('52f8aceb-c77a-4889-9687-9f65c4fcf804'), UUID('c652b829-1569-4085-b8bf-93d3f8e565a5')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @require_POST\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 2:0:     @require_POST\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1768/1800 [20:26<00:06,  4.94it/s]ERROR:src.process_code_changes:Error processing commit 7d270bacbe29a90a10f1855abc3b50dac0f08022\n",
      "ERROR:src.process_code_changes:{'repo': 'openstack/barbican', 'vulnerability_id': '2022-23451', 'commit': '7d270bacbe29a90a10f1855abc3b50dac0f08022', 'commit_source': 'github', 'cwe_id': ['CWE-863'], 'patch': ['@@ -82,6 +82,9 @@\\n         name=\\'secret_project_creator\\',\\n         check_str=\"rule:creator and rule:secret_project_match and \" +\\n                   \"rule:secret_creator_user\"),\\n+    policy.RuleDefault(\\n+        name=\\'secret_project_creator_role\\',\\n+        check_str=\"rule:creator and rule:secret_project_match\"),\\n     policy.RuleDefault(\\n         name=\\'container_project_admin\\',\\n         check_str=\"rule:admin and rule:container_project_match\"),', '@@ -28,7 +28,7 @@ def _secret_metadata_not_found():\\n     pecan.abort(404, u._(\\'Secret metadata not found.\\'))\\n \\n \\n-class SecretMetadataController(controllers.ACLMixin):\\n+class SecretMetadataController(controllers.SecretACLMixin):\\n     \"\"\"Handles SecretMetadata requests by a given secret id.\"\"\"\\n \\n     def __init__(self, secret):\\n@@ -106,7 +106,7 @@ def on_post(self, external_project_id, **kwargs):\\n         return {\\'key\\': key, \\'value\\': value}\\n \\n \\n-class SecretMetadatumController(controllers.ACLMixin):\\n+class SecretMetadatumController(controllers.SecretACLMixin):\\n \\n     def __init__(self, secret):\\n         LOG.debug(\\'=== Creating SecretMetadatumController ===\\')', '@@ -71,7 +71,7 @@ def _request_has_twsk_but_no_transport_key_id():\\n                          \\'transport key id has not been provided.\\'))\\n \\n \\n-class SecretController(controllers.ACLMixin):\\n+class SecretController(controllers.SecretACLMixin):\\n     \"\"\"Handles Secret retrieval and deletion requests.\"\"\"\\n \\n     def __init__(self, secret):\\n@@ -81,12 +81,6 @@ def __init__(self, secret):\\n         self.consumer_repo = repo.get_secret_consumer_repository()\\n         self.transport_key_repo = repo.get_transport_key_repository()\\n \\n-    def get_acl_tuple(self, req, **kwargs):\\n-        d = self.get_acl_dict_for_user(req, self.secret.secret_acls)\\n-        d[\\'project_id\\'] = self.secret.project.external_id\\n-        d[\\'creator_id\\'] = self.secret.creator_id\\n-        return \\'secret\\', d\\n-\\n     @pecan.expose()\\n     def _lookup(self, sub_resource, *remainder):\\n         if sub_resource == \\'acl\\':', \"@@ -17,7 +17,10 @@\\n rules = [\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:get',\\n-        check_str=f'rule:all_but_audit or {_MEMBER}',\\n+        check_str='rule:secret_non_private_read or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  'rule:secret_project_admin or rule:secret_acl_read or ' +\\n+                  f'{_MEMBER}',\\n         scope_types=['project'],\\n         description='metadata/: Lists a secrets user-defined metadata. || ' +\\n                     'metadata/{key}: Retrieves a secrets user-added metadata.',\\n@@ -34,7 +37,10 @@\\n     ),\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:post',\\n-        check_str=f'rule:admin_or_creator or {_MEMBER}',\\n+        check_str='rule:secret_project_admin or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  '(rule:secret_project_creator_role and ' +\\n+                  f'rule:secret_non_private_read) or {_MEMBER}',\\n         scope_types=['project'],\\n         description='Adds a new key/value pair to the secrets user-defined ' +\\n                     'metadata.',\\n@@ -47,7 +53,10 @@\\n     ),\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:put',\\n-        check_str=f'rule:admin_or_creator or {_MEMBER}',\\n+        check_str='rule:secret_project_admin or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  '(rule:secret_project_creator_role and ' +\\n+                  f'rule:secret_non_private_read) or {_MEMBER}',\\n         scope_types=['project'],\\n         description='metadata/: Sets the user-defined metadata for a secret ' +\\n                     '|| metadata/{key}: Updates an existing key/value pair ' +\\n@@ -65,7 +74,10 @@\\n     ),\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:delete',\\n-        check_str=f'rule:admin_or_creator or {_MEMBER}',\\n+        check_str='rule:secret_project_admin or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  '(rule:secret_project_creator_role and ' +\\n+                  f'rule:secret_non_private_read) or {_MEMBER}',\\n         scope_types=['project'],\\n         description='Delete secret user-defined metadata by key.',\\n         operations=[\"], 'file': ['barbican/common/policies/base.py', 'barbican/api/controllers/secretmeta.py', 'barbican/api/controllers/secrets.py', 'barbican/common/policies/secretmeta.py'], 'language': ['Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('7d0bc194-cc4f-40f1-a96d-fbd50318389e'), UUID('ca9cff81-0050-4a46-9314-2a05874968b8'), UUID('07580f8e-2fbe-4521-b55a-8589933a85cf'), UUID('824e486f-96d7-4a91-b447-24c48d863a08')]}\n",
      "ERROR:root:Error in {'repo': 'openstack/barbican', 'vulnerability_id': '2022-23451', 'commit': '7d270bacbe29a90a10f1855abc3b50dac0f08022', 'commit_source': 'github', 'cwe_id': ['CWE-863'], 'patch': ['@@ -82,6 +82,9 @@\\n         name=\\'secret_project_creator\\',\\n         check_str=\"rule:creator and rule:secret_project_match and \" +\\n                   \"rule:secret_creator_user\"),\\n+    policy.RuleDefault(\\n+        name=\\'secret_project_creator_role\\',\\n+        check_str=\"rule:creator and rule:secret_project_match\"),\\n     policy.RuleDefault(\\n         name=\\'container_project_admin\\',\\n         check_str=\"rule:admin and rule:container_project_match\"),', '@@ -28,7 +28,7 @@ def _secret_metadata_not_found():\\n     pecan.abort(404, u._(\\'Secret metadata not found.\\'))\\n \\n \\n-class SecretMetadataController(controllers.ACLMixin):\\n+class SecretMetadataController(controllers.SecretACLMixin):\\n     \"\"\"Handles SecretMetadata requests by a given secret id.\"\"\"\\n \\n     def __init__(self, secret):\\n@@ -106,7 +106,7 @@ def on_post(self, external_project_id, **kwargs):\\n         return {\\'key\\': key, \\'value\\': value}\\n \\n \\n-class SecretMetadatumController(controllers.ACLMixin):\\n+class SecretMetadatumController(controllers.SecretACLMixin):\\n \\n     def __init__(self, secret):\\n         LOG.debug(\\'=== Creating SecretMetadatumController ===\\')', '@@ -71,7 +71,7 @@ def _request_has_twsk_but_no_transport_key_id():\\n                          \\'transport key id has not been provided.\\'))\\n \\n \\n-class SecretController(controllers.ACLMixin):\\n+class SecretController(controllers.SecretACLMixin):\\n     \"\"\"Handles Secret retrieval and deletion requests.\"\"\"\\n \\n     def __init__(self, secret):\\n@@ -81,12 +81,6 @@ def __init__(self, secret):\\n         self.consumer_repo = repo.get_secret_consumer_repository()\\n         self.transport_key_repo = repo.get_transport_key_repository()\\n \\n-    def get_acl_tuple(self, req, **kwargs):\\n-        d = self.get_acl_dict_for_user(req, self.secret.secret_acls)\\n-        d[\\'project_id\\'] = self.secret.project.external_id\\n-        d[\\'creator_id\\'] = self.secret.creator_id\\n-        return \\'secret\\', d\\n-\\n     @pecan.expose()\\n     def _lookup(self, sub_resource, *remainder):\\n         if sub_resource == \\'acl\\':', \"@@ -17,7 +17,10 @@\\n rules = [\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:get',\\n-        check_str=f'rule:all_but_audit or {_MEMBER}',\\n+        check_str='rule:secret_non_private_read or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  'rule:secret_project_admin or rule:secret_acl_read or ' +\\n+                  f'{_MEMBER}',\\n         scope_types=['project'],\\n         description='metadata/: Lists a secrets user-defined metadata. || ' +\\n                     'metadata/{key}: Retrieves a secrets user-added metadata.',\\n@@ -34,7 +37,10 @@\\n     ),\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:post',\\n-        check_str=f'rule:admin_or_creator or {_MEMBER}',\\n+        check_str='rule:secret_project_admin or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  '(rule:secret_project_creator_role and ' +\\n+                  f'rule:secret_non_private_read) or {_MEMBER}',\\n         scope_types=['project'],\\n         description='Adds a new key/value pair to the secrets user-defined ' +\\n                     'metadata.',\\n@@ -47,7 +53,10 @@\\n     ),\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:put',\\n-        check_str=f'rule:admin_or_creator or {_MEMBER}',\\n+        check_str='rule:secret_project_admin or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  '(rule:secret_project_creator_role and ' +\\n+                  f'rule:secret_non_private_read) or {_MEMBER}',\\n         scope_types=['project'],\\n         description='metadata/: Sets the user-defined metadata for a secret ' +\\n                     '|| metadata/{key}: Updates an existing key/value pair ' +\\n@@ -65,7 +74,10 @@\\n     ),\\n     policy.DocumentedRuleDefault(\\n         name='secret_meta:delete',\\n-        check_str=f'rule:admin_or_creator or {_MEMBER}',\\n+        check_str='rule:secret_project_admin or ' +\\n+                  'rule:secret_project_creator or ' +\\n+                  '(rule:secret_project_creator_role and ' +\\n+                  f'rule:secret_non_private_read) or {_MEMBER}',\\n         scope_types=['project'],\\n         description='Delete secret user-defined metadata by key.',\\n         operations=[\"], 'file': ['barbican/common/policies/base.py', 'barbican/api/controllers/secretmeta.py', 'barbican/api/controllers/secrets.py', 'barbican/common/policies/secretmeta.py'], 'language': ['Python', 'Python', 'Python', 'Python'], 'temp_id': [UUID('7d0bc194-cc4f-40f1-a96d-fbd50318389e'), UUID('ca9cff81-0050-4a46-9314-2a05874968b8'), UUID('07580f8e-2fbe-4521-b55a-8589933a85cf'), UUID('824e486f-96d7-4a91-b447-24c48d863a08')]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 103, in clear_file_content\n",
      "    new_content = format_str(new_content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 4:0: class SecretMetadatumController(controllers.SecretACLMixin):\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/cy/xkclg4xd0gjglf9yxn33fv6r0000gn/T/ipykernel_17648/250454671.py\", line 27, in <module>\n",
      "    get_changes(commit_data_row)\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 197, in get_changes\n",
      "    code_context_after_fix = clear_file_content(code_unit_after_fix)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/somen/Zavodi/unik/open_source_code_vulnerabilities_dataset/src/process_code_changes.py\", line 106, in clear_file_content\n",
      "    new_content = format_str(content, mode=FileMode())\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"src/black/__init__.py\", line 1208, in format_str\n",
      "  File \"src/black/__init__.py\", line 1222, in _format_str_once\n",
      "  File \"src/black/parsing.py\", line 98, in lib2to3_parse\n",
      "black.parsing.InvalidInput: Cannot parse: 6:0: <line number missing in source>\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1800/1800 [20:31<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "if PYTHON_CODE_FIXES_DATA_PATH.exists():\n",
    "    print(\"Reading code fixes\")\n",
    "    code_unit_changes = pl.read_parquet(PYTHON_CODE_FIXES_DATA_PATH).to_dicts()\n",
    "else:\n",
    "    code_unit_changes: list[dict[str, Any]] = []\n",
    "\n",
    "repos: dict[str, Repo] = {}\n",
    "\n",
    "grouped_vulnerabilities = (\n",
    "    python_vulnerability_fixes.group_by(\n",
    "        \"repo\", \"vulnerability_id\", \"commit\", \"commit_source\", \"cwe_id\"\n",
    "    )\n",
    "    .agg(pl.col(\"patch\"), pl.col(\"file\"), pl.col(\"language\"))\n",
    "    .sample(fraction=1, shuffle=True)\n",
    ")\n",
    "errors: list[dict[str, Any]] = []\n",
    "checked_commits = set([change[\"commit\"] for change in code_unit_changes])\n",
    "\n",
    "\n",
    "vulnerabilities_to_check = grouped_vulnerabilities.to_dicts()\n",
    "\n",
    "\n",
    "for commit_data_row in tqdm(vulnerabilities_to_check):\n",
    "    if commit_data_row[\"commit\"] in checked_commits:\n",
    "        continue\n",
    "    try:\n",
    "        get_changes(commit_data_row)\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error in {commit_data_row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_code_unit_changes = []\n",
    "new_code_context_changes = []\n",
    "\n",
    "for commit_data_file in PYTHON_CODE_UNITS_DATA_PATH.rglob(\"*.json\"):\n",
    "    try:\n",
    "        with commit_data_file.open() as f:\n",
    "            new_code_unit_changes.append(json.load(f))\n",
    "    except:\n",
    "        print(commit_data_file)\n",
    "        commit_data_file.unlink()\n",
    "for commit_data_file in PYTHON_CODE_CONTEXT_DATA_PATH.rglob(\"*.json\"):\n",
    "    try:\n",
    "        with commit_data_file.open() as f:\n",
    "            new_code_context_changes.append(json.load(f))\n",
    "    except:\n",
    "        print(commit_data_file)\n",
    "        commit_data_file.unlink()\n",
    "\n",
    "pl.DataFrame(new_code_unit_changes).write_parquet(PYTHON_CODE_FIXES_DATA_PATH)\n",
    "pl.DataFrame(new_code_context_changes).write_parquet(PYTHON_CODE_FIXES_WITH_CONTEXT_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1140939597315436"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.read_parquet(PYTHON_CODE_FIXES_DATA_PATH).unique(\"vulnerability_id\").shape[0] / grouped_vulnerabilities.unique(\"vulnerability_id\").shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>commit</th><th>repo</th><th>new_file</th><th>patch</th><th>code_unit_after_fix</th><th>vulnerability_id</th><th>cwe_id</th><th>old_file</th><th>code_unit_before_fix</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;4401&quot;</td><td>&quot;4401&quot;</td><td>&quot;4401&quot;</td><td>&quot;4401&quot;</td><td>&quot;4401&quot;</td><td>&quot;4401&quot;</td><td>4401.0</td><td>&quot;4401&quot;</td><td>&quot;4401&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;001b0634cd309e372edb6d7d95d083â€¦</td><td>&quot;389ds/389-ds-base&quot;</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;@@ -1 +1 @@\n",
       "-__version__ = &quot;1.â€¦</td><td>&quot;&quot;</td><td>&quot;2013-0208&quot;</td><td>null</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;ffc095a3e5acc1c404773a0510e6d0â€¦</td><td>&quot;zwczou/weixin-python&quot;</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;@@ -99,7 +99,9 @@ extern &quot;C&quot; {â€¦</td><td>&quot;} else {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;actionsTemplaâ€¦</td><td>&quot;GHSA-x563-6hqv-26mr&quot;</td><td>null</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;},\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;scope&quot;: {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† commit     â”† repo      â”† new_file  â”† â€¦ â”† vulnerabi â”† cwe_id â”† old_file  â”† code_unit â”‚\n",
       "â”‚ ---        â”† ---        â”† ---       â”† ---       â”†   â”† lity_id   â”† ---    â”† ---       â”† _before_f â”‚\n",
       "â”‚ str        â”† str        â”† str       â”† str       â”†   â”† ---       â”† f64    â”† str       â”† ix        â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”† str       â”†        â”†           â”† ---       â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”†           â”†        â”†           â”† str       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 4401       â”† 4401      â”† 4401      â”† â€¦ â”† 4401      â”† 4401.0 â”† 4401      â”† 4401      â”‚\n",
       "â”‚ null_count â”† 0          â”† 0         â”† 0         â”† â€¦ â”† 0         â”† 0.0    â”† 0         â”† 0         â”‚\n",
       "â”‚ mean       â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ std        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ min        â”† 001b0634cd â”† 389ds/389 â”† .devconta â”† â€¦ â”† 2013-0208 â”† null   â”† .devconta â”†           â”‚\n",
       "â”‚            â”† 309e372edb â”† -ds-base  â”† iner/libr â”†   â”†           â”†        â”† iner/libr â”†           â”‚\n",
       "â”‚            â”† 6d7d95d083 â”†           â”† ary-scrip â”†   â”†           â”†        â”† ary-scrip â”†           â”‚\n",
       "â”‚            â”† â€¦          â”†           â”† ts/â€¦      â”†   â”†           â”†        â”† ts/â€¦      â”†           â”‚\n",
       "â”‚ 25%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 50%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 75%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ max        â”† ffc095a3e5 â”† zwczou/we â”† zproject/ â”† â€¦ â”† GHSA-x563 â”† null   â”† zproject/ â”† },        â”‚\n",
       "â”‚            â”† acc1c40477 â”† ixin-pyth â”† urls.py   â”†   â”† -6hqv-26m â”†        â”† urls.py   â”† \"scope\":  â”‚\n",
       "â”‚            â”† 3a0510e6d0 â”† on        â”†           â”†   â”† r         â”†        â”†           â”† {         â”‚\n",
       "â”‚            â”† â€¦          â”†           â”†           â”†   â”†           â”†        â”†           â”†         â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_unit_changes_df = pl.read_parquet(PYTHON_CODE_FIXES_DATA_PATH)\n",
    "code_unit_changes_df = code_unit_changes_df.filter(\n",
    "    # pl.col(\"vulnerability_id\").is_in(excluded_vulns).not_(),\n",
    "    pl.col(\"new_file\") != \"setup.py\",\n",
    "    pl.col(\"old_file\") != \"setup.py\"\n",
    ").with_columns(\n",
    "    pl.col(\"code_unit_after_fix\").str.replace_all(r\"\\n\\s*\\n\", \"\\n\").str.strip_chars(),\n",
    "    pl.col(\"code_unit_before_fix\").str.replace_all(r\"\\n\\s*\\n\", \"\\n\").str.strip_chars(),\n",
    ")\n",
    "code_unit_changes_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>commit</th><th>repo</th><th>new_file</th><th>patch</th><th>code_unit_after_fix</th><th>vulnerability_id</th><th>cwe_id</th><th>old_file</th><th>code_unit_before_fix</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;4378&quot;</td><td>&quot;4378&quot;</td><td>&quot;4378&quot;</td><td>&quot;4378&quot;</td><td>&quot;4378&quot;</td><td>&quot;4378&quot;</td><td>4378.0</td><td>&quot;4378&quot;</td><td>&quot;4378&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;001b0634cd309e372edb6d7d95d083â€¦</td><td>&quot;389ds/389-ds-base&quot;</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;@@ -1 +1 @@\n",
       "-__version__ = &quot;1.â€¦</td><td>&quot;&quot;</td><td>&quot;2013-0208&quot;</td><td>null</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;ffc095a3e5acc1c404773a0510e6d0â€¦</td><td>&quot;zwczou/weixin-python&quot;</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;@@ -99,7 +99,9 @@ extern &quot;C&quot; {â€¦</td><td>&quot;} else {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;actionsTemplaâ€¦</td><td>&quot;GHSA-x563-6hqv-26mr&quot;</td><td>null</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;},\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;scope&quot;: {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† commit     â”† repo      â”† new_file  â”† â€¦ â”† vulnerabi â”† cwe_id â”† old_file  â”† code_unit â”‚\n",
       "â”‚ ---        â”† ---        â”† ---       â”† ---       â”†   â”† lity_id   â”† ---    â”† ---       â”† _before_f â”‚\n",
       "â”‚ str        â”† str        â”† str       â”† str       â”†   â”† ---       â”† f64    â”† str       â”† ix        â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”† str       â”†        â”†           â”† ---       â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”†           â”†        â”†           â”† str       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 4378       â”† 4378      â”† 4378      â”† â€¦ â”† 4378      â”† 4378.0 â”† 4378      â”† 4378      â”‚\n",
       "â”‚ null_count â”† 0          â”† 0         â”† 0         â”† â€¦ â”† 0         â”† 0.0    â”† 0         â”† 0         â”‚\n",
       "â”‚ mean       â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ std        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ min        â”† 001b0634cd â”† 389ds/389 â”† .devconta â”† â€¦ â”† 2013-0208 â”† null   â”† .devconta â”†           â”‚\n",
       "â”‚            â”† 309e372edb â”† -ds-base  â”† iner/libr â”†   â”†           â”†        â”† iner/libr â”†           â”‚\n",
       "â”‚            â”† 6d7d95d083 â”†           â”† ary-scrip â”†   â”†           â”†        â”† ary-scrip â”†           â”‚\n",
       "â”‚            â”† â€¦          â”†           â”† ts/â€¦      â”†   â”†           â”†        â”† ts/â€¦      â”†           â”‚\n",
       "â”‚ 25%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 50%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 75%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ max        â”† ffc095a3e5 â”† zwczou/we â”† zproject/ â”† â€¦ â”† GHSA-x563 â”† null   â”† zproject/ â”† },        â”‚\n",
       "â”‚            â”† acc1c40477 â”† ixin-pyth â”† urls.py   â”†   â”† -6hqv-26m â”†        â”† urls.py   â”† \"scope\":  â”‚\n",
       "â”‚            â”† 3a0510e6d0 â”† on        â”†           â”†   â”† r         â”†        â”†           â”† {         â”‚\n",
       "â”‚            â”† â€¦          â”†           â”†           â”†   â”†           â”†        â”†           â”†         â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_unit_changes_df = code_unit_changes_df.filter(\n",
    "    ((pl.col(\"code_unit_after_fix\").str.strip_chars() == \"\") & (pl.col(\"code_unit_before_fix\").str.strip_chars() == \"\")).not_(), \n",
    ")\n",
    "code_unit_changes_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-10861',\n",
       " '2018-5729',\n",
       " '2020-5224',\n",
       " '2021-41104',\n",
       " '2022-25882',\n",
       " '2022-31116',\n",
       " '2022-35999',\n",
       " '2022-41893',\n",
       " '2023-24817',\n",
       " '2023-46249',\n",
       " '2023-52266',\n",
       " '2023-6507',\n",
       " '2024-21485',\n",
       " '2024-32979',\n",
       " '2024-49377']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(code_unit_changes_df.with_columns(\n",
    "    pl.col(\"new_file\").str.split(\".\").list.last().alias(\"lang\")\n",
    ").group_by(\"vulnerability_id\").agg(\"lang\").with_columns(pl.col(\"lang\").list.unique()).unique(\"lang\").filter(pl.col(\"lang\").list.contains(\"py\").not_()).select(\"vulnerability_id\").to_series().to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate code detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578\n"
     ]
    }
   ],
   "source": [
    "concated_code_units = pl.concat(\n",
    "    [code_unit_changes_df.filter(pl.col(\"code_unit_after_fix\").str.strip_chars() != \"\").select(\n",
    "        pl.col(\"code_unit_after_fix\").alias(\"code_unit\"), pl.col(\"commit\"), pl.col(\"repo\")\n",
    "    ),\n",
    "    code_unit_changes_df.filter(pl.col(\"code_unit_before_fix\").str.strip_chars() != \"\").select(\n",
    "        pl.col(\"code_unit_before_fix\").alias(\"code_unit\"), pl.col(\"commit\"), pl.col(\"repo\")\n",
    "    )]\n",
    ")\n",
    "\n",
    "print(concated_code_units.group_by(\"code_unit\").agg(\"commit\", \"repo\").filter(pl.col(\"commit\").list.len() > 1).shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter duplicates in vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (9, 10)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ statistic  â”† commit     â”† repo      â”† new_file  â”† â€¦ â”† vulnerabi â”† cwe_id â”† old_file  â”† code_unit â”‚\n",
      "â”‚ ---        â”† ---        â”† ---       â”† ---       â”†   â”† lity_id   â”† ---    â”† ---       â”† _before_f â”‚\n",
      "â”‚ str        â”† str        â”† str       â”† str       â”†   â”† ---       â”† f64    â”† str       â”† ix        â”‚\n",
      "â”‚            â”†            â”†           â”†           â”†   â”† str       â”†        â”†           â”† ---       â”‚\n",
      "â”‚            â”†            â”†           â”†           â”†   â”†           â”†        â”†           â”† str       â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ count      â”† 3769       â”† 3769      â”† 3769      â”† â€¦ â”† 3769      â”† 3769.0 â”† 3769      â”† 3769      â”‚\n",
      "â”‚ null_count â”† 0          â”† 0         â”† 0         â”† â€¦ â”† 0         â”† 0.0    â”† 0         â”† 0         â”‚\n",
      "â”‚ mean       â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
      "â”‚ std        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
      "â”‚ min        â”† 001b0634cd â”† 389ds/389 â”† .devconta â”† â€¦ â”† 2013-0208 â”† null   â”† .devconta â”†           â”‚\n",
      "â”‚            â”† 309e372edb â”† -ds-base  â”† iner/libr â”†   â”†           â”†        â”† iner/libr â”†           â”‚\n",
      "â”‚            â”† 6d7d95d083 â”†           â”† ary-scrip â”†   â”†           â”†        â”† ary-scrip â”†           â”‚\n",
      "â”‚            â”† â€¦          â”†           â”† ts/â€¦      â”†   â”†           â”†        â”† ts/â€¦      â”†           â”‚\n",
      "â”‚ 25%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
      "â”‚ 50%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
      "â”‚ 75%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
      "â”‚ max        â”† ffc095a3e5 â”† zwczou/we â”† zproject/ â”† â€¦ â”† GHSA-x563 â”† null   â”† zproject/ â”† },        â”‚\n",
      "â”‚            â”† acc1c40477 â”† ixin-pyth â”† urls.py   â”†   â”† -6hqv-26m â”†        â”† urls.py   â”† \"scope\":  â”‚\n",
      "â”‚            â”† 3a0510e6d0 â”† on        â”†           â”†   â”† r         â”†        â”†           â”† {         â”‚\n",
      "â”‚            â”† â€¦          â”†           â”†           â”†   â”†           â”†        â”†           â”†         â€¦ â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "without_duplicates = code_unit_changes_df.unique([\"vulnerability_id\", \"code_unit_before_fix\", \"code_unit_after_fix\"])\n",
    "without_duplicates = without_duplicates.filter(\n",
    "    pl.col(\"code_unit_after_fix\").str.strip_chars() != pl.col(\"code_unit_before_fix\").str.strip_chars()\n",
    ")\n",
    "print(without_duplicates.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (41, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>vulnerability_id</th><th>code_unit_before_fix</th><th>vulnerability_id_right</th><th>code_unit_after_fix</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;2021-36219&quot;</td><td>&quot;#include &quot;SGXWALLET_VERSION&quot;&quot;</td><td>&quot;2021-36219&quot;</td><td>&quot;#include &quot;SGXWALLET_VERSION&quot;&quot;</td></tr><tr><td>&quot;2019-14859&quot;</td><td>&quot;def read_length(string):\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;iâ€¦</td><td>&quot;2019-14859&quot;</td><td>&quot;def read_length(string):\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;iâ€¦</td></tr><tr><td>&quot;2020-15142&quot;</td><td>&quot;def _sanitize(value: str) -&gt; sâ€¦</td><td>&quot;2020-15141&quot;</td><td>&quot;def _sanitize(value: str) -&gt; sâ€¦</td></tr><tr><td>&quot;2022-3363&quot;</td><td>&quot;def get_parser():\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;parser =â€¦</td><td>&quot;2022-4018&quot;</td><td>&quot;def get_parser():\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;parser =â€¦</td></tr><tr><td>&quot;2023-51449&quot;</td><td>&quot;def is_in_or_equal(path_1: strâ€¦</td><td>&quot;2023-34239&quot;</td><td>&quot;def is_in_or_equal(path_1: strâ€¦</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;2015-2317&quot;</td><td>&quot;def is_safe_url(url, host=Noneâ€¦</td><td>&quot;2015-0220&quot;</td><td>&quot;def is_safe_url(url, host=Noneâ€¦</td></tr><tr><td>&quot;2015-2317&quot;</td><td>&quot;def is_safe_url(url, host=Noneâ€¦</td><td>&quot;2015-0220&quot;</td><td>&quot;def is_safe_url(url, host=Noneâ€¦</td></tr><tr><td>&quot;2024-21649&quot;</td><td>&quot;def serialize(data: any) -&gt; byâ€¦</td><td>&quot;2023-23930&quot;</td><td>&quot;def serialize(data: any) -&gt; byâ€¦</td></tr><tr><td>&quot;2021-41194&quot;</td><td>&quot;class FirstUseAuthenticator(Auâ€¦</td><td>&quot;2021-41194&quot;</td><td>&quot;class FirstUseAuthenticator(Auâ€¦</td></tr><tr><td>&quot;2023-24580&quot;</td><td>&quot;class MultiPartParser:\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;defâ€¦</td><td>&quot;2022-23833&quot;</td><td>&quot;class MultiPartParser:\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;defâ€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (41, 4)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ vulnerability_id â”† code_unit_before_fix      â”† vulnerability_id_right â”† code_unit_after_fix      â”‚\n",
       "â”‚ ---              â”† ---                       â”† ---                    â”† ---                      â”‚\n",
       "â”‚ str              â”† str                       â”† str                    â”† str                      â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2021-36219       â”† #include                  â”† 2021-36219             â”† #include                 â”‚\n",
       "â”‚                  â”† \"SGXWALLET_VERSION\"       â”†                        â”† \"SGXWALLET_VERSION\"      â”‚\n",
       "â”‚ 2019-14859       â”† def read_length(string):  â”† 2019-14859             â”† def read_length(string): â”‚\n",
       "â”‚                  â”†     iâ€¦                    â”†                        â”†     iâ€¦                   â”‚\n",
       "â”‚ 2020-15142       â”† def _sanitize(value: str) â”† 2020-15141             â”† def _sanitize(value:     â”‚\n",
       "â”‚                  â”† -> sâ€¦                     â”†                        â”† str) -> sâ€¦               â”‚\n",
       "â”‚ 2022-3363        â”† def get_parser():         â”† 2022-4018              â”† def get_parser():        â”‚\n",
       "â”‚                  â”†     parser =â€¦             â”†                        â”†     parser =â€¦            â”‚\n",
       "â”‚ 2023-51449       â”† def                       â”† 2023-34239             â”† def                      â”‚\n",
       "â”‚                  â”† is_in_or_equal(path_1:    â”†                        â”† is_in_or_equal(path_1:   â”‚\n",
       "â”‚                  â”† strâ€¦                      â”†                        â”† strâ€¦                     â”‚\n",
       "â”‚ â€¦                â”† â€¦                         â”† â€¦                      â”† â€¦                        â”‚\n",
       "â”‚ 2015-2317        â”† def is_safe_url(url,      â”† 2015-0220              â”† def is_safe_url(url,     â”‚\n",
       "â”‚                  â”† host=Noneâ€¦                â”†                        â”† host=Noneâ€¦               â”‚\n",
       "â”‚ 2015-2317        â”† def is_safe_url(url,      â”† 2015-0220              â”† def is_safe_url(url,     â”‚\n",
       "â”‚                  â”† host=Noneâ€¦                â”†                        â”† host=Noneâ€¦               â”‚\n",
       "â”‚ 2024-21649       â”† def serialize(data: any)  â”† 2023-23930             â”† def serialize(data: any) â”‚\n",
       "â”‚                  â”† -> byâ€¦                    â”†                        â”† -> byâ€¦                   â”‚\n",
       "â”‚ 2021-41194       â”† class                     â”† 2021-41194             â”† class FirstUseAuthentica â”‚\n",
       "â”‚                  â”† FirstUseAuthenticator(Auâ€¦ â”†                        â”† tor(Auâ€¦                  â”‚\n",
       "â”‚ 2023-24580       â”† class MultiPartParser:    â”† 2022-23833             â”† class MultiPartParser:   â”‚\n",
       "â”‚                  â”†     defâ€¦                  â”†                        â”†     defâ€¦                 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Looking for code units that are the same before and after the fix\n",
    "\n",
    "inconsistent_code_units = without_duplicates.select(\"vulnerability_id\", \"code_unit_before_fix\").filter(\n",
    "    pl.col(\"code_unit_before_fix\").str.strip_chars() != \"\"\n",
    ").join(\n",
    "    without_duplicates.select(\"vulnerability_id\", \"code_unit_after_fix\").filter(\n",
    "        pl.col(\"code_unit_after_fix\").str.strip_chars() != \"\"\n",
    "    ), \n",
    "    how=\"cross\"\n",
    ").filter(pl.col(\"code_unit_before_fix\") == pl.col(\"code_unit_after_fix\"))\n",
    "\n",
    "inconsistent_code_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix and check consistency\n",
    "\n",
    "Usually, there are a few vulnerabilities in code unit. Some of them was fixed later.\n",
    "There aren't so much bad cases, so we can just drop inconsistent rows\n",
    "\n",
    "Code examples on right have hidden vulnerability. We can exclude such vulnerabilities from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>commit</th><th>repo</th><th>new_file</th><th>patch</th><th>code_unit_after_fix</th><th>vulnerability_id</th><th>cwe_id</th><th>old_file</th><th>code_unit_before_fix</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;3604&quot;</td><td>&quot;3604&quot;</td><td>&quot;3604&quot;</td><td>&quot;3604&quot;</td><td>&quot;3604&quot;</td><td>&quot;3604&quot;</td><td>3604.0</td><td>&quot;3604&quot;</td><td>&quot;3604&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;001b0634cd309e372edb6d7d95d083â€¦</td><td>&quot;389ds/389-ds-base&quot;</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;@@ -1 +1 @@\n",
       "-__version__ = &quot;1.â€¦</td><td>&quot;&quot;</td><td>&quot;2013-0208&quot;</td><td>null</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;ffc095a3e5acc1c404773a0510e6d0â€¦</td><td>&quot;zwczou/weixin-python&quot;</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;@@ -99,7 +99,9 @@ extern &quot;C&quot; {â€¦</td><td>&quot;} else {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;actionsTemplaâ€¦</td><td>&quot;GHSA-x563-6hqv-26mr&quot;</td><td>null</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;},\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;scope&quot;: {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† commit     â”† repo      â”† new_file  â”† â€¦ â”† vulnerabi â”† cwe_id â”† old_file  â”† code_unit â”‚\n",
       "â”‚ ---        â”† ---        â”† ---       â”† ---       â”†   â”† lity_id   â”† ---    â”† ---       â”† _before_f â”‚\n",
       "â”‚ str        â”† str        â”† str       â”† str       â”†   â”† ---       â”† f64    â”† str       â”† ix        â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”† str       â”†        â”†           â”† ---       â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”†           â”†        â”†           â”† str       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 3604       â”† 3604      â”† 3604      â”† â€¦ â”† 3604      â”† 3604.0 â”† 3604      â”† 3604      â”‚\n",
       "â”‚ null_count â”† 0          â”† 0         â”† 0         â”† â€¦ â”† 0         â”† 0.0    â”† 0         â”† 0         â”‚\n",
       "â”‚ mean       â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ std        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ min        â”† 001b0634cd â”† 389ds/389 â”† .devconta â”† â€¦ â”† 2013-0208 â”† null   â”† .devconta â”†           â”‚\n",
       "â”‚            â”† 309e372edb â”† -ds-base  â”† iner/libr â”†   â”†           â”†        â”† iner/libr â”†           â”‚\n",
       "â”‚            â”† 6d7d95d083 â”†           â”† ary-scrip â”†   â”†           â”†        â”† ary-scrip â”†           â”‚\n",
       "â”‚            â”† â€¦          â”†           â”† ts/â€¦      â”†   â”†           â”†        â”† ts/â€¦      â”†           â”‚\n",
       "â”‚ 25%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 50%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 75%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ max        â”† ffc095a3e5 â”† zwczou/we â”† zproject/ â”† â€¦ â”† GHSA-x563 â”† null   â”† zproject/ â”† },        â”‚\n",
       "â”‚            â”† acc1c40477 â”† ixin-pyth â”† urls.py   â”†   â”† -6hqv-26m â”†        â”† urls.py   â”† \"scope\":  â”‚\n",
       "â”‚            â”† 3a0510e6d0 â”† on        â”†           â”†   â”† r         â”†        â”†           â”† {         â”‚\n",
       "â”‚            â”† â€¦          â”†           â”†           â”†   â”†           â”†        â”†           â”†         â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inconsistent_vulns = set(inconsistent_code_units.select(\"vulnerability_id_right\").to_series().to_list())\n",
    "consistency_fix_vulnerabilities = without_duplicates.filter(\n",
    "    pl.col(\"vulnerability_id\").is_in(inconsistent_vulns).not_()\n",
    ")\n",
    "consistency_fix_vulnerabilities.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_fix_vulnerabilities.select(\"vulnerability_id\", \"code_unit_before_fix\").filter(\n",
    "    pl.col(\"code_unit_before_fix\").str.strip_chars() != \"\"\n",
    ").join(\n",
    "    consistency_fix_vulnerabilities.select(\"vulnerability_id\", \"code_unit_after_fix\").filter(\n",
    "        pl.col(\"code_unit_after_fix\").str.strip_chars() != \"\"\n",
    "    ), \n",
    "    how=\"cross\"\n",
    ").filter(pl.col(\"code_unit_before_fix\") == pl.col(\"code_unit_after_fix\")).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced filtering with duplicate searcher tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4252202"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_data = without_duplicates.slice(offset=0, length=100)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    for row in slice_data.iter_rows(named=True):\n",
    "        path = Path(f\"{temp_dir}/new_data/{row['commit']}/{row['new_file']}\")\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(row[\"code_unit_after_fix\"])\n",
    "            \n",
    "        path = Path(f\"{temp_dir}/old_data/{row['commit']}/{row['old_file']}\")\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(row[\"code_unit_before_fix\"])\n",
    "\n",
    "    similarity = json.loads(subprocess.getoutput(\n",
    "        \"cd /Users/somen/Zavodi/unik/duplicate-code-detection-tool/ && \"\n",
    "        \"source .venv/bin/activate && \"\n",
    "        f\"python -W ignore duplicate_code_detection.py --json True --file-extensions py -d {temp_dir}/new_data/ {temp_dir}/old_data/\",\n",
    "    ))\n",
    "\n",
    "SIMILARITY_DATA_PATH = Path(\"data/similarity.json\")\n",
    "SIMILARITY_DATA_PATH.write_text(json.dumps(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>path_1</th><th>path_2</th><th>similarity</th><th>commit_1</th><th>file_path_1</th><th>is_new_1</th><th>commit_2</th><th>file_path_2</th><th>is_new_2</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;28056&quot;</td><td>&quot;28056&quot;</td><td>28056.0</td><td>&quot;28056&quot;</td><td>&quot;28056&quot;</td><td>28056.0</td><td>&quot;28056&quot;</td><td>&quot;28056&quot;</td><td>28056.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>2.466187</td><td>null</td><td>null</td><td>0.5</td><td>null</td><td>null</td><td>0.5</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>6.643144</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>0.0</td><td>&quot;0bdcf656d469e5f675cb56fd644d82â€¦</td><td>&quot;CalendarinhoApp/authenticationâ€¦</td><td>0.0</td><td>&quot;0bdcf656d469e5f675cb56fd644d82â€¦</td><td>&quot;CalendarinhoApp/authenticationâ€¦</td><td>0.0</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>0.41</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>1.38</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>2.87</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>100.0</td><td>&quot;fab24457bcf8ede882abd11419769câ€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>1.0</td><td>&quot;fab24457bcf8ede882abd11419769câ€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>1.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† path_1    â”† path_2    â”† similarit â”† â€¦ â”† is_new_1 â”† commit_2  â”† file_path â”† is_new_2 â”‚\n",
       "â”‚ ---        â”† ---       â”† ---       â”† y         â”†   â”† ---      â”† ---       â”† _2        â”† ---      â”‚\n",
       "â”‚ str        â”† str       â”† str       â”† ---       â”†   â”† f64      â”† str       â”† ---       â”† f64      â”‚\n",
       "â”‚            â”†           â”†           â”† f64       â”†   â”†          â”†           â”† str       â”†          â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 28056     â”† 28056     â”† 28056.0   â”† â€¦ â”† 28056.0  â”† 28056     â”† 28056     â”† 28056.0  â”‚\n",
       "â”‚ null_count â”† 0         â”† 0         â”† 0.0       â”† â€¦ â”† 0.0      â”† 0         â”† 0         â”† 0.0      â”‚\n",
       "â”‚ mean       â”† null      â”† null      â”† 2.466187  â”† â€¦ â”† 0.5      â”† null      â”† null      â”† 0.5      â”‚\n",
       "â”‚ std        â”† null      â”† null      â”† 6.643144  â”† â€¦ â”† null     â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ min        â”† /var/fold â”† /var/fold â”† 0.0       â”† â€¦ â”† 0.0      â”† 0bdcf656d â”† Calendari â”† 0.0      â”‚\n",
       "â”‚            â”† ers/cy/xk â”† ers/cy/xk â”†           â”†   â”†          â”† 469e5f675 â”† nhoApp/au â”†          â”‚\n",
       "â”‚            â”† clg4xd0gj â”† clg4xd0gj â”†           â”†   â”†          â”† cb56fd644 â”† thenticat â”†          â”‚\n",
       "â”‚            â”† glfâ€¦      â”† glfâ€¦      â”†           â”†   â”†          â”† d82â€¦      â”† ionâ€¦      â”†          â”‚\n",
       "â”‚ 25%        â”† null      â”† null      â”† 0.41      â”† â€¦ â”† null     â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 50%        â”† null      â”† null      â”† 1.38      â”† â€¦ â”† null     â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ 75%        â”† null      â”† null      â”† 2.87      â”† â€¦ â”† null     â”† null      â”† null      â”† null     â”‚\n",
       "â”‚ max        â”† /var/fold â”† /var/fold â”† 100.0     â”† â€¦ â”† 1.0      â”† fab24457b â”† zerver/vi â”† 1.0      â”‚\n",
       "â”‚            â”† ers/cy/xk â”† ers/cy/xk â”†           â”†   â”†          â”† cf8ede882 â”† ews/regis â”†          â”‚\n",
       "â”‚            â”† clg4xd0gj â”† clg4xd0gj â”†           â”†   â”†          â”† abd114197 â”† tration.p â”†          â”‚\n",
       "â”‚            â”† glfâ€¦      â”† glfâ€¦      â”†           â”†   â”†          â”† 69câ€¦      â”† y         â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = json.loads(SIMILARITY_DATA_PATH.read_text())\n",
    "similarity = pl.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"path_1\": left_path, \n",
    "            \"path_2\": right_path, \n",
    "            \"similarity\": value\n",
    "        }  \n",
    "        for left_path, values in similarity.items() for right_path, value in values.items()\n",
    "    ]\n",
    ")\n",
    "similarity = similarity.with_columns(\n",
    "    # THIS LINES ARE NOT CROSS PLATFORM. Change index in .get(...) call accordingly to your own path\n",
    "    pl.col(\"path_1\").str.split(\"/\").list.get(8).alias(\"commit_1\"), \n",
    "    pl.col(\"path_1\").str.split(\"/\").list.slice(9).list.join(\"/\").alias(\"file_path_1\"), \n",
    "    (pl.col(\"path_1\").str.split(\"/\").list.get(7) == \"new_data\").alias(\"is_new_1\"),\n",
    "    pl.col(\"path_2\").str.split(\"/\").list.get(8).alias(\"commit_2\"),\n",
    "    pl.col(\"path_2\").str.split(\"/\").list.slice(9).list.join(\"/\").alias(\"file_path_2\"),\n",
    "    (pl.col(\"path_2\").str.split(\"/\").list.get(7) == \"new_data\").alias(\"is_new_2\"),\n",
    ")\n",
    "\n",
    "similarity.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
      "â”‚ similarity â”† len â”‚\n",
      "â”‚ ---        â”† --- â”‚\n",
      "â”‚ i64        â”† u32 â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
      "â”‚ 84         â”† 2   â”‚\n",
      "â”‚ 54         â”† 2   â”‚\n",
      "â”‚ 82         â”† 2   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "print(similarity.filter(\n",
    "    (pl.col(\"is_new_1\").and_(pl.col(\"is_new_2\"))).or_(pl.col(\"is_new_1\").not_().and_(pl.col(\"is_new_2\").not_())), \n",
    "    pl.col(\"commit_1\") != pl.col(\"commit_2\")\n",
    ").filter(pl.col(\"similarity\") > 50).with_columns(\n",
    "    pl.col(\"similarity\").cast(pl.Int64)\n",
    ").group_by(\"similarity\").agg(pl.len()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>path_1</th><th>path_2</th><th>similarity</th><th>commit_1</th><th>file_path_1</th><th>is_new_1</th><th>commit_2</th><th>file_path_2</th><th>is_new_2</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>bool</td><td>str</td><td>str</td><td>bool</td></tr></thead><tbody><tr><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>54.58</td><td>&quot;2fcb4998887959b4fa11894a068d68â€¦</td><td>&quot;src/deployment/deploy.py&quot;</td><td>true</td><td>&quot;ee98e5af78ec60db8a17fef6ea0ca2â€¦</td><td>&quot;rdiffweb/core/config.py&quot;</td><td>true</td></tr><tr><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>82.92</td><td>&quot;960d736e55cbb9386a68e4ee45f805â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>true</td><td>&quot;a014ef75a3a0ed7f24ebb157632ba5â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>true</td></tr><tr><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>82.92</td><td>&quot;a014ef75a3a0ed7f24ebb157632ba5â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>true</td><td>&quot;960d736e55cbb9386a68e4ee45f805â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>true</td></tr><tr><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>54.58</td><td>&quot;ee98e5af78ec60db8a17fef6ea0ca2â€¦</td><td>&quot;rdiffweb/core/config.py&quot;</td><td>true</td><td>&quot;2fcb4998887959b4fa11894a068d68â€¦</td><td>&quot;src/deployment/deploy.py&quot;</td><td>true</td></tr><tr><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>84.39</td><td>&quot;960d736e55cbb9386a68e4ee45f805â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>false</td><td>&quot;a014ef75a3a0ed7f24ebb157632ba5â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>false</td></tr><tr><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>&quot;/var/folders/cy/xkclg4xd0gjglfâ€¦</td><td>84.39</td><td>&quot;a014ef75a3a0ed7f24ebb157632ba5â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>false</td><td>&quot;960d736e55cbb9386a68e4ee45f805â€¦</td><td>&quot;zerver/views/registration.py&quot;</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ path_1     â”† path_2    â”† similarit â”† commit_1  â”† â€¦ â”† is_new_1 â”† commit_2  â”† file_path â”† is_new_2 â”‚\n",
       "â”‚ ---        â”† ---       â”† y         â”† ---       â”†   â”† ---      â”† ---       â”† _2        â”† ---      â”‚\n",
       "â”‚ str        â”† str       â”† ---       â”† str       â”†   â”† bool     â”† str       â”† ---       â”† bool     â”‚\n",
       "â”‚            â”†           â”† f64       â”†           â”†   â”†          â”†           â”† str       â”†          â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ /var/folde â”† /var/fold â”† 54.58     â”† 2fcb49988 â”† â€¦ â”† true     â”† ee98e5af7 â”† rdiffweb/ â”† true     â”‚\n",
       "â”‚ rs/cy/xkcl â”† ers/cy/xk â”†           â”† 87959b4fa â”†   â”†          â”† 8ec60db8a â”† core/conf â”†          â”‚\n",
       "â”‚ g4xd0gjglf â”† clg4xd0gj â”†           â”† 11894a068 â”†   â”†          â”† 17fef6ea0 â”† ig.py     â”†          â”‚\n",
       "â”‚ â€¦          â”† glfâ€¦      â”†           â”† d68â€¦      â”†   â”†          â”† ca2â€¦      â”†           â”†          â”‚\n",
       "â”‚ /var/folde â”† /var/fold â”† 82.92     â”† 960d736e5 â”† â€¦ â”† true     â”† a014ef75a â”† zerver/vi â”† true     â”‚\n",
       "â”‚ rs/cy/xkcl â”† ers/cy/xk â”†           â”† 5cbb9386a â”†   â”†          â”† 3a0ed7f24 â”† ews/regis â”†          â”‚\n",
       "â”‚ g4xd0gjglf â”† clg4xd0gj â”†           â”† 68e4ee45f â”†   â”†          â”† ebb157632 â”† tration.p â”†          â”‚\n",
       "â”‚ â€¦          â”† glfâ€¦      â”†           â”† 805â€¦      â”†   â”†          â”† ba5â€¦      â”† y         â”†          â”‚\n",
       "â”‚ /var/folde â”† /var/fold â”† 82.92     â”† a014ef75a â”† â€¦ â”† true     â”† 960d736e5 â”† zerver/vi â”† true     â”‚\n",
       "â”‚ rs/cy/xkcl â”† ers/cy/xk â”†           â”† 3a0ed7f24 â”†   â”†          â”† 5cbb9386a â”† ews/regis â”†          â”‚\n",
       "â”‚ g4xd0gjglf â”† clg4xd0gj â”†           â”† ebb157632 â”†   â”†          â”† 68e4ee45f â”† tration.p â”†          â”‚\n",
       "â”‚ â€¦          â”† glfâ€¦      â”†           â”† ba5â€¦      â”†   â”†          â”† 805â€¦      â”† y         â”†          â”‚\n",
       "â”‚ /var/folde â”† /var/fold â”† 54.58     â”† ee98e5af7 â”† â€¦ â”† true     â”† 2fcb49988 â”† src/deplo â”† true     â”‚\n",
       "â”‚ rs/cy/xkcl â”† ers/cy/xk â”†           â”† 8ec60db8a â”†   â”†          â”† 87959b4fa â”† yment/dep â”†          â”‚\n",
       "â”‚ g4xd0gjglf â”† clg4xd0gj â”†           â”† 17fef6ea0 â”†   â”†          â”† 11894a068 â”† loy.py    â”†          â”‚\n",
       "â”‚ â€¦          â”† glfâ€¦      â”†           â”† ca2â€¦      â”†   â”†          â”† d68â€¦      â”†           â”†          â”‚\n",
       "â”‚ /var/folde â”† /var/fold â”† 84.39     â”† 960d736e5 â”† â€¦ â”† false    â”† a014ef75a â”† zerver/vi â”† false    â”‚\n",
       "â”‚ rs/cy/xkcl â”† ers/cy/xk â”†           â”† 5cbb9386a â”†   â”†          â”† 3a0ed7f24 â”† ews/regis â”†          â”‚\n",
       "â”‚ g4xd0gjglf â”† clg4xd0gj â”†           â”† 68e4ee45f â”†   â”†          â”† ebb157632 â”† tration.p â”†          â”‚\n",
       "â”‚ â€¦          â”† glfâ€¦      â”†           â”† 805â€¦      â”†   â”†          â”† ba5â€¦      â”† y         â”†          â”‚\n",
       "â”‚ /var/folde â”† /var/fold â”† 84.39     â”† a014ef75a â”† â€¦ â”† false    â”† 960d736e5 â”† zerver/vi â”† false    â”‚\n",
       "â”‚ rs/cy/xkcl â”† ers/cy/xk â”†           â”† 3a0ed7f24 â”†   â”†          â”† 5cbb9386a â”† ews/regis â”†          â”‚\n",
       "â”‚ g4xd0gjglf â”† clg4xd0gj â”†           â”† ebb157632 â”†   â”†          â”† 68e4ee45f â”† tration.p â”†          â”‚\n",
       "â”‚ â€¦          â”† glfâ€¦      â”†           â”† ba5â€¦      â”†   â”†          â”† 805â€¦      â”† y         â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity.filter(\n",
    "    (pl.col(\"is_new_1\").and_(pl.col(\"is_new_2\"))).or_(pl.col(\"is_new_1\").not_().and_(pl.col(\"is_new_2\").not_())), \n",
    "    pl.col(\"commit_1\") != pl.col(\"commit_2\")\n",
    ").filter(pl.col(\"similarity\") > 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "commits_to_exclude: set[str] = {\n",
    "    # Duplicate with 800828887a0509ad1162d6d407e94d8de7eafc60\n",
    "    \"2c09e68ec911919360d5f8502cefc312f9e03c5d\",\n",
    "    # duplicate with e1592e0f26302e79856cc7f2218ae848ae19b0f6\n",
    "    \"4cb35b384ceef52123fc66411a73c36a706825e1\",\n",
    "    # Consistency error with accff72ecc2f6cf5a76d9570198a93ac7c90270e\n",
    "    \"01220354d389cd05474713f8c982d05c9b17aafb\",\n",
    "    \"644124ecd0b6e417c527191f866daa05a5a2056d\",\n",
    "    \"40b6d1605814dd1db0a46e202d6e56f2e4c9a468\"\n",
    "    # Duplicate with 9f75e2e562fa0c0482f3dde6fc7399a9070b4a3d\n",
    "    \"f27c38ab5d90f68c9dd60cabef248a570c0be8fc\",\n",
    "    # Duplicate with 3dfb8e81bb5f776a6b00c7a90dd087e85b71f8bb\n",
    "    \"3dfb8e81bb5f776a6b00c7a90dd087e85b71f8bb\",\n",
    "    # Duplicate with cc82d986c40328d4ae81298a9d287c95a6326bb0\n",
    "    \"d74a1b1d1325af2a24848044cf2858987f5a3ecc\",\n",
    "    # Duplicate with f50cc0b8cb399bb7b7c1ad23b94c9404f0cc6d23\n",
    "    \"b618339c321c387230d3ea523e80ad47af3de5cf\",\n",
    "    # Duplicate with 03daf774d0d80fb7235910ed1c2b4fbcaebdfe65\n",
    "    \"3b6de811abea0a811e03e3029222a7e459922892\",\n",
    "    # Duplicate with 70e83e72b43e05e57eb42a6d52d01a4d9768f510\n",
    "    \"2936b80dbbc7efb889934aeec80f6142c10266ce\",\n",
    "    # Duplicate with https://github.com/apache/airflow/commit/1cbb0ad26dd17f218c6ab1c2ae59b262c443a443\n",
    "    \"09be0c5c7e847dda1d0be5776f8d5e327ff2281a\",\n",
    "    # Duplicate with 24f43aac0f4116b3d89fdbe973ba92c6cfb0d998\n",
    "    \"54b02d9f3a94de94e4fb471908b8cf798e62e411\",\n",
    "    # Duplicate with fa0d4829f9c81eefb37cc058e2fa1b6a918741da\n",
    "    \"ab2a5d82b4ee3c909d2456704388ccf90e367c9b\",\n",
    "    # Duplicate with de4466d88b816437fb29eff5ab23b9b964cd3985\n",
    "    \"32a7b713468161282f2ea01d5e2faff980d924cd\",\n",
    "    # Duplicate with 813de2672bd7361e9a453ab62cd6e52f96b6525b\n",
    "    \"7b7b909579c8311c140c89b8a9431bf537febf93\",\n",
    "    # Strange commit, no useful info\n",
    "    \"31cb25adecba930bdeee4556709f5a1c42d88fd6\",\n",
    "    # Duplicate with https://github.com/django/django/commit/8cc41ce7a7a8f6bebfdd89d5ab276cd0109f4fc5\n",
    "    \"2f5485346ee6f84b4e52068c04e043092daf55f7\",\n",
    "    # Duplicate with https://github.com/django/django/commit/f27c38ab5d90f68c9dd60cabef248a570c0be8fc\n",
    "    \"9f75e2e562fa0c0482f3dde6fc7399a9070b4a3d\",\n",
    "}\n",
    "\n",
    "without_duplicates = without_duplicates.filter(\n",
    "    pl.col(\"commit\").is_in(commits_to_exclude).not_()\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
      "â”‚ similarity â”† len â”‚\n",
      "â”‚ ---        â”† --- â”‚\n",
      "â”‚ i64        â”† u32 â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
      "â”‚ 54         â”† 1   â”‚\n",
      "â”‚ 58         â”† 1   â”‚\n",
      "â”‚ 84         â”† 1   â”‚\n",
      "â”‚ 82         â”† 1   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "## Looking for code units that are the same before and after the fix\n",
    "print(similarity.filter(\n",
    "    pl.col(\"is_new_1\").not_(), pl.col(\"is_new_2\"), pl.col(\"commit_1\") != pl.col(\"commit_2\")\n",
    ").filter(pl.col(\"similarity\") > 50).with_columns(\n",
    "    pl.col(\"similarity\").cast(pl.Int64)\n",
    ").group_by(\"similarity\").agg(pl.len()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CWE-130',\n",
       " 'CWE-131',\n",
       " 'CWE-1333',\n",
       " 'CWE-190',\n",
       " 'CWE-20',\n",
       " 'CWE-200',\n",
       " 'CWE-22',\n",
       " 'CWE-276',\n",
       " 'CWE-284',\n",
       " 'CWE-287',\n",
       " 'CWE-29',\n",
       " 'CWE-290',\n",
       " 'CWE-295',\n",
       " 'CWE-306',\n",
       " 'CWE-347',\n",
       " 'CWE-352',\n",
       " 'CWE-400',\n",
       " 'CWE-444',\n",
       " 'CWE-476',\n",
       " 'CWE-502',\n",
       " 'CWE-521',\n",
       " 'CWE-522',\n",
       " 'CWE-552',\n",
       " 'CWE-59',\n",
       " 'CWE-601',\n",
       " 'CWE-611',\n",
       " 'CWE-613',\n",
       " 'CWE-667',\n",
       " 'CWE-74',\n",
       " 'CWE-77',\n",
       " 'CWE-770',\n",
       " 'CWE-78',\n",
       " 'CWE-787',\n",
       " 'CWE-79',\n",
       " 'CWE-824',\n",
       " 'CWE-835',\n",
       " 'CWE-840',\n",
       " 'CWE-89',\n",
       " 'CWE-918',\n",
       " 'CWE-94',\n",
       " None}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concated_code_units = pl.concat(\n",
    "    [without_duplicates.filter(pl.col(\"code_unit_after_fix\").str.strip_chars() != \"\").select(\n",
    "        pl.col(\"code_unit_after_fix\").alias(\"code_unit\"), pl.col(\"commit\"), pl.col(\"repo\"), pl.col(\"vulnerability_id\"), pl.col(\"cwe_id\")\n",
    "    ),\n",
    "    without_duplicates.filter(pl.col(\"code_unit_before_fix\").str.strip_chars() != \"\").select(\n",
    "        pl.col(\"code_unit_before_fix\").alias(\"code_unit\"), pl.col(\"commit\"), pl.col(\"repo\"), pl.col(\"vulnerability_id\"), pl.col(\"cwe_id\")\n",
    "    )]\n",
    ")\n",
    "\n",
    "duplicate_code_units = concated_code_units.group_by(\"code_unit\").agg(\"commit\", \"repo\", \"vulnerability_id\", \"cwe_id\").filter(pl.col(\"commit\").list.len() > 1).sort(\"repo\").slice(2)\n",
    "set(duplicate_code_units.select(\"cwe_id\").explode(\"cwe_id\").explode(\"cwe_id\").to_series().to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 3109\n"
     ]
    }
   ],
   "source": [
    "def try_convert_to_python3(code: str) -> str:\n",
    "    try:\n",
    "        with tempfile.TemporaryFile(\"w\") as temp_file:\n",
    "            temp_file.write(code)\n",
    "            temp_file.seek(0)\n",
    "            \n",
    "            os.system(f\"2to3 -w {temp_file.name}\")\n",
    "            return temp_file.readlines()\n",
    "    except:\n",
    "        return code\n",
    "    \n",
    "\n",
    "\n",
    "total = 0\n",
    "errors_count = 0\n",
    "bad_commits = set()\n",
    "for row in without_duplicates.sample(fraction=1, shuffle=True).iter_rows(named=True):\n",
    "    file_extension = row[\"new_file\"].split(\".\")[-1]\n",
    "    if file_extension in {\"py\", \"pyi\", \"pyx\", \"pxi\"}:\n",
    "        total += 1\n",
    "        script = jedi.Script(code=row[\"code_unit_after_fix\"])\n",
    "        errors = script.get_syntax_errors()\n",
    "        if errors and row[\"commit\"]:\n",
    "        #     try_convert_to_python3(row[\"code_unit_after_fix\"])\n",
    "            # # shutil.rmtree(PYTHON_CODE_UNITS_DATA_PATH / row[\"commit\"], ignore_errors=True)\n",
    "            # # shutil.rmtree(PYTHON_CODE_CONTEXT_DATA_PATH / row[\"commit\"], ignore_errors=True)\n",
    "            # print(row[\"vulnerability_id\"], row[\"repo\"], row[\"new_file\"], row[\"commit\"])\n",
    "            # print(row[\"code_unit_after_fix\"])\n",
    "            # print(errors)\n",
    "            # break\n",
    "            bad_commits.add(row[\"commit\"])\n",
    "            errors_count += 1\n",
    "\n",
    "print(errors_count, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>commit</th><th>repo</th><th>new_file</th><th>patch</th><th>code_unit_after_fix</th><th>vulnerability_id</th><th>cwe_id</th><th>old_file</th><th>code_unit_before_fix</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;3716&quot;</td><td>&quot;3716&quot;</td><td>&quot;3716&quot;</td><td>&quot;3716&quot;</td><td>&quot;3716&quot;</td><td>&quot;3716&quot;</td><td>3716.0</td><td>&quot;3716&quot;</td><td>&quot;3716&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;001b0634cd309e372edb6d7d95d083â€¦</td><td>&quot;389ds/389-ds-base&quot;</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;@@ -1 +1 @@\n",
       "-__version__ = &quot;1.â€¦</td><td>&quot;&quot;</td><td>&quot;2013-0208&quot;</td><td>null</td><td>&quot;.devcontainer/library-scripts/â€¦</td><td>&quot;&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;ffc095a3e5acc1c404773a0510e6d0â€¦</td><td>&quot;zwczou/weixin-python&quot;</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;@@ -99,7 +99,9 @@ extern &quot;C&quot; {â€¦</td><td>&quot;} else {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;actionsTemplaâ€¦</td><td>&quot;GHSA-x563-6hqv-26mr&quot;</td><td>null</td><td>&quot;zproject/urls.py&quot;</td><td>&quot;},\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;scope&quot;: {\n",
       "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;â€¦</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† commit     â”† repo      â”† new_file  â”† â€¦ â”† vulnerabi â”† cwe_id â”† old_file  â”† code_unit â”‚\n",
       "â”‚ ---        â”† ---        â”† ---       â”† ---       â”†   â”† lity_id   â”† ---    â”† ---       â”† _before_f â”‚\n",
       "â”‚ str        â”† str        â”† str       â”† str       â”†   â”† ---       â”† f64    â”† str       â”† ix        â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”† str       â”†        â”†           â”† ---       â”‚\n",
       "â”‚            â”†            â”†           â”†           â”†   â”†           â”†        â”†           â”† str       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 3716       â”† 3716      â”† 3716      â”† â€¦ â”† 3716      â”† 3716.0 â”† 3716      â”† 3716      â”‚\n",
       "â”‚ null_count â”† 0          â”† 0         â”† 0         â”† â€¦ â”† 0         â”† 0.0    â”† 0         â”† 0         â”‚\n",
       "â”‚ mean       â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ std        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ min        â”† 001b0634cd â”† 389ds/389 â”† .devconta â”† â€¦ â”† 2013-0208 â”† null   â”† .devconta â”†           â”‚\n",
       "â”‚            â”† 309e372edb â”† -ds-base  â”† iner/libr â”†   â”†           â”†        â”† iner/libr â”†           â”‚\n",
       "â”‚            â”† 6d7d95d083 â”†           â”† ary-scrip â”†   â”†           â”†        â”† ary-scrip â”†           â”‚\n",
       "â”‚            â”† â€¦          â”†           â”† ts/â€¦      â”†   â”†           â”†        â”† ts/â€¦      â”†           â”‚\n",
       "â”‚ 25%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 50%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ 75%        â”† null       â”† null      â”† null      â”† â€¦ â”† null      â”† null   â”† null      â”† null      â”‚\n",
       "â”‚ max        â”† ffc095a3e5 â”† zwczou/we â”† zproject/ â”† â€¦ â”† GHSA-x563 â”† null   â”† zproject/ â”† },        â”‚\n",
       "â”‚            â”† acc1c40477 â”† ixin-pyth â”† urls.py   â”†   â”† -6hqv-26m â”†        â”† urls.py   â”† \"scope\":  â”‚\n",
       "â”‚            â”† 3a0510e6d0 â”† on        â”†           â”†   â”† r         â”†        â”†           â”† {         â”‚\n",
       "â”‚            â”† â€¦          â”†           â”†           â”†   â”†           â”†        â”†           â”†         â€¦ â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_duplicates.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cwe_id': None, 'count': 177}, {'cwe_id': 'CWE-400', 'count': 145}, {'cwe_id': 'CWE-94', 'count': 141}, {'cwe_id': 'CWE-601', 'count': 120}, {'cwe_id': 'CWE-863', 'count': 104}, {'cwe_id': 'CWE-918', 'count': 102}, {'cwe_id': 'CWE-770', 'count': 97}, {'cwe_id': 'CWE-284', 'count': 96}, {'cwe_id': 'CWE-287', 'count': 90}, {'cwe_id': 'CWE-502', 'count': 70}, {'cwe_id': 'CWE-78', 'count': 66}, {'cwe_id': 'CWE-532', 'count': 66}, {'cwe_id': 'CWE-74', 'count': 63}, {'cwe_id': 'CWE-89', 'count': 59}, {'cwe_id': 'CWE-77', 'count': 59}, {'cwe_id': 'CWE-352', 'count': 57}, {'cwe_id': 'CWE-1333', 'count': 51}, {'cwe_id': 'CWE-613', 'count': 49}, {'cwe_id': 'CWE-617', 'count': 48}, {'cwe_id': 'CWE-312', 'count': 44}, {'cwe_id': 'CWE-444', 'count': 42}, {'cwe_id': 'CWE-125', 'count': 42}, {'cwe_id': 'CWE-116', 'count': 42}, {'cwe_id': 'CWE-285', 'count': 40}, {'cwe_id': 'CWE-190', 'count': 38}, {'cwe_id': 'CWE-667', 'count': 37}, {'cwe_id': 'CWE-476', 'count': 37}, {'cwe_id': 'CWE-269', 'count': 36}, {'cwe_id': 'CWE-347', 'count': 35}, {'cwe_id': 'CWE-203', 'count': 35}, {'cwe_id': 'CWE-405', 'count': 34}, {'cwe_id': 'CWE-787', 'count': 31}, {'cwe_id': 'CWE-362', 'count': 31}, {'cwe_id': 'CWE-29', 'count': 29}, {'cwe_id': 'CWE-611', 'count': 26}, {'cwe_id': 'CWE-208', 'count': 26}, {'cwe_id': 'CWE-120', 'count': 26}, {'cwe_id': 'CWE-306', 'count': 24}, {'cwe_id': 'CWE-295', 'count': 24}, {'cwe_id': 'CWE-122', 'count': 24}, {'cwe_id': 'CWE-80', 'count': 23}, {'cwe_id': 'CWE-354', 'count': 22}, {'cwe_id': 'CWE-276', 'count': 21}, {'cwe_id': 'CWE-862', 'count': 21}, {'cwe_id': 'CWE-668', 'count': 20}, {'cwe_id': 'CWE-755', 'count': 20}, {'cwe_id': 'CWE-835', 'count': 19}, {'cwe_id': 'CWE-522', 'count': 19}, {'cwe_id': 'CWE-434', 'count': 19}, {'cwe_id': 'CWE-119', 'count': 18}, {'cwe_id': 'CWE-59', 'count': 18}, {'cwe_id': 'CWE-185', 'count': 16}, {'cwe_id': 'CWE-117', 'count': 16}, {'cwe_id': 'CWE-209', 'count': 16}, {'cwe_id': 'CWE-824', 'count': 16}, {'cwe_id': 'CWE-330', 'count': 14}, {'cwe_id': 'CWE-250', 'count': 13}, {'cwe_id': 'CWE-610', 'count': 13}, {'cwe_id': 'CWE-290', 'count': 13}, {'cwe_id': 'CWE-23', 'count': 13}, {'cwe_id': 'CWE-426', 'count': 12}, {'cwe_id': 'CWE-401', 'count': 12}, {'cwe_id': 'CWE-1336', 'count': 12}, {'cwe_id': 'CWE-697', 'count': 12}, {'cwe_id': 'CWE-307', 'count': 12}, {'cwe_id': 'CWE-416', 'count': 12}, {'cwe_id': 'CWE-288', 'count': 11}, {'cwe_id': 'CWE-552', 'count': 11}, {'cwe_id': 'CWE-73', 'count': 11}, {'cwe_id': 'CWE-346', 'count': 11}, {'cwe_id': 'CWE-95', 'count': 11}, {'cwe_id': 'CWE-776', 'count': 11}, {'cwe_id': 'CWE-384', 'count': 11}, {'cwe_id': 'CWE-754', 'count': 11}, {'cwe_id': 'CWE-326', 'count': 11}, {'cwe_id': 'CWE-681', 'count': 10}, {'cwe_id': 'CWE-377', 'count': 10}, {'cwe_id': 'CWE-359', 'count': 10}, {'cwe_id': 'CWE-75', 'count': 10}, {'cwe_id': 'CWE-670', 'count': 10}, {'cwe_id': 'CWE-521', 'count': 10}, {'cwe_id': 'CWE-26', 'count': 9}, {'cwe_id': 'CWE-639', 'count': 9}, {'cwe_id': 'CWE-840', 'count': 9}, {'cwe_id': 'CWE-843', 'count': 8}, {'cwe_id': 'CWE-281', 'count': 8}, {'cwe_id': 'CWE-130', 'count': 8}, {'cwe_id': 'CWE-204', 'count': 8}, {'cwe_id': 'CWE-345', 'count': 8}, {'cwe_id': 'CWE-565', 'count': 8}, {'cwe_id': 'CWE-427', 'count': 8}, {'cwe_id': 'CWE-369', 'count': 8}, {'cwe_id': 'CWE-1236', 'count': 8}, {'cwe_id': 'CWE-497', 'count': 8}, {'cwe_id': 'CWE-93', 'count': 8}, {'cwe_id': 'CWE-280', 'count': 8}, {'cwe_id': 'CWE-707', 'count': 7}, {'cwe_id': 'CWE-311', 'count': 7}, {'cwe_id': 'CWE-91', 'count': 7}, {'cwe_id': 'CWE-1284', 'count': 7}, {'cwe_id': 'CWE-305', 'count': 7}, {'cwe_id': 'CWE-315', 'count': 7}, {'cwe_id': 'CWE-367', 'count': 7}, {'cwe_id': 'CWE-97', 'count': 6}, {'cwe_id': 'CWE-131', 'count': 6}, {'cwe_id': 'CWE-172', 'count': 6}, {'cwe_id': 'CWE-1188', 'count': 6}, {'cwe_id': 'CWE-88', 'count': 6}, {'cwe_id': 'CWE-327', 'count': 6}, {'cwe_id': 'CWE-331', 'count': 6}, {'cwe_id': 'CWE-134', 'count': 5}, {'cwe_id': 'CWE-834', 'count': 5}, {'cwe_id': 'CWE-214', 'count': 5}, {'cwe_id': 'CWE-653', 'count': 5}, {'cwe_id': 'CWE-620', 'count': 5}, {'cwe_id': 'CWE-248', 'count': 5}, {'cwe_id': 'CWE-328', 'count': 5}, {'cwe_id': 'CWE-252', 'count': 5}, {'cwe_id': 'CWE-36', 'count': 5}, {'cwe_id': 'CWE-358', 'count': 5}, {'cwe_id': 'CWE-15', 'count': 4}, {'cwe_id': 'CWE-349', 'count': 4}, {'cwe_id': 'CWE-772', 'count': 4}, {'cwe_id': 'CWE-708', 'count': 4}, {'cwe_id': 'CWE-842', 'count': 4}, {'cwe_id': 'CWE-833', 'count': 4}, {'cwe_id': 'CWE-602', 'count': 4}, {'cwe_id': 'CWE-640', 'count': 4}, {'cwe_id': 'CWE-233', 'count': 4}, {'cwe_id': 'CWE-674', 'count': 4}, {'cwe_id': 'CWE-1270', 'count': 4}, {'cwe_id': 'CWE-173', 'count': 4}, {'cwe_id': 'CWE-798', 'count': 4}, {'cwe_id': 'CWE-129', 'count': 3}, {'cwe_id': 'CWE-913', 'count': 3}, {'cwe_id': 'CWE-696', 'count': 3}, {'cwe_id': 'CWE-662', 'count': 3}, {'cwe_id': 'CWE-749', 'count': 3}, {'cwe_id': 'CWE-279', 'count': 3}, {'cwe_id': 'CWE-237', 'count': 3}, {'cwe_id': 'CWE-212', 'count': 3}, {'cwe_id': 'CWE-665', 'count': 3}, {'cwe_id': 'CWE-213', 'count': 3}, {'cwe_id': 'CWE-274', 'count': 3}, {'cwe_id': 'CWE-1057', 'count': 3}, {'cwe_id': 'CWE-922', 'count': 2}, {'cwe_id': 'CWE-344', 'count': 2}, {'cwe_id': 'CWE-682', 'count': 2}, {'cwe_id': 'CWE-669', 'count': 2}, {'cwe_id': 'CWE-425', 'count': 2}, {'cwe_id': 'CWE-385', 'count': 2}, {'cwe_id': 'CWE-415', 'count': 2}, {'cwe_id': 'CWE-76', 'count': 2}, {'cwe_id': 'CWE-294', 'count': 2}, {'cwe_id': 'CWE-289', 'count': 2}, {'cwe_id': 'CWE-791', 'count': 2}, {'cwe_id': 'CWE-420', 'count': 2}, {'cwe_id': 'CWE-525', 'count': 2}, {'cwe_id': 'CWE-407', 'count': 2}, {'cwe_id': 'CWE-325', 'count': 2}, {'cwe_id': 'CWE-404', 'count': 2}, {'cwe_id': 'CWE-191', 'count': 2}, {'cwe_id': 'CWE-494', 'count': 2}, {'cwe_id': 'CWE-829', 'count': 2}, {'cwe_id': 'CWE-475', 'count': 2}, {'cwe_id': 'CWE-789', 'count': 2}, {'cwe_id': 'CWE-459', 'count': 2}, {'cwe_id': 'CWE-732', 'count': 2}, {'cwe_id': 'CWE-366', 'count': 2}, {'cwe_id': 'CWE-35', 'count': 2}, {'cwe_id': 'CWE-256', 'count': 2}, {'cwe_id': 'CWE-940', 'count': 2}, {'cwe_id': 'CWE-332', 'count': 2}, {'cwe_id': 'CWE-703', 'count': 2}, {'cwe_id': 'CWE-539', 'count': 2}, {'cwe_id': 'CWE-1283', 'count': 1}, {'cwe_id': 'CWE-323', 'count': 1}, {'cwe_id': 'CWE-31', 'count': 1}, {'cwe_id': 'CWE-300', 'count': 1}, {'cwe_id': 'CWE-921', 'count': 1}, {'cwe_id': 'CWE-693', 'count': 1}, {'cwe_id': 'CWE-394', 'count': 1}, {'cwe_id': 'CWE-942', 'count': 1}, {'cwe_id': 'CWE-409', 'count': 1}, {'cwe_id': 'CWE-338', 'count': 1}, {'cwe_id': 'CWE-1327', 'count': 1}, {'cwe_id': 'CWE-395', 'count': 1}, {'cwe_id': 'CWE-614', 'count': 1}, {'cwe_id': 'CWE-379', 'count': 1}, {'cwe_id': 'CWE-114', 'count': 1}, {'cwe_id': 'CWE-304', 'count': 1}, {'cwe_id': 'CWE-830', 'count': 1}, {'cwe_id': 'CWE-627', 'count': 1}, {'cwe_id': 'CWE-178', 'count': 1}, {'cwe_id': 'CWE-1021', 'count': 1}, {'cwe_id': 'CWE-683', 'count': 1}, {'cwe_id': 'CWE-277', 'count': 1}, {'cwe_id': 'CWE-303', 'count': 1}, {'cwe_id': 'CWE-253', 'count': 1}, {'cwe_id': 'CWE-706', 'count': 1}, {'cwe_id': 'CWE-1395', 'count': 1}, {'cwe_id': 'CWE-61', 'count': 1}, {'cwe_id': 'CWE-779', 'count': 1}, {'cwe_id': 'CWE-460', 'count': 1}, {'cwe_id': 'CWE-322', 'count': 1}, {'cwe_id': 'CWE-440', 'count': 1}, {'cwe_id': 'CWE-915', 'count': 1}, {'cwe_id': 'CWE-81', 'count': 1}, {'cwe_id': 'CWE-1321', 'count': 1}, {'cwe_id': 'CWE-916', 'count': 1}, {'cwe_id': 'CWE-150', 'count': 1}, {'cwe_id': 'CWE-202', 'count': 1}]\n",
      "shape: (4, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ cwe_id   â”† count â”‚\n",
      "â”‚ ---      â”† ---   â”‚\n",
      "â”‚ str      â”† u32   â”‚\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚ CWE-345  â”† 141   â”‚\n",
      "â”‚ CWE-119  â”† 133   â”‚\n",
      "â”‚ CWE-1390 â”† 130   â”‚\n",
      "â”‚ CWE-863  â”† 114   â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "print(without_duplicates.select(\"cwe_id\").explode(\"cwe_id\").to_series().value_counts().sort(\"count\", descending=True).filter(pl.col(\"count\") < 200).to_dicts())\n",
    "\n",
    "VULNERABILITY_TYPES_MAPPING = {\n",
    "    \"CWE-770\": \"CWE-400\",\n",
    "    \"CWE-405\": \"CWE-400\",\n",
    "    \"CWE-407\": \"CWE-400\",\n",
    "    \"CWE-1333\": \"CWE-400\",\n",
    "    # \"CWE-920\": \"CWE-400\",\n",
    "    \"CWE-639\": \"CWE-863\",\n",
    "    \"CWE-256\": \"CWE-1390\",\n",
    "    \"CWE-262\": \"CWE-1390\",\n",
    "    \"CWE-263\": \"CWE-1390\",\n",
    "    \"CWE-289\": \"CWE-1390\",\n",
    "    \"CWE-290\": \"CWE-1390\",\n",
    "    \"CWE-294\": \"CWE-1390\",\n",
    "    \"CWE-295\": \"CWE-1390\",\n",
    "    \"CWE-301\": \"CWE-1390\",\n",
    "    \"CWE-302\": \"CWE-1390\",\n",
    "    \"CWE-303\": \"CWE-1390\",\n",
    "    \"CWE-304\": \"CWE-1390\",\n",
    "    \"CWE-305\": \"CWE-1390\",\n",
    "    \"CWE-306\": \"CWE-1390\",\n",
    "    \"CWE-307\": \"CWE-1390\",\n",
    "    \"CWE-308\": \"CWE-1390\",\n",
    "    \"CWE-309\": \"CWE-1390\",\n",
    "    \"CWE-312\": \"CWE-1390\",\n",
    "    \"CWE-324\": \"CWE-1390\",\n",
    "    \"CWE-521\": \"CWE-1390\",\n",
    "    \"CWE-522\": \"CWE-1390\",\n",
    "    \"CWE-593\": \"CWE-1390\",\n",
    "    \"CWE-603\": \"CWE-1390\",\n",
    "    \"CWE-620\": \"CWE-1390\",\n",
    "    \"CWE-640\": \"CWE-1390\",\n",
    "    \"CWE-798\": \"CWE-1390\",\n",
    "    \"CWE-804\": \"CWE-1390\",\n",
    "    \"CWE-836\": \"CWE-1390\",\n",
    "    \"CWE-1391\": \"CWE-1390\",\n",
    "    \"CWE-77\": \"CWE-74\",\n",
    "    \"CWE-78\": \"CWE-74\",\n",
    "    # \"CWE-88\": \"CWE-74\",\n",
    "    \"CWE-89\": \"CWE-74\",\n",
    "    # \"CWE-91\": \"CWE-74\",\n",
    "    \"CWE-94\": \"CWE-74\",\n",
    "    # \"CWE-917\": \"CWE-74\",\n",
    "    # \"CWE-1236\": \"CWE-74\",\n",
    "    # \"CWE-384\": \"CWE-610\",\n",
    "    \"CWE-601\": \"CWE-610\",\n",
    "    \"CWE-611\": \"CWE-610\",\n",
    "    \"CWE-918\": \"CWE-610\",\n",
    "    # \"CWE-1021\": \"CWE-610\",\n",
    "    # \"CWE-532\": \"CWE-200\",\n",
    "    \"CWE-203\": \"CWE-200\",\n",
    "    \"CWE-208\": \"CWE-200\",\n",
    "    # \"CWE-209\": \"CWE-200\",\n",
    "    \"CWE-532\": \"CWE-200\",\n",
    "    \"CWE-470\": \"CWE-913\",\n",
    "    \"CWE-502\": \"CWE-913\",\n",
    "    \"CWE-1321\": \"CWE-913\",\n",
    "    \"CWE-346\": \"CWE-345\",\n",
    "    \"CWE-347\": \"CWE-345\",\n",
    "    \"CWE-348\": \"CWE-345\",\n",
    "    \"CWE-349\": \"CWE-345\",\n",
    "    \"CWE-351\": \"CWE-345\",\n",
    "    \"CWE-352\": \"CWE-345\",\n",
    "    \"CWE-353\": \"CWE-345\",\n",
    "    \"CWE-354\": \"CWE-345\",\n",
    "    \"CWE-360\": \"CWE-345\",\n",
    "    \"CWE-494\": \"CWE-345\",\n",
    "    \"CWE-616\": \"CWE-345\",\n",
    "    \"CWE-646\": \"CWE-345\",\n",
    "    \"CWE-649\": \"CWE-345\",\n",
    "    \"CWE-924\": \"CWE-345\",\n",
    "    \"CWE-940\": \"CWE-345\",\n",
    "    \"CWE-1333\": \"CWE-407\",\n",
    "    \"CWE-312\": \"CWE-311\",\n",
    "    \"CWE-319\": \"CWE-311\",\n",
    "    \"CWE-415\": \"CWE-672\",\n",
    "    \"CWE-416\": \"CWE-672\",\n",
    "    \"CWE-613\": \"CWE-672\",\n",
    "    \"CWE-617\": \"CWE-670\",\n",
    "    \"CWE-838\": \"CWE-116\",\n",
    "    \"CWE-117\": \"CWE-116\",\n",
    "    \"CWE-362\": \"CWE-367\",\n",
    "    \"CWE-120\": \"CWE-119\",\n",
    "    \"CWE-125\": \"CWE-119\",\n",
    "    \"CWE-787\": \"CWE-119\",\n",
    "    \"CWE-824\": \"CWE-119\",\n",
    "    \"CWE-662\": \"CWE-667\",\n",
    "    \"CWE-131\": \"CWE-682\",\n",
    "    \"CWE-190\": \"CWE-682\",\n",
    "    \"CWE-191\": \"CWE-682\",\n",
    "    \"CWE-193\": \"CWE-682\",\n",
    "    \"CWE-369\": \"CWE-682\",\n",
    "    \"CWE-444\": \"CWE-436\",\n",
    "    \"CWE-942\": \"CWE-863\"\n",
    "    # ??? CWE-284\n",
    "    # ??? CWE-285\n",
    "    # ??? CWE-122\n",
    "}\n",
    "\n",
    "# clustered_vulnerabilities = code_unit_changes_df.explode(\"cwe_id\").with_columns(pl.col(\"cwe_id\").replace(VULNERABILITY_TYPES_MAPPING)).group_by([col for col in code_unit_changes_df.columns if col != \"cwe_id\"]).agg(pl.col(\"cwe_id\").alias(\"cwe_id\"))\n",
    "clustered_cwes = without_duplicates.explode(\"cwe_id\").with_columns(pl.col(\"cwe_id\").replace(VULNERABILITY_TYPES_MAPPING)).group_by(\"vulnerability_id\").agg(pl.col(\"cwe_id\"))\n",
    "# We filter vulnerabilities, which cwe types are too rare (<100 examples)\n",
    "cwes_to_exclude = clustered_cwes.select(\"cwe_id\").explode(\"cwe_id\").to_series().value_counts().sort(\"count\", descending=True).filter(pl.col(\"count\") < 100).select(pl.col(\"cwe_id\")).to_series().to_list()\n",
    "code_unit_without_rare_cwes = without_duplicates.explode(\"cwe_id\").with_columns(pl.col(\"cwe_id\").replace(VULNERABILITY_TYPES_MAPPING)).filter(pl.col(\"cwe_id\").is_in(cwes_to_exclude).not_()).group_by(\"vulnerability_id\").agg(pl.col(\"cwe_id\"))\n",
    "possible_to_sample_cwes = code_unit_without_rare_cwes.select(\"cwe_id\").explode(\"cwe_id\").to_series().value_counts().sort(\"count\", descending=True).filter(pl.col(\"count\") < 200)\n",
    "print(possible_to_sample_cwes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (163, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cwe_id</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;CWE-377&quot;</td></tr><tr><td>&quot;CWE-80&quot;</td></tr><tr><td>&quot;CWE-706&quot;</td></tr><tr><td>&quot;CWE-279&quot;</td></tr><tr><td>&quot;CWE-789&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;CWE-379&quot;</td></tr><tr><td>&quot;CWE-116&quot;</td></tr><tr><td>&quot;CWE-459&quot;</td></tr><tr><td>&quot;CWE-779&quot;</td></tr><tr><td>&quot;CWE-22&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (163, 1)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ cwe_id  â”‚\n",
       "â”‚ ---     â”‚\n",
       "â”‚ str     â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ CWE-377 â”‚\n",
       "â”‚ CWE-80  â”‚\n",
       "â”‚ CWE-706 â”‚\n",
       "â”‚ CWE-279 â”‚\n",
       "â”‚ CWE-789 â”‚\n",
       "â”‚ â€¦       â”‚\n",
       "â”‚ CWE-379 â”‚\n",
       "â”‚ CWE-116 â”‚\n",
       "â”‚ CWE-459 â”‚\n",
       "â”‚ CWE-779 â”‚\n",
       "â”‚ CWE-22  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_cwes.select(\"cwe_id\").explode(\"cwe_id\").unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze a relevance of rare cwes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (0, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>vulnerability_id</th><th>cwe_id</th></tr><tr><td>str</td><td>list[str]</td></tr></thead><tbody></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (0, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ vulnerability_id â”† cwe_id    â”‚\n",
       "â”‚ ---              â”† ---       â”‚\n",
       "â”‚ str              â”† list[str] â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_cwes.filter(pl.col(\"cwe_id\").list.contains(\"CWE-942\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerability_year = python_vulnerability_fixes.select(\"vulnerability_id\", \"patch_time\").with_columns(pl.col(\"patch_time\").cast(pl.Date).dt.year().alias(\"year\"))\n",
    "vulnerabilities_types_per_year = without_duplicates.join(\n",
    "    vulnerability_year, on=\"vulnerability_id\", how=\"left\",\n",
    ").explode(\"cwe_id\").with_columns(pl.col(\"cwe_id\").replace(VULNERABILITY_TYPES_MAPPING)).unique([\"cwe_id\", \"vulnerability_id\", \"year\"]).group_by(\"cwe_id\", \"year\").agg(pl.col(\"vulnerability_id\").count().alias(\"count\")).sort(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE-345\n",
      "CWE-119\n",
      "CWE-1390\n",
      "CWE-863\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x700 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,axes = plt.subplots(nrows=2, ncols=2,figsize=(11, 7))\n",
    "\n",
    "for idx, cwe_id in enumerate(possible_to_sample_cwes.select(\"cwe_id\").to_series()):\n",
    "    print(cwe_id)\n",
    "    data = vulnerabilities_types_per_year.filter(pl.col(\"cwe_id\") == cwe_id).to_pandas()\n",
    "    ax = plt.subplot(2, 2, idx+1)\n",
    "    ax.set_title(cwe_id)\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Amount\")\n",
    "    ax.plot(data[\"year\"], data[\"count\"], label=cwe_id)\n",
    "fig.tight_layout()\n",
    "\n",
    "# data = {'apple': 10, 'orange': 15, 'lemon': 5, 'lime': 20}\n",
    "# names = list(data.keys())\n",
    "# values = list(data.values())\n",
    "\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(9, 3), sharey=True)\n",
    "# axs[0].bar(names, values)\n",
    "# axs[1].scatter(names, values)\n",
    "# axs[2].plot(names, values)\n",
    "# fig.suptitle('Categorical Plotting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>commit</th><th>repo</th><th>new_file</th><th>patch</th><th>code_unit_after_fix</th><th>vulnerability_id</th><th>cwe_id</th><th>old_file</th><th>code_unit_before_fix</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>11</td><td>11</td><td>11</td><td>11</td><td>11</td><td>11</td><td>11</td><td>11</td><td>11</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ commit â”† repo â”† new_file â”† patch â”† â€¦ â”† vulnerability_id â”† cwe_id â”† old_file â”† code_unit_before_f â”‚\n",
       "â”‚ ---    â”† ---  â”† ---      â”† ---   â”†   â”† ---              â”† ---    â”† ---      â”† ix                 â”‚\n",
       "â”‚ u32    â”† u32  â”† u32      â”† u32   â”†   â”† u32              â”† u32    â”† u32      â”† ---                â”‚\n",
       "â”‚        â”†      â”†          â”†       â”†   â”†                  â”†        â”†          â”† u32                â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 11     â”† 11   â”† 11       â”† 11    â”† â€¦ â”† 11               â”† 11     â”† 11       â”† 11                 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_unit_changes_df.explode(\"cwe_id\").filter(pl.col(\"cwe_id\") == \"CWE-369\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out rare cwes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of vulnerabilities without rare cwes: 1048\n",
      "Number of excluded vulnerabilities: 597\n"
     ]
    }
   ],
   "source": [
    "cwes_to_exclude = clustered_cwes.select(\"cwe_id\").explode(\"cwe_id\").to_series().value_counts().sort(\"count\", descending=True).filter(pl.col(\"count\") < 100).select(pl.col(\"cwe_id\")).to_series().to_list()\n",
    "code_unit_without_rare_cwes = without_duplicates.explode(\"cwe_id\").with_columns(pl.col(\"cwe_id\").replace(VULNERABILITY_TYPES_MAPPING)).filter(pl.col(\"cwe_id\").is_in(cwes_to_exclude).not_()).group_by(\"vulnerability_id\").agg(pl.col(\"cwe_id\"))\n",
    "print(f\"Amount of vulnerabilities without rare cwes: {code_unit_without_rare_cwes.shape[0]}\")\n",
    "print(f\"Number of excluded vulnerabilities: {without_duplicates.unique(\"vulnerability_id\").shape[0] - code_unit_without_rare_cwes.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>vulnerability_id</th><th>cwe_id</th></tr><tr><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;1025&quot;</td><td>1025.0</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;2013-0212&quot;</td><td>null</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;GHSA-w228-rfpx-fhm4&quot;</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 3)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ statistic  â”† vulnerability_id    â”† cwe_id â”‚\n",
       "â”‚ ---        â”† ---                 â”† ---    â”‚\n",
       "â”‚ str        â”† str                 â”† f64    â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ count      â”† 1025                â”† 1025.0 â”‚\n",
       "â”‚ null_count â”† 0                   â”† 0.0    â”‚\n",
       "â”‚ mean       â”† null                â”† null   â”‚\n",
       "â”‚ std        â”† null                â”† null   â”‚\n",
       "â”‚ min        â”† 2013-0212           â”† null   â”‚\n",
       "â”‚ 25%        â”† null                â”† null   â”‚\n",
       "â”‚ 50%        â”† null                â”† null   â”‚\n",
       "â”‚ 75%        â”† null                â”† null   â”‚\n",
       "â”‚ max        â”† GHSA-w228-rfpx-fhm4 â”† null   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_unit_without_rare_cwes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_048, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>vulnerability_id</th><th>cwe_id</th></tr><tr><td>str</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;2018-14505&quot;</td><td>[&quot;CWE-20&quot;]</td></tr><tr><td>&quot;2021-39371&quot;</td><td>[&quot;CWE-610&quot;, &quot;CWE-610&quot;, â€¦ &quot;CWE-610&quot;]</td></tr><tr><td>&quot;2023-25668&quot;</td><td>[&quot;CWE-119&quot;]</td></tr><tr><td>&quot;2021-33503&quot;</td><td>[&quot;CWE-400&quot;]</td></tr><tr><td>&quot;2019-7539&quot;</td><td>[&quot;CWE-74&quot;, &quot;CWE-74&quot;, &quot;CWE-74&quot;]</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;GHSA-vx3h-qwqw-r2wq&quot;</td><td>[&quot;CWE-610&quot;]</td></tr><tr><td>&quot;2023-47631&quot;</td><td>[&quot;CWE-345&quot;]</td></tr><tr><td>&quot;2023-27494&quot;</td><td>[&quot;CWE-79&quot;, &quot;CWE-79&quot;]</td></tr><tr><td>&quot;2023-45803&quot;</td><td>[&quot;CWE-200&quot;, &quot;CWE-200&quot;, â€¦ &quot;CWE-200&quot;]</td></tr><tr><td>&quot;2021-23727&quot;</td><td>[&quot;CWE-74&quot;, &quot;CWE-74&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_048, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ vulnerability_id    â”† cwe_id                          â”‚\n",
       "â”‚ ---                 â”† ---                             â”‚\n",
       "â”‚ str                 â”† list[str]                       â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2018-14505          â”† [\"CWE-20\"]                      â”‚\n",
       "â”‚ 2021-39371          â”† [\"CWE-610\", \"CWE-610\", â€¦ \"CWE-â€¦ â”‚\n",
       "â”‚ 2023-25668          â”† [\"CWE-119\"]                     â”‚\n",
       "â”‚ 2021-33503          â”† [\"CWE-400\"]                     â”‚\n",
       "â”‚ 2019-7539           â”† [\"CWE-74\", \"CWE-74\", \"CWE-74\"]  â”‚\n",
       "â”‚ â€¦                   â”† â€¦                               â”‚\n",
       "â”‚ GHSA-vx3h-qwqw-r2wq â”† [\"CWE-610\"]                     â”‚\n",
       "â”‚ 2023-47631          â”† [\"CWE-345\"]                     â”‚\n",
       "â”‚ 2023-27494          â”† [\"CWE-79\", \"CWE-79\"]            â”‚\n",
       "â”‚ 2023-45803          â”† [\"CWE-200\", \"CWE-200\", â€¦ \"CWE-â€¦ â”‚\n",
       "â”‚ 2021-23727          â”† [\"CWE-74\", \"CWE-74\"]            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_unit_without_rare_cwes.unique(\"vulnerability_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (11, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>cwe_id</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;CWE-1390&quot;</td></tr><tr><td>&quot;CWE-74&quot;</td></tr><tr><td>&quot;CWE-200&quot;</td></tr><tr><td>&quot;CWE-610&quot;</td></tr><tr><td>&quot;CWE-79&quot;</td></tr><tr><td>&hellip;</td></tr><tr><td>&quot;CWE-863&quot;</td></tr><tr><td>&quot;CWE-119&quot;</td></tr><tr><td>&quot;CWE-400&quot;</td></tr><tr><td>&quot;CWE-22&quot;</td></tr><tr><td>&quot;CWE-345&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (11, 1)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ cwe_id   â”‚\n",
       "â”‚ ---      â”‚\n",
       "â”‚ str      â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ CWE-1390 â”‚\n",
       "â”‚ CWE-74   â”‚\n",
       "â”‚ CWE-200  â”‚\n",
       "â”‚ CWE-610  â”‚\n",
       "â”‚ CWE-79   â”‚\n",
       "â”‚ â€¦        â”‚\n",
       "â”‚ CWE-863  â”‚\n",
       "â”‚ CWE-119  â”‚\n",
       "â”‚ CWE-400  â”‚\n",
       "â”‚ CWE-22   â”‚\n",
       "â”‚ CWE-345  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_unit_without_rare_cwes.select(\"cwe_id\").explode(\"cwe_id\").unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
