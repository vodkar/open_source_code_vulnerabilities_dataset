{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Qwen2 patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 12.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vodkar\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\open-source-code-vulnerabilities-dataset-7Yku5KZk-py3.12\\Lib\\site-packages\\unsloth\\models\\llama.py:1189: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  self.register_buffer(\"cos_cached\", emb.cos().to(dtype=dtype, device=device, non_blocking=True), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-Coder-7B\"\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support ROPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.6 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabiliged LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    "    lora_alpha=16,\n",
    "    # Supports any, but = \"none\" is optimized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\vodkar\\AppData\\Local\\Temp\\ipykernel_25068\\2624096182.py:31: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  PYTHON_FIXES_CODE_PATH = \"data\\python_vulnerability_fixes_code_unit_changes.parquet\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bf59ae9bab446e89ea31544b9e47a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d028fe3cad0946da86e74a68eb8d5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Input is a python file name and a part of python code with possible software vulnerabilities.\n",
    "Review this code and find vulnerabilities in input code and identify the number of Common Weakness Enumeration (CWE), if vulnerability not exists, tell about it.\n",
    "\n",
    "Input will be provided in format:\n",
    "```\n",
    "python/code/file1.py\n",
    "Code:\n",
    "import sys\n",
    "\n",
    "print(\"hello world!\")\n",
    "\n",
    "python/code/file2.py\n",
    "Code:\n",
    "import os\n",
    "\n",
    "print(\"hello world from another file!\")\n",
    "```\n",
    "\n",
    "\n",
    "### Input:\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "PYTHON_FIXES_CODE_PATH = \"data\\python_vulnerability_fixes_code_unit_changes.parquet\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(row):\n",
    "    codes = row[\"code\"]\n",
    "    files = row[\"file\"]\n",
    "    is_vulnerability_exists = row[\"is_vulnerability_exists\"]\n",
    "    cwe = row[\"cwe_id\"]\n",
    "    texts = []\n",
    "    if is_vulnerability_exists[0]:\n",
    "        if len(cwe) == 1:\n",
    "            output = f\"Found vulnerability: {cwe[0]}.\"\n",
    "        else:\n",
    "            output = f\"Found vulnerabilities: {', '.join(cwe)}.\"\n",
    "    else:\n",
    "        output = \"No vulnerabilities found.\"\n",
    "\n",
    "    input_data = \"\"\n",
    "    for code_file, code_unit in zip(files, codes):\n",
    "        input_data = (\n",
    "            f\"File name: {code_file}\\nCode: \\n{code_unit.replace('\\n\\n', '\\n')}\\n\\n\"\n",
    "        )\n",
    "    return {\"text\": alpaca_prompt.format(input_data, output) + EOS_TOKEN}\n",
    "    texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "code_without_vulnerabilities = pl.read_parquet(PYTHON_FIXES_CODE_PATH).drop(\n",
    "    \"repo\", \"patch\"\n",
    ")\n",
    "code_with_vulnerabilites = code_without_vulnerabilities.clone()\n",
    "# TO BE DELETED\n",
    "vulns = (\n",
    "    pl.read_parquet(\"data\\\\python_vulnerability_fixes.parquet\")\n",
    "    .unique([\"commit\", \"vulnerability_id\", \"cwe_id\"])\n",
    "    .select([\"commit\", \"vulnerability_id\", \"cwe_id\"])\n",
    "    .drop_nulls()\n",
    ")\n",
    "code_with_vulnerabilites = code_with_vulnerabilites.join(vulns, on=\"commit\")\n",
    "code_without_vulnerabilities = code_without_vulnerabilities.join(vulns, on=\"commit\")\n",
    "###\n",
    "\n",
    "code_with_vulnerabilites = (\n",
    "    (\n",
    "        code_with_vulnerabilites.drop(\"code_unit_after_fix\", \"new_file\")\n",
    "        .rename({\"code_unit_before_fix\": \"code\", \"old_file\": \"file\"})\n",
    "        .with_columns(pl.lit(True).alias(\"is_vulnerability_exists\"))\n",
    "    )\n",
    "    .group_by(by=\"vulnerability_id\")\n",
    "    .agg(\"code\", \"file\", \"is_vulnerability_exists\", \"cwe_id\")\n",
    ")\n",
    "code_without_vulnerabilities = (\n",
    "    (\n",
    "        code_without_vulnerabilities.drop(\"code_unit_before_fix\", \"old_file\")\n",
    "        .rename({\"code_unit_after_fix\": \"code\", \"new_file\": \"file\"})\n",
    "        .with_columns(pl.lit(False).alias(\"is_vulnerability_exists\"))\n",
    "    )\n",
    "    .group_by(by=\"vulnerability_id\")\n",
    "    .agg(\"code\", \"file\", \"is_vulnerability_exists\", \"cwe_id\")\n",
    ")\n",
    "\n",
    "vulnerability_dataset = pl.concat(\n",
    "    [code_with_vulnerabilites, code_without_vulnerabilities]\n",
    ")\n",
    "\n",
    "\n",
    "vulnerability_dataset = vulnerability_dataset.sample(fraction=1, shuffle=True)\n",
    "dataset = Dataset.from_pandas(vulnerability_dataset.to_pandas(), split=\"train[:90%]\")\n",
    "validation_dataset = Dataset.from_pandas(vulnerability_dataset.to_pandas(), split=\"train[10%:]\")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    ")\n",
    "validation_dataset = validation_dataset.map(\n",
    "    formatting_prompts_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08d1ee2115e476da16440cdfafb5d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d22d51b9274d1798effa4879fccce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060. Max memory = 12.0 GB.\n",
      "5.439 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 11:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.780900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.475600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.629400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.845800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.621700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.900900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.746400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.608100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.717700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.679900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.780100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.633900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.810200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.602900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.805500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729.2421 seconds used for training.\n",
      "12.15 minutes used for training.\n",
      "Peak reserved memory = 8.795 GB.\n",
      "Peak reserved memory for training = 3.356 GB.\n",
      "Peak reserved memory % of max memory = 73.292 %.\n",
      "Peak reserved memory for training % of max memory = 27.967 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"vulnerability_searcher\")\n",
    "model.save_pretrained(\"pretrained_vulnerability_searcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='131' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [131/306 14:46 < 19:52, 0.15 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation_result = trainer.evaluate()\n",
    "print(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Input is a python file name and a part of python code with possible software vulnerabilities.\n",
    "Review this code and find vulnerabilities in input code and identify the number of Common Weakness Enumeration (CWE), if vulnerability not exists, tell about it.\n",
    "\n",
    "Input will be provided in format:\n",
    "```\n",
    "python/code/file1.py\n",
    "Code:\n",
    "import sys\n",
    "\n",
    "print(\"hello world!\")\n",
    "\n",
    "python/code/file2.py\n",
    "Code:\n",
    "import os\n",
    "\n",
    "print(\"hello world from another file!\")\n",
    "```\n",
    "\n",
    "\n",
    "### Input:\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "def formatting_input(row):\n",
    "    codes = row[\"code\"]\n",
    "    files = row[\"file\"]\n",
    "\n",
    "    input_data = \"\"\n",
    "    for code_file, code_unit in zip(files, codes):\n",
    "        input_data = (\n",
    "            f\"File name: {code_file}\\nCode: \\n{code_unit.replace('\\n\\n', '\\n')}\\n\\n\"\n",
    "        )\n",
    "    return {\"text\": alpaca_prompt.format(input_data) + EOS_TOKEN}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(Path(\"pretrained_vulnerability_searcher\"), device_map=\"auto\", load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Path(\"pretrained_vulnerability_searcher\"), padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_inputs = tokenizer([formatting_input(validation_dataset.data.slice)], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=200)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open-source-code-vulnerabilities-dataset-7Yku5KZk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
